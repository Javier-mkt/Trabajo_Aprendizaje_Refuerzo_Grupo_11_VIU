# Q-value_layer para dueling network DQN
def q_value_layer(inputs_q):
    v, a = inputs_q
    return v + (a - tf.reduce_mean(a, axis=1, keepdims=True)) # Uso del average (no max ni naive)


# Red neuronal dueling_DQN
def dueling_network_DQN(input_shape = (4,84,84), nb_actions = nb_actions):
    inputs = Input(shape=input_shape)
    x = Permute((2,3,1))(inputs)

    # Red convolucional
    x = Convolution2D(32, (8,8), strides = (4,4), activation = "relu")(x)
    x = Convolution2D(64, (4,4), strides = (2,2), activation = "relu")(x)
    x = Convolution2D(64, (3,3), strides = (1,1), activation = "relu")(x)
    x = Flatten()(x)

    # Value
    v = Dense(512, activation = "relu")(x)
    v = Dense(1)(v)

    # Advantage
    a = Dense(512, activation = "relu")(x)
    a = Dense(nb_actions)(a)

   

    q = q_value_layer([v, a])

    model_dueling_DQN = Model(inputs = inputs, outputs = q)
    return model_dueling_DQN


processor = AtariProcessor()
memory = SequentialMemory(limit=500000, window_length = 4)
policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr = "eps", value_max = 1.0, value_min = 0.1, value_test = 0, nb_steps = 500000) 

DQN = DQNAgent(model = dueling_network_DQN(input_shape = (4,84,84), nb_actions = nb_actions),
               nb_actions = nb_actions,
               memory = memory,
               gamma = 0.99,
               batch_size = 32,
               train_interval = 4,
               nb_steps_warmup = 25000,
               target_model_update = 10000,
               policy = policy,
               processor = processor)

DQN.compile(Adam(learning_rate = 1e-4),metrics=['mae'])  #Usar en futuro hubber clipping?


folder = f"version_{version}"
weights_filename = os.path.join(folder, f'dqn_v_{version}_final_weights.h5f')

DQN.fit(env, nb_steps=600000, visualize=False, verbose=2, callbacks=[metricas, guardar_pesos_iteraciones, guardar_mejores_pesos])
DQN.save_weights(weights_filename, overwrite=True)
