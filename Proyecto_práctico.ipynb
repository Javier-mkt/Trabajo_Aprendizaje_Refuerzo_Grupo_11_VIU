{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUehXgCyIRdq"
   },
   "source": [
    "# Actividad - Proyecto práctico\n",
    "\n",
    "\n",
    "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
    "*   Alumno 1: Gil Garcera, Javier\n",
    "*   Alumno 2: Palomares Mateo, Abel\n",
    "*   Alumno 3: Serrano López, Francisco Rubén\n",
    "*   Alumno 4: Vegas Romero, David\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwpYlnjWJhS9"
   },
   "source": [
    "---\n",
    "## **PARTE 1** - Instalación y requisitos previos\n",
    "\n",
    "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
    "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
    "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
    "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
    "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RU2BPrK2JkP0"
   },
   "source": [
    "---\n",
    "### 1.1. Preparar enviroment (solo local)\n",
    "\n",
    "\n",
    "\n",
    "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
    "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
    "2. Instalar Anaconda\n",
    "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
    "\n",
    "\n",
    "```\n",
    "conda create --name miar_rl python=3.8\n",
    "conda activate miar_rl\n",
    "cd \"PATH_TO_FOLDER\"\n",
    "conda install git\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "\n",
    "4. Abrir la notebook con *jupyter-notebook*.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "jupyter-notebook\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-kixNPiJqTc"
   },
   "source": [
    "---\n",
    "### 1.2. Localizar entorno de trabajo: Google colab o local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "shfOpqKTEhCu"
   },
   "outputs": [],
   "source": [
    "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
    "mount='/content/gdrive'\n",
    "drive_root = mount + \"/My Drive/08_MIAR/actividades/proyecto practico_v4\"\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Dp_a1iBJ0tf"
   },
   "source": [
    "---\n",
    "### 1.3. Montar carpeta de datos local (solo Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I6n7MIefJ21i",
    "outputId": "e7819f31-a959-4f97-b676-4b9e1aaa2833"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're running Colab\n",
      "Colab: mounting Google drive on  /content/gdrive\n",
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
      "\n",
      "Colab: making sure  /content/gdrive/My Drive/08_MIAR/actividades/proyecto practico_v4  exists.\n",
      "\n",
      "Colab: Changing directory to  /content/gdrive/My Drive/08_MIAR/actividades/proyecto practico_v4\n",
      "/content/gdrive/My Drive/08_MIAR/actividades/proyecto practico_v4\n",
      "Archivos en el directorio: \n",
      "['dqn_SpaceInvaders-v0_log.json', 'dqn_SpaceInvaders-v0_weights.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights.h5f.index', 'checkpoint', 'dqn_SpaceInvaders-v0_weights_50000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_50000.h5f.index']\n"
     ]
    }
   ],
   "source": [
    "# Switch to the directory on the Google Drive that you want to use\n",
    "import os\n",
    "if IN_COLAB:\n",
    "  print(\"We're running Colab\")\n",
    "\n",
    "  if IN_COLAB:\n",
    "    # Mount the Google Drive at mount\n",
    "    print(\"Colab: mounting Google drive on \", mount)\n",
    "\n",
    "    drive.mount(mount)\n",
    "\n",
    "    # Create drive_root if it doesn't exist\n",
    "    create_drive_root = True\n",
    "    if create_drive_root:\n",
    "      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
    "      os.makedirs(drive_root, exist_ok=True)\n",
    "\n",
    "    # Change to the directory\n",
    "    print(\"\\nColab: Changing directory to \", drive_root)\n",
    "    %cd $drive_root\n",
    "# Verify we're in the correct working directory\n",
    "%pwd\n",
    "print(\"Archivos en el directorio: \")\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1ZSL5bpJ560"
   },
   "source": [
    "---\n",
    "### 1.4. Instalar librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UbVRjvHCJ8UF",
    "outputId": "00240407-c543-4a34-92c7-9882280d5bba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym==0.17.3 in /usr/local/lib/python3.11/dist-packages (0.17.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.15.3)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.23.5)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.6.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (1.0.0)\n",
      "Collecting git+https://github.com/Kojoley/atari-py.git\n",
      "  Cloning https://github.com/Kojoley/atari-py.git to /tmp/pip-req-build-cxvkre37\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/Kojoley/atari-py.git /tmp/pip-req-build-cxvkre37\n",
      "  Resolved https://github.com/Kojoley/atari-py.git to commit 86a1e05c0a95e9e6233c3a413521fdb34ca8a089\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from atari-py==1.2.2) (1.23.5)\n",
      "Requirement already satisfied: keras-rl2==1.0.5 in /usr/local/lib/python3.11/dist-packages (1.0.5)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (from keras-rl2==1.0.5) (2.12.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (25.2.10)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.73.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.14.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.4.30)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.12.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (18.1.1)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (4.25.8)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (75.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.17.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.12.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.12.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (4.14.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2==1.0.5) (0.45.1)\n",
      "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow->keras-rl2==1.0.5) (0.4.30)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow->keras-rl2==1.0.5) (0.4.1)\n",
      "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow->keras-rl2==1.0.5) (1.15.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.38.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.8.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.1.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.0.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.3.1)\n",
      "Requirement already satisfied: tensorflow==2.12 in /usr/local/lib/python3.11/dist-packages (2.12.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (25.2.10)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.73.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.14.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.4.30)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (2.12.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (18.1.1)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (4.25.8)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (75.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.17.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (2.12.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (2.12.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (4.14.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12) (0.45.1)\n",
      "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.4.30)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.4.1)\n",
      "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (1.15.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.38.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.8.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.1.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.0.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.3.1)\n"
     ]
    }
   ],
   "source": [
    "if IN_COLAB:\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install tensorflow==2.12\n",
    "else:\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install pyglet==1.5.0\n",
    "  %pip install h5py==3.1.0\n",
    "  %pip install Pillow==9.5.0\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install Keras==2.2.4\n",
    "  %pip install tensorflow==2.12.1\n",
    "  %pip install torch==2.0.1\n",
    "  %pip install tensorflow-gpu==2.5.3\n",
    "  %pip install agents==1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hzP_5ZuGb2X"
   },
   "source": [
    "---\n",
    "## **PARTE 2**. Enunciado\n",
    "\n",
    "Consideraciones a tener en cuenta:\n",
    "\n",
    "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
    "\n",
    "- Para nuestro ejercicio, el requisito mínimo será alcanzado cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
    "\n",
    "Este proyecto práctico consta de tres partes:\n",
    "\n",
    "1.   Implementar la red neuronal que se usará en la solución\n",
    "2.   Implementar las distintas piezas de la solución DQN\n",
    "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
    "\n",
    "**Rúbrica**: Se valorará la originalidad en la solución aportada, así como la capacidad de discutir los resultados de forma detallada. El requisito mínimo servirá para aprobar la actividad, bajo premisa de que la discusión del resultado sera apropiada.\n",
    "\n",
    "IMPORTANTE:\n",
    "\n",
    "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
    "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
    "* Se deberá entregar unicamente el notebook y los pesos del mejor modelo en un fichero .zip, de forma organizada.\n",
    "* Cada alumno deberá de subir la solución de forma individual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_b3mzw8IzJP"
   },
   "source": [
    "---\n",
    "## **PARTE 3**. Desarrollo y preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duPmUNOVGb2a"
   },
   "source": [
    "#### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "j3eRhgI-Gb2a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from PIL import Image\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Conv2D, Permute\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4jgQjzoGb2a"
   },
   "source": [
    "#### Configuración base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "jwOE6I_KGb2a"
   },
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n",
    "\n",
    "env_name = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "9jGEZUcpGb2a"
   },
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n",
    "\n",
    "class CustomAtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        assert observation.ndim == 3  # (210, 160, 3)\n",
    "        grayscale = np.mean(observation, axis=2).astype(np.uint8) # Convierte la imagen a escala de grises haciendo la media de los canales RGB\n",
    "        cropped = grayscale[30:190, 10:150]  # zona útil - Recorta la imagen para quedarse con la zona donde ocurre el juego (elimina HUD)\n",
    "        img = Image.fromarray(cropped)\n",
    "        img = img.resize(INPUT_SHAPE, Image.BILINEAR) # Redimensionamos la imagen a 84x84 usando interpolación bilineal\n",
    "        processed = np.array(img).astype('uint8')\n",
    "        return processed\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        return batch.astype('float32') / 255. # Normalizamos\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "QKCT5cLiFEtS"
   },
   "outputs": [],
   "source": [
    "class RewardShapingWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.last_action = None\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "\n",
    "        # Incentivamos las acciones de disparo para fomentar un comportamiento ofensivo:\n",
    "        if action == 3: # Disparar Izquierda\n",
    "            reward += 0.1\n",
    "        elif action == 4: # Disparar Derecha\n",
    "            reward += 0.2 # (mayor incentivo por el patrón observado en las primeras pruebas)\n",
    "        return obs, reward, done, info\n",
    "\n",
    "env_name = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_name)\n",
    "env = RewardShapingWrapper(env)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKSaPiFJLhUA"
   },
   "source": [
    "**Justificación de la utilización: classe RewardShapingWrapper**\n",
    "\n",
    "Tras visualizar los resultados en las primeras pruebas, detectamos que al agente le costaba más realizar disparos hacia la derecha, lo cual podía limitar su desempeño ofensivo. Por ello, hemos incorporado un reward shaping ligero, incentivando las acciones de disparo, especialmente hacia la derecha, para guiar al agente durante la fase de aprendizaje sin alterar el entorno original durante el test.\n",
    "\n",
    "**IMPORTANTE**\n",
    "\n",
    "Este ajuste tiene como consecuencia que las recompensas durante el entrenamiento estén artificialmente infladas, ya que se introducen bonificaciones externas al entorno original. Sin embargo, para obtener una evaluación objetiva, hemos realizado el test final en un entorno limpio, sin aplicar el `RewardShapingWrapper`, de forma que las recompensas finales no estén alteradas y reflejen fielmente la política aprendida por el agente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yitXTADGb2b"
   },
   "source": [
    "### 1. Implementación de la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofzetTUWNCbD"
   },
   "source": [
    "Se ha implementado una red convolucional sencilla pero efectiva, basada en la arquitectura propuesta en el paper original de DQN para juegos de Atari. Este tipo de red permite extraer patrones espaciales relevantes de las observaciones visuales (como posiciones de enemigos o disparos), y está especialmente diseñada para procesar secuencias de imágenes apiladas (4 frames). Su estructura ligera facilita un aprendizaje eficiente sin requerir demasiados recursos computacionales, siendo adecuada para entornos visuales como SpaceInvaders-v0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O4GKrfWSGb2b",
    "outputId": "813b7d42-ce33-48cd-dac6-4504b1c62df0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " permute_6 (Permute)         (None, 84, 84, 4)         0         \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 20, 20, 32)        8224      \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 9, 9, 64)          32832     \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 7, 7, 64)          36928     \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 512)               1606144   \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 6)                 3078      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,687,206\n",
      "Trainable params: 1,687,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Permute((2, 3, 1), input_shape=input_shape))  # canales al final\n",
    "\n",
    "model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu'))\n",
    "model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(nb_actions, activation='linear'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB9-_5HPGb2b"
   },
   "source": [
    "### 2. Implementación de la solución DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "foSlxWH1Gb2b"
   },
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=1_500_000, window_length=WINDOW_LENGTH)\n",
    "\n",
    "processor = CustomAtariProcessor()\n",
    "\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),\n",
    "                              attr='eps',\n",
    "                              value_max=1.0,\n",
    "                              value_min=0.1,  # Termina con un 10% de exploración mínima\n",
    "                              value_test=0.05,\n",
    "                              nb_steps=int(1_500_000 * 0.8)) # ε disminuye linealmente hasta el 80% de los pasos:\n",
    "                                                             # permite explorar lo suficiente antes de explotar\n",
    "\n",
    "dqn = DQNAgent(model=model,\n",
    "               nb_actions=nb_actions,\n",
    "               policy=policy,\n",
    "               memory=memory,\n",
    "               processor=processor,\n",
    "               nb_steps_warmup=50_000,\n",
    "               gamma=0.99,\n",
    "               target_model_update=10_000,\n",
    "               train_interval=4)\n",
    "\n",
    "dqn.compile(Adam(learning_rate=1e-4), metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-2esOkZFP5a"
   },
   "source": [
    "### 3. Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQ5r9K9BN0G2"
   },
   "source": [
    "Durante el entrenamiento, **se han creado dos entornos diferenciados**:\n",
    "\n",
    "- `env_train` (entorno con reward shaping)\n",
    "Se utiliza una versión modificada del entorno original (RewardShapingWrapper) que añade pequeñas recompensas extra cuando el agente dispara (más aún si lo hace hacia la derecha).\n",
    "Esta modificación tiene como objetivo guiar el aprendizaje inicial incentivando un comportamiento ofensivo.\n",
    "\n",
    "- `env_test` (entorno limpio, sin shaping)\n",
    "Para garantizar una evaluación honesta, se construye un segundo entorno idéntico al original, pero sin ningún tipo de modificación.\n",
    "Esto asegura que las recompensas obtenidas durante el test no estén infladas artificialmente, cumpliendo así con los requisitos del enunciado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "uNfc_15MOTFz"
   },
   "outputs": [],
   "source": [
    "env_name = 'SpaceInvaders-v0'\n",
    "\n",
    "# ENTRENAMIENTO (con reward shaping)\n",
    "env_train = gym.make(env_name)\n",
    "env_train = RewardShapingWrapper(env_train)\n",
    "\n",
    "# TESTEO (entorno limpio, sin shaping)\n",
    "env_test = gym.make(env_name)\n",
    "\n",
    "# Semillas\n",
    "np.random.seed(123)\n",
    "env_train.seed(123)\n",
    "env_test.seed(123)\n",
    "\n",
    "nb_actions = env_train.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mT33M2PFFOT1",
    "outputId": "0d72b3a3-17db-4606-9e20-ed9780f7d742"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 44s 4ms/step - reward: 0.0623\n",
      "14 episodes - episode_reward: 43.657 [25.500, 71.800] - ale.lives: 2.049\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 44s 4ms/step - reward: 0.0622\n",
      "15 episodes - episode_reward: 41.127 [22.900, 64.100] - ale.lives: 2.135\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 44s 4ms/step - reward: 0.0642\n",
      "16 episodes - episode_reward: 40.938 [23.000, 81.400] - ale.lives: 2.000\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 44s 4ms/step - reward: 0.0624\n",
      "15 episodes - episode_reward: 40.300 [25.000, 65.600] - ale.lives: 2.152\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 44s 4ms/step - reward: 0.0630\n",
      "13 episodes - episode_reward: 47.623 [24.300, 93.500] - ale.lives: 2.122\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 295s 29ms/step - reward: 0.0699\n",
      "15 episodes - episode_reward: 45.100 [29.200, 83.400] - loss: 0.006 - mae: 0.069 - mean_q: 0.221 - mean_eps: 0.959 - ale.lives: 2.143\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 294s 29ms/step - reward: 0.0687\n",
      "15 episodes - episode_reward: 47.313 [29.600, 65.100] - loss: 0.006 - mae: 0.243 - mean_q: 0.432 - mean_eps: 0.951 - ale.lives: 2.040\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 295s 29ms/step - reward: 0.0731\n",
      "15 episodes - episode_reward: 49.847 [28.900, 85.000] - loss: 0.006 - mae: 0.419 - mean_q: 0.645 - mean_eps: 0.944 - ale.lives: 2.058\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 295s 29ms/step - reward: 0.0736\n",
      "14 episodes - episode_reward: 53.307 [25.200, 70.900] - loss: 0.006 - mae: 0.611 - mean_q: 0.875 - mean_eps: 0.936 - ale.lives: 2.189\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 295s 29ms/step - reward: 0.0745\n",
      "12 episodes - episode_reward: 58.258 [31.000, 107.900] - loss: 0.007 - mae: 0.780 - mean_q: 1.077 - mean_eps: 0.929 - ale.lives: 2.013\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 295s 30ms/step - reward: 0.0775\n",
      "16 episodes - episode_reward: 51.519 [28.700, 132.700] - loss: 0.007 - mae: 0.954 - mean_q: 1.285 - mean_eps: 0.921 - ale.lives: 1.998\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 297s 30ms/step - reward: 0.0787\n",
      "16 episodes - episode_reward: 48.581 [27.000, 98.400] - loss: 0.007 - mae: 1.132 - mean_q: 1.499 - mean_eps: 0.914 - ale.lives: 2.214\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 297s 30ms/step - reward: 0.0789\n",
      "14 episodes - episode_reward: 54.343 [26.600, 95.900] - loss: 0.008 - mae: 1.311 - mean_q: 1.713 - mean_eps: 0.906 - ale.lives: 2.121\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 298s 30ms/step - reward: 0.0822\n",
      "16 episodes - episode_reward: 53.438 [32.100, 88.500] - loss: 0.008 - mae: 1.478 - mean_q: 1.914 - mean_eps: 0.899 - ale.lives: 2.090\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 298s 30ms/step - reward: 0.0830\n",
      "12 episodes - episode_reward: 67.383 [29.100, 114.700] - loss: 0.009 - mae: 1.646 - mean_q: 2.116 - mean_eps: 0.891 - ale.lives: 2.135\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 300s 30ms/step - reward: 0.0826\n",
      "14 episodes - episode_reward: 58.436 [30.800, 93.200] - loss: 0.010 - mae: 1.833 - mean_q: 2.337 - mean_eps: 0.884 - ale.lives: 2.116\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 301s 30ms/step - reward: 0.0835\n",
      "16 episodes - episode_reward: 52.138 [29.000, 88.900] - loss: 0.010 - mae: 1.986 - mean_q: 2.522 - mean_eps: 0.876 - ale.lives: 2.154\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 302s 30ms/step - reward: 0.0827\n",
      "16 episodes - episode_reward: 52.356 [30.100, 128.000] - loss: 0.011 - mae: 2.168 - mean_q: 2.740 - mean_eps: 0.869 - ale.lives: 1.884\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 302s 30ms/step - reward: 0.0894\n",
      "14 episodes - episode_reward: 64.171 [34.200, 110.600] - loss: 0.011 - mae: 2.312 - mean_q: 2.913 - mean_eps: 0.861 - ale.lives: 2.189\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 303s 30ms/step - reward: 0.0859\n",
      "17 episodes - episode_reward: 50.600 [25.600, 107.000] - loss: 0.011 - mae: 2.454 - mean_q: 3.086 - mean_eps: 0.854 - ale.lives: 2.109\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 306s 31ms/step - reward: 0.0919\n",
      "14 episodes - episode_reward: 65.193 [39.400, 99.300] - loss: 0.014 - mae: 2.634 - mean_q: 3.301 - mean_eps: 0.846 - ale.lives: 2.098\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 307s 31ms/step - reward: 0.0895\n",
      "13 episodes - episode_reward: 66.038 [35.900, 101.200] - loss: 0.015 - mae: 2.765 - mean_q: 3.454 - mean_eps: 0.839 - ale.lives: 2.072\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0918\n",
      "17 episodes - episode_reward: 56.288 [30.900, 88.200] - loss: 0.014 - mae: 2.911 - mean_q: 3.631 - mean_eps: 0.831 - ale.lives: 2.048\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0924\n",
      "14 episodes - episode_reward: 66.193 [32.400, 108.400] - loss: 0.015 - mae: 3.068 - mean_q: 3.822 - mean_eps: 0.824 - ale.lives: 2.008\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0919\n",
      "14 episodes - episode_reward: 60.107 [26.600, 108.800] - loss: 0.017 - mae: 3.220 - mean_q: 4.000 - mean_eps: 0.816 - ale.lives: 1.971\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0928\n",
      "16 episodes - episode_reward: 62.425 [34.700, 117.700] - loss: 0.018 - mae: 3.370 - mean_q: 4.184 - mean_eps: 0.809 - ale.lives: 2.136\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0971\n",
      "14 episodes - episode_reward: 67.893 [30.600, 109.800] - loss: 0.017 - mae: 3.511 - mean_q: 4.348 - mean_eps: 0.801 - ale.lives: 2.084\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0977\n",
      "15 episodes - episode_reward: 65.533 [32.300, 118.500] - loss: 0.020 - mae: 3.683 - mean_q: 4.557 - mean_eps: 0.794 - ale.lives: 2.100\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 315s 31ms/step - reward: 0.0986\n",
      "14 episodes - episode_reward: 70.757 [38.400, 126.200] - loss: 0.022 - mae: 3.817 - mean_q: 4.716 - mean_eps: 0.786 - ale.lives: 2.081\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 316s 32ms/step - reward: 0.1014\n",
      "16 episodes - episode_reward: 60.481 [33.300, 103.800] - loss: 0.024 - mae: 3.999 - mean_q: 4.935 - mean_eps: 0.779 - ale.lives: 1.965\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 318s 32ms/step - reward: 0.1009\n",
      "15 episodes - episode_reward: 71.753 [33.000, 109.800] - loss: 0.021 - mae: 4.114 - mean_q: 5.074 - mean_eps: 0.771 - ale.lives: 1.963\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 319s 32ms/step - reward: 0.1005\n",
      "13 episodes - episode_reward: 76.038 [37.000, 143.300] - loss: 0.023 - mae: 4.244 - mean_q: 5.230 - mean_eps: 0.764 - ale.lives: 2.260\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.1021\n",
      "17 episodes - episode_reward: 59.294 [37.200, 106.600] - loss: 0.024 - mae: 4.374 - mean_q: 5.387 - mean_eps: 0.756 - ale.lives: 2.094\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.1039\n",
      "18 episodes - episode_reward: 58.117 [34.200, 99.000] - loss: 0.027 - mae: 4.466 - mean_q: 5.497 - mean_eps: 0.749 - ale.lives: 2.041\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.1087\n",
      "15 episodes - episode_reward: 74.253 [40.000, 122.700] - loss: 0.029 - mae: 4.586 - mean_q: 5.641 - mean_eps: 0.741 - ale.lives: 2.198\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 336s 34ms/step - reward: 0.1059\n",
      "15 episodes - episode_reward: 69.080 [35.000, 116.700] - loss: 0.027 - mae: 4.707 - mean_q: 5.783 - mean_eps: 0.734 - ale.lives: 2.086\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 335s 33ms/step - reward: 0.1088\n",
      "13 episodes - episode_reward: 79.085 [41.600, 114.000] - loss: 0.029 - mae: 4.836 - mean_q: 5.939 - mean_eps: 0.726 - ale.lives: 1.933\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 340s 34ms/step - reward: 0.1108\n",
      "15 episodes - episode_reward: 77.367 [35.700, 133.800] - loss: 0.033 - mae: 4.948 - mean_q: 6.073 - mean_eps: 0.719 - ale.lives: 2.060\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 335s 33ms/step - reward: 0.1073\n",
      "14 episodes - episode_reward: 78.429 [40.500, 118.500] - loss: 0.030 - mae: 5.067 - mean_q: 6.217 - mean_eps: 0.711 - ale.lives: 2.007\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 333s 33ms/step - reward: 0.1093\n",
      "14 episodes - episode_reward: 77.007 [38.700, 134.500] - loss: 0.035 - mae: 5.201 - mean_q: 6.378 - mean_eps: 0.704 - ale.lives: 2.099\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 338s 34ms/step - reward: 0.1110\n",
      "16 episodes - episode_reward: 69.800 [39.300, 115.100] - loss: 0.032 - mae: 5.345 - mean_q: 6.550 - mean_eps: 0.696 - ale.lives: 2.048\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 335s 34ms/step - reward: 0.1100\n",
      "14 episodes - episode_reward: 77.750 [35.100, 131.300] - loss: 0.035 - mae: 5.467 - mean_q: 6.692 - mean_eps: 0.689 - ale.lives: 2.212\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 337s 34ms/step - reward: 0.1153\n",
      "14 episodes - episode_reward: 81.929 [36.300, 130.300] - loss: 0.036 - mae: 5.526 - mean_q: 6.766 - mean_eps: 0.681 - ale.lives: 2.127\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 338s 34ms/step - reward: 0.1161\n",
      "16 episodes - episode_reward: 73.606 [39.600, 109.900] - loss: 0.036 - mae: 5.603 - mean_q: 6.859 - mean_eps: 0.674 - ale.lives: 2.030\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 339s 34ms/step - reward: 0.1178\n",
      "14 episodes - episode_reward: 80.350 [37.300, 126.600] - loss: 0.032 - mae: 5.693 - mean_q: 6.970 - mean_eps: 0.666 - ale.lives: 2.046\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 342s 34ms/step - reward: 0.1184\n",
      "15 episodes - episode_reward: 82.093 [57.300, 124.300] - loss: 0.037 - mae: 5.732 - mean_q: 7.012 - mean_eps: 0.659 - ale.lives: 1.972\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 343s 34ms/step - reward: 0.1186\n",
      "15 episodes - episode_reward: 80.287 [35.100, 119.700] - loss: 0.036 - mae: 5.852 - mean_q: 7.155 - mean_eps: 0.651 - ale.lives: 1.984\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 345s 34ms/step - reward: 0.1214\n",
      "15 episodes - episode_reward: 74.973 [35.800, 154.500] - loss: 0.037 - mae: 5.964 - mean_q: 7.289 - mean_eps: 0.644 - ale.lives: 2.051\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 348s 35ms/step - reward: 0.1215\n",
      "17 episodes - episode_reward: 76.976 [35.300, 127.500] - loss: 0.038 - mae: 6.070 - mean_q: 7.419 - mean_eps: 0.636 - ale.lives: 1.951\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 349s 35ms/step - reward: 0.1213\n",
      "13 episodes - episode_reward: 87.346 [36.000, 149.100] - loss: 0.039 - mae: 6.188 - mean_q: 7.558 - mean_eps: 0.629 - ale.lives: 2.035\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      "10000/10000 [==============================] - 350s 35ms/step - reward: 0.1231\n",
      "16 episodes - episode_reward: 78.081 [51.200, 132.900] - loss: 0.038 - mae: 6.286 - mean_q: 7.679 - mean_eps: 0.621 - ale.lives: 2.000\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      "10000/10000 [==============================] - 356s 36ms/step - reward: 0.1242\n",
      "18 episodes - episode_reward: 71.433 [32.700, 145.900] - loss: 0.043 - mae: 6.415 - mean_q: 7.833 - mean_eps: 0.614 - ale.lives: 2.043\n",
      "\n",
      "Interval 53 (520000 steps performed)\n",
      "10000/10000 [==============================] - 354s 35ms/step - reward: 0.1225\n",
      "14 episodes - episode_reward: 86.786 [44.900, 127.700] - loss: 0.042 - mae: 6.604 - mean_q: 8.060 - mean_eps: 0.606 - ale.lives: 2.059\n",
      "\n",
      "Interval 54 (530000 steps performed)\n",
      "10000/10000 [==============================] - 358s 36ms/step - reward: 0.1247\n",
      "17 episodes - episode_reward: 70.247 [41.700, 134.300] - loss: 0.047 - mae: 6.699 - mean_q: 8.169 - mean_eps: 0.599 - ale.lives: 2.115\n",
      "\n",
      "Interval 55 (540000 steps performed)\n",
      "10000/10000 [==============================] - 367s 37ms/step - reward: 0.1261\n",
      "15 episodes - episode_reward: 83.127 [42.500, 158.300] - loss: 0.047 - mae: 6.857 - mean_q: 8.360 - mean_eps: 0.591 - ale.lives: 2.005\n",
      "\n",
      "Interval 56 (550000 steps performed)\n",
      "10000/10000 [==============================] - 363s 36ms/step - reward: 0.1298\n",
      "14 episodes - episode_reward: 94.971 [39.900, 154.400] - loss: 0.048 - mae: 7.024 - mean_q: 8.561 - mean_eps: 0.584 - ale.lives: 2.000\n",
      "\n",
      "Interval 57 (560000 steps performed)\n",
      "10000/10000 [==============================] - 367s 37ms/step - reward: 0.1270\n",
      "16 episodes - episode_reward: 80.069 [35.600, 152.800] - loss: 0.051 - mae: 7.119 - mean_q: 8.675 - mean_eps: 0.576 - ale.lives: 1.924\n",
      "\n",
      "Interval 58 (570000 steps performed)\n",
      "10000/10000 [==============================] - 367s 37ms/step - reward: 0.1307\n",
      "15 episodes - episode_reward: 88.513 [39.600, 153.700] - loss: 0.049 - mae: 7.256 - mean_q: 8.841 - mean_eps: 0.569 - ale.lives: 2.101\n",
      "\n",
      "Interval 59 (580000 steps performed)\n",
      "10000/10000 [==============================] - 370s 37ms/step - reward: 0.1302\n",
      "13 episodes - episode_reward: 102.492 [63.700, 150.700] - loss: 0.053 - mae: 7.382 - mean_q: 8.987 - mean_eps: 0.561 - ale.lives: 1.762\n",
      "\n",
      "Interval 60 (590000 steps performed)\n",
      "10000/10000 [==============================] - 374s 37ms/step - reward: 0.1316\n",
      "15 episodes - episode_reward: 87.660 [41.100, 159.500] - loss: 0.060 - mae: 7.470 - mean_q: 9.094 - mean_eps: 0.554 - ale.lives: 2.012\n",
      "\n",
      "Interval 61 (600000 steps performed)\n",
      "10000/10000 [==============================] - 377s 38ms/step - reward: 0.1361\n",
      "14 episodes - episode_reward: 94.764 [48.700, 121.600] - loss: 0.055 - mae: 7.541 - mean_q: 9.179 - mean_eps: 0.546 - ale.lives: 1.974\n",
      "\n",
      "Interval 62 (610000 steps performed)\n",
      "10000/10000 [==============================] - 378s 38ms/step - reward: 0.1355\n",
      "12 episodes - episode_reward: 106.758 [71.100, 162.400] - loss: 0.056 - mae: 7.589 - mean_q: 9.240 - mean_eps: 0.539 - ale.lives: 1.996\n",
      "\n",
      "Interval 63 (620000 steps performed)\n",
      "10000/10000 [==============================] - 386s 39ms/step - reward: 0.1358\n",
      "14 episodes - episode_reward: 102.786 [37.900, 163.400] - loss: 0.052 - mae: 7.737 - mean_q: 9.414 - mean_eps: 0.531 - ale.lives: 1.960\n",
      "\n",
      "Interval 64 (630000 steps performed)\n",
      "10000/10000 [==============================] - 390s 39ms/step - reward: 0.1382\n",
      "13 episodes - episode_reward: 102.162 [86.200, 146.600] - loss: 0.057 - mae: 7.840 - mean_q: 9.539 - mean_eps: 0.524 - ale.lives: 1.877\n",
      "\n",
      "Interval 65 (640000 steps performed)\n",
      "10000/10000 [==============================] - 394s 39ms/step - reward: 0.1358\n",
      "15 episodes - episode_reward: 92.467 [40.200, 171.300] - loss: 0.051 - mae: 7.922 - mean_q: 9.635 - mean_eps: 0.516 - ale.lives: 2.050\n",
      "\n",
      "Interval 66 (650000 steps performed)\n",
      "10000/10000 [==============================] - 396s 40ms/step - reward: 0.1384\n",
      "12 episodes - episode_reward: 106.100 [50.600, 234.200] - loss: 0.060 - mae: 8.020 - mean_q: 9.753 - mean_eps: 0.509 - ale.lives: 1.871\n",
      "\n",
      "Interval 67 (660000 steps performed)\n",
      "10000/10000 [==============================] - 393s 39ms/step - reward: 0.1403\n",
      "15 episodes - episode_reward: 104.667 [43.200, 203.500] - loss: 0.064 - mae: 8.151 - mean_q: 9.911 - mean_eps: 0.501 - ale.lives: 1.904\n",
      "\n",
      "Interval 68 (670000 steps performed)\n",
      "10000/10000 [==============================] - 394s 39ms/step - reward: 0.1412\n",
      "11 episodes - episode_reward: 116.000 [43.800, 199.500] - loss: 0.066 - mae: 8.275 - mean_q: 10.061 - mean_eps: 0.494 - ale.lives: 2.036\n",
      "\n",
      "Interval 69 (680000 steps performed)\n",
      "10000/10000 [==============================] - 390s 39ms/step - reward: 0.1398\n",
      "14 episodes - episode_reward: 105.600 [40.500, 192.500] - loss: 0.063 - mae: 8.328 - mean_q: 10.121 - mean_eps: 0.486 - ale.lives: 1.931\n",
      "\n",
      "Interval 70 (690000 steps performed)\n",
      "10000/10000 [==============================] - 391s 39ms/step - reward: 0.1438\n",
      "13 episodes - episode_reward: 106.154 [62.000, 153.800] - loss: 0.066 - mae: 8.374 - mean_q: 10.173 - mean_eps: 0.479 - ale.lives: 1.917\n",
      "\n",
      "Interval 71 (700000 steps performed)\n",
      "10000/10000 [==============================] - 394s 39ms/step - reward: 0.1436\n",
      "10 episodes - episode_reward: 142.480 [86.800, 221.900] - loss: 0.060 - mae: 8.482 - mean_q: 10.307 - mean_eps: 0.471 - ale.lives: 1.829\n",
      "\n",
      "Interval 72 (710000 steps performed)\n",
      "10000/10000 [==============================] - 396s 40ms/step - reward: 0.1460\n",
      "13 episodes - episode_reward: 113.338 [84.000, 165.700] - loss: 0.065 - mae: 8.568 - mean_q: 10.413 - mean_eps: 0.464 - ale.lives: 1.966\n",
      "\n",
      "Interval 73 (720000 steps performed)\n",
      "10000/10000 [==============================] - 396s 40ms/step - reward: 0.1467\n",
      "15 episodes - episode_reward: 103.840 [49.900, 170.800] - loss: 0.068 - mae: 8.601 - mean_q: 10.450 - mean_eps: 0.456 - ale.lives: 1.952\n",
      "\n",
      "Interval 74 (730000 steps performed)\n",
      "10000/10000 [==============================] - 405s 41ms/step - reward: 0.1467\n",
      "12 episodes - episode_reward: 114.917 [68.200, 187.600] - loss: 0.065 - mae: 8.603 - mean_q: 10.452 - mean_eps: 0.449 - ale.lives: 2.027\n",
      "\n",
      "Interval 75 (740000 steps performed)\n",
      "10000/10000 [==============================] - 412s 41ms/step - reward: 0.1495\n",
      "12 episodes - episode_reward: 120.467 [60.700, 183.400] - loss: 0.065 - mae: 8.630 - mean_q: 10.484 - mean_eps: 0.441 - ale.lives: 1.853\n",
      "\n",
      "Interval 76 (750000 steps performed)\n",
      "10000/10000 [==============================] - 413s 41ms/step - reward: 0.1503\n",
      "14 episodes - episode_reward: 117.800 [82.200, 177.800] - loss: 0.062 - mae: 8.680 - mean_q: 10.548 - mean_eps: 0.434 - ale.lives: 1.997\n",
      "\n",
      "Interval 77 (760000 steps performed)\n",
      "10000/10000 [==============================] - 419s 42ms/step - reward: 0.1497\n",
      "13 episodes - episode_reward: 115.708 [57.600, 211.300] - loss: 0.068 - mae: 8.880 - mean_q: 10.787 - mean_eps: 0.426 - ale.lives: 1.875\n",
      "\n",
      "Interval 78 (770000 steps performed)\n",
      "10000/10000 [==============================] - 410s 41ms/step - reward: 0.1528\n",
      "15 episodes - episode_reward: 96.413 [49.000, 162.200] - loss: 0.067 - mae: 8.923 - mean_q: 10.838 - mean_eps: 0.419 - ale.lives: 2.053\n",
      "\n",
      "Interval 79 (780000 steps performed)\n",
      "10000/10000 [==============================] - 413s 41ms/step - reward: 0.1514\n",
      "12 episodes - episode_reward: 120.800 [52.200, 186.700] - loss: 0.075 - mae: 8.940 - mean_q: 10.856 - mean_eps: 0.411 - ale.lives: 1.959\n",
      "\n",
      "Interval 80 (790000 steps performed)\n",
      "10000/10000 [==============================] - 415s 41ms/step - reward: 0.1539\n",
      "12 episodes - episode_reward: 138.408 [59.100, 225.500] - loss: 0.066 - mae: 9.022 - mean_q: 10.956 - mean_eps: 0.404 - ale.lives: 2.015\n",
      "\n",
      "Interval 81 (800000 steps performed)\n",
      "10000/10000 [==============================] - 418s 42ms/step - reward: 0.1555\n",
      "13 episodes - episode_reward: 115.246 [52.100, 193.200] - loss: 0.068 - mae: 9.169 - mean_q: 11.132 - mean_eps: 0.396 - ale.lives: 1.935\n",
      "\n",
      "Interval 82 (810000 steps performed)\n",
      "10000/10000 [==============================] - 423s 42ms/step - reward: 0.1559\n",
      "13 episodes - episode_reward: 120.769 [78.400, 223.600] - loss: 0.071 - mae: 9.247 - mean_q: 11.223 - mean_eps: 0.389 - ale.lives: 1.871\n",
      "\n",
      "Interval 83 (820000 steps performed)\n",
      "10000/10000 [==============================] - 425s 42ms/step - reward: 0.1541\n",
      "12 episodes - episode_reward: 129.917 [86.000, 197.100] - loss: 0.073 - mae: 9.310 - mean_q: 11.300 - mean_eps: 0.381 - ale.lives: 1.796\n",
      "\n",
      "Interval 84 (830000 steps performed)\n",
      "10000/10000 [==============================] - 424s 42ms/step - reward: 0.1575\n",
      "13 episodes - episode_reward: 125.338 [54.700, 179.000] - loss: 0.078 - mae: 9.375 - mean_q: 11.382 - mean_eps: 0.374 - ale.lives: 1.941\n",
      "\n",
      "Interval 85 (840000 steps performed)\n",
      "10000/10000 [==============================] - 426s 43ms/step - reward: 0.1606\n",
      "14 episodes - episode_reward: 107.129 [51.900, 147.300] - loss: 0.073 - mae: 9.394 - mean_q: 11.402 - mean_eps: 0.366 - ale.lives: 1.992\n",
      "\n",
      "Interval 86 (850000 steps performed)\n",
      "10000/10000 [==============================] - 427s 43ms/step - reward: 0.1592\n",
      "13 episodes - episode_reward: 125.246 [60.200, 201.900] - loss: 0.072 - mae: 9.518 - mean_q: 11.550 - mean_eps: 0.359 - ale.lives: 1.943\n",
      "\n",
      "Interval 87 (860000 steps performed)\n",
      "10000/10000 [==============================] - 429s 43ms/step - reward: 0.1608\n",
      "10 episodes - episode_reward: 151.840 [107.800, 186.300] - loss: 0.074 - mae: 9.612 - mean_q: 11.666 - mean_eps: 0.351 - ale.lives: 1.908\n",
      "\n",
      "Interval 88 (870000 steps performed)\n",
      "10000/10000 [==============================] - 434s 43ms/step - reward: 0.1615\n",
      "14 episodes - episode_reward: 123.164 [80.400, 235.700] - loss: 0.076 - mae: 9.680 - mean_q: 11.742 - mean_eps: 0.344 - ale.lives: 2.071\n",
      "\n",
      "Interval 89 (880000 steps performed)\n",
      "10000/10000 [==============================] - 430s 43ms/step - reward: 0.1638\n",
      "12 episodes - episode_reward: 130.942 [73.700, 197.900] - loss: 0.075 - mae: 9.740 - mean_q: 11.816 - mean_eps: 0.336 - ale.lives: 1.901\n",
      "\n",
      "Interval 90 (890000 steps performed)\n",
      "10000/10000 [==============================] - 429s 43ms/step - reward: 0.1641\n",
      "12 episodes - episode_reward: 143.767 [87.500, 248.900] - loss: 0.079 - mae: 9.826 - mean_q: 11.918 - mean_eps: 0.329 - ale.lives: 2.040\n",
      "\n",
      "Interval 91 (900000 steps performed)\n",
      "10000/10000 [==============================] - 433s 43ms/step - reward: 0.1644\n",
      "11 episodes - episode_reward: 151.536 [89.800, 234.500] - loss: 0.072 - mae: 9.827 - mean_q: 11.923 - mean_eps: 0.321 - ale.lives: 1.959\n",
      "\n",
      "Interval 92 (910000 steps performed)\n",
      "10000/10000 [==============================] - 437s 44ms/step - reward: 0.1659\n",
      "13 episodes - episode_reward: 128.585 [85.700, 186.900] - loss: 0.070 - mae: 9.874 - mean_q: 11.975 - mean_eps: 0.314 - ale.lives: 1.955\n",
      "\n",
      "Interval 93 (920000 steps performed)\n",
      "10000/10000 [==============================] - 443s 44ms/step - reward: 0.1695\n",
      "11 episodes - episode_reward: 149.900 [89.600, 206.000] - loss: 0.072 - mae: 9.946 - mean_q: 12.062 - mean_eps: 0.306 - ale.lives: 2.014\n",
      "\n",
      "Interval 94 (930000 steps performed)\n",
      "10000/10000 [==============================] - 435s 43ms/step - reward: 0.1678\n",
      "10 episodes - episode_reward: 151.220 [90.900, 220.100] - loss: 0.073 - mae: 9.990 - mean_q: 12.112 - mean_eps: 0.299 - ale.lives: 1.902\n",
      "\n",
      "Interval 95 (940000 steps performed)\n",
      "10000/10000 [==============================] - 445s 45ms/step - reward: 0.1654\n",
      "12 episodes - episode_reward: 151.575 [62.100, 304.200] - loss: 0.072 - mae: 10.083 - mean_q: 12.229 - mean_eps: 0.291 - ale.lives: 2.004\n",
      "\n",
      "Interval 96 (950000 steps performed)\n",
      "10000/10000 [==============================] - 448s 45ms/step - reward: 0.1728\n",
      "13 episodes - episode_reward: 134.685 [62.100, 209.300] - loss: 0.087 - mae: 10.118 - mean_q: 12.270 - mean_eps: 0.284 - ale.lives: 1.927\n",
      "\n",
      "Interval 97 (960000 steps performed)\n",
      "10000/10000 [==============================] - 451s 45ms/step - reward: 0.1711\n",
      "11 episodes - episode_reward: 144.091 [85.300, 218.500] - loss: 0.081 - mae: 10.113 - mean_q: 12.260 - mean_eps: 0.276 - ale.lives: 1.918\n",
      "\n",
      "Interval 98 (970000 steps performed)\n",
      "10000/10000 [==============================] - 458s 46ms/step - reward: 0.1719\n",
      "10 episodes - episode_reward: 170.880 [80.200, 259.200] - loss: 0.079 - mae: 10.245 - mean_q: 12.420 - mean_eps: 0.269 - ale.lives: 1.947\n",
      "\n",
      "Interval 99 (980000 steps performed)\n",
      "10000/10000 [==============================] - 460s 46ms/step - reward: 0.1747\n",
      "13 episodes - episode_reward: 133.423 [94.300, 212.100] - loss: 0.088 - mae: 10.303 - mean_q: 12.491 - mean_eps: 0.261 - ale.lives: 2.017\n",
      "\n",
      "Interval 100 (990000 steps performed)\n",
      "10000/10000 [==============================] - 455s 46ms/step - reward: 0.1716\n",
      "13 episodes - episode_reward: 144.462 [92.000, 212.100] - loss: 0.081 - mae: 10.340 - mean_q: 12.536 - mean_eps: 0.254 - ale.lives: 1.991\n",
      "\n",
      "Interval 101 (1000000 steps performed)\n",
      "10000/10000 [==============================] - 457s 46ms/step - reward: 0.1765\n",
      "12 episodes - episode_reward: 142.492 [77.900, 222.600] - loss: 0.079 - mae: 10.451 - mean_q: 12.669 - mean_eps: 0.246 - ale.lives: 2.061\n",
      "\n",
      "Interval 102 (1010000 steps performed)\n",
      "10000/10000 [==============================] - 455s 45ms/step - reward: 0.1747\n",
      "11 episodes - episode_reward: 160.118 [112.400, 216.300] - loss: 0.083 - mae: 10.563 - mean_q: 12.798 - mean_eps: 0.239 - ale.lives: 1.873\n",
      "\n",
      "Interval 103 (1020000 steps performed)\n",
      "10000/10000 [==============================] - 461s 46ms/step - reward: 0.1776\n",
      "11 episodes - episode_reward: 160.955 [83.000, 244.500] - loss: 0.081 - mae: 10.576 - mean_q: 12.821 - mean_eps: 0.231 - ale.lives: 1.944\n",
      "\n",
      "Interval 104 (1030000 steps performed)\n",
      "10000/10000 [==============================] - 465s 47ms/step - reward: 0.1767\n",
      "12 episodes - episode_reward: 147.175 [83.100, 307.500] - loss: 0.080 - mae: 10.595 - mean_q: 12.841 - mean_eps: 0.224 - ale.lives: 1.976\n",
      "\n",
      "Interval 105 (1040000 steps performed)\n",
      "10000/10000 [==============================] - 460s 46ms/step - reward: 0.1806\n",
      "10 episodes - episode_reward: 162.970 [97.200, 277.400] - loss: 0.085 - mae: 10.689 - mean_q: 12.959 - mean_eps: 0.216 - ale.lives: 1.949\n",
      "\n",
      "Interval 106 (1050000 steps performed)\n",
      "10000/10000 [==============================] - 465s 46ms/step - reward: 0.1819\n",
      "11 episodes - episode_reward: 176.955 [113.900, 249.900] - loss: 0.089 - mae: 10.784 - mean_q: 13.074 - mean_eps: 0.209 - ale.lives: 1.994\n",
      "\n",
      "Interval 107 (1060000 steps performed)\n",
      "10000/10000 [==============================] - 464s 46ms/step - reward: 0.1823\n",
      "11 episodes - episode_reward: 162.245 [107.000, 277.500] - loss: 0.081 - mae: 10.830 - mean_q: 13.131 - mean_eps: 0.201 - ale.lives: 1.931\n",
      "\n",
      "Interval 108 (1070000 steps performed)\n",
      "10000/10000 [==============================] - 466s 47ms/step - reward: 0.1811\n",
      "9 episodes - episode_reward: 194.267 [127.900, 253.200] - loss: 0.074 - mae: 10.824 - mean_q: 13.119 - mean_eps: 0.194 - ale.lives: 1.953\n",
      "\n",
      "Interval 109 (1080000 steps performed)\n",
      "10000/10000 [==============================] - 473s 47ms/step - reward: 0.1843\n",
      "12 episodes - episode_reward: 167.300 [107.100, 234.200] - loss: 0.081 - mae: 10.893 - mean_q: 13.200 - mean_eps: 0.186 - ale.lives: 1.938\n",
      "\n",
      "Interval 110 (1090000 steps performed)\n",
      "10000/10000 [==============================] - 467s 47ms/step - reward: 0.1829\n",
      "9 episodes - episode_reward: 185.578 [114.800, 241.000] - loss: 0.085 - mae: 11.024 - mean_q: 13.357 - mean_eps: 0.179 - ale.lives: 1.898\n",
      "\n",
      "Interval 111 (1100000 steps performed)\n",
      "10000/10000 [==============================] - 468s 47ms/step - reward: 0.1858\n",
      "12 episodes - episode_reward: 167.550 [110.400, 275.500] - loss: 0.091 - mae: 11.041 - mean_q: 13.378 - mean_eps: 0.171 - ale.lives: 2.016\n",
      "\n",
      "Interval 112 (1110000 steps performed)\n",
      "10000/10000 [==============================] - 471s 47ms/step - reward: 0.1848\n",
      "10 episodes - episode_reward: 183.770 [98.400, 268.200] - loss: 0.088 - mae: 11.153 - mean_q: 13.510 - mean_eps: 0.164 - ale.lives: 1.970\n",
      "\n",
      "Interval 113 (1120000 steps performed)\n",
      "10000/10000 [==============================] - 475s 47ms/step - reward: 0.1857\n",
      "9 episodes - episode_reward: 194.456 [110.000, 262.400] - loss: 0.095 - mae: 11.208 - mean_q: 13.578 - mean_eps: 0.156 - ale.lives: 1.905\n",
      "\n",
      "Interval 114 (1130000 steps performed)\n",
      "10000/10000 [==============================] - 478s 48ms/step - reward: 0.1865\n",
      "10 episodes - episode_reward: 182.040 [103.000, 234.900] - loss: 0.090 - mae: 11.245 - mean_q: 13.624 - mean_eps: 0.149 - ale.lives: 1.967\n",
      "\n",
      "Interval 115 (1140000 steps performed)\n",
      "10000/10000 [==============================] - 479s 48ms/step - reward: 0.1905\n",
      "13 episodes - episode_reward: 159.800 [99.700, 223.900] - loss: 0.097 - mae: 11.269 - mean_q: 13.649 - mean_eps: 0.141 - ale.lives: 2.026\n",
      "\n",
      "Interval 116 (1150000 steps performed)\n",
      "10000/10000 [==============================] - 480s 48ms/step - reward: 0.1899\n",
      "9 episodes - episode_reward: 201.067 [127.700, 293.800] - loss: 0.085 - mae: 11.310 - mean_q: 13.704 - mean_eps: 0.134 - ale.lives: 1.948\n",
      "\n",
      "Interval 117 (1160000 steps performed)\n",
      "10000/10000 [==============================] - 485s 49ms/step - reward: 0.1921\n",
      "11 episodes - episode_reward: 183.655 [102.100, 302.700] - loss: 0.081 - mae: 11.294 - mean_q: 13.686 - mean_eps: 0.126 - ale.lives: 1.911\n",
      "\n",
      "Interval 118 (1170000 steps performed)\n",
      "10000/10000 [==============================] - 491s 49ms/step - reward: 0.1938\n",
      "11 episodes - episode_reward: 171.691 [115.600, 238.700] - loss: 0.097 - mae: 11.359 - mean_q: 13.758 - mean_eps: 0.119 - ale.lives: 2.006\n",
      "\n",
      "Interval 119 (1180000 steps performed)\n",
      "10000/10000 [==============================] - 496s 50ms/step - reward: 0.1940\n",
      "12 episodes - episode_reward: 162.100 [101.800, 234.500] - loss: 0.095 - mae: 11.442 - mean_q: 13.855 - mean_eps: 0.111 - ale.lives: 1.935\n",
      "\n",
      "Interval 120 (1190000 steps performed)\n",
      "10000/10000 [==============================] - 496s 50ms/step - reward: 0.1972\n",
      "12 episodes - episode_reward: 155.550 [102.900, 233.300] - loss: 0.080 - mae: 11.553 - mean_q: 13.993 - mean_eps: 0.104 - ale.lives: 1.971\n",
      "\n",
      "Interval 121 (1200000 steps performed)\n",
      "10000/10000 [==============================] - 497s 50ms/step - reward: 0.1982\n",
      "12 episodes - episode_reward: 178.842 [101.700, 312.500] - loss: 0.085 - mae: 11.482 - mean_q: 13.912 - mean_eps: 0.100 - ale.lives: 1.980\n",
      "\n",
      "Interval 122 (1210000 steps performed)\n",
      "10000/10000 [==============================] - 499s 50ms/step - reward: 0.1994\n",
      "12 episodes - episode_reward: 165.317 [112.400, 256.900] - loss: 0.087 - mae: 11.519 - mean_q: 13.959 - mean_eps: 0.100 - ale.lives: 1.971\n",
      "\n",
      "Interval 123 (1220000 steps performed)\n",
      "10000/10000 [==============================] - 501s 50ms/step - reward: 0.1950\n",
      "9 episodes - episode_reward: 194.056 [114.700, 252.400] - loss: 0.094 - mae: 11.689 - mean_q: 14.156 - mean_eps: 0.100 - ale.lives: 1.971\n",
      "\n",
      "Interval 124 (1230000 steps performed)\n",
      "10000/10000 [==============================] - 500s 50ms/step - reward: 0.1946\n",
      "10 episodes - episode_reward: 203.920 [117.800, 336.500] - loss: 0.090 - mae: 11.681 - mean_q: 14.150 - mean_eps: 0.100 - ale.lives: 1.946\n",
      "\n",
      "Interval 125 (1240000 steps performed)\n",
      "10000/10000 [==============================] - 501s 50ms/step - reward: 0.1963\n",
      "10 episodes - episode_reward: 203.670 [116.400, 287.800] - loss: 0.089 - mae: 11.776 - mean_q: 14.270 - mean_eps: 0.100 - ale.lives: 2.006\n",
      "\n",
      "Interval 126 (1250000 steps performed)\n",
      "10000/10000 [==============================] - 509s 51ms/step - reward: 0.1995\n",
      "12 episodes - episode_reward: 169.775 [120.000, 247.600] - loss: 0.080 - mae: 11.840 - mean_q: 14.347 - mean_eps: 0.100 - ale.lives: 1.949\n",
      "\n",
      "Interval 127 (1260000 steps performed)\n",
      "10000/10000 [==============================] - 509s 51ms/step - reward: 0.1953\n",
      "9 episodes - episode_reward: 216.611 [131.600, 298.000] - loss: 0.087 - mae: 11.840 - mean_q: 14.345 - mean_eps: 0.100 - ale.lives: 1.993\n",
      "\n",
      "Interval 128 (1270000 steps performed)\n",
      "10000/10000 [==============================] - 518s 52ms/step - reward: 0.1962\n",
      "10 episodes - episode_reward: 181.930 [125.700, 236.800] - loss: 0.087 - mae: 11.806 - mean_q: 14.303 - mean_eps: 0.100 - ale.lives: 1.750\n",
      "\n",
      "Interval 129 (1280000 steps performed)\n",
      "10000/10000 [==============================] - 525s 52ms/step - reward: 0.1953\n",
      "11 episodes - episode_reward: 188.600 [83.000, 266.800] - loss: 0.095 - mae: 11.819 - mean_q: 14.314 - mean_eps: 0.100 - ale.lives: 2.039\n",
      "\n",
      "Interval 130 (1290000 steps performed)\n",
      "10000/10000 [==============================] - 526s 53ms/step - reward: 0.1954\n",
      "11 episodes - episode_reward: 173.700 [129.100, 256.100] - loss: 0.100 - mae: 11.950 - mean_q: 14.466 - mean_eps: 0.100 - ale.lives: 1.933\n",
      "\n",
      "Interval 131 (1300000 steps performed)\n",
      "10000/10000 [==============================] - 529s 53ms/step - reward: 0.1979\n",
      "10 episodes - episode_reward: 190.140 [125.800, 265.800] - loss: 0.088 - mae: 12.039 - mean_q: 14.588 - mean_eps: 0.100 - ale.lives: 1.909\n",
      "\n",
      "Interval 132 (1310000 steps performed)\n",
      "10000/10000 [==============================] - 532s 53ms/step - reward: 0.1973\n",
      "11 episodes - episode_reward: 180.855 [123.400, 264.400] - loss: 0.098 - mae: 12.103 - mean_q: 14.653 - mean_eps: 0.100 - ale.lives: 1.982\n",
      "\n",
      "Interval 133 (1320000 steps performed)\n",
      "10000/10000 [==============================] - 535s 53ms/step - reward: 0.1965\n",
      "11 episodes - episode_reward: 174.582 [101.500, 240.700] - loss: 0.097 - mae: 12.234 - mean_q: 14.814 - mean_eps: 0.100 - ale.lives: 1.948\n",
      "\n",
      "Interval 134 (1330000 steps performed)\n",
      "10000/10000 [==============================] - 532s 53ms/step - reward: 0.1987\n",
      "13 episodes - episode_reward: 163.892 [115.200, 250.100] - loss: 0.108 - mae: 12.289 - mean_q: 14.880 - mean_eps: 0.100 - ale.lives: 1.972\n",
      "\n",
      "Interval 135 (1340000 steps performed)\n",
      "10000/10000 [==============================] - 535s 54ms/step - reward: 0.1962\n",
      "10 episodes - episode_reward: 198.290 [127.200, 313.500] - loss: 0.111 - mae: 12.294 - mean_q: 14.887 - mean_eps: 0.100 - ale.lives: 1.986\n",
      "\n",
      "Interval 136 (1350000 steps performed)\n",
      "10000/10000 [==============================] - 545s 55ms/step - reward: 0.1930\n",
      "12 episodes - episode_reward: 157.067 [107.500, 289.400] - loss: 0.098 - mae: 12.274 - mean_q: 14.863 - mean_eps: 0.100 - ale.lives: 1.982\n",
      "\n",
      "Interval 137 (1360000 steps performed)\n",
      "10000/10000 [==============================] - 545s 54ms/step - reward: 0.1954\n",
      "9 episodes - episode_reward: 207.367 [119.800, 285.300] - loss: 0.099 - mae: 12.339 - mean_q: 14.933 - mean_eps: 0.100 - ale.lives: 1.887\n",
      "\n",
      "Interval 138 (1370000 steps performed)\n",
      "10000/10000 [==============================] - 546s 55ms/step - reward: 0.1945\n",
      "10 episodes - episode_reward: 195.400 [125.900, 269.900] - loss: 0.100 - mae: 12.474 - mean_q: 15.097 - mean_eps: 0.100 - ale.lives: 1.921\n",
      "\n",
      "Interval 139 (1380000 steps performed)\n",
      "10000/10000 [==============================] - 552s 55ms/step - reward: 0.1960\n",
      "10 episodes - episode_reward: 204.080 [101.100, 342.200] - loss: 0.110 - mae: 12.526 - mean_q: 15.162 - mean_eps: 0.100 - ale.lives: 1.943\n",
      "\n",
      "Interval 140 (1390000 steps performed)\n",
      "10000/10000 [==============================] - 553s 55ms/step - reward: 0.1948\n",
      "10 episodes - episode_reward: 193.920 [120.300, 306.100] - loss: 0.101 - mae: 12.530 - mean_q: 15.174 - mean_eps: 0.100 - ale.lives: 1.843\n",
      "\n",
      "Interval 141 (1400000 steps performed)\n",
      "10000/10000 [==============================] - 555s 56ms/step - reward: 0.1952\n",
      "10 episodes - episode_reward: 180.330 [116.100, 257.800] - loss: 0.106 - mae: 12.626 - mean_q: 15.287 - mean_eps: 0.100 - ale.lives: 1.959\n",
      "\n",
      "Interval 142 (1410000 steps performed)\n",
      "10000/10000 [==============================] - 555s 55ms/step - reward: 0.1974\n",
      "12 episodes - episode_reward: 172.075 [117.900, 254.000] - loss: 0.103 - mae: 12.699 - mean_q: 15.379 - mean_eps: 0.100 - ale.lives: 2.020\n",
      "\n",
      "Interval 143 (1420000 steps performed)\n",
      "10000/10000 [==============================] - 559s 56ms/step - reward: 0.1970\n",
      "10 episodes - episode_reward: 199.410 [101.200, 285.700] - loss: 0.103 - mae: 12.711 - mean_q: 15.394 - mean_eps: 0.100 - ale.lives: 1.875\n",
      "\n",
      "Interval 144 (1430000 steps performed)\n",
      "10000/10000 [==============================] - 561s 56ms/step - reward: 0.1962\n",
      "11 episodes - episode_reward: 178.500 [95.700, 253.400] - loss: 0.093 - mae: 12.778 - mean_q: 15.467 - mean_eps: 0.100 - ale.lives: 1.982\n",
      "\n",
      "Interval 145 (1440000 steps performed)\n",
      "10000/10000 [==============================] - 564s 56ms/step - reward: 0.1971\n",
      "12 episodes - episode_reward: 168.667 [100.800, 256.200] - loss: 0.097 - mae: 12.807 - mean_q: 15.510 - mean_eps: 0.100 - ale.lives: 2.124\n",
      "\n",
      "Interval 146 (1450000 steps performed)\n",
      "10000/10000 [==============================] - 568s 57ms/step - reward: 0.1965\n",
      "10 episodes - episode_reward: 181.170 [126.400, 247.200] - loss: 0.102 - mae: 12.804 - mean_q: 15.503 - mean_eps: 0.100 - ale.lives: 2.034\n",
      "\n",
      "Interval 147 (1460000 steps performed)\n",
      "10000/10000 [==============================] - 573s 57ms/step - reward: 0.1976\n",
      "11 episodes - episode_reward: 182.345 [91.500, 282.100] - loss: 0.106 - mae: 12.858 - mean_q: 15.572 - mean_eps: 0.100 - ale.lives: 1.940\n",
      "\n",
      "Interval 148 (1470000 steps performed)\n",
      "10000/10000 [==============================] - 566s 57ms/step - reward: 0.1941\n",
      "11 episodes - episode_reward: 187.791 [85.900, 251.300] - loss: 0.098 - mae: 12.961 - mean_q: 15.689 - mean_eps: 0.100 - ale.lives: 1.824\n",
      "\n",
      "Interval 149 (1480000 steps performed)\n",
      "10000/10000 [==============================] - 561s 56ms/step - reward: 0.1971\n",
      "13 episodes - episode_reward: 152.108 [113.300, 224.300] - loss: 0.107 - mae: 13.024 - mean_q: 15.760 - mean_eps: 0.100 - ale.lives: 2.003\n",
      "\n",
      "Interval 150 (1490000 steps performed)\n",
      "10000/10000 [==============================] - 559s 56ms/step - reward: 0.1956\n",
      "10 episodes - episode_reward: 194.270 [122.800, 307.200] - loss: 0.106 - mae: 13.080 - mean_q: 15.834 - mean_eps: 0.100 - ale.lives: 1.850\n",
      "\n",
      "Interval 151 (1500000 steps performed)\n",
      " 5217/10000 [==============>...............] - ETA: 4:31 - reward: 0.1906done, took 60672.507 seconds\n"
     ]
    }
   ],
   "source": [
    "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
    "checkpoint_weights_filename = 'dqn_' + env_name + '_weights_{step}.h5f'\n",
    "log_filename = 'dqn_{}_log.json'.format(env_name)\n",
    "\n",
    "callbacks = [\n",
    "    ModelIntervalCheckpoint(checkpoint_weights_filename, interval=50000),\n",
    "    FileLogger(log_filename, interval=100)\n",
    "]\n",
    "\n",
    "dqn.fit(env, callbacks=callbacks, nb_steps=1500000, log_interval=10000, visualize=False)\n",
    "dqn.save_weights(weights_filename, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DhqD3IbHGAg"
   },
   "source": [
    "### 4. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "IdbiLyy9-LgD"
   },
   "outputs": [],
   "source": [
    "dqn.save_weights(weights_filename, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_NoydooSC7sZ",
    "outputId": "8bee087b-a8b7-48ea-c4dd-dbd5ca0ae170"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 19.000, steps: 811\n",
      "Episode 2: reward: 18.000, steps: 715\n",
      "Episode 3: reward: 20.000, steps: 742\n",
      "Episode 4: reward: 20.000, steps: 702\n",
      "Episode 5: reward: 20.000, steps: 734\n",
      "Episode 6: reward: 16.000, steps: 817\n",
      "Episode 7: reward: 19.000, steps: 1193\n",
      "Episode 8: reward: 27.000, steps: 1515\n",
      "Episode 9: reward: 21.000, steps: 1053\n",
      "Episode 10: reward: 15.000, steps: 1114\n",
      "Media de la recompensa en 10 episodios: 19.50\n",
      "Desviación estándar de la recompensa: 3.07\n",
      "Media de pasos por episodio: 939.60\n",
      "Desviación estándar de los pasos: 256.58\n"
     ]
    }
   ],
   "source": [
    "# Carga de pesos y evaluación\n",
    "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
    "dqn.load_weights(weights_filename)\n",
    "\n",
    "# Test en 10 episodios\n",
    "test_history = dqn.test(env_test, nb_episodes=10, visualize=False)\n",
    "\n",
    "# Métricas\n",
    "recompensas = test_history.history['episode_reward']\n",
    "pasos = test_history.history['nb_steps']\n",
    "\n",
    "print(f\"Media de la recompensa en 10 episodios: {np.mean(recompensas):.2f}\")\n",
    "print(f\"Desviación estándar de la recompensa: {np.std(recompensas):.2f}\")\n",
    "\n",
    "print(f\"Media de pasos por episodio: {np.mean(pasos):.2f}\")\n",
    "print(f\"Desviación estándar de los pasos: {np.std(pasos):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NAlu8b1Gb2b"
   },
   "source": [
    "3. Justificación de los parámetros seleccionados y de los resultados obtenidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYHyz6yhJeZ0"
   },
   "source": [
    "En este proyecto se ha entrenado un agente **DQN (Deep Q-Network)** para el entorno `SpaceInvaders-v0`, utilizando una arquitectura de red neuronal convolucional inspirada en el diseño original de DeepMind para juegos de Atari. La red incorpora tres capas `Conv2D` seguidas de `Flatten` y capas `Dense`, permitiendo una extracción jerárquica de características espaciales y la estimación de los valores Q para cada acción posible.\n",
    "\n",
    "Durante el entrenamiento, se ha incorporado un **reward shaping ligero** que premia las acciones de disparo (izquierda y derecha), con el objetivo de acelerar la exploración de políticas ofensivas y evitar conductas pasivas en fases tempranas. Este shaping se ha desactivado completamente durante el test, siguiendo buenas prácticas para no inflar artificialmente los resultados. Para ello, se ha utilizado un entorno separado, sin modificaciones ni bonificaciones adicionales, garantizando así que las recompensas obtenidas en test reflejen exclusivamente la política aprendida por el agente.\n",
    "\n",
    "La política de exploración empleada ha sido **ε-greedy con decaimiento lineal**, comenzando en **ε=1.0** (exploración total) y descendiendo hasta **ε=0.1** de forma progresiva. Este valor mínimo se alcanza justo al **80% del total de pasos de entrenamiento**, lo cual permite una amplia fase de exploración antes de consolidar la explotación. Esta estrategia es común en entornos complejos porque **favorece el descubrimiento de comportamientos variados al inicio**, mientras que **reserva el tramo final del entrenamiento para afinar la política con un comportamiento más determinista**.\n",
    "\n",
    "\n",
    "**Resultados obtenidos (modo test)**\n",
    "\n",
    "- **Media de recompensa en 10 episodios:** 19.50  \n",
    "- **Desviación estándar de la recompensa:** 3.07  \n",
    "- **Media de pasos por episodio:** 939.60  \n",
    "- **Desviación estándar de pasos:** 256.58  \n",
    "\n",
    "Estos resultados reflejan un comportamiento **estable y cercano al umbral exigido de 20 puntos**, con una desviación moderada que indica que el agente ha aprendido una política funcional y reproducible. Se observa una ligera variabilidad entre episodios, típica de entornos con cierto grado de aleatoriedad en la dinámica de los enemigos.\n",
    "\n",
    "**Posibles líneas de mejora**\n",
    "\n",
    "Aunque la media obtenida es sólida, se podrían considerar mejoras como:\n",
    "\n",
    "- Aumentar ligeramente el número de pasos de entrenamiento.\n",
    "- Ajustar el *learning rate* o la tasa de exploración final.\n",
    "- Incorporar técnicas como **Double DQN** o **Prioritized Experience Replay**.\n",
    "- Incrementar la regularización o el uso de `BatchNormalization` para mejorar la generalización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANFQiicXK3sO"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
