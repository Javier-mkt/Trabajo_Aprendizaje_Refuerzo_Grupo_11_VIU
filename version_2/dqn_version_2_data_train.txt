Training for 1200000 steps ...
c:\Users\jgilg\anaconda3\envs\miar_rl\lib\site-packages\tensorflow\python\keras\engine\training.py:2424: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
     420/1200000: episode: 1, duration: 1.941s, episode steps: 420, steps per second: 216, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    1131/1200000: episode: 2, duration: 2.665s, episode steps: 711, steps per second: 267, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    1941/1200000: episode: 3, duration: 3.539s, episode steps: 810, steps per second: 229, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    2831/1200000: episode: 4, duration: 4.669s, episode steps: 890, steps per second: 191, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    3206/1200000: episode: 5, duration: 1.841s, episode steps: 375, steps per second: 204, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    3813/1200000: episode: 6, duration: 2.272s, episode steps: 607, steps per second: 267, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    4535/1200000: episode: 7, duration: 2.397s, episode steps: 722, steps per second: 301, episode reward:  6.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    5186/1200000: episode: 8, duration: 2.680s, episode steps: 651, steps per second: 243, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    6017/1200000: episode: 9, duration: 3.473s, episode steps: 831, steps per second: 239, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    6548/1200000: episode: 10, duration: 2.004s, episode steps: 531, steps per second: 265, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    7406/1200000: episode: 11, duration: 3.218s, episode steps: 858, steps per second: 267, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    7939/1200000: episode: 12, duration: 2.289s, episode steps: 533, steps per second: 233, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    8692/1200000: episode: 13, duration: 2.978s, episode steps: 753, steps per second: 253, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    9336/1200000: episode: 14, duration: 2.596s, episode steps: 644, steps per second: 248, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    9698/1200000: episode: 15, duration: 1.534s, episode steps: 362, steps per second: 236, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   10680/1200000: episode: 16, duration: 4.066s, episode steps: 982, steps per second: 242, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   11301/1200000: episode: 17, duration: 2.532s, episode steps: 621, steps per second: 245, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   11749/1200000: episode: 18, duration: 1.619s, episode steps: 448, steps per second: 277, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   12965/1200000: episode: 19, duration: 4.801s, episode steps: 1216, steps per second: 253, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   14127/1200000: episode: 20, duration: 4.084s, episode steps: 1162, steps per second: 284, episode reward: 11.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   14764/1200000: episode: 21, duration: 2.386s, episode steps: 637, steps per second: 267, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   15373/1200000: episode: 22, duration: 2.123s, episode steps: 609, steps per second: 287, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   15764/1200000: episode: 23, duration: 1.655s, episode steps: 391, steps per second: 236, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   16465/1200000: episode: 24, duration: 2.697s, episode steps: 701, steps per second: 260, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   17069/1200000: episode: 25, duration: 2.355s, episode steps: 604, steps per second: 256, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   17924/1200000: episode: 26, duration: 3.206s, episode steps: 855, steps per second: 267, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   18735/1200000: episode: 27, duration: 2.963s, episode steps: 811, steps per second: 274, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   19390/1200000: episode: 28, duration: 2.379s, episode steps: 655, steps per second: 275, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   19852/1200000: episode: 29, duration: 1.601s, episode steps: 462, steps per second: 289, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   20988/1200000: episode: 30, duration: 4.104s, episode steps: 1136, steps per second: 277, episode reward: 14.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   21460/1200000: episode: 31, duration: 1.592s, episode steps: 472, steps per second: 296, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   22200/1200000: episode: 32, duration: 3.304s, episode steps: 740, steps per second: 224, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   23146/1200000: episode: 33, duration: 3.936s, episode steps: 946, steps per second: 240, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   24493/1200000: episode: 34, duration: 5.523s, episode steps: 1347, steps per second: 244, episode reward: 17.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
c:\Users\jgilg\anaconda3\envs\miar_rl\lib\site-packages\tensorflow\python\keras\engine\training.py:2424: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
   25715/1200000: episode: 35, duration: 14.751s, episode steps: 1222, steps per second:  83, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.005355, mae: 0.030328, mean_q: 0.046177, mean_eps: 0.961963
   26382/1200000: episode: 36, duration: 10.455s, episode steps: 667, steps per second:  64, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.005824, mae: 0.031926, mean_q: 0.046575, mean_eps: 0.960928
   26783/1200000: episode: 37, duration: 6.120s, episode steps: 401, steps per second:  66, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.005622, mae: 0.031854, mean_q: 0.042913, mean_eps: 0.960127
   27783/1200000: episode: 38, duration: 15.690s, episode steps: 1000, steps per second:  64, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.008162, mae: 0.037903, mean_q: 0.050547, mean_eps: 0.959077
   28374/1200000: episode: 39, duration: 9.347s, episode steps: 591, steps per second:  63, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.006802, mae: 0.034157, mean_q: 0.047089, mean_eps: 0.957883
   29384/1200000: episode: 40, duration: 15.816s, episode steps: 1010, steps per second:  64, episode reward: 10.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.006644, mae: 0.034060, mean_q: 0.046436, mean_eps: 0.956683
   29983/1200000: episode: 41, duration: 9.294s, episode steps: 599, steps per second:  64, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.007767, mae: 0.037416, mean_q: 0.052428, mean_eps: 0.955477
   30521/1200000: episode: 42, duration: 8.311s, episode steps: 538, steps per second:  65, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.006011, mae: 0.048121, mean_q: 0.065744, mean_eps: 0.954622
   31236/1200000: episode: 43, duration: 11.142s, episode steps: 715, steps per second:  64, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.007132, mae: 0.052294, mean_q: 0.070997, mean_eps: 0.953683
   32167/1200000: episode: 44, duration: 14.672s, episode steps: 931, steps per second:  63, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.006074, mae: 0.050319, mean_q: 0.068388, mean_eps: 0.952450
   32561/1200000: episode: 45, duration: 6.598s, episode steps: 394, steps per second:  60, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.005863, mae: 0.049895, mean_q: 0.068178, mean_eps: 0.951454
   33231/1200000: episode: 46, duration: 10.886s, episode steps: 670, steps per second:  62, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.006248, mae: 0.050798, mean_q: 0.070453, mean_eps: 0.950656
   33874/1200000: episode: 47, duration: 10.150s, episode steps: 643, steps per second:  63, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.006177, mae: 0.050065, mean_q: 0.070692, mean_eps: 0.949672
   34573/1200000: episode: 48, duration: 11.126s, episode steps: 699, steps per second:  63, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.005787, mae: 0.050247, mean_q: 0.070097, mean_eps: 0.948664
   35163/1200000: episode: 49, duration: 10.362s, episode steps: 590, steps per second:  57, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.006985, mae: 0.052925, mean_q: 0.070964, mean_eps: 0.947698
   35803/1200000: episode: 50, duration: 10.139s, episode steps: 640, steps per second:  63, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.007282, mae: 0.052819, mean_q: 0.072503, mean_eps: 0.946777
   36512/1200000: episode: 51, duration: 12.435s, episode steps: 709, steps per second:  57, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.006108, mae: 0.050292, mean_q: 0.069826, mean_eps: 0.945766
   37214/1200000: episode: 52, duration: 13.703s, episode steps: 702, steps per second:  51, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.006883, mae: 0.053436, mean_q: 0.076163, mean_eps: 0.944707
   37705/1200000: episode: 53, duration: 8.720s, episode steps: 491, steps per second:  56, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.007793, mae: 0.055385, mean_q: 0.080065, mean_eps: 0.943810
   38469/1200000: episode: 54, duration: 13.716s, episode steps: 764, steps per second:  56, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.006081, mae: 0.050738, mean_q: 0.073219, mean_eps: 0.942868
   39250/1200000: episode: 55, duration: 14.061s, episode steps: 781, steps per second:  56, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.006344, mae: 0.050811, mean_q: 0.074957, mean_eps: 0.941710
   39744/1200000: episode: 56, duration: 8.639s, episode steps: 494, steps per second:  57, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.006744, mae: 0.053257, mean_q: 0.080555, mean_eps: 0.940756
   40238/1200000: episode: 57, duration: 9.583s, episode steps: 494, steps per second:  52, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.006405, mae: 0.058800, mean_q: 0.085772, mean_eps: 0.940015
   41016/1200000: episode: 58, duration: 13.974s, episode steps: 778, steps per second:  56, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.005589, mae: 0.068805, mean_q: 0.098550, mean_eps: 0.939061
   41947/1200000: episode: 59, duration: 16.396s, episode steps: 931, steps per second:  57, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.007050, mae: 0.072371, mean_q: 0.102141, mean_eps: 0.937780
   42606/1200000: episode: 60, duration: 12.644s, episode steps: 659, steps per second:  52, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.007064, mae: 0.072355, mean_q: 0.100193, mean_eps: 0.936586
   43185/1200000: episode: 61, duration: 11.519s, episode steps: 579, steps per second:  50, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.005617, mae: 0.069273, mean_q: 0.095300, mean_eps: 0.935656
   43678/1200000: episode: 62, duration: 9.307s, episode steps: 493, steps per second:  53, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.006754, mae: 0.073069, mean_q: 0.103563, mean_eps: 0.934852
   44085/1200000: episode: 63, duration: 6.870s, episode steps: 407, steps per second:  59, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.008002, mae: 0.075632, mean_q: 0.104579, mean_eps: 0.934177
   44930/1200000: episode: 64, duration: 14.169s, episode steps: 845, steps per second:  60, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.007356, mae: 0.073362, mean_q: 0.102128, mean_eps: 0.933238
   45967/1200000: episode: 65, duration: 18.024s, episode steps: 1037, steps per second:  58, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.006043, mae: 0.070155, mean_q: 0.097820, mean_eps: 0.931828
   46515/1200000: episode: 66, duration: 9.638s, episode steps: 548, steps per second:  57, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.005243, mae: 0.068573, mean_q: 0.095449, mean_eps: 0.930640
   47059/1200000: episode: 67, duration: 9.784s, episode steps: 544, steps per second:  56, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.006499, mae: 0.072787, mean_q: 0.102164, mean_eps: 0.929821
   47426/1200000: episode: 68, duration: 6.805s, episode steps: 367, steps per second:  54, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.006603, mae: 0.070886, mean_q: 0.099873, mean_eps: 0.929137
   48071/1200000: episode: 69, duration: 11.808s, episode steps: 645, steps per second:  55, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.006799, mae: 0.074521, mean_q: 0.104692, mean_eps: 0.928378
   48784/1200000: episode: 70, duration: 13.313s, episode steps: 713, steps per second:  54, episode reward:  6.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.006128, mae: 0.069990, mean_q: 0.099896, mean_eps: 0.927361
   49352/1200000: episode: 71, duration: 10.402s, episode steps: 568, steps per second:  55, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.006776, mae: 0.073709, mean_q: 0.104163, mean_eps: 0.926401
   50225/1200000: episode: 72, duration: 15.673s, episode steps: 873, steps per second:  56, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.006318, mae: 0.080159, mean_q: 0.113515, mean_eps: 0.925318
   50725/1200000: episode: 73, duration: 8.209s, episode steps: 500, steps per second:  61, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.006976, mae: 0.110958, mean_q: 0.151682, mean_eps: 0.924286
   51263/1200000: episode: 74, duration: 8.741s, episode steps: 538, steps per second:  62, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.005309, mae: 0.106141, mean_q: 0.145419, mean_eps: 0.923509
   51870/1200000: episode: 75, duration: 10.632s, episode steps: 607, steps per second:  57, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.005192, mae: 0.105360, mean_q: 0.141816, mean_eps: 0.922651
   52604/1200000: episode: 76, duration: 12.550s, episode steps: 734, steps per second:  58, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.005531, mae: 0.106671, mean_q: 0.142583, mean_eps: 0.921646
   53107/1200000: episode: 77, duration: 8.521s, episode steps: 503, steps per second:  59, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.005700, mae: 0.106974, mean_q: 0.141299, mean_eps: 0.920719
   53631/1200000: episode: 78, duration: 9.026s, episode steps: 524, steps per second:  58, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.006381, mae: 0.108382, mean_q: 0.144044, mean_eps: 0.919948
   54332/1200000: episode: 79, duration: 11.676s, episode steps: 701, steps per second:  60, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.005851, mae: 0.107079, mean_q: 0.142650, mean_eps: 0.919030
   54961/1200000: episode: 80, duration: 10.401s, episode steps: 629, steps per second:  60, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.004505, mae: 0.104621, mean_q: 0.140434, mean_eps: 0.918031
   55644/1200000: episode: 81, duration: 11.442s, episode steps: 683, steps per second:  60, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.007068, mae: 0.110881, mean_q: 0.150771, mean_eps: 0.917047
   56356/1200000: episode: 82, duration: 11.889s, episode steps: 712, steps per second:  60, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.007238, mae: 0.110046, mean_q: 0.148429, mean_eps: 0.916003
   57498/1200000: episode: 83, duration: 19.625s, episode steps: 1142, steps per second:  58, episode reward: 13.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.006384, mae: 0.111264, mean_q: 0.151360, mean_eps: 0.914611
   58121/1200000: episode: 84, duration: 11.147s, episode steps: 623, steps per second:  56, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.006430, mae: 0.109582, mean_q: 0.150716, mean_eps: 0.913285
   58891/1200000: episode: 85, duration: 13.438s, episode steps: 770, steps per second:  57, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.005425, mae: 0.107576, mean_q: 0.145254, mean_eps: 0.912241
   59645/1200000: episode: 86, duration: 13.880s, episode steps: 754, steps per second:  54, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.006642, mae: 0.109173, mean_q: 0.149373, mean_eps: 0.911098
   60787/1200000: episode: 87, duration: 20.931s, episode steps: 1142, steps per second:  55, episode reward: 14.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.006772, mae: 0.126110, mean_q: 0.169427, mean_eps: 0.909676
   61509/1200000: episode: 88, duration: 13.093s, episode steps: 722, steps per second:  55, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.006051, mae: 0.133767, mean_q: 0.174500, mean_eps: 0.908278
   61898/1200000: episode: 89, duration: 6.719s, episode steps: 389, steps per second:  58, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.005406, mae: 0.129335, mean_q: 0.167444, mean_eps: 0.907444
   62695/1200000: episode: 90, duration: 14.118s, episode steps: 797, steps per second:  56, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.006882, mae: 0.134673, mean_q: 0.174866, mean_eps: 0.906556
   64127/1200000: episode: 91, duration: 25.317s, episode steps: 1432, steps per second:  57, episode reward: 15.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.006393, mae: 0.133890, mean_q: 0.174846, mean_eps: 0.904885
   65490/1200000: episode: 92, duration: 23.709s, episode steps: 1363, steps per second:  57, episode reward: 14.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.005718, mae: 0.131988, mean_q: 0.173273, mean_eps: 0.902788
   66714/1200000: episode: 93, duration: 23.468s, episode steps: 1224, steps per second:  52, episode reward: 15.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.005585, mae: 0.131325, mean_q: 0.173221, mean_eps: 0.900847
   67357/1200000: episode: 94, duration: 12.516s, episode steps: 643, steps per second:  51, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.004728, mae: 0.130261, mean_q: 0.171477, mean_eps: 0.899446
   67796/1200000: episode: 95, duration: 7.925s, episode steps: 439, steps per second:  55, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.005374, mae: 0.130682, mean_q: 0.171356, mean_eps: 0.898636
   68417/1200000: episode: 96, duration: 11.231s, episode steps: 621, steps per second:  55, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.006314, mae: 0.132512, mean_q: 0.172837, mean_eps: 0.897841
   69310/1200000: episode: 97, duration: 16.238s, episode steps: 893, steps per second:  55, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.006063, mae: 0.133804, mean_q: 0.177939, mean_eps: 0.896704
   70093/1200000: episode: 98, duration: 14.317s, episode steps: 783, steps per second:  55, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.006624, mae: 0.138620, mean_q: 0.184939, mean_eps: 0.895447
   70492/1200000: episode: 99, duration: 7.300s, episode steps: 399, steps per second:  55, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.006046, mae: 0.163542, mean_q: 0.214521, mean_eps: 0.894562
   71415/1200000: episode: 100, duration: 15.409s, episode steps: 923, steps per second:  60, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.006800, mae: 0.166752, mean_q: 0.218098, mean_eps: 0.893572
   71918/1200000: episode: 101, duration: 8.094s, episode steps: 503, steps per second:  62, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.007653, mae: 0.171401, mean_q: 0.223023, mean_eps: 0.892501
   73124/1200000: episode: 102, duration: 20.804s, episode steps: 1206, steps per second:  58, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.006737, mae: 0.165409, mean_q: 0.215171, mean_eps: 0.891220
   73788/1200000: episode: 103, duration: 12.318s, episode steps: 664, steps per second:  54, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.006381, mae: 0.166492, mean_q: 0.216588, mean_eps: 0.889819
   74706/1200000: episode: 104, duration: 15.586s, episode steps: 918, steps per second:  59, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.006312, mae: 0.164615, mean_q: 0.214857, mean_eps: 0.888631
   75428/1200000: episode: 105, duration: 12.561s, episode steps: 722, steps per second:  57, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.006435, mae: 0.166508, mean_q: 0.216698, mean_eps: 0.887401
   75948/1200000: episode: 106, duration: 8.730s, episode steps: 520, steps per second:  60, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.006518, mae: 0.167813, mean_q: 0.217605, mean_eps: 0.886471
   76322/1200000: episode: 107, duration: 6.519s, episode steps: 374, steps per second:  57, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.006488, mae: 0.170218, mean_q: 0.219494, mean_eps: 0.885799
   76740/1200000: episode: 108, duration: 7.530s, episode steps: 418, steps per second:  56, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.006129, mae: 0.166195, mean_q: 0.215720, mean_eps: 0.885205
   77655/1200000: episode: 109, duration: 16.196s, episode steps: 915, steps per second:  56, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.006437, mae: 0.167685, mean_q: 0.215745, mean_eps: 0.884206
   78424/1200000: episode: 110, duration: 13.795s, episode steps: 769, steps per second:  56, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.007300, mae: 0.167352, mean_q: 0.216473, mean_eps: 0.882943
   79055/1200000: episode: 111, duration: 11.816s, episode steps: 631, steps per second:  53, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.007124, mae: 0.170309, mean_q: 0.219201, mean_eps: 0.881893
   79842/1200000: episode: 112, duration: 15.162s, episode steps: 787, steps per second:  52, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.007065, mae: 0.167544, mean_q: 0.219450, mean_eps: 0.880828
   80417/1200000: episode: 113, duration: 11.279s, episode steps: 575, steps per second:  51, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.005675, mae: 0.183299, mean_q: 0.236963, mean_eps: 0.879805
   81198/1200000: episode: 114, duration: 14.954s, episode steps: 781, steps per second:  52, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.006283, mae: 0.192766, mean_q: 0.248960, mean_eps: 0.878788
   81594/1200000: episode: 115, duration: 7.001s, episode steps: 396, steps per second:  57, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.006978, mae: 0.196479, mean_q: 0.252030, mean_eps: 0.877906
   82236/1200000: episode: 116, duration: 11.620s, episode steps: 642, steps per second:  55, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.006712, mae: 0.195559, mean_q: 0.250834, mean_eps: 0.877129
   83103/1200000: episode: 117, duration: 18.134s, episode steps: 867, steps per second:  48, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.005870, mae: 0.191759, mean_q: 0.245365, mean_eps: 0.875998
   84333/1200000: episode: 118, duration: 25.266s, episode steps: 1230, steps per second:  49, episode reward: 15.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.006960, mae: 0.198046, mean_q: 0.252955, mean_eps: 0.874423
   84840/1200000: episode: 119, duration: 9.097s, episode steps: 507, steps per second:  56, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.007131, mae: 0.197006, mean_q: 0.251477, mean_eps: 0.873121
   85272/1200000: episode: 120, duration: 8.263s, episode steps: 432, steps per second:  52, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.789 [0.000, 5.000],  loss: 0.005829, mae: 0.195345, mean_q: 0.248574, mean_eps: 0.872419
   85951/1200000: episode: 121, duration: 12.607s, episode steps: 679, steps per second:  54, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.006139, mae: 0.193183, mean_q: 0.247994, mean_eps: 0.871585
   86744/1200000: episode: 122, duration: 15.121s, episode steps: 793, steps per second:  52, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.006758, mae: 0.198654, mean_q: 0.255009, mean_eps: 0.870481
   87255/1200000: episode: 123, duration: 10.060s, episode steps: 511, steps per second:  51, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.005848, mae: 0.193608, mean_q: 0.246232, mean_eps: 0.869503
   88435/1200000: episode: 124, duration: 22.900s, episode steps: 1180, steps per second:  52, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.006771, mae: 0.194776, mean_q: 0.249173, mean_eps: 0.868234
   89505/1200000: episode: 125, duration: 21.729s, episode steps: 1070, steps per second:  49, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.006456, mae: 0.193616, mean_q: 0.248068, mean_eps: 0.866545
   90147/1200000: episode: 126, duration: 12.339s, episode steps: 642, steps per second:  52, episode reward:  4.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.006879, mae: 0.200948, mean_q: 0.257190, mean_eps: 0.865261
   90670/1200000: episode: 127, duration: 8.766s, episode steps: 523, steps per second:  60, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.006915, mae: 0.217648, mean_q: 0.275027, mean_eps: 0.864388
   91154/1200000: episode: 128, duration: 8.451s, episode steps: 484, steps per second:  57, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.006562, mae: 0.219318, mean_q: 0.277656, mean_eps: 0.863632
   91932/1200000: episode: 129, duration: 13.279s, episode steps: 778, steps per second:  59, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.005484, mae: 0.217082, mean_q: 0.276019, mean_eps: 0.862687
   92434/1200000: episode: 130, duration: 9.166s, episode steps: 502, steps per second:  55, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.006269, mae: 0.217685, mean_q: 0.277088, mean_eps: 0.861727
   93129/1200000: episode: 131, duration: 13.387s, episode steps: 695, steps per second:  52, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.007149, mae: 0.221203, mean_q: 0.279871, mean_eps: 0.860827
   94072/1200000: episode: 132, duration: 16.613s, episode steps: 943, steps per second:  57, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.006928, mae: 0.220330, mean_q: 0.279804, mean_eps: 0.859600
   94915/1200000: episode: 133, duration: 14.164s, episode steps: 843, steps per second:  60, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.006229, mae: 0.221260, mean_q: 0.281150, mean_eps: 0.858262
   95690/1200000: episode: 134, duration: 13.761s, episode steps: 775, steps per second:  56, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.006538, mae: 0.223653, mean_q: 0.285083, mean_eps: 0.857047
   96223/1200000: episode: 135, duration: 9.346s, episode steps: 533, steps per second:  57, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.006192, mae: 0.218625, mean_q: 0.277035, mean_eps: 0.856066
   96868/1200000: episode: 136, duration: 11.205s, episode steps: 645, steps per second:  58, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.007077, mae: 0.222539, mean_q: 0.282693, mean_eps: 0.855184
   97274/1200000: episode: 137, duration: 7.356s, episode steps: 406, steps per second:  55, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.650 [0.000, 5.000],  loss: 0.005518, mae: 0.216663, mean_q: 0.277177, mean_eps: 0.854395
   98506/1200000: episode: 138, duration: 21.982s, episode steps: 1232, steps per second:  56, episode reward: 17.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.005840, mae: 0.217820, mean_q: 0.276635, mean_eps: 0.853165
   99595/1200000: episode: 139, duration: 22.978s, episode steps: 1089, steps per second:  47, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.005957, mae: 0.221455, mean_q: 0.282608, mean_eps: 0.851425
  100246/1200000: episode: 140, duration: 12.734s, episode steps: 651, steps per second:  51, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.004622, mae: 0.225509, mean_q: 0.285831, mean_eps: 0.850120
  101222/1200000: episode: 141, duration: 18.174s, episode steps: 976, steps per second:  54, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.008106, mae: 0.247934, mean_q: 0.312115, mean_eps: 0.848899
  101735/1200000: episode: 142, duration: 9.607s, episode steps: 513, steps per second:  53, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.324 [0.000, 5.000],  loss: 0.007089, mae: 0.248833, mean_q: 0.315515, mean_eps: 0.847783
  102384/1200000: episode: 143, duration: 11.317s, episode steps: 649, steps per second:  57, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.006025, mae: 0.245286, mean_q: 0.309332, mean_eps: 0.846913
  103055/1200000: episode: 144, duration: 12.416s, episode steps: 671, steps per second:  54, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.235 [0.000, 5.000],  loss: 0.005234, mae: 0.242619, mean_q: 0.305969, mean_eps: 0.845923
  103807/1200000: episode: 145, duration: 12.959s, episode steps: 752, steps per second:  58, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.007991, mae: 0.249644, mean_q: 0.312534, mean_eps: 0.844855
  104308/1200000: episode: 146, duration: 8.736s, episode steps: 501, steps per second:  57, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.006886, mae: 0.245820, mean_q: 0.309852, mean_eps: 0.843916
  104948/1200000: episode: 147, duration: 11.256s, episode steps: 640, steps per second:  57, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.723 [0.000, 5.000],  loss: 0.006192, mae: 0.242920, mean_q: 0.306959, mean_eps: 0.843061
  105473/1200000: episode: 148, duration: 9.399s, episode steps: 525, steps per second:  56, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.265 [0.000, 5.000],  loss: 0.006357, mae: 0.247153, mean_q: 0.309829, mean_eps: 0.842185
  106196/1200000: episode: 149, duration: 13.996s, episode steps: 723, steps per second:  52, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.006790, mae: 0.246230, mean_q: 0.308291, mean_eps: 0.841249
  106926/1200000: episode: 150, duration: 13.803s, episode steps: 730, steps per second:  53, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.006640, mae: 0.243496, mean_q: 0.304202, mean_eps: 0.840160
  107778/1200000: episode: 151, duration: 15.824s, episode steps: 852, steps per second:  54, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.006984, mae: 0.245230, mean_q: 0.309688, mean_eps: 0.838972
  108712/1200000: episode: 152, duration: 17.638s, episode steps: 934, steps per second:  53, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.006349, mae: 0.244040, mean_q: 0.308526, mean_eps: 0.837634
  109179/1200000: episode: 153, duration: 8.810s, episode steps: 467, steps per second:  53, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.007419, mae: 0.243609, mean_q: 0.307220, mean_eps: 0.836584
  109616/1200000: episode: 154, duration: 8.525s, episode steps: 437, steps per second:  51, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.006839, mae: 0.246866, mean_q: 0.311718, mean_eps: 0.835906
  110517/1200000: episode: 155, duration: 14.915s, episode steps: 901, steps per second:  60, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.006381, mae: 0.262701, mean_q: 0.332661, mean_eps: 0.834901
  111420/1200000: episode: 156, duration: 15.764s, episode steps: 903, steps per second:  57, episode reward:  8.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.005817, mae: 0.274907, mean_q: 0.345849, mean_eps: 0.833548
  112105/1200000: episode: 157, duration: 13.282s, episode steps: 685, steps per second:  52, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.007472, mae: 0.277892, mean_q: 0.349901, mean_eps: 0.832357
  112683/1200000: episode: 158, duration: 11.592s, episode steps: 578, steps per second:  50, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.006501, mae: 0.272576, mean_q: 0.341417, mean_eps: 0.831409
  113282/1200000: episode: 159, duration: 11.450s, episode steps: 599, steps per second:  52, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.006527, mae: 0.280915, mean_q: 0.353872, mean_eps: 0.830527
  113976/1200000: episode: 160, duration: 11.869s, episode steps: 694, steps per second:  58, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.005834, mae: 0.273923, mean_q: 0.344545, mean_eps: 0.829558
  114671/1200000: episode: 161, duration: 12.305s, episode steps: 695, steps per second:  56, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.006761, mae: 0.279130, mean_q: 0.349212, mean_eps: 0.828517
  115272/1200000: episode: 162, duration: 10.579s, episode steps: 601, steps per second:  57, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.006507, mae: 0.272355, mean_q: 0.340195, mean_eps: 0.827545
  115832/1200000: episode: 163, duration: 9.296s, episode steps: 560, steps per second:  60, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.006070, mae: 0.271550, mean_q: 0.340108, mean_eps: 0.826675
  116234/1200000: episode: 164, duration: 6.985s, episode steps: 402, steps per second:  58, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.004768, mae: 0.271203, mean_q: 0.339421, mean_eps: 0.825952
  116755/1200000: episode: 165, duration: 9.132s, episode steps: 521, steps per second:  57, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.676 [0.000, 5.000],  loss: 0.007304, mae: 0.275419, mean_q: 0.342422, mean_eps: 0.825259
  117140/1200000: episode: 166, duration: 6.629s, episode steps: 385, steps per second:  58, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.008781, mae: 0.282136, mean_q: 0.352535, mean_eps: 0.824581
  117551/1200000: episode: 167, duration: 7.598s, episode steps: 411, steps per second:  54, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.006057, mae: 0.277968, mean_q: 0.349723, mean_eps: 0.823984
  117938/1200000: episode: 168, duration: 6.519s, episode steps: 387, steps per second:  59, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.755 [0.000, 5.000],  loss: 0.006497, mae: 0.276494, mean_q: 0.348155, mean_eps: 0.823384
  118467/1200000: episode: 169, duration: 10.064s, episode steps: 529, steps per second:  53, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.227 [0.000, 5.000],  loss: 0.007465, mae: 0.276327, mean_q: 0.345837, mean_eps: 0.822697
  119019/1200000: episode: 170, duration: 10.511s, episode steps: 552, steps per second:  53, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.005978, mae: 0.277817, mean_q: 0.350486, mean_eps: 0.821887
  119365/1200000: episode: 171, duration: 6.377s, episode steps: 346, steps per second:  54, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.006828, mae: 0.271172, mean_q: 0.341053, mean_eps: 0.821212
  120231/1200000: episode: 172, duration: 15.340s, episode steps: 866, steps per second:  56, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.006412, mae: 0.278243, mean_q: 0.349085, mean_eps: 0.820303
  120577/1200000: episode: 173, duration: 6.111s, episode steps: 346, steps per second:  57, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.006128, mae: 0.298190, mean_q: 0.373218, mean_eps: 0.819394
  121618/1200000: episode: 174, duration: 18.705s, episode steps: 1041, steps per second:  56, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.008374, mae: 0.302218, mean_q: 0.377987, mean_eps: 0.818353
  122277/1200000: episode: 175, duration: 12.219s, episode steps: 659, steps per second:  54, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.007587, mae: 0.298880, mean_q: 0.373164, mean_eps: 0.817078
  122895/1200000: episode: 176, duration: 11.232s, episode steps: 618, steps per second:  55, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.006881, mae: 0.302031, mean_q: 0.378876, mean_eps: 0.816121
  123531/1200000: episode: 177, duration: 11.647s, episode steps: 636, steps per second:  55, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.006475, mae: 0.294885, mean_q: 0.368019, mean_eps: 0.815182
  124593/1200000: episode: 178, duration: 18.358s, episode steps: 1062, steps per second:  58, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.006535, mae: 0.298982, mean_q: 0.372381, mean_eps: 0.813907
  125341/1200000: episode: 179, duration: 13.685s, episode steps: 748, steps per second:  55, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.006664, mae: 0.302132, mean_q: 0.377789, mean_eps: 0.812548
  126033/1200000: episode: 180, duration: 13.356s, episode steps: 692, steps per second:  52, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.006603, mae: 0.300159, mean_q: 0.374645, mean_eps: 0.811468
  127155/1200000: episode: 181, duration: 22.334s, episode steps: 1122, steps per second:  50, episode reward: 15.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.007286, mae: 0.307400, mean_q: 0.383854, mean_eps: 0.810109
  127766/1200000: episode: 182, duration: 11.877s, episode steps: 611, steps per second:  51, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.006797, mae: 0.301523, mean_q: 0.376203, mean_eps: 0.808810
  128157/1200000: episode: 183, duration: 7.776s, episode steps: 391, steps per second:  50, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.006570, mae: 0.301806, mean_q: 0.375507, mean_eps: 0.808057
  129065/1200000: episode: 184, duration: 18.313s, episode steps: 908, steps per second:  50, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.007436, mae: 0.301870, mean_q: 0.376725, mean_eps: 0.807082
  129734/1200000: episode: 185, duration: 12.180s, episode steps: 669, steps per second:  55, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.007261, mae: 0.299506, mean_q: 0.372823, mean_eps: 0.805900
  130328/1200000: episode: 186, duration: 10.173s, episode steps: 594, steps per second:  58, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.008275, mae: 0.308724, mean_q: 0.382994, mean_eps: 0.804955
  130977/1200000: episode: 187, duration: 10.972s, episode steps: 649, steps per second:  59, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.006202, mae: 0.310583, mean_q: 0.386338, mean_eps: 0.804022
  131516/1200000: episode: 188, duration: 10.028s, episode steps: 539, steps per second:  54, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.007677, mae: 0.305725, mean_q: 0.380496, mean_eps: 0.803131
  131883/1200000: episode: 189, duration: 6.977s, episode steps: 367, steps per second:  53, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.006674, mae: 0.310904, mean_q: 0.386158, mean_eps: 0.802453
  132399/1200000: episode: 190, duration: 9.888s, episode steps: 516, steps per second:  52, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.007066, mae: 0.310200, mean_q: 0.387442, mean_eps: 0.801790
  132906/1200000: episode: 191, duration: 9.798s, episode steps: 507, steps per second:  52, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.006617, mae: 0.317725, mean_q: 0.396973, mean_eps: 0.801022
  133620/1200000: episode: 192, duration: 11.966s, episode steps: 714, steps per second:  60, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.007624, mae: 0.313417, mean_q: 0.388868, mean_eps: 0.800107
  134218/1200000: episode: 193, duration: 10.644s, episode steps: 598, steps per second:  56, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.007551, mae: 0.309877, mean_q: 0.385697, mean_eps: 0.799123
  134603/1200000: episode: 194, duration: 6.965s, episode steps: 385, steps per second:  55, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.197 [0.000, 5.000],  loss: 0.007227, mae: 0.310063, mean_q: 0.389108, mean_eps: 0.798385
  135088/1200000: episode: 195, duration: 8.410s, episode steps: 485, steps per second:  58, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.006489, mae: 0.309401, mean_q: 0.386877, mean_eps: 0.797734
  136013/1200000: episode: 196, duration: 15.531s, episode steps: 925, steps per second:  60, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.008203, mae: 0.316226, mean_q: 0.394189, mean_eps: 0.796675
  136858/1200000: episode: 197, duration: 15.201s, episode steps: 845, steps per second:  56, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.006326, mae: 0.305783, mean_q: 0.383173, mean_eps: 0.795346
  137627/1200000: episode: 198, duration: 13.687s, episode steps: 769, steps per second:  56, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.006625, mae: 0.313599, mean_q: 0.391048, mean_eps: 0.794137
  138254/1200000: episode: 199, duration: 11.378s, episode steps: 627, steps per second:  55, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.007003, mae: 0.310512, mean_q: 0.386141, mean_eps: 0.793090
  139184/1200000: episode: 200, duration: 18.804s, episode steps: 930, steps per second:  49, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.006351, mae: 0.305551, mean_q: 0.381140, mean_eps: 0.791923
  139818/1200000: episode: 201, duration: 11.886s, episode steps: 634, steps per second:  53, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.007087, mae: 0.307733, mean_q: 0.383156, mean_eps: 0.790750
  140443/1200000: episode: 202, duration: 11.374s, episode steps: 625, steps per second:  55, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.005791, mae: 0.329027, mean_q: 0.411269, mean_eps: 0.789805
  141080/1200000: episode: 203, duration: 12.267s, episode steps: 637, steps per second:  52, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.006929, mae: 0.332925, mean_q: 0.417567, mean_eps: 0.788860
  141778/1200000: episode: 204, duration: 12.121s, episode steps: 698, steps per second:  58, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.007378, mae: 0.333615, mean_q: 0.416614, mean_eps: 0.787858
  142593/1200000: episode: 205, duration: 14.460s, episode steps: 815, steps per second:  56, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.008359, mae: 0.342205, mean_q: 0.427400, mean_eps: 0.786721
  142976/1200000: episode: 206, duration: 6.900s, episode steps: 383, steps per second:  56, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.710 [0.000, 5.000],  loss: 0.007328, mae: 0.341800, mean_q: 0.427610, mean_eps: 0.785824
  143583/1200000: episode: 207, duration: 10.491s, episode steps: 607, steps per second:  58, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.687 [0.000, 5.000],  loss: 0.007905, mae: 0.337113, mean_q: 0.421303, mean_eps: 0.785083
  143948/1200000: episode: 208, duration: 6.533s, episode steps: 365, steps per second:  56, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.006882, mae: 0.336423, mean_q: 0.420013, mean_eps: 0.784354
  144491/1200000: episode: 209, duration: 10.206s, episode steps: 543, steps per second:  53, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.006889, mae: 0.340536, mean_q: 0.426124, mean_eps: 0.783673
  145188/1200000: episode: 210, duration: 13.322s, episode steps: 697, steps per second:  52, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.278 [0.000, 5.000],  loss: 0.007075, mae: 0.335344, mean_q: 0.417898, mean_eps: 0.782743
  145562/1200000: episode: 211, duration: 6.950s, episode steps: 374, steps per second:  54, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.007513, mae: 0.330541, mean_q: 0.411560, mean_eps: 0.781939
  146165/1200000: episode: 212, duration: 11.352s, episode steps: 603, steps per second:  53, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.007956, mae: 0.340657, mean_q: 0.425319, mean_eps: 0.781204
  147737/1200000: episode: 213, duration: 31.366s, episode steps: 1572, steps per second:  50, episode reward: 29.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.007303, mae: 0.338905, mean_q: 0.422295, mean_eps: 0.779572
  148391/1200000: episode: 214, duration: 12.164s, episode steps: 654, steps per second:  54, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.008082, mae: 0.341734, mean_q: 0.425111, mean_eps: 0.777904
  148910/1200000: episode: 215, duration: 9.748s, episode steps: 519, steps per second:  53, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.008176, mae: 0.336540, mean_q: 0.417654, mean_eps: 0.777025
  149446/1200000: episode: 216, duration: 10.059s, episode steps: 536, steps per second:  53, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.319 [0.000, 5.000],  loss: 0.007150, mae: 0.338166, mean_q: 0.422191, mean_eps: 0.776233
  150480/1200000: episode: 217, duration: 17.596s, episode steps: 1034, steps per second:  59, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.008237, mae: 0.355228, mean_q: 0.442466, mean_eps: 0.775057
  150909/1200000: episode: 218, duration: 8.289s, episode steps: 429, steps per second:  52, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.061 [0.000, 5.000],  loss: 0.008936, mae: 0.372235, mean_q: 0.463241, mean_eps: 0.773959
  151889/1200000: episode: 219, duration: 19.365s, episode steps: 980, steps per second:  51, episode reward: 11.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.008764, mae: 0.374972, mean_q: 0.467332, mean_eps: 0.772900
  152569/1200000: episode: 220, duration: 13.908s, episode steps: 680, steps per second:  49, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.007756, mae: 0.372165, mean_q: 0.465069, mean_eps: 0.771655
  153168/1200000: episode: 221, duration: 11.898s, episode steps: 599, steps per second:  50, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.007568, mae: 0.369370, mean_q: 0.462155, mean_eps: 0.770698
  153807/1200000: episode: 222, duration: 12.210s, episode steps: 639, steps per second:  52, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.008642, mae: 0.373705, mean_q: 0.466296, mean_eps: 0.769771
  154269/1200000: episode: 223, duration: 9.134s, episode steps: 462, steps per second:  51, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.007366, mae: 0.368172, mean_q: 0.458596, mean_eps: 0.768943
  155164/1200000: episode: 224, duration: 16.462s, episode steps: 895, steps per second:  54, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.008259, mae: 0.368787, mean_q: 0.458736, mean_eps: 0.767926
  155981/1200000: episode: 225, duration: 15.498s, episode steps: 817, steps per second:  53, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.007336, mae: 0.371433, mean_q: 0.461863, mean_eps: 0.766642
  156363/1200000: episode: 226, duration: 6.765s, episode steps: 382, steps per second:  56, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.007756, mae: 0.372378, mean_q: 0.460830, mean_eps: 0.765742
  157146/1200000: episode: 227, duration: 15.584s, episode steps: 783, steps per second:  50, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 0.007524, mae: 0.372375, mean_q: 0.462096, mean_eps: 0.764869
  157658/1200000: episode: 228, duration: 10.090s, episode steps: 512, steps per second:  51, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.008408, mae: 0.371614, mean_q: 0.462650, mean_eps: 0.763897
  158281/1200000: episode: 229, duration: 12.999s, episode steps: 623, steps per second:  48, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.007751, mae: 0.381675, mean_q: 0.475835, mean_eps: 0.763045
  158718/1200000: episode: 230, duration: 8.694s, episode steps: 437, steps per second:  50, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.007957, mae: 0.381794, mean_q: 0.475467, mean_eps: 0.762250
  159301/1200000: episode: 231, duration: 10.951s, episode steps: 583, steps per second:  53, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.007773, mae: 0.372825, mean_q: 0.465344, mean_eps: 0.761485
  159847/1200000: episode: 232, duration: 9.792s, episode steps: 546, steps per second:  56, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.006873, mae: 0.377003, mean_q: 0.470106, mean_eps: 0.760639
  160313/1200000: episode: 233, duration: 8.860s, episode steps: 466, steps per second:  53, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.007280, mae: 0.384315, mean_q: 0.479922, mean_eps: 0.759880
  160955/1200000: episode: 234, duration: 12.016s, episode steps: 642, steps per second:  53, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.007358, mae: 0.387275, mean_q: 0.480851, mean_eps: 0.759049
  161505/1200000: episode: 235, duration: 11.058s, episode steps: 550, steps per second:  50, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.007745, mae: 0.385559, mean_q: 0.477153, mean_eps: 0.758155
  162190/1200000: episode: 236, duration: 12.521s, episode steps: 685, steps per second:  55, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.008869, mae: 0.384262, mean_q: 0.475922, mean_eps: 0.757228
  162571/1200000: episode: 237, duration: 6.313s, episode steps: 381, steps per second:  60, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.008226, mae: 0.390685, mean_q: 0.485042, mean_eps: 0.756430
  163178/1200000: episode: 238, duration: 10.383s, episode steps: 607, steps per second:  58, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.007450, mae: 0.379319, mean_q: 0.472326, mean_eps: 0.755689
  163969/1200000: episode: 239, duration: 15.320s, episode steps: 791, steps per second:  52, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.009076, mae: 0.382080, mean_q: 0.476759, mean_eps: 0.754639
  164471/1200000: episode: 240, duration: 10.204s, episode steps: 502, steps per second:  49, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.007523, mae: 0.380291, mean_q: 0.473515, mean_eps: 0.753670
  164983/1200000: episode: 241, duration: 10.350s, episode steps: 512, steps per second:  49, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.006744, mae: 0.377587, mean_q: 0.471295, mean_eps: 0.752911
  165718/1200000: episode: 242, duration: 13.702s, episode steps: 735, steps per second:  54, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.007871, mae: 0.390023, mean_q: 0.486141, mean_eps: 0.751975
  166162/1200000: episode: 243, duration: 8.227s, episode steps: 444, steps per second:  54, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.007353, mae: 0.384792, mean_q: 0.480023, mean_eps: 0.751090
  166579/1200000: episode: 244, duration: 7.326s, episode steps: 417, steps per second:  57, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.007552, mae: 0.381308, mean_q: 0.474550, mean_eps: 0.750445
  167568/1200000: episode: 245, duration: 18.006s, episode steps: 989, steps per second:  55, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.008254, mae: 0.384172, mean_q: 0.476489, mean_eps: 0.749392
  168185/1200000: episode: 246, duration: 11.227s, episode steps: 617, steps per second:  55, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.008039, mae: 0.378115, mean_q: 0.470447, mean_eps: 0.748186
  169129/1200000: episode: 247, duration: 16.161s, episode steps: 944, steps per second:  58, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.008072, mae: 0.380746, mean_q: 0.473156, mean_eps: 0.747013
  169780/1200000: episode: 248, duration: 11.296s, episode steps: 651, steps per second:  58, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.007404, mae: 0.380716, mean_q: 0.473113, mean_eps: 0.745819
  170179/1200000: episode: 249, duration: 8.047s, episode steps: 399, steps per second:  50, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.008157, mae: 0.405923, mean_q: 0.503849, mean_eps: 0.745033
  170697/1200000: episode: 250, duration: 10.223s, episode steps: 518, steps per second:  51, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.811 [0.000, 5.000],  loss: 0.009617, mae: 0.428410, mean_q: 0.532766, mean_eps: 0.744343
  171093/1200000: episode: 251, duration: 7.331s, episode steps: 396, steps per second:  54, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.009220, mae: 0.418070, mean_q: 0.517932, mean_eps: 0.743656
  171605/1200000: episode: 252, duration: 9.580s, episode steps: 512, steps per second:  53, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.008775, mae: 0.423619, mean_q: 0.525364, mean_eps: 0.742975
  172167/1200000: episode: 253, duration: 10.384s, episode steps: 562, steps per second:  54, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.008830, mae: 0.425741, mean_q: 0.527152, mean_eps: 0.742171
  172646/1200000: episode: 254, duration: 8.563s, episode steps: 479, steps per second:  56, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.007474, mae: 0.407833, mean_q: 0.502629, mean_eps: 0.741391
  173392/1200000: episode: 255, duration: 13.283s, episode steps: 746, steps per second:  56, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.008653, mae: 0.423645, mean_q: 0.523236, mean_eps: 0.740473
  174436/1200000: episode: 256, duration: 18.430s, episode steps: 1044, steps per second:  57, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.007888, mae: 0.421947, mean_q: 0.521739, mean_eps: 0.739132
  175056/1200000: episode: 257, duration: 12.221s, episode steps: 620, steps per second:  51, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.008803, mae: 0.431926, mean_q: 0.534989, mean_eps: 0.737884
  175740/1200000: episode: 258, duration: 14.014s, episode steps: 684, steps per second:  49, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.784 [0.000, 5.000],  loss: 0.008941, mae: 0.414509, mean_q: 0.513887, mean_eps: 0.736906
  176958/1200000: episode: 259, duration: 24.235s, episode steps: 1218, steps per second:  50, episode reward: 18.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.009077, mae: 0.429768, mean_q: 0.533194, mean_eps: 0.735478
  177447/1200000: episode: 260, duration: 9.547s, episode steps: 489, steps per second:  51, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.007670, mae: 0.420385, mean_q: 0.520332, mean_eps: 0.734197
  178095/1200000: episode: 261, duration: 12.169s, episode steps: 648, steps per second:  53, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.008547, mae: 0.425752, mean_q: 0.527969, mean_eps: 0.733345
  178937/1200000: episode: 262, duration: 16.056s, episode steps: 842, steps per second:  52, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.007825, mae: 0.426172, mean_q: 0.528351, mean_eps: 0.732226
  179306/1200000: episode: 263, duration: 7.368s, episode steps: 369, steps per second:  50, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.007329, mae: 0.418198, mean_q: 0.517881, mean_eps: 0.731317
  179696/1200000: episode: 264, duration: 7.643s, episode steps: 390, steps per second:  51, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.008340, mae: 0.421160, mean_q: 0.522423, mean_eps: 0.730750
  180080/1200000: episode: 265, duration: 7.185s, episode steps: 384, steps per second:  53, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.009153, mae: 0.438243, mean_q: 0.542597, mean_eps: 0.730171
  180459/1200000: episode: 266, duration: 7.497s, episode steps: 379, steps per second:  51, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.760 [0.000, 5.000],  loss: 0.009164, mae: 0.470200, mean_q: 0.581201, mean_eps: 0.729598
  181308/1200000: episode: 267, duration: 15.870s, episode steps: 849, steps per second:  53, episode reward:  8.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.008865, mae: 0.475184, mean_q: 0.586496, mean_eps: 0.728677
  181831/1200000: episode: 268, duration: 9.541s, episode steps: 523, steps per second:  55, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.086 [0.000, 5.000],  loss: 0.009593, mae: 0.470376, mean_q: 0.581420, mean_eps: 0.727648
  182310/1200000: episode: 269, duration: 8.531s, episode steps: 479, steps per second:  56, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.008787, mae: 0.466869, mean_q: 0.579735, mean_eps: 0.726895
  182741/1200000: episode: 270, duration: 8.312s, episode steps: 431, steps per second:  52, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.009014, mae: 0.471488, mean_q: 0.580788, mean_eps: 0.726211
  183645/1200000: episode: 271, duration: 17.966s, episode steps: 904, steps per second:  50, episode reward: 10.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.008752, mae: 0.477342, mean_q: 0.587962, mean_eps: 0.725209
  184095/1200000: episode: 272, duration: 8.501s, episode steps: 450, steps per second:  53, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.647 [0.000, 5.000],  loss: 0.007357, mae: 0.476481, mean_q: 0.587114, mean_eps: 0.724195
  184728/1200000: episode: 273, duration: 12.201s, episode steps: 633, steps per second:  52, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.009272, mae: 0.475242, mean_q: 0.585560, mean_eps: 0.723385
  185126/1200000: episode: 274, duration: 7.061s, episode steps: 398, steps per second:  56, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.008147, mae: 0.472060, mean_q: 0.582898, mean_eps: 0.722611
  185792/1200000: episode: 275, duration: 11.950s, episode steps: 666, steps per second:  56, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.007733, mae: 0.466965, mean_q: 0.577173, mean_eps: 0.721813
  186177/1200000: episode: 276, duration: 7.948s, episode steps: 385, steps per second:  48, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.008104, mae: 0.467847, mean_q: 0.575822, mean_eps: 0.721024
  186670/1200000: episode: 277, duration: 9.503s, episode steps: 493, steps per second:  52, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.009300, mae: 0.479753, mean_q: 0.588678, mean_eps: 0.720364
  187310/1200000: episode: 278, duration: 11.981s, episode steps: 640, steps per second:  53, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.008500, mae: 0.466703, mean_q: 0.575124, mean_eps: 0.719515
  188002/1200000: episode: 279, duration: 11.961s, episode steps: 692, steps per second:  58, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.009016, mae: 0.478554, mean_q: 0.590937, mean_eps: 0.718516
  188740/1200000: episode: 280, duration: 12.537s, episode steps: 738, steps per second:  59, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.710 [0.000, 5.000],  loss: 0.008327, mae: 0.475019, mean_q: 0.586926, mean_eps: 0.717445
  189120/1200000: episode: 281, duration: 7.291s, episode steps: 380, steps per second:  52, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.007331, mae: 0.476324, mean_q: 0.587243, mean_eps: 0.716608
  189565/1200000: episode: 282, duration: 9.233s, episode steps: 445, steps per second:  48, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.008142, mae: 0.474273, mean_q: 0.584821, mean_eps: 0.715987
  189990/1200000: episode: 283, duration: 8.474s, episode steps: 425, steps per second:  50, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.271 [0.000, 5.000],  loss: 0.008053, mae: 0.461723, mean_q: 0.568306, mean_eps: 0.715333
  190444/1200000: episode: 284, duration: 8.773s, episode steps: 454, steps per second:  52, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.007405, mae: 0.497908, mean_q: 0.613945, mean_eps: 0.714676
  191855/1200000: episode: 285, duration: 24.225s, episode steps: 1411, steps per second:  58, episode reward: 24.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.009282, mae: 0.511559, mean_q: 0.631287, mean_eps: 0.713278
  192351/1200000: episode: 286, duration: 8.276s, episode steps: 496, steps per second:  60, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.010239, mae: 0.508754, mean_q: 0.625335, mean_eps: 0.711847
  192751/1200000: episode: 287, duration: 7.121s, episode steps: 400, steps per second:  56, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.010510, mae: 0.510478, mean_q: 0.628609, mean_eps: 0.711175
  193846/1200000: episode: 288, duration: 19.005s, episode steps: 1095, steps per second:  58, episode reward: 12.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.009338, mae: 0.512473, mean_q: 0.632225, mean_eps: 0.710053
  194643/1200000: episode: 289, duration: 14.167s, episode steps: 797, steps per second:  56, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.009170, mae: 0.520147, mean_q: 0.641755, mean_eps: 0.708634
  195535/1200000: episode: 290, duration: 15.965s, episode steps: 892, steps per second:  56, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.007642, mae: 0.505758, mean_q: 0.624363, mean_eps: 0.707368
  196145/1200000: episode: 291, duration: 12.128s, episode steps: 610, steps per second:  50, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.008369, mae: 0.516357, mean_q: 0.636338, mean_eps: 0.706240
  197154/1200000: episode: 292, duration: 20.266s, episode steps: 1009, steps per second:  50, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.010583, mae: 0.505692, mean_q: 0.622498, mean_eps: 0.705025
  197658/1200000: episode: 293, duration: 9.411s, episode steps: 504, steps per second:  54, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.871 [0.000, 5.000],  loss: 0.009459, mae: 0.509748, mean_q: 0.628466, mean_eps: 0.703891
  198595/1200000: episode: 294, duration: 17.086s, episode steps: 937, steps per second:  55, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.008983, mae: 0.515511, mean_q: 0.634163, mean_eps: 0.702811
  199567/1200000: episode: 295, duration: 18.847s, episode steps: 972, steps per second:  52, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.008501, mae: 0.507804, mean_q: 0.624153, mean_eps: 0.701380
  199911/1200000: episode: 296, duration: 6.431s, episode steps: 344, steps per second:  53, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.642 [0.000, 5.000],  loss: 0.008528, mae: 0.512445, mean_q: 0.630053, mean_eps: 0.700393
  200284/1200000: episode: 297, duration: 7.811s, episode steps: 373, steps per second:  48, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.009539, mae: 0.538816, mean_q: 0.662798, mean_eps: 0.699856
  200922/1200000: episode: 298, duration: 12.233s, episode steps: 638, steps per second:  52, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.008799, mae: 0.547323, mean_q: 0.672716, mean_eps: 0.699097
  201785/1200000: episode: 299, duration: 15.503s, episode steps: 863, steps per second:  56, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.642 [0.000, 5.000],  loss: 0.010184, mae: 0.556637, mean_q: 0.683843, mean_eps: 0.697969
  202327/1200000: episode: 300, duration: 12.050s, episode steps: 542, steps per second:  45, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.738 [0.000, 5.000],  loss: 0.008890, mae: 0.556708, mean_q: 0.685899, mean_eps: 0.696916
  202805/1200000: episode: 301, duration: 11.516s, episode steps: 478, steps per second:  42, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.010547, mae: 0.554500, mean_q: 0.685164, mean_eps: 0.696151
  203942/1200000: episode: 302, duration: 22.048s, episode steps: 1137, steps per second:  52, episode reward: 14.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.009069, mae: 0.548377, mean_q: 0.675735, mean_eps: 0.694939
  204573/1200000: episode: 303, duration: 12.468s, episode steps: 631, steps per second:  51, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.009668, mae: 0.549101, mean_q: 0.674821, mean_eps: 0.693613
  205076/1200000: episode: 304, duration: 10.937s, episode steps: 503, steps per second:  46, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.008915, mae: 0.556720, mean_q: 0.686600, mean_eps: 0.692764
  205478/1200000: episode: 305, duration: 8.629s, episode steps: 402, steps per second:  47, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.009741, mae: 0.555497, mean_q: 0.687003, mean_eps: 0.692086
  206330/1200000: episode: 306, duration: 16.892s, episode steps: 852, steps per second:  50, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.008792, mae: 0.551250, mean_q: 0.680479, mean_eps: 0.691144
  206725/1200000: episode: 307, duration: 7.297s, episode steps: 395, steps per second:  54, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.009767, mae: 0.543338, mean_q: 0.670352, mean_eps: 0.690208
  207324/1200000: episode: 308, duration: 10.737s, episode steps: 599, steps per second:  56, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.009544, mae: 0.552709, mean_q: 0.681335, mean_eps: 0.689464
  208479/1200000: episode: 309, duration: 22.014s, episode steps: 1155, steps per second:  52, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.009381, mae: 0.550266, mean_q: 0.678116, mean_eps: 0.688150
  209002/1200000: episode: 310, duration: 10.503s, episode steps: 523, steps per second:  50, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.009953, mae: 0.550505, mean_q: 0.677202, mean_eps: 0.686890
  209505/1200000: episode: 311, duration: 9.488s, episode steps: 503, steps per second:  53, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.009543, mae: 0.542725, mean_q: 0.666323, mean_eps: 0.686119
  210035/1200000: episode: 312, duration: 9.429s, episode steps: 530, steps per second:  56, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.008488, mae: 0.556763, mean_q: 0.683342, mean_eps: 0.685345
  210819/1200000: episode: 313, duration: 13.876s, episode steps: 784, steps per second:  56, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.008620, mae: 0.562589, mean_q: 0.690853, mean_eps: 0.684361
  211537/1200000: episode: 314, duration: 13.531s, episode steps: 718, steps per second:  53, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: 0.008781, mae: 0.556656, mean_q: 0.683231, mean_eps: 0.683233
  211938/1200000: episode: 315, duration: 7.586s, episode steps: 401, steps per second:  53, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.010577, mae: 0.567346, mean_q: 0.696073, mean_eps: 0.682393
  212320/1200000: episode: 316, duration: 7.423s, episode steps: 382, steps per second:  51, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.796 [0.000, 5.000],  loss: 0.009575, mae: 0.552312, mean_q: 0.677548, mean_eps: 0.681808
  212932/1200000: episode: 317, duration: 10.578s, episode steps: 612, steps per second:  58, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.708 [0.000, 5.000],  loss: 0.008891, mae: 0.556351, mean_q: 0.685228, mean_eps: 0.681064
  213469/1200000: episode: 318, duration: 9.574s, episode steps: 537, steps per second:  56, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.250 [0.000, 5.000],  loss: 0.009574, mae: 0.558192, mean_q: 0.687273, mean_eps: 0.680200
  214061/1200000: episode: 319, duration: 11.026s, episode steps: 592, steps per second:  54, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.194 [0.000, 5.000],  loss: 0.009017, mae: 0.546497, mean_q: 0.674679, mean_eps: 0.679351
  214745/1200000: episode: 320, duration: 13.186s, episode steps: 684, steps per second:  52, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.278 [0.000, 5.000],  loss: 0.009762, mae: 0.558780, mean_q: 0.688593, mean_eps: 0.678394
  215104/1200000: episode: 321, duration: 7.589s, episode steps: 359, steps per second:  47, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.256 [0.000, 5.000],  loss: 0.009600, mae: 0.561506, mean_q: 0.692623, mean_eps: 0.677614
  215730/1200000: episode: 322, duration: 13.556s, episode steps: 626, steps per second:  46, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.233 [0.000, 5.000],  loss: 0.009598, mae: 0.550619, mean_q: 0.677602, mean_eps: 0.676876
  216841/1200000: episode: 323, duration: 20.417s, episode steps: 1111, steps per second:  54, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.009944, mae: 0.559167, mean_q: 0.688950, mean_eps: 0.675571
  217834/1200000: episode: 324, duration: 19.053s, episode steps: 993, steps per second:  52, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.009786, mae: 0.564329, mean_q: 0.695732, mean_eps: 0.673993
  218576/1200000: episode: 325, duration: 14.680s, episode steps: 742, steps per second:  51, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.334 [0.000, 5.000],  loss: 0.009415, mae: 0.543171, mean_q: 0.668146, mean_eps: 0.672694
  219111/1200000: episode: 326, duration: 9.864s, episode steps: 535, steps per second:  54, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.008909, mae: 0.546773, mean_q: 0.673228, mean_eps: 0.671737
  219764/1200000: episode: 327, duration: 12.143s, episode steps: 653, steps per second:  54, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.010299, mae: 0.571395, mean_q: 0.704008, mean_eps: 0.670846
  220409/1200000: episode: 328, duration: 11.712s, episode steps: 645, steps per second:  55, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.172 [0.000, 5.000],  loss: 0.008941, mae: 0.564409, mean_q: 0.695182, mean_eps: 0.669871
  221074/1200000: episode: 329, duration: 13.506s, episode steps: 665, steps per second:  49, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.010821, mae: 0.579981, mean_q: 0.711968, mean_eps: 0.668887
  221688/1200000: episode: 330, duration: 12.701s, episode steps: 614, steps per second:  48, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.008176, mae: 0.580268, mean_q: 0.714050, mean_eps: 0.667930
  222416/1200000: episode: 331, duration: 14.781s, episode steps: 728, steps per second:  49, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.243 [0.000, 5.000],  loss: 0.011104, mae: 0.582777, mean_q: 0.715208, mean_eps: 0.666925
  222956/1200000: episode: 332, duration: 10.787s, episode steps: 540, steps per second:  50, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.156 [0.000, 5.000],  loss: 0.010323, mae: 0.584952, mean_q: 0.719052, mean_eps: 0.665974
  223938/1200000: episode: 333, duration: 19.314s, episode steps: 982, steps per second:  51, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.239 [0.000, 5.000],  loss: 0.009055, mae: 0.581061, mean_q: 0.714320, mean_eps: 0.664831
  224592/1200000: episode: 334, duration: 13.000s, episode steps: 654, steps per second:  50, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.009078, mae: 0.586366, mean_q: 0.720841, mean_eps: 0.663604
  225514/1200000: episode: 335, duration: 16.501s, episode steps: 922, steps per second:  56, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.318 [0.000, 5.000],  loss: 0.010045, mae: 0.575622, mean_q: 0.707475, mean_eps: 0.662422
  226227/1200000: episode: 336, duration: 12.814s, episode steps: 713, steps per second:  56, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.009772, mae: 0.587651, mean_q: 0.722816, mean_eps: 0.661195
  226684/1200000: episode: 337, duration: 8.118s, episode steps: 457, steps per second:  56, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.010817, mae: 0.583415, mean_q: 0.717525, mean_eps: 0.660319
  227636/1200000: episode: 338, duration: 19.373s, episode steps: 952, steps per second:  49, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.009270, mae: 0.578612, mean_q: 0.711112, mean_eps: 0.659263
  228144/1200000: episode: 339, duration: 10.094s, episode steps: 508, steps per second:  50, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.150 [0.000, 5.000],  loss: 0.008549, mae: 0.579798, mean_q: 0.713452, mean_eps: 0.658168
  228668/1200000: episode: 340, duration: 9.466s, episode steps: 524, steps per second:  55, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.008907, mae: 0.589704, mean_q: 0.725653, mean_eps: 0.657394
  229308/1200000: episode: 341, duration: 11.661s, episode steps: 640, steps per second:  55, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.010220, mae: 0.579382, mean_q: 0.709496, mean_eps: 0.656521
  230027/1200000: episode: 342, duration: 13.522s, episode steps: 719, steps per second:  53, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.178 [0.000, 5.000],  loss: 0.009666, mae: 0.590801, mean_q: 0.724748, mean_eps: 0.655501
  230535/1200000: episode: 343, duration: 10.619s, episode steps: 508, steps per second:  48, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.281 [0.000, 5.000],  loss: 0.009609, mae: 0.633556, mean_q: 0.776524, mean_eps: 0.654580
  231033/1200000: episode: 344, duration: 10.162s, episode steps: 498, steps per second:  49, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.009884, mae: 0.620372, mean_q: 0.760958, mean_eps: 0.653824
  231729/1200000: episode: 345, duration: 13.823s, episode steps: 696, steps per second:  50, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.009560, mae: 0.637461, mean_q: 0.780622, mean_eps: 0.652927
  232241/1200000: episode: 346, duration: 10.094s, episode steps: 512, steps per second:  51, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.180 [0.000, 5.000],  loss: 0.010611, mae: 0.640200, mean_q: 0.785149, mean_eps: 0.652021
  233179/1200000: episode: 347, duration: 20.213s, episode steps: 938, steps per second:  46, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.010078, mae: 0.639566, mean_q: 0.784782, mean_eps: 0.650935
  234353/1200000: episode: 348, duration: 26.416s, episode steps: 1174, steps per second:  44, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.010897, mae: 0.644892, mean_q: 0.789843, mean_eps: 0.649351
  235246/1200000: episode: 349, duration: 19.840s, episode steps: 893, steps per second:  45, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.011808, mae: 0.640965, mean_q: 0.785116, mean_eps: 0.647800
  235778/1200000: episode: 350, duration: 11.379s, episode steps: 532, steps per second:  47, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.012487, mae: 0.638133, mean_q: 0.782464, mean_eps: 0.646732
  236314/1200000: episode: 351, duration: 11.397s, episode steps: 536, steps per second:  47, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.010811, mae: 0.635568, mean_q: 0.778084, mean_eps: 0.645931
  237086/1200000: episode: 352, duration: 14.982s, episode steps: 772, steps per second:  52, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.010569, mae: 0.638255, mean_q: 0.781041, mean_eps: 0.644950
  238223/1200000: episode: 353, duration: 21.887s, episode steps: 1137, steps per second:  52, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.009651, mae: 0.630916, mean_q: 0.774406, mean_eps: 0.643519
  239385/1200000: episode: 354, duration: 24.661s, episode steps: 1162, steps per second:  47, episode reward: 11.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.010665, mae: 0.640591, mean_q: 0.786849, mean_eps: 0.641794
  239745/1200000: episode: 355, duration: 7.588s, episode steps: 360, steps per second:  47, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.014 [0.000, 5.000],  loss: 0.009282, mae: 0.612348, mean_q: 0.754109, mean_eps: 0.640651
  240961/1200000: episode: 356, duration: 26.615s, episode steps: 1216, steps per second:  46, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.307 [0.000, 5.000],  loss: 0.010320, mae: 0.652168, mean_q: 0.800725, mean_eps: 0.639469
  241768/1200000: episode: 357, duration: 18.550s, episode steps: 807, steps per second:  44, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.186 [0.000, 5.000],  loss: 0.010172, mae: 0.655981, mean_q: 0.802914, mean_eps: 0.637954
  242490/1200000: episode: 358, duration: 17.897s, episode steps: 722, steps per second:  40, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.010107, mae: 0.660768, mean_q: 0.808400, mean_eps: 0.636808
  243034/1200000: episode: 359, duration: 11.911s, episode steps: 544, steps per second:  46, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.009111, mae: 0.664915, mean_q: 0.815326, mean_eps: 0.635857
  243532/1200000: episode: 360, duration: 9.551s, episode steps: 498, steps per second:  52, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.009778, mae: 0.662112, mean_q: 0.810639, mean_eps: 0.635077
  244060/1200000: episode: 361, duration: 10.424s, episode steps: 528, steps per second:  51, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.010706, mae: 0.675504, mean_q: 0.829340, mean_eps: 0.634309
  244601/1200000: episode: 362, duration: 11.718s, episode steps: 541, steps per second:  46, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.012648, mae: 0.677930, mean_q: 0.830905, mean_eps: 0.633505
  245575/1200000: episode: 363, duration: 18.637s, episode steps: 974, steps per second:  52, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.679 [0.000, 5.000],  loss: 0.010139, mae: 0.665635, mean_q: 0.815198, mean_eps: 0.632368
  246103/1200000: episode: 364, duration: 11.179s, episode steps: 528, steps per second:  47, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.011104, mae: 0.665732, mean_q: 0.815877, mean_eps: 0.631243
  246489/1200000: episode: 365, duration: 8.079s, episode steps: 386, steps per second:  48, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.153 [0.000, 5.000],  loss: 0.009942, mae: 0.664414, mean_q: 0.814949, mean_eps: 0.630556
  247238/1200000: episode: 366, duration: 15.647s, episode steps: 749, steps per second:  48, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.010931, mae: 0.671566, mean_q: 0.823892, mean_eps: 0.629704
  248218/1200000: episode: 367, duration: 20.399s, episode steps: 980, steps per second:  48, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.009979, mae: 0.657958, mean_q: 0.805979, mean_eps: 0.628408
  248861/1200000: episode: 368, duration: 14.152s, episode steps: 643, steps per second:  45, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.010315, mae: 0.663342, mean_q: 0.812666, mean_eps: 0.627190
  249272/1200000: episode: 369, duration: 8.975s, episode steps: 411, steps per second:  46, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.672 [0.000, 5.000],  loss: 0.010852, mae: 0.664615, mean_q: 0.813730, mean_eps: 0.626401
  249856/1200000: episode: 370, duration: 12.803s, episode steps: 584, steps per second:  46, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.687 [0.000, 5.000],  loss: 0.010815, mae: 0.666136, mean_q: 0.816488, mean_eps: 0.625657
  250445/1200000: episode: 371, duration: 12.453s, episode steps: 589, steps per second:  47, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.010496, mae: 0.702398, mean_q: 0.863351, mean_eps: 0.624775
  251355/1200000: episode: 372, duration: 18.205s, episode steps: 910, steps per second:  50, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.009577, mae: 0.696204, mean_q: 0.853924, mean_eps: 0.623650
  251763/1200000: episode: 373, duration: 8.864s, episode steps: 408, steps per second:  46, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.284 [0.000, 5.000],  loss: 0.011127, mae: 0.697598, mean_q: 0.856520, mean_eps: 0.622663
  252603/1200000: episode: 374, duration: 15.526s, episode steps: 840, steps per second:  54, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 0.011161, mae: 0.701166, mean_q: 0.860215, mean_eps: 0.621727
  253379/1200000: episode: 375, duration: 15.096s, episode steps: 776, steps per second:  51, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.011075, mae: 0.687425, mean_q: 0.842907, mean_eps: 0.620515
  253888/1200000: episode: 376, duration: 9.911s, episode steps: 509, steps per second:  51, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.204 [0.000, 5.000],  loss: 0.008649, mae: 0.681700, mean_q: 0.836556, mean_eps: 0.619552
  254590/1200000: episode: 377, duration: 13.424s, episode steps: 702, steps per second:  52, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.010327, mae: 0.686892, mean_q: 0.842332, mean_eps: 0.618643
  255373/1200000: episode: 378, duration: 14.796s, episode steps: 783, steps per second:  53, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.010682, mae: 0.708113, mean_q: 0.867765, mean_eps: 0.617527
  255897/1200000: episode: 379, duration: 9.487s, episode steps: 524, steps per second:  55, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.009824, mae: 0.691726, mean_q: 0.846856, mean_eps: 0.616546
  256297/1200000: episode: 380, duration: 8.123s, episode steps: 400, steps per second:  49, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.009658, mae: 0.691473, mean_q: 0.848028, mean_eps: 0.615853
  256634/1200000: episode: 381, duration: 6.568s, episode steps: 337, steps per second:  51, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.807 [0.000, 5.000],  loss: 0.009380, mae: 0.676800, mean_q: 0.827532, mean_eps: 0.615301
  257392/1200000: episode: 382, duration: 15.217s, episode steps: 758, steps per second:  50, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.009473, mae: 0.699002, mean_q: 0.855055, mean_eps: 0.614482
  257776/1200000: episode: 383, duration: 7.858s, episode steps: 384, steps per second:  49, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.742 [0.000, 5.000],  loss: 0.010020, mae: 0.696435, mean_q: 0.851712, mean_eps: 0.613627
  258559/1200000: episode: 384, duration: 16.306s, episode steps: 783, steps per second:  48, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.010102, mae: 0.705459, mean_q: 0.862198, mean_eps: 0.612751
  259275/1200000: episode: 385, duration: 14.515s, episode steps: 716, steps per second:  49, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.012798, mae: 0.701533, mean_q: 0.857698, mean_eps: 0.611626
  259801/1200000: episode: 386, duration: 11.131s, episode steps: 526, steps per second:  47, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.696 [0.000, 5.000],  loss: 0.011252, mae: 0.701468, mean_q: 0.859259, mean_eps: 0.610693
  260288/1200000: episode: 387, duration: 10.077s, episode steps: 487, steps per second:  48, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.010784, mae: 0.701218, mean_q: 0.859308, mean_eps: 0.609934
  260837/1200000: episode: 388, duration: 10.135s, episode steps: 549, steps per second:  54, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.774 [0.000, 5.000],  loss: 0.011740, mae: 0.723228, mean_q: 0.886961, mean_eps: 0.609157
  261434/1200000: episode: 389, duration: 10.709s, episode steps: 597, steps per second:  56, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.010492, mae: 0.705118, mean_q: 0.863814, mean_eps: 0.608296
  262079/1200000: episode: 390, duration: 11.792s, episode steps: 645, steps per second:  55, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.012611, mae: 0.714344, mean_q: 0.875496, mean_eps: 0.607366
  262560/1200000: episode: 391, duration: 9.564s, episode steps: 481, steps per second:  50, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.011592, mae: 0.711850, mean_q: 0.873120, mean_eps: 0.606523
  263331/1200000: episode: 392, duration: 14.773s, episode steps: 771, steps per second:  52, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: 0.010940, mae: 0.718309, mean_q: 0.880725, mean_eps: 0.605584
  263819/1200000: episode: 393, duration: 8.895s, episode steps: 488, steps per second:  55, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.011204, mae: 0.721574, mean_q: 0.883195, mean_eps: 0.604639
  264317/1200000: episode: 394, duration: 9.325s, episode steps: 498, steps per second:  53, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.010960, mae: 0.702492, mean_q: 0.860399, mean_eps: 0.603898
  264719/1200000: episode: 395, duration: 7.581s, episode steps: 402, steps per second:  53, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.985 [0.000, 5.000],  loss: 0.011192, mae: 0.716766, mean_q: 0.878689, mean_eps: 0.603223
  265443/1200000: episode: 396, duration: 13.509s, episode steps: 724, steps per second:  54, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 0.010299, mae: 0.715519, mean_q: 0.878922, mean_eps: 0.602380
  265973/1200000: episode: 397, duration: 10.039s, episode steps: 530, steps per second:  53, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.010508, mae: 0.708981, mean_q: 0.870022, mean_eps: 0.601438
  266515/1200000: episode: 398, duration: 9.920s, episode steps: 542, steps per second:  55, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.057 [0.000, 5.000],  loss: 0.014145, mae: 0.706773, mean_q: 0.864149, mean_eps: 0.600634
  267333/1200000: episode: 399, duration: 15.920s, episode steps: 818, steps per second:  51, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.009641, mae: 0.715221, mean_q: 0.876689, mean_eps: 0.599614
  267932/1200000: episode: 400, duration: 10.916s, episode steps: 599, steps per second:  55, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.010829, mae: 0.714097, mean_q: 0.873657, mean_eps: 0.598552
  268460/1200000: episode: 401, duration: 10.961s, episode steps: 528, steps per second:  48, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.009369, mae: 0.703617, mean_q: 0.863682, mean_eps: 0.597709
  269287/1200000: episode: 402, duration: 18.033s, episode steps: 827, steps per second:  46, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.011728, mae: 0.709142, mean_q: 0.868075, mean_eps: 0.596692
  270113/1200000: episode: 403, duration: 17.037s, episode steps: 826, steps per second:  48, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.200 [0.000, 5.000],  loss: 0.011578, mae: 0.719387, mean_q: 0.878934, mean_eps: 0.595450
  270758/1200000: episode: 404, duration: 13.822s, episode steps: 645, steps per second:  47, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.011180, mae: 0.741543, mean_q: 0.908219, mean_eps: 0.594346
  271518/1200000: episode: 405, duration: 15.518s, episode steps: 760, steps per second:  49, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.139 [0.000, 5.000],  loss: 0.011719, mae: 0.746035, mean_q: 0.912758, mean_eps: 0.593293
  272010/1200000: episode: 406, duration: 10.022s, episode steps: 492, steps per second:  49, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.010978, mae: 0.746336, mean_q: 0.914555, mean_eps: 0.592354
  272669/1200000: episode: 407, duration: 13.097s, episode steps: 659, steps per second:  50, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.010311, mae: 0.756832, mean_q: 0.925640, mean_eps: 0.591490
  273506/1200000: episode: 408, duration: 16.526s, episode steps: 837, steps per second:  51, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.010280, mae: 0.743597, mean_q: 0.910809, mean_eps: 0.590368
  274055/1200000: episode: 409, duration: 10.479s, episode steps: 549, steps per second:  52, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.011447, mae: 0.742821, mean_q: 0.909744, mean_eps: 0.589330
  274843/1200000: episode: 410, duration: 15.897s, episode steps: 788, steps per second:  50, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.010268, mae: 0.738022, mean_q: 0.904242, mean_eps: 0.588328
  275468/1200000: episode: 411, duration: 13.013s, episode steps: 625, steps per second:  48, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.010096, mae: 0.744716, mean_q: 0.912754, mean_eps: 0.587269
  276176/1200000: episode: 412, duration: 14.242s, episode steps: 708, steps per second:  50, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.747 [0.000, 5.000],  loss: 0.011805, mae: 0.742187, mean_q: 0.907637, mean_eps: 0.586270
  277279/1200000: episode: 413, duration: 21.954s, episode steps: 1103, steps per second:  50, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.011417, mae: 0.741013, mean_q: 0.908751, mean_eps: 0.584911
  278198/1200000: episode: 414, duration: 18.511s, episode steps: 919, steps per second:  50, episode reward: 10.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.010678, mae: 0.745051, mean_q: 0.914230, mean_eps: 0.583393
  278836/1200000: episode: 415, duration: 11.849s, episode steps: 638, steps per second:  54, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.828 [0.000, 5.000],  loss: 0.009884, mae: 0.732234, mean_q: 0.896790, mean_eps: 0.582226
  279299/1200000: episode: 416, duration: 8.040s, episode steps: 463, steps per second:  58, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.010459, mae: 0.745970, mean_q: 0.913581, mean_eps: 0.581401
  279966/1200000: episode: 417, duration: 11.891s, episode steps: 667, steps per second:  56, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.286 [0.000, 5.000],  loss: 0.012426, mae: 0.739518, mean_q: 0.904962, mean_eps: 0.580552
  280636/1200000: episode: 418, duration: 12.995s, episode steps: 670, steps per second:  52, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.012643, mae: 0.765397, mean_q: 0.936239, mean_eps: 0.579550
  281396/1200000: episode: 419, duration: 15.462s, episode steps: 760, steps per second:  49, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: 0.009699, mae: 0.765417, mean_q: 0.937463, mean_eps: 0.578479
  282088/1200000: episode: 420, duration: 13.358s, episode steps: 692, steps per second:  52, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.754 [0.000, 5.000],  loss: 0.010955, mae: 0.762532, mean_q: 0.933223, mean_eps: 0.577390
  282601/1200000: episode: 421, duration: 9.938s, episode steps: 513, steps per second:  52, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.010405, mae: 0.759126, mean_q: 0.930315, mean_eps: 0.576484
  283577/1200000: episode: 422, duration: 18.336s, episode steps: 976, steps per second:  53, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.010365, mae: 0.770911, mean_q: 0.945654, mean_eps: 0.575365
  284500/1200000: episode: 423, duration: 17.779s, episode steps: 923, steps per second:  52, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.012963, mae: 0.768807, mean_q: 0.940473, mean_eps: 0.573943
  285060/1200000: episode: 424, duration: 12.103s, episode steps: 560, steps per second:  46, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.010977, mae: 0.766309, mean_q: 0.936869, mean_eps: 0.572833
  285578/1200000: episode: 425, duration: 11.570s, episode steps: 518, steps per second:  45, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.012 [0.000, 5.000],  loss: 0.011020, mae: 0.772880, mean_q: 0.945673, mean_eps: 0.572023
  286798/1200000: episode: 426, duration: 29.359s, episode steps: 1220, steps per second:  42, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.011159, mae: 0.765085, mean_q: 0.936006, mean_eps: 0.570718
  287460/1200000: episode: 427, duration: 16.513s, episode steps: 662, steps per second:  40, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.012558, mae: 0.768201, mean_q: 0.938684, mean_eps: 0.569308
  288276/1200000: episode: 428, duration: 17.375s, episode steps: 816, steps per second:  47, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.011787, mae: 0.787911, mean_q: 0.962803, mean_eps: 0.568201
  289359/1200000: episode: 429, duration: 22.030s, episode steps: 1083, steps per second:  49, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.011582, mae: 0.770973, mean_q: 0.944681, mean_eps: 0.566776
  289730/1200000: episode: 430, duration: 7.381s, episode steps: 371, steps per second:  50, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.272 [0.000, 5.000],  loss: 0.011041, mae: 0.776975, mean_q: 0.951553, mean_eps: 0.565684
  290236/1200000: episode: 431, duration: 9.477s, episode steps: 506, steps per second:  53, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.628 [0.000, 5.000],  loss: 0.012495, mae: 0.774698, mean_q: 0.949221, mean_eps: 0.565027
  290615/1200000: episode: 432, duration: 7.685s, episode steps: 379, steps per second:  49, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.010007, mae: 0.779595, mean_q: 0.955689, mean_eps: 0.564364
  291312/1200000: episode: 433, duration: 13.233s, episode steps: 697, steps per second:  53, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.011426, mae: 0.785641, mean_q: 0.961558, mean_eps: 0.563557
  292000/1200000: episode: 434, duration: 13.346s, episode steps: 688, steps per second:  52, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.762 [0.000, 5.000],  loss: 0.012876, mae: 0.787752, mean_q: 0.965100, mean_eps: 0.562519
  292641/1200000: episode: 435, duration: 13.134s, episode steps: 641, steps per second:  49, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.265 [0.000, 5.000],  loss: 0.010789, mae: 0.774438, mean_q: 0.950680, mean_eps: 0.561520
  293275/1200000: episode: 436, duration: 13.045s, episode steps: 634, steps per second:  49, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.012199, mae: 0.794929, mean_q: 0.973338, mean_eps: 0.560563
  293983/1200000: episode: 437, duration: 14.296s, episode steps: 708, steps per second:  50, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.011026, mae: 0.785997, mean_q: 0.961876, mean_eps: 0.559558
  294909/1200000: episode: 438, duration: 18.653s, episode steps: 926, steps per second:  50, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.202 [0.000, 5.000],  loss: 0.011449, mae: 0.784583, mean_q: 0.961239, mean_eps: 0.558331
  295850/1200000: episode: 439, duration: 18.908s, episode steps: 941, steps per second:  50, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.012372, mae: 0.781779, mean_q: 0.957633, mean_eps: 0.556930
  296563/1200000: episode: 440, duration: 12.959s, episode steps: 713, steps per second:  55, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.010996, mae: 0.785402, mean_q: 0.963338, mean_eps: 0.555691
  297322/1200000: episode: 441, duration: 13.188s, episode steps: 759, steps per second:  58, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.287 [0.000, 5.000],  loss: 0.010492, mae: 0.783771, mean_q: 0.961070, mean_eps: 0.554587
  297902/1200000: episode: 442, duration: 10.377s, episode steps: 580, steps per second:  56, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.010515, mae: 0.777538, mean_q: 0.951757, mean_eps: 0.553582
  298689/1200000: episode: 443, duration: 15.944s, episode steps: 787, steps per second:  49, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.012229, mae: 0.778571, mean_q: 0.953571, mean_eps: 0.552556
  299380/1200000: episode: 444, duration: 14.763s, episode steps: 691, steps per second:  47, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.009915, mae: 0.781529, mean_q: 0.957142, mean_eps: 0.551449
  300219/1200000: episode: 445, duration: 16.087s, episode steps: 839, steps per second:  52, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.011749, mae: 0.796740, mean_q: 0.976882, mean_eps: 0.550303
  300779/1200000: episode: 446, duration: 10.927s, episode steps: 560, steps per second:  51, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.010749, mae: 0.813202, mean_q: 0.995068, mean_eps: 0.549253
  301528/1200000: episode: 447, duration: 13.534s, episode steps: 749, steps per second:  55, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.010843, mae: 0.825166, mean_q: 1.009266, mean_eps: 0.548272
  302287/1200000: episode: 448, duration: 14.835s, episode steps: 759, steps per second:  51, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.011129, mae: 0.819134, mean_q: 1.002272, mean_eps: 0.547141
  302924/1200000: episode: 449, duration: 12.240s, episode steps: 637, steps per second:  52, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.010870, mae: 0.828202, mean_q: 1.014731, mean_eps: 0.546094
  303454/1200000: episode: 450, duration: 10.485s, episode steps: 530, steps per second:  51, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.728 [0.000, 5.000],  loss: 0.011201, mae: 0.826885, mean_q: 1.012425, mean_eps: 0.545218
  304174/1200000: episode: 451, duration: 13.940s, episode steps: 720, steps per second:  52, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.011409, mae: 0.828040, mean_q: 1.012524, mean_eps: 0.544279
  304760/1200000: episode: 452, duration: 11.848s, episode steps: 586, steps per second:  49, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.838 [0.000, 5.000],  loss: 0.011649, mae: 0.822731, mean_q: 1.004986, mean_eps: 0.543301
  305577/1200000: episode: 453, duration: 17.239s, episode steps: 817, steps per second:  47, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.760 [0.000, 5.000],  loss: 0.010860, mae: 0.817654, mean_q: 0.999937, mean_eps: 0.542248
  306319/1200000: episode: 454, duration: 13.389s, episode steps: 742, steps per second:  55, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.011178, mae: 0.824759, mean_q: 1.008222, mean_eps: 0.541078
  306845/1200000: episode: 455, duration: 10.189s, episode steps: 526, steps per second:  52, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.011546, mae: 0.813316, mean_q: 0.993703, mean_eps: 0.540127
  307434/1200000: episode: 456, duration: 11.002s, episode steps: 589, steps per second:  54, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.011737, mae: 0.827516, mean_q: 1.011156, mean_eps: 0.539290
  307951/1200000: episode: 457, duration: 10.140s, episode steps: 517, steps per second:  51, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.011122, mae: 0.813753, mean_q: 0.993690, mean_eps: 0.538462
  308854/1200000: episode: 458, duration: 17.122s, episode steps: 903, steps per second:  53, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.011007, mae: 0.822826, mean_q: 1.007023, mean_eps: 0.537397
  309651/1200000: episode: 459, duration: 14.583s, episode steps: 797, steps per second:  55, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.011008, mae: 0.821042, mean_q: 1.006137, mean_eps: 0.536122
  310686/1200000: episode: 460, duration: 20.737s, episode steps: 1035, steps per second:  50, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.010427, mae: 0.847550, mean_q: 1.038899, mean_eps: 0.534748
  311368/1200000: episode: 461, duration: 15.555s, episode steps: 682, steps per second:  44, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.796 [0.000, 5.000],  loss: 0.012075, mae: 0.838746, mean_q: 1.024935, mean_eps: 0.533461
  312085/1200000: episode: 462, duration: 14.580s, episode steps: 717, steps per second:  49, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.802 [0.000, 5.000],  loss: 0.011340, mae: 0.849798, mean_q: 1.038670, mean_eps: 0.532411
  312781/1200000: episode: 463, duration: 14.099s, episode steps: 696, steps per second:  49, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.724 [0.000, 5.000],  loss: 0.011353, mae: 0.844699, mean_q: 1.034652, mean_eps: 0.531349
  313488/1200000: episode: 464, duration: 14.639s, episode steps: 707, steps per second:  48, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.011743, mae: 0.847281, mean_q: 1.035671, mean_eps: 0.530299
  314200/1200000: episode: 465, duration: 14.838s, episode steps: 712, steps per second:  48, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.011897, mae: 0.845787, mean_q: 1.033880, mean_eps: 0.529237
  314740/1200000: episode: 466, duration: 10.497s, episode steps: 540, steps per second:  51, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.010911, mae: 0.834395, mean_q: 1.020201, mean_eps: 0.528298
  315818/1200000: episode: 467, duration: 19.251s, episode steps: 1078, steps per second:  56, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.010534, mae: 0.855283, mean_q: 1.047009, mean_eps: 0.527083
  316604/1200000: episode: 468, duration: 14.833s, episode steps: 786, steps per second:  53, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.332 [0.000, 5.000],  loss: 0.010717, mae: 0.847345, mean_q: 1.037003, mean_eps: 0.525685
  317332/1200000: episode: 469, duration: 13.751s, episode steps: 728, steps per second:  53, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.011299, mae: 0.833012, mean_q: 1.019433, mean_eps: 0.524551
  317750/1200000: episode: 470, duration: 8.771s, episode steps: 418, steps per second:  48, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.013551, mae: 0.844519, mean_q: 1.034124, mean_eps: 0.523690
  318402/1200000: episode: 471, duration: 11.593s, episode steps: 652, steps per second:  56, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.011394, mae: 0.840074, mean_q: 1.028694, mean_eps: 0.522886
  319028/1200000: episode: 472, duration: 11.752s, episode steps: 626, steps per second:  53, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.260 [0.000, 5.000],  loss: 0.011660, mae: 0.843053, mean_q: 1.029893, mean_eps: 0.521929
  319653/1200000: episode: 473, duration: 11.990s, episode steps: 625, steps per second:  52, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.834 [0.000, 5.000],  loss: 0.012234, mae: 0.838394, mean_q: 1.025146, mean_eps: 0.520990
  320402/1200000: episode: 474, duration: 13.570s, episode steps: 749, steps per second:  55, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.010931, mae: 0.851347, mean_q: 1.043636, mean_eps: 0.519958
  321450/1200000: episode: 475, duration: 20.481s, episode steps: 1048, steps per second:  51, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.011004, mae: 0.879161, mean_q: 1.076626, mean_eps: 0.518611
  322395/1200000: episode: 476, duration: 18.177s, episode steps: 945, steps per second:  52, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 0.013028, mae: 0.881853, mean_q: 1.077882, mean_eps: 0.517117
  323034/1200000: episode: 477, duration: 14.762s, episode steps: 639, steps per second:  43, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.011752, mae: 0.871545, mean_q: 1.064750, mean_eps: 0.515929
  324008/1200000: episode: 478, duration: 23.157s, episode steps: 974, steps per second:  42, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.012165, mae: 0.888243, mean_q: 1.085860, mean_eps: 0.514720
  324444/1200000: episode: 479, duration: 8.893s, episode steps: 436, steps per second:  49, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.012040, mae: 0.895187, mean_q: 1.094089, mean_eps: 0.513664
  324998/1200000: episode: 480, duration: 11.014s, episode steps: 554, steps per second:  50, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.011216, mae: 0.874231, mean_q: 1.068515, mean_eps: 0.512920
  326136/1200000: episode: 481, duration: 21.899s, episode steps: 1138, steps per second:  52, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.012745, mae: 0.878274, mean_q: 1.072011, mean_eps: 0.511651
  326874/1200000: episode: 482, duration: 14.233s, episode steps: 738, steps per second:  52, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.012543, mae: 0.875208, mean_q: 1.069486, mean_eps: 0.510244
  327508/1200000: episode: 483, duration: 11.514s, episode steps: 634, steps per second:  55, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.012364, mae: 0.878196, mean_q: 1.072744, mean_eps: 0.509215
  328670/1200000: episode: 484, duration: 22.433s, episode steps: 1162, steps per second:  52, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.010789, mae: 0.877436, mean_q: 1.073167, mean_eps: 0.507868
  329276/1200000: episode: 485, duration: 13.970s, episode steps: 606, steps per second:  43, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 0.011756, mae: 0.867512, mean_q: 1.058962, mean_eps: 0.506542
  330014/1200000: episode: 486, duration: 15.984s, episode steps: 738, steps per second:  46, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.011611, mae: 0.871868, mean_q: 1.064459, mean_eps: 0.505534
  330777/1200000: episode: 487, duration: 15.857s, episode steps: 763, steps per second:  48, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.012019, mae: 0.921372, mean_q: 1.125599, mean_eps: 0.504406
  331572/1200000: episode: 488, duration: 16.405s, episode steps: 795, steps per second:  48, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.012379, mae: 0.904173, mean_q: 1.103284, mean_eps: 0.503239
  332412/1200000: episode: 489, duration: 17.007s, episode steps: 840, steps per second:  49, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.886 [0.000, 5.000],  loss: 0.012770, mae: 0.921568, mean_q: 1.124815, mean_eps: 0.502015
  332885/1200000: episode: 490, duration: 9.249s, episode steps: 473, steps per second:  51, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.013916, mae: 0.914355, mean_q: 1.115982, mean_eps: 0.501028
  333762/1200000: episode: 491, duration: 15.295s, episode steps: 877, steps per second:  57, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.300 [0.000, 5.000],  loss: 0.012511, mae: 0.913914, mean_q: 1.117291, mean_eps: 0.500014
  334697/1200000: episode: 492, duration: 17.299s, episode steps: 935, steps per second:  54, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.013550, mae: 0.915304, mean_q: 1.115950, mean_eps: 0.498655
  335435/1200000: episode: 493, duration: 14.684s, episode steps: 738, steps per second:  50, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.187 [0.000, 5.000],  loss: 0.013011, mae: 0.924437, mean_q: 1.128135, mean_eps: 0.497401
  336098/1200000: episode: 494, duration: 12.713s, episode steps: 663, steps per second:  52, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.010922, mae: 0.922713, mean_q: 1.124996, mean_eps: 0.496351
  336612/1200000: episode: 495, duration: 9.526s, episode steps: 514, steps per second:  54, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.774 [0.000, 5.000],  loss: 0.011865, mae: 0.912810, mean_q: 1.113142, mean_eps: 0.495469
  337666/1200000: episode: 496, duration: 19.391s, episode steps: 1054, steps per second:  54, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.011971, mae: 0.908919, mean_q: 1.106918, mean_eps: 0.494293
  338282/1200000: episode: 497, duration: 11.458s, episode steps: 616, steps per second:  54, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.326 [0.000, 5.000],  loss: 0.014619, mae: 0.915161, mean_q: 1.114481, mean_eps: 0.493039
  338964/1200000: episode: 498, duration: 13.383s, episode steps: 682, steps per second:  51, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.191 [0.000, 5.000],  loss: 0.013975, mae: 0.912092, mean_q: 1.111865, mean_eps: 0.492067
  339509/1200000: episode: 499, duration: 11.112s, episode steps: 545, steps per second:  49, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.130 [0.000, 5.000],  loss: 0.012579, mae: 0.929282, mean_q: 1.134968, mean_eps: 0.491146
  340412/1200000: episode: 500, duration: 18.183s, episode steps: 903, steps per second:  50, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.204 [0.000, 5.000],  loss: 0.012230, mae: 0.931780, mean_q: 1.137937, mean_eps: 0.490060
  341147/1200000: episode: 501, duration: 15.174s, episode steps: 735, steps per second:  48, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.011751, mae: 0.957870, mean_q: 1.169100, mean_eps: 0.488833
  341842/1200000: episode: 502, duration: 14.847s, episode steps: 695, steps per second:  47, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.011307, mae: 0.948052, mean_q: 1.156296, mean_eps: 0.487759
  342532/1200000: episode: 503, duration: 14.179s, episode steps: 690, steps per second:  49, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.011484, mae: 0.952114, mean_q: 1.162815, mean_eps: 0.486721
  343147/1200000: episode: 504, duration: 12.261s, episode steps: 615, steps per second:  50, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.010618, mae: 0.959910, mean_q: 1.171943, mean_eps: 0.485743
  343958/1200000: episode: 505, duration: 16.787s, episode steps: 811, steps per second:  48, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.666 [0.000, 5.000],  loss: 0.011504, mae: 0.935809, mean_q: 1.142109, mean_eps: 0.484672
  344442/1200000: episode: 506, duration: 10.019s, episode steps: 484, steps per second:  48, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.833 [0.000, 5.000],  loss: 0.012955, mae: 0.951334, mean_q: 1.159570, mean_eps: 0.483700
  345056/1200000: episode: 507, duration: 12.184s, episode steps: 614, steps per second:  50, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.184 [0.000, 5.000],  loss: 0.011093, mae: 0.934379, mean_q: 1.140208, mean_eps: 0.482878
  345673/1200000: episode: 508, duration: 11.894s, episode steps: 617, steps per second:  52, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.010434, mae: 0.926065, mean_q: 1.128785, mean_eps: 0.481954
  346311/1200000: episode: 509, duration: 13.026s, episode steps: 638, steps per second:  49, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.157 [0.000, 5.000],  loss: 0.011203, mae: 0.939196, mean_q: 1.145508, mean_eps: 0.481012
  347068/1200000: episode: 510, duration: 15.079s, episode steps: 757, steps per second:  50, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.011259, mae: 0.931599, mean_q: 1.134315, mean_eps: 0.479968
  347571/1200000: episode: 511, duration: 10.326s, episode steps: 503, steps per second:  49, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.011282, mae: 0.960551, mean_q: 1.170956, mean_eps: 0.479023
  348243/1200000: episode: 512, duration: 13.731s, episode steps: 672, steps per second:  49, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.010962, mae: 0.944078, mean_q: 1.150761, mean_eps: 0.478141
  348965/1200000: episode: 513, duration: 14.480s, episode steps: 722, steps per second:  50, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.010979, mae: 0.949451, mean_q: 1.159776, mean_eps: 0.477094
  349999/1200000: episode: 514, duration: 20.897s, episode steps: 1034, steps per second:  49, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.012173, mae: 0.953814, mean_q: 1.164031, mean_eps: 0.475777
  350366/1200000: episode: 515, duration: 7.401s, episode steps: 367, steps per second:  50, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.008772, mae: 0.964933, mean_q: 1.179259, mean_eps: 0.474727
  351162/1200000: episode: 516, duration: 15.970s, episode steps: 796, steps per second:  50, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.012161, mae: 0.990934, mean_q: 1.208784, mean_eps: 0.473854
  351652/1200000: episode: 517, duration: 8.773s, episode steps: 490, steps per second:  56, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.011374, mae: 1.004237, mean_q: 1.225131, mean_eps: 0.472891
  352501/1200000: episode: 518, duration: 15.266s, episode steps: 849, steps per second:  56, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.012405, mae: 0.977909, mean_q: 1.191622, mean_eps: 0.471886
  353309/1200000: episode: 519, duration: 16.815s, episode steps: 808, steps per second:  48, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.011740, mae: 0.974439, mean_q: 1.186497, mean_eps: 0.470641
  354191/1200000: episode: 520, duration: 18.813s, episode steps: 882, steps per second:  47, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.296 [0.000, 5.000],  loss: 0.011505, mae: 0.995458, mean_q: 1.212713, mean_eps: 0.469375
  355074/1200000: episode: 521, duration: 16.290s, episode steps: 883, steps per second:  54, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.012154, mae: 0.989315, mean_q: 1.205019, mean_eps: 0.468052
  355904/1200000: episode: 522, duration: 15.282s, episode steps: 830, steps per second:  54, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.147 [0.000, 5.000],  loss: 0.011864, mae: 0.997462, mean_q: 1.214890, mean_eps: 0.466768
  357121/1200000: episode: 523, duration: 23.111s, episode steps: 1217, steps per second:  53, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.272 [0.000, 5.000],  loss: 0.012580, mae: 0.998261, mean_q: 1.216168, mean_eps: 0.465232
  357804/1200000: episode: 524, duration: 12.511s, episode steps: 683, steps per second:  55, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.255 [0.000, 5.000],  loss: 0.011619, mae: 0.985111, mean_q: 1.200139, mean_eps: 0.463807
  358502/1200000: episode: 525, duration: 13.122s, episode steps: 698, steps per second:  53, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.010710, mae: 0.983805, mean_q: 1.198186, mean_eps: 0.462772
  359345/1200000: episode: 526, duration: 18.406s, episode steps: 843, steps per second:  46, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.011863, mae: 0.986434, mean_q: 1.202014, mean_eps: 0.461614
  360222/1200000: episode: 527, duration: 19.697s, episode steps: 877, steps per second:  45, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.012124, mae: 1.000548, mean_q: 1.219322, mean_eps: 0.460324
  361009/1200000: episode: 528, duration: 16.566s, episode steps: 787, steps per second:  48, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.010635, mae: 1.025209, mean_q: 1.251237, mean_eps: 0.459076
  362004/1200000: episode: 529, duration: 20.562s, episode steps: 995, steps per second:  48, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.676 [0.000, 5.000],  loss: 0.010957, mae: 1.021612, mean_q: 1.246078, mean_eps: 0.457741
  362390/1200000: episode: 530, duration: 8.189s, episode steps: 386, steps per second:  47, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.011131, mae: 1.008487, mean_q: 1.229923, mean_eps: 0.456706
  363030/1200000: episode: 531, duration: 12.840s, episode steps: 640, steps per second:  50, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.011975, mae: 1.011667, mean_q: 1.231669, mean_eps: 0.455935
  363523/1200000: episode: 532, duration: 8.832s, episode steps: 493, steps per second:  56, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.012305, mae: 1.002974, mean_q: 1.222070, mean_eps: 0.455086
  363999/1200000: episode: 533, duration: 8.654s, episode steps: 476, steps per second:  55, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.187 [0.000, 5.000],  loss: 0.011959, mae: 1.013157, mean_q: 1.236668, mean_eps: 0.454360
  364601/1200000: episode: 534, duration: 10.550s, episode steps: 602, steps per second:  57, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.150 [0.000, 5.000],  loss: 0.011801, mae: 1.015584, mean_q: 1.238964, mean_eps: 0.453550
  364985/1200000: episode: 535, duration: 7.240s, episode steps: 384, steps per second:  53, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.203 [0.000, 5.000],  loss: 0.011265, mae: 1.018817, mean_q: 1.244152, mean_eps: 0.452809
  365654/1200000: episode: 536, duration: 13.542s, episode steps: 669, steps per second:  49, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.010801, mae: 1.013439, mean_q: 1.236732, mean_eps: 0.452020
  366475/1200000: episode: 537, duration: 16.344s, episode steps: 821, steps per second:  50, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.659 [0.000, 5.000],  loss: 0.012600, mae: 1.005799, mean_q: 1.225334, mean_eps: 0.450904
  367240/1200000: episode: 538, duration: 14.836s, episode steps: 765, steps per second:  52, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.010442, mae: 1.008327, mean_q: 1.229280, mean_eps: 0.449716
  367784/1200000: episode: 539, duration: 11.434s, episode steps: 544, steps per second:  48, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.680 [0.000, 5.000],  loss: 0.012164, mae: 1.018856, mean_q: 1.240077, mean_eps: 0.448735
  368287/1200000: episode: 540, duration: 10.872s, episode steps: 503, steps per second:  46, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.011620, mae: 1.003369, mean_q: 1.222009, mean_eps: 0.447949
  368919/1200000: episode: 541, duration: 13.118s, episode steps: 632, steps per second:  48, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.010523, mae: 1.010543, mean_q: 1.231927, mean_eps: 0.447097
  369431/1200000: episode: 542, duration: 10.220s, episode steps: 512, steps per second:  50, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.012291, mae: 1.007197, mean_q: 1.226759, mean_eps: 0.446239
  369997/1200000: episode: 543, duration: 10.167s, episode steps: 566, steps per second:  56, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.012392, mae: 1.020427, mean_q: 1.245382, mean_eps: 0.445429
  370508/1200000: episode: 544, duration: 9.326s, episode steps: 511, steps per second:  55, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.011295, mae: 1.040014, mean_q: 1.269717, mean_eps: 0.444622
  371548/1200000: episode: 545, duration: 20.883s, episode steps: 1040, steps per second:  50, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.011909, mae: 1.057360, mean_q: 1.288145, mean_eps: 0.443461
  372122/1200000: episode: 546, duration: 11.775s, episode steps: 574, steps per second:  49, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.178 [0.000, 5.000],  loss: 0.011787, mae: 1.072668, mean_q: 1.306037, mean_eps: 0.442249
  372771/1200000: episode: 547, duration: 12.399s, episode steps: 649, steps per second:  52, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.013234, mae: 1.071334, mean_q: 1.304823, mean_eps: 0.441331
  373415/1200000: episode: 548, duration: 12.624s, episode steps: 644, steps per second:  51, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.011838, mae: 1.065568, mean_q: 1.297447, mean_eps: 0.440362
  373791/1200000: episode: 549, duration: 7.547s, episode steps: 376, steps per second:  50, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.011983, mae: 1.058247, mean_q: 1.289164, mean_eps: 0.439597
  374175/1200000: episode: 550, duration: 7.138s, episode steps: 384, steps per second:  54, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.013282, mae: 1.073998, mean_q: 1.308099, mean_eps: 0.439027
  375037/1200000: episode: 551, duration: 16.213s, episode steps: 862, steps per second:  53, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.012885, mae: 1.073720, mean_q: 1.306763, mean_eps: 0.438091
  375774/1200000: episode: 552, duration: 15.422s, episode steps: 737, steps per second:  48, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.034 [0.000, 5.000],  loss: 0.012183, mae: 1.047001, mean_q: 1.273824, mean_eps: 0.436891
  376593/1200000: episode: 553, duration: 17.263s, episode steps: 819, steps per second:  47, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.011391, mae: 1.061739, mean_q: 1.291825, mean_eps: 0.435724
  376957/1200000: episode: 554, duration: 7.682s, episode steps: 364, steps per second:  47, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.113 [0.000, 5.000],  loss: 0.013388, mae: 1.079319, mean_q: 1.312023, mean_eps: 0.434836
  377722/1200000: episode: 555, duration: 15.777s, episode steps: 765, steps per second:  48, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.011864, mae: 1.064900, mean_q: 1.295548, mean_eps: 0.433990
  378114/1200000: episode: 556, duration: 8.425s, episode steps: 392, steps per second:  47, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 0.012316, mae: 1.077975, mean_q: 1.310968, mean_eps: 0.433123
  379283/1200000: episode: 557, duration: 22.493s, episode steps: 1169, steps per second:  52, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.012117, mae: 1.078094, mean_q: 1.310956, mean_eps: 0.431953
  379956/1200000: episode: 558, duration: 13.162s, episode steps: 673, steps per second:  51, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.220 [0.000, 5.000],  loss: 0.011231, mae: 1.058646, mean_q: 1.288486, mean_eps: 0.430573
  380969/1200000: episode: 559, duration: 20.289s, episode steps: 1013, steps per second:  50, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.012708, mae: 1.093604, mean_q: 1.329170, mean_eps: 0.429307
  381529/1200000: episode: 560, duration: 11.258s, episode steps: 560, steps per second:  50, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.648 [0.000, 5.000],  loss: 0.012323, mae: 1.081868, mean_q: 1.317012, mean_eps: 0.428125
  382232/1200000: episode: 561, duration: 13.774s, episode steps: 703, steps per second:  51, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.011219, mae: 1.069523, mean_q: 1.301046, mean_eps: 0.427180
  382903/1200000: episode: 562, duration: 13.433s, episode steps: 671, steps per second:  50, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.011369, mae: 1.091825, mean_q: 1.327413, mean_eps: 0.426151
  383567/1200000: episode: 563, duration: 13.669s, episode steps: 664, steps per second:  49, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.267 [0.000, 5.000],  loss: 0.011660, mae: 1.101387, mean_q: 1.340521, mean_eps: 0.425149
  384232/1200000: episode: 564, duration: 13.233s, episode steps: 665, steps per second:  50, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.654 [0.000, 5.000],  loss: 0.011201, mae: 1.085089, mean_q: 1.321485, mean_eps: 0.424153
  384743/1200000: episode: 565, duration: 10.400s, episode steps: 511, steps per second:  49, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.998 [0.000, 5.000],  loss: 0.012862, mae: 1.094083, mean_q: 1.330847, mean_eps: 0.423271
  385278/1200000: episode: 566, duration: 10.751s, episode steps: 535, steps per second:  50, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.014497, mae: 1.106166, mean_q: 1.343747, mean_eps: 0.422485
  386104/1200000: episode: 567, duration: 16.334s, episode steps: 826, steps per second:  51, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.013051, mae: 1.086882, mean_q: 1.320287, mean_eps: 0.421465
  386497/1200000: episode: 568, duration: 8.255s, episode steps: 393, steps per second:  48, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.020 [0.000, 5.000],  loss: 0.011732, mae: 1.081939, mean_q: 1.315693, mean_eps: 0.420550
  386881/1200000: episode: 569, duration: 7.860s, episode steps: 384, steps per second:  49, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.107 [0.000, 5.000],  loss: 0.012852, mae: 1.095583, mean_q: 1.332388, mean_eps: 0.419965
  387629/1200000: episode: 570, duration: 14.161s, episode steps: 748, steps per second:  53, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.012783, mae: 1.085681, mean_q: 1.319024, mean_eps: 0.419116
  388283/1200000: episode: 571, duration: 11.721s, episode steps: 654, steps per second:  56, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.332 [0.000, 5.000],  loss: 0.012318, mae: 1.091606, mean_q: 1.327306, mean_eps: 0.418066
  388933/1200000: episode: 572, duration: 12.197s, episode steps: 650, steps per second:  53, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.812 [0.000, 5.000],  loss: 0.012515, mae: 1.091893, mean_q: 1.329201, mean_eps: 0.417088
  390472/1200000: episode: 573, duration: 30.341s, episode steps: 1539, steps per second:  51, episode reward: 32.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.244 [0.000, 5.000],  loss: 0.011586, mae: 1.100487, mean_q: 1.340368, mean_eps: 0.415447
  391223/1200000: episode: 574, duration: 13.956s, episode steps: 751, steps per second:  54, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.272 [0.000, 5.000],  loss: 0.012291, mae: 1.135174, mean_q: 1.381096, mean_eps: 0.413731
  391863/1200000: episode: 575, duration: 11.666s, episode steps: 640, steps per second:  55, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.078 [0.000, 5.000],  loss: 0.013153, mae: 1.136639, mean_q: 1.383672, mean_eps: 0.412687
  392430/1200000: episode: 576, duration: 10.717s, episode steps: 567, steps per second:  53, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.055 [0.000, 5.000],  loss: 0.014668, mae: 1.122814, mean_q: 1.364473, mean_eps: 0.411781
  393297/1200000: episode: 577, duration: 16.175s, episode steps: 867, steps per second:  54, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.091 [0.000, 5.000],  loss: 0.016179, mae: 1.131418, mean_q: 1.375513, mean_eps: 0.410704
  394165/1200000: episode: 578, duration: 16.211s, episode steps: 868, steps per second:  54, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.011995, mae: 1.128453, mean_q: 1.371943, mean_eps: 0.409402
  394988/1200000: episode: 579, duration: 15.212s, episode steps: 823, steps per second:  54, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.915 [0.000, 5.000],  loss: 0.012786, mae: 1.140065, mean_q: 1.386398, mean_eps: 0.408136
  395785/1200000: episode: 580, duration: 16.136s, episode steps: 797, steps per second:  49, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.243 [0.000, 5.000],  loss: 0.011801, mae: 1.126523, mean_q: 1.369446, mean_eps: 0.406921
  396482/1200000: episode: 581, duration: 16.021s, episode steps: 697, steps per second:  44, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.628 [0.000, 5.000],  loss: 0.011995, mae: 1.126805, mean_q: 1.370052, mean_eps: 0.405799
  397289/1200000: episode: 582, duration: 15.535s, episode steps: 807, steps per second:  52, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.013374, mae: 1.140299, mean_q: 1.386496, mean_eps: 0.404671
  398077/1200000: episode: 583, duration: 14.948s, episode steps: 788, steps per second:  53, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.013395, mae: 1.124352, mean_q: 1.366983, mean_eps: 0.403474
  398585/1200000: episode: 584, duration: 11.033s, episode steps: 508, steps per second:  46, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.012784, mae: 1.133004, mean_q: 1.377995, mean_eps: 0.402502
  399716/1200000: episode: 585, duration: 24.944s, episode steps: 1131, steps per second:  45, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.048 [0.000, 5.000],  loss: 0.012919, mae: 1.129276, mean_q: 1.373326, mean_eps: 0.401275
  400345/1200000: episode: 586, duration: 12.198s, episode steps: 629, steps per second:  52, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.714 [0.000, 5.000],  loss: 0.012553, mae: 1.144378, mean_q: 1.392156, mean_eps: 0.399955
  401362/1200000: episode: 587, duration: 19.659s, episode steps: 1017, steps per second:  52, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.011661, mae: 1.138704, mean_q: 1.383670, mean_eps: 0.398719
  402230/1200000: episode: 588, duration: 17.396s, episode steps: 868, steps per second:  50, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.011625, mae: 1.151380, mean_q: 1.398888, mean_eps: 0.397306
  402967/1200000: episode: 589, duration: 14.778s, episode steps: 737, steps per second:  50, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.828 [0.000, 5.000],  loss: 0.013891, mae: 1.156873, mean_q: 1.404996, mean_eps: 0.396103
  403496/1200000: episode: 590, duration: 10.054s, episode steps: 529, steps per second:  53, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.654 [0.000, 5.000],  loss: 0.012773, mae: 1.152867, mean_q: 1.402045, mean_eps: 0.395155
  404401/1200000: episode: 591, duration: 16.983s, episode steps: 905, steps per second:  53, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.013031, mae: 1.154990, mean_q: 1.404270, mean_eps: 0.394078
  405662/1200000: episode: 592, duration: 23.996s, episode steps: 1261, steps per second:  53, episode reward: 27.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.012819, mae: 1.156351, mean_q: 1.406448, mean_eps: 0.392452
  406265/1200000: episode: 593, duration: 10.526s, episode steps: 603, steps per second:  57, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.179 [0.000, 5.000],  loss: 0.011289, mae: 1.158104, mean_q: 1.407549, mean_eps: 0.391054
  406792/1200000: episode: 594, duration: 9.365s, episode steps: 527, steps per second:  56, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.012630, mae: 1.146163, mean_q: 1.393542, mean_eps: 0.390208
  407581/1200000: episode: 595, duration: 13.770s, episode steps: 789, steps per second:  57, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.682 [0.000, 5.000],  loss: 0.011830, mae: 1.149541, mean_q: 1.398200, mean_eps: 0.389221
  408358/1200000: episode: 596, duration: 14.084s, episode steps: 777, steps per second:  55, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.014108, mae: 1.134599, mean_q: 1.378586, mean_eps: 0.388045
  408897/1200000: episode: 597, duration: 9.950s, episode steps: 539, steps per second:  54, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.013271, mae: 1.149254, mean_q: 1.395043, mean_eps: 0.387058
  409705/1200000: episode: 598, duration: 14.938s, episode steps: 808, steps per second:  54, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.012903, mae: 1.155437, mean_q: 1.402257, mean_eps: 0.386047
  410202/1200000: episode: 599, duration: 8.520s, episode steps: 497, steps per second:  58, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.296 [0.000, 5.000],  loss: 0.011714, mae: 1.135051, mean_q: 1.379138, mean_eps: 0.385069
  410684/1200000: episode: 600, duration: 8.789s, episode steps: 482, steps per second:  55, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.011905, mae: 1.159226, mean_q: 1.409548, mean_eps: 0.384337
  411598/1200000: episode: 601, duration: 16.622s, episode steps: 914, steps per second:  55, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.164 [0.000, 5.000],  loss: 0.011626, mae: 1.165286, mean_q: 1.417088, mean_eps: 0.383290
  412183/1200000: episode: 602, duration: 10.019s, episode steps: 585, steps per second:  58, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.843 [0.000, 5.000],  loss: 0.012776, mae: 1.170720, mean_q: 1.422808, mean_eps: 0.382165
  412947/1200000: episode: 603, duration: 14.729s, episode steps: 764, steps per second:  52, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.639 [0.000, 5.000],  loss: 0.011814, mae: 1.185688, mean_q: 1.442655, mean_eps: 0.381154
  413633/1200000: episode: 604, duration: 13.170s, episode steps: 686, steps per second:  52, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.153 [0.000, 5.000],  loss: 0.012736, mae: 1.173145, mean_q: 1.427311, mean_eps: 0.380065
  414492/1200000: episode: 605, duration: 18.793s, episode steps: 859, steps per second:  46, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.012479, mae: 1.188728, mean_q: 1.445028, mean_eps: 0.378907
  415116/1200000: episode: 606, duration: 13.484s, episode steps: 624, steps per second:  46, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.012715, mae: 1.155137, mean_q: 1.402681, mean_eps: 0.377797
  416488/1200000: episode: 607, duration: 27.378s, episode steps: 1372, steps per second:  50, episode reward: 26.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.157 [0.000, 5.000],  loss: 0.012716, mae: 1.177262, mean_q: 1.429813, mean_eps: 0.376300
  417228/1200000: episode: 608, duration: 14.043s, episode steps: 740, steps per second:  53, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.013113, mae: 1.168303, mean_q: 1.420152, mean_eps: 0.374716
  418341/1200000: episode: 609, duration: 21.591s, episode steps: 1113, steps per second:  52, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.015209, mae: 1.177669, mean_q: 1.429459, mean_eps: 0.373324
  419002/1200000: episode: 610, duration: 12.001s, episode steps: 661, steps per second:  55, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.012663, mae: 1.170815, mean_q: 1.421473, mean_eps: 0.371992
  419621/1200000: episode: 611, duration: 11.013s, episode steps: 619, steps per second:  56, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.013299, mae: 1.160925, mean_q: 1.409566, mean_eps: 0.371032
  420144/1200000: episode: 612, duration: 10.273s, episode steps: 523, steps per second:  51, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.013427, mae: 1.163922, mean_q: 1.413678, mean_eps: 0.370177
  420646/1200000: episode: 613, duration: 10.392s, episode steps: 502, steps per second:  48, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.133 [0.000, 5.000],  loss: 0.014062, mae: 1.201916, mean_q: 1.458916, mean_eps: 0.369409
  421160/1200000: episode: 614, duration: 11.689s, episode steps: 514, steps per second:  44, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.732 [0.000, 5.000],  loss: 0.012971, mae: 1.194955, mean_q: 1.451288, mean_eps: 0.368647
  421833/1200000: episode: 615, duration: 13.449s, episode steps: 673, steps per second:  50, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.011391, mae: 1.194392, mean_q: 1.451530, mean_eps: 0.367756
  422440/1200000: episode: 616, duration: 11.364s, episode steps: 607, steps per second:  53, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.008 [0.000, 5.000],  loss: 0.011051, mae: 1.198472, mean_q: 1.456097, mean_eps: 0.366796
  423174/1200000: episode: 617, duration: 13.842s, episode steps: 734, steps per second:  53, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.312 [0.000, 5.000],  loss: 0.012396, mae: 1.199555, mean_q: 1.456821, mean_eps: 0.365791
  423789/1200000: episode: 618, duration: 11.491s, episode steps: 615, steps per second:  54, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.012301, mae: 1.189426, mean_q: 1.444003, mean_eps: 0.364777
  424432/1200000: episode: 619, duration: 12.141s, episode steps: 643, steps per second:  53, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.264 [0.000, 5.000],  loss: 0.012787, mae: 1.211661, mean_q: 1.472464, mean_eps: 0.363835
  425055/1200000: episode: 620, duration: 11.061s, episode steps: 623, steps per second:  56, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.313 [0.000, 5.000],  loss: 0.011425, mae: 1.209382, mean_q: 1.470703, mean_eps: 0.362887
  425562/1200000: episode: 621, duration: 8.961s, episode steps: 507, steps per second:  57, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.738 [0.000, 5.000],  loss: 0.011938, mae: 1.187837, mean_q: 1.444099, mean_eps: 0.362038
  426269/1200000: episode: 622, duration: 12.682s, episode steps: 707, steps per second:  56, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.140 [0.000, 5.000],  loss: 0.011884, mae: 1.192449, mean_q: 1.448433, mean_eps: 0.361126
  427487/1200000: episode: 623, duration: 23.329s, episode steps: 1218, steps per second:  52, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.213 [0.000, 5.000],  loss: 0.013276, mae: 1.190037, mean_q: 1.444365, mean_eps: 0.359683
  428239/1200000: episode: 624, duration: 13.993s, episode steps: 752, steps per second:  54, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.097 [0.000, 5.000],  loss: 0.013165, mae: 1.184472, mean_q: 1.438864, mean_eps: 0.358207
  429159/1200000: episode: 625, duration: 16.342s, episode steps: 920, steps per second:  56, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.208 [0.000, 5.000],  loss: 0.013835, mae: 1.200983, mean_q: 1.458027, mean_eps: 0.356953
  429743/1200000: episode: 626, duration: 10.629s, episode steps: 584, steps per second:  55, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.098 [0.000, 5.000],  loss: 0.014058, mae: 1.192771, mean_q: 1.448542, mean_eps: 0.355825
  430805/1200000: episode: 627, duration: 19.406s, episode steps: 1062, steps per second:  55, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.012105, mae: 1.216077, mean_q: 1.479074, mean_eps: 0.354589
  431375/1200000: episode: 628, duration: 11.709s, episode steps: 570, steps per second:  49, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.016 [0.000, 5.000],  loss: 0.012769, mae: 1.212748, mean_q: 1.473956, mean_eps: 0.353365
  432299/1200000: episode: 629, duration: 20.818s, episode steps: 924, steps per second:  44, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.012056, mae: 1.214877, mean_q: 1.476280, mean_eps: 0.352246
  432980/1200000: episode: 630, duration: 16.738s, episode steps: 681, steps per second:  41, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.012747, mae: 1.198628, mean_q: 1.456951, mean_eps: 0.351043
  434220/1200000: episode: 631, duration: 25.382s, episode steps: 1240, steps per second:  49, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.013110, mae: 1.219171, mean_q: 1.481233, mean_eps: 0.349603
  435624/1200000: episode: 632, duration: 28.645s, episode steps: 1404, steps per second:  49, episode reward: 30.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.012991, mae: 1.212435, mean_q: 1.472976, mean_eps: 0.347620
  436100/1200000: episode: 633, duration: 9.609s, episode steps: 476, steps per second:  50, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.010772, mae: 1.217635, mean_q: 1.478156, mean_eps: 0.346210
  436871/1200000: episode: 634, duration: 14.633s, episode steps: 771, steps per second:  53, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.012619, mae: 1.216959, mean_q: 1.478014, mean_eps: 0.345274
  437485/1200000: episode: 635, duration: 10.874s, episode steps: 614, steps per second:  56, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.086 [0.000, 5.000],  loss: 0.014475, mae: 1.220644, mean_q: 1.481877, mean_eps: 0.344233
  438264/1200000: episode: 636, duration: 13.806s, episode steps: 779, steps per second:  56, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.012707, mae: 1.216813, mean_q: 1.478141, mean_eps: 0.343189
  438875/1200000: episode: 637, duration: 11.367s, episode steps: 611, steps per second:  54, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.791 [0.000, 5.000],  loss: 0.012187, mae: 1.206304, mean_q: 1.464464, mean_eps: 0.342148
  439427/1200000: episode: 638, duration: 10.772s, episode steps: 552, steps per second:  51, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.012449, mae: 1.215340, mean_q: 1.476229, mean_eps: 0.341275
  440142/1200000: episode: 639, duration: 13.798s, episode steps: 715, steps per second:  52, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.011754, mae: 1.203213, mean_q: 1.461775, mean_eps: 0.340324
  441204/1200000: episode: 640, duration: 19.963s, episode steps: 1062, steps per second:  53, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.185 [0.000, 5.000],  loss: 0.012385, mae: 1.237383, mean_q: 1.503932, mean_eps: 0.338992
  442099/1200000: episode: 641, duration: 17.370s, episode steps: 895, steps per second:  52, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.095 [0.000, 5.000],  loss: 0.013734, mae: 1.240390, mean_q: 1.506770, mean_eps: 0.337525
  442841/1200000: episode: 642, duration: 14.167s, episode steps: 742, steps per second:  52, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.012385, mae: 1.249949, mean_q: 1.518325, mean_eps: 0.336295
  443689/1200000: episode: 643, duration: 15.656s, episode steps: 848, steps per second:  54, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.212 [0.000, 5.000],  loss: 0.011782, mae: 1.232412, mean_q: 1.497675, mean_eps: 0.335101
  444455/1200000: episode: 644, duration: 13.766s, episode steps: 766, steps per second:  56, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.913 [0.000, 5.000],  loss: 0.012231, mae: 1.222436, mean_q: 1.484598, mean_eps: 0.333892
  445261/1200000: episode: 645, duration: 15.604s, episode steps: 806, steps per second:  52, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.056 [0.000, 5.000],  loss: 0.013343, mae: 1.234142, mean_q: 1.497497, mean_eps: 0.332713
  446152/1200000: episode: 646, duration: 17.905s, episode steps: 891, steps per second:  50, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.012662, mae: 1.232003, mean_q: 1.495489, mean_eps: 0.331441
  446672/1200000: episode: 647, duration: 10.040s, episode steps: 520, steps per second:  52, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.014431, mae: 1.234986, mean_q: 1.498943, mean_eps: 0.330385
  447539/1200000: episode: 648, duration: 17.519s, episode steps: 867, steps per second:  49, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.012443, mae: 1.237497, mean_q: 1.501666, mean_eps: 0.329344
  448250/1200000: episode: 649, duration: 13.632s, episode steps: 711, steps per second:  52, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.012461, mae: 1.224631, mean_q: 1.484530, mean_eps: 0.328159
  449054/1200000: episode: 650, duration: 14.544s, episode steps: 804, steps per second:  55, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.193 [0.000, 5.000],  loss: 0.012737, mae: 1.227404, mean_q: 1.489290, mean_eps: 0.327022
  449870/1200000: episode: 651, duration: 15.356s, episode steps: 816, steps per second:  53, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.214 [0.000, 5.000],  loss: 0.012193, mae: 1.242662, mean_q: 1.507729, mean_eps: 0.325807
  450643/1200000: episode: 652, duration: 14.036s, episode steps: 773, steps per second:  55, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.164 [0.000, 5.000],  loss: 0.012725, mae: 1.250703, mean_q: 1.518604, mean_eps: 0.324616
  451384/1200000: episode: 653, duration: 14.483s, episode steps: 741, steps per second:  51, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.251 [0.000, 5.000],  loss: 0.013901, mae: 1.249033, mean_q: 1.515418, mean_eps: 0.323482
  451772/1200000: episode: 654, duration: 8.067s, episode steps: 388, steps per second:  48, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.835 [0.000, 5.000],  loss: 0.012304, mae: 1.263582, mean_q: 1.534561, mean_eps: 0.322636
  452860/1200000: episode: 655, duration: 22.361s, episode steps: 1088, steps per second:  49, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.049 [0.000, 5.000],  loss: 0.013168, mae: 1.243017, mean_q: 1.509324, mean_eps: 0.321529
  453564/1200000: episode: 656, duration: 13.648s, episode steps: 704, steps per second:  52, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.122 [0.000, 5.000],  loss: 0.013611, mae: 1.264282, mean_q: 1.533648, mean_eps: 0.320185
  454479/1200000: episode: 657, duration: 17.696s, episode steps: 915, steps per second:  52, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.013449, mae: 1.255931, mean_q: 1.524513, mean_eps: 0.318970
  455137/1200000: episode: 658, duration: 12.410s, episode steps: 658, steps per second:  53, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.021 [0.000, 5.000],  loss: 0.014303, mae: 1.253056, mean_q: 1.522157, mean_eps: 0.317788
  456105/1200000: episode: 659, duration: 17.653s, episode steps: 968, steps per second:  55, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.121 [0.000, 5.000],  loss: 0.013000, mae: 1.247565, mean_q: 1.515197, mean_eps: 0.316567
  457312/1200000: episode: 660, duration: 21.214s, episode steps: 1207, steps per second:  57, episode reward: 30.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.013022, mae: 1.251156, mean_q: 1.518977, mean_eps: 0.314938
  457976/1200000: episode: 661, duration: 13.042s, episode steps: 664, steps per second:  51, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.012761, mae: 1.225738, mean_q: 1.487915, mean_eps: 0.313537
  458358/1200000: episode: 662, duration: 7.137s, episode steps: 382, steps per second:  54, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.026 [0.000, 5.000],  loss: 0.012754, mae: 1.286427, mean_q: 1.562978, mean_eps: 0.312751
  458998/1200000: episode: 663, duration: 12.599s, episode steps: 640, steps per second:  51, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.178 [0.000, 5.000],  loss: 0.013232, mae: 1.265409, mean_q: 1.536111, mean_eps: 0.311983
  459793/1200000: episode: 664, duration: 14.759s, episode steps: 795, steps per second:  54, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.127 [0.000, 5.000],  loss: 0.012766, mae: 1.262890, mean_q: 1.534842, mean_eps: 0.310906
  460423/1200000: episode: 665, duration: 12.136s, episode steps: 630, steps per second:  52, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.256 [0.000, 5.000],  loss: 0.012028, mae: 1.267414, mean_q: 1.539513, mean_eps: 0.309838
  461138/1200000: episode: 666, duration: 13.488s, episode steps: 715, steps per second:  53, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.011208, mae: 1.287610, mean_q: 1.563904, mean_eps: 0.308830
  461775/1200000: episode: 667, duration: 11.981s, episode steps: 637, steps per second:  53, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.015023, mae: 1.286486, mean_q: 1.561616, mean_eps: 0.307816
  462489/1200000: episode: 668, duration: 12.680s, episode steps: 714, steps per second:  56, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.889 [0.000, 5.000],  loss: 0.013857, mae: 1.299297, mean_q: 1.577723, mean_eps: 0.306802
  463138/1200000: episode: 669, duration: 11.263s, episode steps: 649, steps per second:  58, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.773 [0.000, 5.000],  loss: 0.013947, mae: 1.310863, mean_q: 1.591174, mean_eps: 0.305779
  464086/1200000: episode: 670, duration: 16.712s, episode steps: 948, steps per second:  57, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.806 [0.000, 5.000],  loss: 0.012796, mae: 1.297301, mean_q: 1.575546, mean_eps: 0.304582
  464803/1200000: episode: 671, duration: 13.196s, episode steps: 717, steps per second:  54, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.070 [0.000, 5.000],  loss: 0.013121, mae: 1.283677, mean_q: 1.558753, mean_eps: 0.303334
  466072/1200000: episode: 672, duration: 23.074s, episode steps: 1269, steps per second:  55, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.130 [0.000, 5.000],  loss: 0.012777, mae: 1.298809, mean_q: 1.576725, mean_eps: 0.301846
  466655/1200000: episode: 673, duration: 10.145s, episode steps: 583, steps per second:  57, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.012698, mae: 1.283395, mean_q: 1.558232, mean_eps: 0.300457
  467697/1200000: episode: 674, duration: 19.373s, episode steps: 1042, steps per second:  54, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.162 [0.000, 5.000],  loss: 0.013332, mae: 1.277400, mean_q: 1.549567, mean_eps: 0.299236
  468557/1200000: episode: 675, duration: 16.758s, episode steps: 860, steps per second:  51, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.098 [0.000, 5.000],  loss: 0.013567, mae: 1.293914, mean_q: 1.570310, mean_eps: 0.297808
  469185/1200000: episode: 676, duration: 12.034s, episode steps: 628, steps per second:  52, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.874 [0.000, 5.000],  loss: 0.014576, mae: 1.297140, mean_q: 1.575423, mean_eps: 0.296692
  469716/1200000: episode: 677, duration: 10.418s, episode steps: 531, steps per second:  51, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.105 [0.000, 5.000],  loss: 0.012765, mae: 1.277166, mean_q: 1.549828, mean_eps: 0.295825
  470269/1200000: episode: 678, duration: 10.972s, episode steps: 553, steps per second:  50, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.705 [0.000, 5.000],  loss: 0.012815, mae: 1.311440, mean_q: 1.592882, mean_eps: 0.295012
  470924/1200000: episode: 679, duration: 15.698s, episode steps: 655, steps per second:  42, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.012948, mae: 1.320485, mean_q: 1.603748, mean_eps: 0.294106
  471695/1200000: episode: 680, duration: 18.053s, episode steps: 771, steps per second:  43, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.012211, mae: 1.328274, mean_q: 1.613113, mean_eps: 0.293038
  472328/1200000: episode: 681, duration: 14.168s, episode steps: 633, steps per second:  45, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.012346, mae: 1.313173, mean_q: 1.594221, mean_eps: 0.291985
  473044/1200000: episode: 682, duration: 15.806s, episode steps: 716, steps per second:  45, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.728 [0.000, 5.000],  loss: 0.011577, mae: 1.326370, mean_q: 1.609207, mean_eps: 0.290974
  473597/1200000: episode: 683, duration: 11.303s, episode steps: 553, steps per second:  49, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.013240, mae: 1.313292, mean_q: 1.593521, mean_eps: 0.290020
  474319/1200000: episode: 684, duration: 15.461s, episode steps: 722, steps per second:  47, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.012605, mae: 1.326472, mean_q: 1.610941, mean_eps: 0.289063
  475115/1200000: episode: 685, duration: 16.190s, episode steps: 796, steps per second:  49, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.041 [0.000, 5.000],  loss: 0.014146, mae: 1.329040, mean_q: 1.614408, mean_eps: 0.287926
  475752/1200000: episode: 686, duration: 13.172s, episode steps: 637, steps per second:  48, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.653 [0.000, 5.000],  loss: 0.013587, mae: 1.311276, mean_q: 1.592646, mean_eps: 0.286852
  476253/1200000: episode: 687, duration: 9.874s, episode steps: 501, steps per second:  51, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.013200, mae: 1.304381, mean_q: 1.583696, mean_eps: 0.285997
  476937/1200000: episode: 688, duration: 14.070s, episode steps: 684, steps per second:  49, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.015236, mae: 1.329178, mean_q: 1.613504, mean_eps: 0.285106
  477553/1200000: episode: 689, duration: 13.264s, episode steps: 616, steps per second:  46, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.713 [0.000, 5.000],  loss: 0.012251, mae: 1.306360, mean_q: 1.586210, mean_eps: 0.284131
  478340/1200000: episode: 690, duration: 17.353s, episode steps: 787, steps per second:  45, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.011846, mae: 1.326137, mean_q: 1.610907, mean_eps: 0.283081
  478948/1200000: episode: 691, duration: 13.336s, episode steps: 608, steps per second:  46, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.011782, mae: 1.309170, mean_q: 1.590765, mean_eps: 0.282037
  479692/1200000: episode: 692, duration: 16.158s, episode steps: 744, steps per second:  46, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.012387, mae: 1.328074, mean_q: 1.612326, mean_eps: 0.281023
  480574/1200000: episode: 693, duration: 17.392s, episode steps: 882, steps per second:  51, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.010972, mae: 1.332511, mean_q: 1.616374, mean_eps: 0.279802
  481118/1200000: episode: 694, duration: 10.402s, episode steps: 544, steps per second:  52, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.061 [0.000, 5.000],  loss: 0.010959, mae: 1.331678, mean_q: 1.616094, mean_eps: 0.278731
  482151/1200000: episode: 695, duration: 20.383s, episode steps: 1033, steps per second:  51, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.993 [0.000, 5.000],  loss: 0.012352, mae: 1.346744, mean_q: 1.633871, mean_eps: 0.277549
  482950/1200000: episode: 696, duration: 16.072s, episode steps: 799, steps per second:  50, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.012662, mae: 1.359166, mean_q: 1.648705, mean_eps: 0.276175
  483521/1200000: episode: 697, duration: 11.116s, episode steps: 571, steps per second:  51, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.215 [0.000, 5.000],  loss: 0.012506, mae: 1.338645, mean_q: 1.623339, mean_eps: 0.275146
  484088/1200000: episode: 698, duration: 10.705s, episode steps: 567, steps per second:  53, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.153 [0.000, 5.000],  loss: 0.014150, mae: 1.332865, mean_q: 1.616202, mean_eps: 0.274294
  484809/1200000: episode: 699, duration: 13.842s, episode steps: 721, steps per second:  52, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.814 [0.000, 5.000],  loss: 0.012897, mae: 1.325418, mean_q: 1.607355, mean_eps: 0.273328
  485507/1200000: episode: 700, duration: 12.665s, episode steps: 698, steps per second:  55, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.160 [0.000, 5.000],  loss: 0.012640, mae: 1.321688, mean_q: 1.605011, mean_eps: 0.272263
  486466/1200000: episode: 701, duration: 18.572s, episode steps: 959, steps per second:  52, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.767 [0.000, 5.000],  loss: 0.012946, mae: 1.348309, mean_q: 1.637015, mean_eps: 0.271021
  486898/1200000: episode: 702, duration: 8.170s, episode steps: 432, steps per second:  53, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.155 [0.000, 5.000],  loss: 0.015878, mae: 1.343943, mean_q: 1.630812, mean_eps: 0.269977
  487690/1200000: episode: 703, duration: 15.791s, episode steps: 792, steps per second:  50, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.011833, mae: 1.346827, mean_q: 1.633842, mean_eps: 0.269059
  488433/1200000: episode: 704, duration: 15.558s, episode steps: 743, steps per second:  48, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.078 [0.000, 5.000],  loss: 0.013308, mae: 1.321636, mean_q: 1.602730, mean_eps: 0.267907
  489462/1200000: episode: 705, duration: 21.913s, episode steps: 1029, steps per second:  47, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.013033, mae: 1.349045, mean_q: 1.637776, mean_eps: 0.266578
  490265/1200000: episode: 706, duration: 15.933s, episode steps: 803, steps per second:  50, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.012738, mae: 1.368205, mean_q: 1.660456, mean_eps: 0.265204
  490754/1200000: episode: 707, duration: 9.929s, episode steps: 489, steps per second:  49, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 0.011697, mae: 1.358219, mean_q: 1.647805, mean_eps: 0.264235
  492597/1200000: episode: 708, duration: 36.818s, episode steps: 1843, steps per second:  50, episode reward: 28.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.012421, mae: 1.381363, mean_q: 1.675548, mean_eps: 0.262486
  493260/1200000: episode: 709, duration: 13.211s, episode steps: 663, steps per second:  50, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.013894, mae: 1.377976, mean_q: 1.671108, mean_eps: 0.260608
  494219/1200000: episode: 710, duration: 20.873s, episode steps: 959, steps per second:  46, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.012139, mae: 1.372504, mean_q: 1.664544, mean_eps: 0.259393
  494764/1200000: episode: 711, duration: 12.011s, episode steps: 545, steps per second:  45, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.012462, mae: 1.379058, mean_q: 1.672933, mean_eps: 0.258265
  495599/1200000: episode: 712, duration: 17.742s, episode steps: 835, steps per second:  47, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.012509, mae: 1.353222, mean_q: 1.641117, mean_eps: 0.257230
  496583/1200000: episode: 713, duration: 19.825s, episode steps: 984, steps per second:  50, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.858 [0.000, 5.000],  loss: 0.012404, mae: 1.359929, mean_q: 1.649961, mean_eps: 0.255865
  497496/1200000: episode: 714, duration: 18.651s, episode steps: 913, steps per second:  49, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.146 [0.000, 5.000],  loss: 0.017044, mae: 1.391121, mean_q: 1.684475, mean_eps: 0.254443
  498303/1200000: episode: 715, duration: 15.238s, episode steps: 807, steps per second:  53, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.727 [0.000, 5.000],  loss: 0.012501, mae: 1.377385, mean_q: 1.671270, mean_eps: 0.253153
  498909/1200000: episode: 716, duration: 11.606s, episode steps: 606, steps per second:  52, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.705 [0.000, 5.000],  loss: 0.011965, mae: 1.347056, mean_q: 1.634225, mean_eps: 0.252091
  499721/1200000: episode: 717, duration: 16.303s, episode steps: 812, steps per second:  50, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.775 [0.000, 5.000],  loss: 0.013008, mae: 1.363484, mean_q: 1.653027, mean_eps: 0.251026
  500259/1200000: episode: 718, duration: 11.656s, episode steps: 538, steps per second:  46, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.247 [0.000, 5.000],  loss: 0.010612, mae: 1.355727, mean_q: 1.646174, mean_eps: 0.250015
  500836/1200000: episode: 719, duration: 13.889s, episode steps: 577, steps per second:  42, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.011559, mae: 1.366960, mean_q: 1.659214, mean_eps: 0.249181
  501658/1200000: episode: 720, duration: 17.912s, episode steps: 822, steps per second:  46, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.746 [0.000, 5.000],  loss: 0.013243, mae: 1.366369, mean_q: 1.657013, mean_eps: 0.248131
  502453/1200000: episode: 721, duration: 17.150s, episode steps: 795, steps per second:  46, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.226 [0.000, 5.000],  loss: 0.013463, mae: 1.378829, mean_q: 1.672113, mean_eps: 0.246916
  503020/1200000: episode: 722, duration: 11.934s, episode steps: 567, steps per second:  48, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.012677, mae: 1.366998, mean_q: 1.657353, mean_eps: 0.245896
  503771/1200000: episode: 723, duration: 15.048s, episode steps: 751, steps per second:  50, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.313 [0.000, 5.000],  loss: 0.012934, mae: 1.372997, mean_q: 1.665493, mean_eps: 0.244909
  504590/1200000: episode: 724, duration: 16.692s, episode steps: 819, steps per second:  49, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.669 [0.000, 5.000],  loss: 0.012124, mae: 1.359343, mean_q: 1.647214, mean_eps: 0.243730
  505270/1200000: episode: 725, duration: 14.325s, episode steps: 680, steps per second:  47, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.193 [0.000, 5.000],  loss: 0.012806, mae: 1.383177, mean_q: 1.676455, mean_eps: 0.242605
  505988/1200000: episode: 726, duration: 16.900s, episode steps: 718, steps per second:  42, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.192 [0.000, 5.000],  loss: 0.014357, mae: 1.345154, mean_q: 1.629630, mean_eps: 0.241558
  506623/1200000: episode: 727, duration: 14.368s, episode steps: 635, steps per second:  44, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.751 [0.000, 5.000],  loss: 0.012817, mae: 1.348967, mean_q: 1.635318, mean_eps: 0.240544
  507274/1200000: episode: 728, duration: 12.771s, episode steps: 651, steps per second:  51, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.261 [0.000, 5.000],  loss: 0.013551, mae: 1.387085, mean_q: 1.681011, mean_eps: 0.239578
  508061/1200000: episode: 729, duration: 15.554s, episode steps: 787, steps per second:  51, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.012726, mae: 1.376232, mean_q: 1.669690, mean_eps: 0.238498
  508897/1200000: episode: 730, duration: 17.172s, episode steps: 836, steps per second:  49, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.013493, mae: 1.372600, mean_q: 1.665269, mean_eps: 0.237280
  509617/1200000: episode: 731, duration: 14.489s, episode steps: 720, steps per second:  50, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.015056, mae: 1.386254, mean_q: 1.681358, mean_eps: 0.236113
  510435/1200000: episode: 732, duration: 16.406s, episode steps: 818, steps per second:  50, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.833 [0.000, 5.000],  loss: 0.012504, mae: 1.377709, mean_q: 1.670772, mean_eps: 0.234961
  511496/1200000: episode: 733, duration: 21.698s, episode steps: 1061, steps per second:  49, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.248 [0.000, 5.000],  loss: 0.012169, mae: 1.406380, mean_q: 1.705686, mean_eps: 0.233554
  512497/1200000: episode: 734, duration: 19.978s, episode steps: 1001, steps per second:  50, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.011867, mae: 1.421589, mean_q: 1.724541, mean_eps: 0.232006
  513329/1200000: episode: 735, duration: 18.298s, episode steps: 832, steps per second:  45, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.081 [0.000, 5.000],  loss: 0.014103, mae: 1.424458, mean_q: 1.727378, mean_eps: 0.230629
  514051/1200000: episode: 736, duration: 15.778s, episode steps: 722, steps per second:  46, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.014541, mae: 1.410314, mean_q: 1.710099, mean_eps: 0.229465
  514771/1200000: episode: 737, duration: 14.974s, episode steps: 720, steps per second:  48, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.013204, mae: 1.422282, mean_q: 1.725057, mean_eps: 0.228385
  515696/1200000: episode: 738, duration: 18.971s, episode steps: 925, steps per second:  49, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.014504, mae: 1.396561, mean_q: 1.694677, mean_eps: 0.227152
  516576/1200000: episode: 739, duration: 16.805s, episode steps: 880, steps per second:  52, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.013388, mae: 1.419452, mean_q: 1.721102, mean_eps: 0.225799
  517678/1200000: episode: 740, duration: 21.620s, episode steps: 1102, steps per second:  51, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.964 [0.000, 5.000],  loss: 0.012113, mae: 1.419934, mean_q: 1.721086, mean_eps: 0.224311
  518418/1200000: episode: 741, duration: 14.481s, episode steps: 740, steps per second:  51, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.951 [0.000, 5.000],  loss: 0.013886, mae: 1.426456, mean_q: 1.729330, mean_eps: 0.222928
  519170/1200000: episode: 742, duration: 14.112s, episode steps: 752, steps per second:  53, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.041 [0.000, 5.000],  loss: 0.014616, mae: 1.426170, mean_q: 1.731338, mean_eps: 0.221809
  519973/1200000: episode: 743, duration: 15.644s, episode steps: 803, steps per second:  51, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.999 [0.000, 5.000],  loss: 0.015096, mae: 1.408174, mean_q: 1.708973, mean_eps: 0.220642
  521240/1200000: episode: 744, duration: 24.343s, episode steps: 1267, steps per second:  52, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.242 [0.000, 5.000],  loss: 0.012172, mae: 1.450691, mean_q: 1.761399, mean_eps: 0.219091
  522020/1200000: episode: 745, duration: 16.495s, episode steps: 780, steps per second:  47, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.186 [0.000, 5.000],  loss: 0.013206, mae: 1.446057, mean_q: 1.754015, mean_eps: 0.217558
  522784/1200000: episode: 746, duration: 16.118s, episode steps: 764, steps per second:  47, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.726 [0.000, 5.000],  loss: 0.014388, mae: 1.447443, mean_q: 1.754089, mean_eps: 0.216400
  523781/1200000: episode: 747, duration: 23.836s, episode steps: 997, steps per second:  42, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.147 [0.000, 5.000],  loss: 0.014263, mae: 1.454239, mean_q: 1.763161, mean_eps: 0.215077
  524463/1200000: episode: 748, duration: 14.901s, episode steps: 682, steps per second:  46, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.014816, mae: 1.461828, mean_q: 1.774505, mean_eps: 0.213817
  525270/1200000: episode: 749, duration: 16.302s, episode steps: 807, steps per second:  50, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.170 [0.000, 5.000],  loss: 0.014467, mae: 1.457105, mean_q: 1.766945, mean_eps: 0.212701
  526740/1200000: episode: 750, duration: 29.880s, episode steps: 1470, steps per second:  49, episode reward: 29.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.013095, mae: 1.442010, mean_q: 1.749125, mean_eps: 0.210994
  527638/1200000: episode: 751, duration: 18.613s, episode steps: 898, steps per second:  48, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.986 [0.000, 5.000],  loss: 0.013613, mae: 1.452583, mean_q: 1.761140, mean_eps: 0.209218
  528635/1200000: episode: 752, duration: 19.897s, episode steps: 997, steps per second:  50, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.211 [0.000, 5.000],  loss: 0.011758, mae: 1.455060, mean_q: 1.763768, mean_eps: 0.207796
  529464/1200000: episode: 753, duration: 18.053s, episode steps: 829, steps per second:  46, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.013818, mae: 1.446995, mean_q: 1.753546, mean_eps: 0.206428
  530464/1200000: episode: 754, duration: 21.515s, episode steps: 1000, steps per second:  46, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.647 [0.000, 5.000],  loss: 0.012137, mae: 1.460655, mean_q: 1.771288, mean_eps: 0.205057
  531805/1200000: episode: 755, duration: 28.225s, episode steps: 1341, steps per second:  48, episode reward: 25.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.203 [0.000, 5.000],  loss: 0.011192, mae: 1.463606, mean_q: 1.774725, mean_eps: 0.203299
  533143/1200000: episode: 756, duration: 26.738s, episode steps: 1338, steps per second:  50, episode reward: 26.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.168 [0.000, 5.000],  loss: 0.012567, mae: 1.474946, mean_q: 1.787536, mean_eps: 0.201289
  533992/1200000: episode: 757, duration: 16.042s, episode steps: 849, steps per second:  53, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.013701, mae: 1.479379, mean_q: 1.791114, mean_eps: 0.199651
  534644/1200000: episode: 758, duration: 12.674s, episode steps: 652, steps per second:  51, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.121 [0.000, 5.000],  loss: 0.014038, mae: 1.494658, mean_q: 1.808868, mean_eps: 0.198526
  535489/1200000: episode: 759, duration: 17.508s, episode steps: 845, steps per second:  48, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.756 [0.000, 5.000],  loss: 0.012687, mae: 1.462976, mean_q: 1.770783, mean_eps: 0.197401
  536145/1200000: episode: 760, duration: 12.761s, episode steps: 656, steps per second:  51, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.012018, mae: 1.469049, mean_q: 1.777562, mean_eps: 0.196273
  537238/1200000: episode: 761, duration: 20.442s, episode steps: 1093, steps per second:  53, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.287 [0.000, 5.000],  loss: 0.012791, mae: 1.483035, mean_q: 1.795530, mean_eps: 0.194962
  537780/1200000: episode: 762, duration: 10.840s, episode steps: 542, steps per second:  50, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.830 [0.000, 5.000],  loss: 0.013746, mae: 1.479067, mean_q: 1.791069, mean_eps: 0.193738
  538702/1200000: episode: 763, duration: 18.522s, episode steps: 922, steps per second:  50, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.928 [0.000, 5.000],  loss: 0.012703, mae: 1.479868, mean_q: 1.792104, mean_eps: 0.192640
  539392/1200000: episode: 764, duration: 13.619s, episode steps: 690, steps per second:  51, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.852 [0.000, 5.000],  loss: 0.012791, mae: 1.488826, mean_q: 1.802744, mean_eps: 0.191431
  540329/1200000: episode: 765, duration: 18.405s, episode steps: 937, steps per second:  51, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.965 [0.000, 5.000],  loss: 0.013523, mae: 1.483079, mean_q: 1.797504, mean_eps: 0.190210
  540995/1200000: episode: 766, duration: 14.357s, episode steps: 666, steps per second:  46, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.806 [0.000, 5.000],  loss: 0.012901, mae: 1.500965, mean_q: 1.818233, mean_eps: 0.189007
  541850/1200000: episode: 767, duration: 17.972s, episode steps: 855, steps per second:  48, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.013160, mae: 1.485455, mean_q: 1.800330, mean_eps: 0.187867
  542374/1200000: episode: 768, duration: 11.003s, episode steps: 524, steps per second:  48, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.281 [0.000, 5.000],  loss: 0.012771, mae: 1.502241, mean_q: 1.822278, mean_eps: 0.186832
  543314/1200000: episode: 769, duration: 19.128s, episode steps: 940, steps per second:  49, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.013933, mae: 1.486947, mean_q: 1.802910, mean_eps: 0.185734
  544120/1200000: episode: 770, duration: 17.018s, episode steps: 806, steps per second:  47, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.013872, mae: 1.485552, mean_q: 1.799796, mean_eps: 0.184426
  544666/1200000: episode: 771, duration: 11.249s, episode steps: 546, steps per second:  49, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.659 [0.000, 5.000],  loss: 0.012930, mae: 1.498075, mean_q: 1.816008, mean_eps: 0.183412
  545590/1200000: episode: 772, duration: 18.820s, episode steps: 924, steps per second:  49, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.014882, mae: 1.482845, mean_q: 1.796609, mean_eps: 0.182308
  546377/1200000: episode: 773, duration: 16.633s, episode steps: 787, steps per second:  47, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.793 [0.000, 5.000],  loss: 0.013130, mae: 1.502808, mean_q: 1.822287, mean_eps: 0.181024
  547030/1200000: episode: 774, duration: 14.828s, episode steps: 653, steps per second:  44, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.968 [0.000, 5.000],  loss: 0.012645, mae: 1.482130, mean_q: 1.797230, mean_eps: 0.179944
  547807/1200000: episode: 775, duration: 16.994s, episode steps: 777, steps per second:  46, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.950 [0.000, 5.000],  loss: 0.013167, mae: 1.485058, mean_q: 1.800859, mean_eps: 0.178873
  548456/1200000: episode: 776, duration: 13.832s, episode steps: 649, steps per second:  47, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.918 [0.000, 5.000],  loss: 0.011804, mae: 1.477796, mean_q: 1.791895, mean_eps: 0.177805
  549118/1200000: episode: 777, duration: 14.438s, episode steps: 662, steps per second:  46, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.014098, mae: 1.472602, mean_q: 1.784830, mean_eps: 0.176821
  549770/1200000: episode: 778, duration: 14.195s, episode steps: 652, steps per second:  46, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.888 [0.000, 5.000],  loss: 0.011984, mae: 1.478041, mean_q: 1.791075, mean_eps: 0.175834
  550349/1200000: episode: 779, duration: 12.508s, episode steps: 579, steps per second:  46, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.736 [0.000, 5.000],  loss: 0.013373, mae: 1.509956, mean_q: 1.830876, mean_eps: 0.174910
  551117/1200000: episode: 780, duration: 14.806s, episode steps: 768, steps per second:  52, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.115 [0.000, 5.000],  loss: 0.012459, mae: 1.505846, mean_q: 1.824470, mean_eps: 0.173899
  551839/1200000: episode: 781, duration: 13.687s, episode steps: 722, steps per second:  53, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.747 [0.000, 5.000],  loss: 0.013672, mae: 1.521006, mean_q: 1.843017, mean_eps: 0.172783
  552931/1200000: episode: 782, duration: 22.722s, episode steps: 1092, steps per second:  48, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.012317, mae: 1.525613, mean_q: 1.849300, mean_eps: 0.171424
  553682/1200000: episode: 783, duration: 14.771s, episode steps: 751, steps per second:  51, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.936 [0.000, 5.000],  loss: 0.013811, mae: 1.535987, mean_q: 1.861152, mean_eps: 0.170041
  554214/1200000: episode: 784, duration: 10.285s, episode steps: 532, steps per second:  52, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.014312, mae: 1.524267, mean_q: 1.846348, mean_eps: 0.169078
  555263/1200000: episode: 785, duration: 20.529s, episode steps: 1049, steps per second:  51, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.931 [0.000, 5.000],  loss: 0.013155, mae: 1.521327, mean_q: 1.844295, mean_eps: 0.167893
  556370/1200000: episode: 786, duration: 22.174s, episode steps: 1107, steps per second:  50, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.714 [0.000, 5.000],  loss: 0.013286, mae: 1.527898, mean_q: 1.853544, mean_eps: 0.166276
  557681/1200000: episode: 787, duration: 27.444s, episode steps: 1311, steps per second:  48, episode reward: 31.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.103 [0.000, 5.000],  loss: 0.013206, mae: 1.522045, mean_q: 1.845910, mean_eps: 0.164461
  558751/1200000: episode: 788, duration: 25.910s, episode steps: 1070, steps per second:  41, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.228 [0.000, 5.000],  loss: 0.015137, mae: 1.549232, mean_q: 1.878401, mean_eps: 0.162676
  559435/1200000: episode: 789, duration: 16.702s, episode steps: 684, steps per second:  41, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.014686, mae: 1.543109, mean_q: 1.872096, mean_eps: 0.161362
  560115/1200000: episode: 790, duration: 16.344s, episode steps: 680, steps per second:  42, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.014341, mae: 1.543788, mean_q: 1.872841, mean_eps: 0.160339
  561175/1200000: episode: 791, duration: 21.955s, episode steps: 1060, steps per second:  48, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.840 [0.000, 5.000],  loss: 0.012703, mae: 1.558338, mean_q: 1.888949, mean_eps: 0.159034
  562066/1200000: episode: 792, duration: 19.052s, episode steps: 891, steps per second:  47, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.200 [0.000, 5.000],  loss: 0.014592, mae: 1.550694, mean_q: 1.877387, mean_eps: 0.157570
  563043/1200000: episode: 793, duration: 20.665s, episode steps: 977, steps per second:  47, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.163 [0.000, 5.000],  loss: 0.014362, mae: 1.544638, mean_q: 1.870081, mean_eps: 0.156169
  563977/1200000: episode: 794, duration: 20.905s, episode steps: 934, steps per second:  45, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.061 [0.000, 5.000],  loss: 0.016062, mae: 1.547425, mean_q: 1.874285, mean_eps: 0.154735
  564603/1200000: episode: 795, duration: 13.570s, episode steps: 626, steps per second:  46, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.968 [0.000, 5.000],  loss: 0.014682, mae: 1.552964, mean_q: 1.881102, mean_eps: 0.153565
  565437/1200000: episode: 796, duration: 17.378s, episode steps: 834, steps per second:  48, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.133 [0.000, 5.000],  loss: 0.012815, mae: 1.570763, mean_q: 1.904296, mean_eps: 0.152470
  566189/1200000: episode: 797, duration: 16.781s, episode steps: 752, steps per second:  45, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.923 [0.000, 5.000],  loss: 0.012076, mae: 1.548219, mean_q: 1.875375, mean_eps: 0.151279
  567040/1200000: episode: 798, duration: 17.807s, episode steps: 851, steps per second:  48, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.012663, mae: 1.525922, mean_q: 1.847667, mean_eps: 0.150079
  567719/1200000: episode: 799, duration: 13.673s, episode steps: 679, steps per second:  50, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.965 [0.000, 5.000],  loss: 0.014772, mae: 1.549520, mean_q: 1.875705, mean_eps: 0.148933
  568434/1200000: episode: 800, duration: 13.856s, episode steps: 715, steps per second:  52, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.041 [0.000, 5.000],  loss: 0.014666, mae: 1.550314, mean_q: 1.875693, mean_eps: 0.147886
  568976/1200000: episode: 801, duration: 11.193s, episode steps: 542, steps per second:  48, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.945 [0.000, 5.000],  loss: 0.013526, mae: 1.579445, mean_q: 1.911579, mean_eps: 0.146944
  569759/1200000: episode: 802, duration: 16.177s, episode steps: 783, steps per second:  48, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.313 [0.000, 5.000],  loss: 0.015003, mae: 1.545360, mean_q: 1.869937, mean_eps: 0.145951
  570602/1200000: episode: 803, duration: 15.988s, episode steps: 843, steps per second:  53, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.013929, mae: 1.559994, mean_q: 1.888219, mean_eps: 0.144730
  571717/1200000: episode: 804, duration: 20.938s, episode steps: 1115, steps per second:  53, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.110 [0.000, 5.000],  loss: 0.014064, mae: 1.546630, mean_q: 1.871763, mean_eps: 0.143260
  572253/1200000: episode: 805, duration: 9.916s, episode steps: 536, steps per second:  54, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.013878, mae: 1.559512, mean_q: 1.888958, mean_eps: 0.142021
  572934/1200000: episode: 806, duration: 12.794s, episode steps: 681, steps per second:  53, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.771 [0.000, 5.000],  loss: 0.013937, mae: 1.537125, mean_q: 1.861375, mean_eps: 0.141109
  573598/1200000: episode: 807, duration: 12.400s, episode steps: 664, steps per second:  54, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.015107, mae: 1.547907, mean_q: 1.873493, mean_eps: 0.140101
  574004/1200000: episode: 808, duration: 8.067s, episode steps: 406, steps per second:  50, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.014420, mae: 1.561503, mean_q: 1.890682, mean_eps: 0.139300
  574417/1200000: episode: 809, duration: 7.762s, episode steps: 413, steps per second:  53, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.014317, mae: 1.573986, mean_q: 1.907405, mean_eps: 0.138685
  574935/1200000: episode: 810, duration: 9.699s, episode steps: 518, steps per second:  53, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.847 [0.000, 5.000],  loss: 0.013813, mae: 1.561932, mean_q: 1.890901, mean_eps: 0.137986
  575828/1200000: episode: 811, duration: 20.588s, episode steps: 893, steps per second:  43, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.014474, mae: 1.547866, mean_q: 1.876342, mean_eps: 0.136930
  576730/1200000: episode: 812, duration: 18.626s, episode steps: 902, steps per second:  48, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.012975, mae: 1.541929, mean_q: 1.869051, mean_eps: 0.135583
  577383/1200000: episode: 813, duration: 13.917s, episode steps: 653, steps per second:  47, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.988 [0.000, 5.000],  loss: 0.015179, mae: 1.532893, mean_q: 1.856824, mean_eps: 0.134416
  578176/1200000: episode: 814, duration: 19.810s, episode steps: 793, steps per second:  40, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.013020, mae: 1.532966, mean_q: 1.857300, mean_eps: 0.133333
  578672/1200000: episode: 815, duration: 9.803s, episode steps: 496, steps per second:  51, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.011483, mae: 1.543771, mean_q: 1.869776, mean_eps: 0.132367
  579333/1200000: episode: 816, duration: 12.849s, episode steps: 661, steps per second:  51, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.113 [0.000, 5.000],  loss: 0.012794, mae: 1.543399, mean_q: 1.868690, mean_eps: 0.131497
  580378/1200000: episode: 817, duration: 20.689s, episode steps: 1045, steps per second:  51, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.927 [0.000, 5.000],  loss: 0.014998, mae: 1.558359, mean_q: 1.886030, mean_eps: 0.130216
  581045/1200000: episode: 818, duration: 14.403s, episode steps: 667, steps per second:  46, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.228 [0.000, 5.000],  loss: 0.013378, mae: 1.571586, mean_q: 1.903478, mean_eps: 0.128932
  581974/1200000: episode: 819, duration: 19.803s, episode steps: 929, steps per second:  47, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.115 [0.000, 5.000],  loss: 0.012632, mae: 1.573770, mean_q: 1.904523, mean_eps: 0.127735
  582827/1200000: episode: 820, duration: 18.377s, episode steps: 853, steps per second:  46, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.176 [0.000, 5.000],  loss: 0.013980, mae: 1.569278, mean_q: 1.898111, mean_eps: 0.126400
  583705/1200000: episode: 821, duration: 19.055s, episode steps: 878, steps per second:  46, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.013188, mae: 1.573237, mean_q: 1.903441, mean_eps: 0.125101
  584385/1200000: episode: 822, duration: 15.000s, episode steps: 680, steps per second:  45, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.014315, mae: 1.600045, mean_q: 1.937791, mean_eps: 0.123931
  585070/1200000: episode: 823, duration: 14.039s, episode steps: 685, steps per second:  49, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.061 [0.000, 5.000],  loss: 0.014309, mae: 1.588484, mean_q: 1.923565, mean_eps: 0.122908
  585956/1200000: episode: 824, duration: 17.220s, episode steps: 886, steps per second:  51, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.014489, mae: 1.586901, mean_q: 1.921643, mean_eps: 0.121732
  586709/1200000: episode: 825, duration: 15.517s, episode steps: 753, steps per second:  49, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.210 [0.000, 5.000],  loss: 0.013000, mae: 1.583549, mean_q: 1.919711, mean_eps: 0.120502
  587528/1200000: episode: 826, duration: 16.737s, episode steps: 819, steps per second:  49, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.957 [0.000, 5.000],  loss: 0.013260, mae: 1.565473, mean_q: 1.895617, mean_eps: 0.119323
  588186/1200000: episode: 827, duration: 12.859s, episode steps: 658, steps per second:  51, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.012975, mae: 1.568928, mean_q: 1.900740, mean_eps: 0.118216
  589348/1200000: episode: 828, duration: 22.596s, episode steps: 1162, steps per second:  51, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.013707, mae: 1.572335, mean_q: 1.904462, mean_eps: 0.116851
  590005/1200000: episode: 829, duration: 14.234s, episode steps: 657, steps per second:  46, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.464 [0.000, 5.000],  loss: 0.013134, mae: 1.569836, mean_q: 1.902100, mean_eps: 0.115486
  590502/1200000: episode: 830, duration: 11.132s, episode steps: 497, steps per second:  45, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.012890, mae: 1.584739, mean_q: 1.920099, mean_eps: 0.114619
  591231/1200000: episode: 831, duration: 15.281s, episode steps: 729, steps per second:  48, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.624 [0.000, 5.000],  loss: 0.013564, mae: 1.606857, mean_q: 1.947697, mean_eps: 0.113701
  591904/1200000: episode: 832, duration: 14.740s, episode steps: 673, steps per second:  46, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.014985, mae: 1.616461, mean_q: 1.958104, mean_eps: 0.112651
  592781/1200000: episode: 833, duration: 18.917s, episode steps: 877, steps per second:  46, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.929 [0.000, 5.000],  loss: 0.014512, mae: 1.594572, mean_q: 1.930950, mean_eps: 0.111487
  594184/1200000: episode: 834, duration: 29.156s, episode steps: 1403, steps per second:  48, episode reward: 29.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.149 [0.000, 5.000],  loss: 0.013681, mae: 1.596105, mean_q: 1.932930, mean_eps: 0.109777
  594994/1200000: episode: 835, duration: 16.688s, episode steps: 810, steps per second:  49, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.205 [0.000, 5.000],  loss: 0.014038, mae: 1.598719, mean_q: 1.937849, mean_eps: 0.108118
  595968/1200000: episode: 836, duration: 18.953s, episode steps: 974, steps per second:  51, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.906 [0.000, 5.000],  loss: 0.015544, mae: 1.579255, mean_q: 1.912762, mean_eps: 0.106780
  596925/1200000: episode: 837, duration: 18.675s, episode steps: 957, steps per second:  51, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.911 [0.000, 5.000],  loss: 0.014243, mae: 1.597651, mean_q: 1.936047, mean_eps: 0.105331
  597936/1200000: episode: 838, duration: 19.672s, episode steps: 1011, steps per second:  51, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.786 [0.000, 5.000],  loss: 0.012488, mae: 1.594476, mean_q: 1.931970, mean_eps: 0.103855
  598976/1200000: episode: 839, duration: 21.171s, episode steps: 1040, steps per second:  49, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.730 [0.000, 5.000],  loss: 0.014829, mae: 1.597773, mean_q: 1.934149, mean_eps: 0.102319
  599626/1200000: episode: 840, duration: 13.059s, episode steps: 650, steps per second:  50, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.717 [0.000, 5.000],  loss: 0.014919, mae: 1.585082, mean_q: 1.919733, mean_eps: 0.101050
  600587/1200000: episode: 841, duration: 18.798s, episode steps: 961, steps per second:  51, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.656 [0.000, 5.000],  loss: 0.013824, mae: 1.604963, mean_q: 1.942271, mean_eps: 0.100109
  601274/1200000: episode: 842, duration: 14.010s, episode steps: 687, steps per second:  49, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.014963, mae: 1.633353, mean_q: 1.977469, mean_eps: 0.100000
  602133/1200000: episode: 843, duration: 17.401s, episode steps: 859, steps per second:  49, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.705 [0.000, 5.000],  loss: 0.014391, mae: 1.593433, mean_q: 1.928288, mean_eps: 0.100000
  602635/1200000: episode: 844, duration: 9.868s, episode steps: 502, steps per second:  51, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.777 [0.000, 5.000],  loss: 0.012688, mae: 1.613957, mean_q: 1.954170, mean_eps: 0.100000
  603277/1200000: episode: 845, duration: 12.245s, episode steps: 642, steps per second:  52, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.997 [0.000, 5.000],  loss: 0.013284, mae: 1.602248, mean_q: 1.939430, mean_eps: 0.100000
  604071/1200000: episode: 846, duration: 14.842s, episode steps: 794, steps per second:  53, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.014557, mae: 1.606080, mean_q: 1.945718, mean_eps: 0.100000
  604591/1200000: episode: 847, duration: 9.962s, episode steps: 520, steps per second:  52, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.885 [0.000, 5.000],  loss: 0.012418, mae: 1.576414, mean_q: 1.909311, mean_eps: 0.100000
  605502/1200000: episode: 848, duration: 17.923s, episode steps: 911, steps per second:  51, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.089 [0.000, 5.000],  loss: 0.013898, mae: 1.592349, mean_q: 1.927311, mean_eps: 0.100000
  606456/1200000: episode: 849, duration: 18.595s, episode steps: 954, steps per second:  51, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.013338, mae: 1.607078, mean_q: 1.947756, mean_eps: 0.100000
  607177/1200000: episode: 850, duration: 14.103s, episode steps: 721, steps per second:  51, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.724 [0.000, 5.000],  loss: 0.015258, mae: 1.599770, mean_q: 1.937847, mean_eps: 0.100000
  607856/1200000: episode: 851, duration: 13.588s, episode steps: 679, steps per second:  50, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.577 [0.000, 5.000],  loss: 0.013154, mae: 1.627744, mean_q: 1.972248, mean_eps: 0.100000
  608678/1200000: episode: 852, duration: 16.260s, episode steps: 822, steps per second:  51, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.704 [0.000, 5.000],  loss: 0.012744, mae: 1.608993, mean_q: 1.948907, mean_eps: 0.100000
  609825/1200000: episode: 853, duration: 23.540s, episode steps: 1147, steps per second:  49, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.061 [0.000, 5.000],  loss: 0.014102, mae: 1.595314, mean_q: 1.932532, mean_eps: 0.100000
  610473/1200000: episode: 854, duration: 13.405s, episode steps: 648, steps per second:  48, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.369 [0.000, 5.000],  loss: 0.012460, mae: 1.636165, mean_q: 1.983799, mean_eps: 0.100000
  611124/1200000: episode: 855, duration: 13.143s, episode steps: 651, steps per second:  50, episode reward:  4.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.318 [0.000, 5.000],  loss: 0.013846, mae: 1.644211, mean_q: 1.992825, mean_eps: 0.100000
  611794/1200000: episode: 856, duration: 14.519s, episode steps: 670, steps per second:  46, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.840 [0.000, 5.000],  loss: 0.015629, mae: 1.643534, mean_q: 1.989046, mean_eps: 0.100000
  612630/1200000: episode: 857, duration: 16.565s, episode steps: 836, steps per second:  50, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.358 [0.000, 5.000],  loss: 0.012895, mae: 1.617761, mean_q: 1.958203, mean_eps: 0.100000
  613478/1200000: episode: 858, duration: 17.099s, episode steps: 848, steps per second:  50, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.732 [0.000, 5.000],  loss: 0.012634, mae: 1.664138, mean_q: 2.013876, mean_eps: 0.100000
  614311/1200000: episode: 859, duration: 17.096s, episode steps: 833, steps per second:  49, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.181 [0.000, 5.000],  loss: 0.014237, mae: 1.644513, mean_q: 1.987615, mean_eps: 0.100000
  615165/1200000: episode: 860, duration: 16.907s, episode steps: 854, steps per second:  51, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.635 [0.000, 5.000],  loss: 0.014625, mae: 1.648195, mean_q: 1.995429, mean_eps: 0.100000
  616154/1200000: episode: 861, duration: 19.609s, episode steps: 989, steps per second:  50, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.089 [0.000, 5.000],  loss: 0.014954, mae: 1.656154, mean_q: 2.004649, mean_eps: 0.100000
  616816/1200000: episode: 862, duration: 14.054s, episode steps: 662, steps per second:  47, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.639 [0.000, 5.000],  loss: 0.014945, mae: 1.654925, mean_q: 2.002108, mean_eps: 0.100000
  617638/1200000: episode: 863, duration: 17.215s, episode steps: 822, steps per second:  48, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.119 [0.000, 5.000],  loss: 0.015372, mae: 1.663379, mean_q: 2.012892, mean_eps: 0.100000
  618271/1200000: episode: 864, duration: 12.462s, episode steps: 633, steps per second:  51, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.125 [0.000, 5.000],  loss: 0.014705, mae: 1.674653, mean_q: 2.027486, mean_eps: 0.100000
  619119/1200000: episode: 865, duration: 17.026s, episode steps: 848, steps per second:  50, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.471 [0.000, 5.000],  loss: 0.014670, mae: 1.650433, mean_q: 1.998344, mean_eps: 0.100000
  620110/1200000: episode: 866, duration: 20.356s, episode steps: 991, steps per second:  49, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.206 [0.000, 5.000],  loss: 0.015402, mae: 1.667929, mean_q: 2.017754, mean_eps: 0.100000
  620657/1200000: episode: 867, duration: 10.510s, episode steps: 547, steps per second:  52, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.014213, mae: 1.637342, mean_q: 1.981519, mean_eps: 0.100000
  621355/1200000: episode: 868, duration: 13.228s, episode steps: 698, steps per second:  53, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.013509, mae: 1.630531, mean_q: 1.973897, mean_eps: 0.100000
  622200/1200000: episode: 869, duration: 16.381s, episode steps: 845, steps per second:  52, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.014169, mae: 1.646322, mean_q: 1.992089, mean_eps: 0.100000
  623281/1200000: episode: 870, duration: 22.471s, episode steps: 1081, steps per second:  48, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.558 [0.000, 5.000],  loss: 0.012260, mae: 1.634845, mean_q: 1.979013, mean_eps: 0.100000
  624144/1200000: episode: 871, duration: 17.990s, episode steps: 863, steps per second:  48, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.885 [0.000, 5.000],  loss: 0.014067, mae: 1.619978, mean_q: 1.959917, mean_eps: 0.100000
  625088/1200000: episode: 872, duration: 19.576s, episode steps: 944, steps per second:  48, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.028 [0.000, 5.000],  loss: 0.014841, mae: 1.619505, mean_q: 1.958225, mean_eps: 0.100000
  625713/1200000: episode: 873, duration: 12.980s, episode steps: 625, steps per second:  48, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.014599, mae: 1.626421, mean_q: 1.968353, mean_eps: 0.100000
  626608/1200000: episode: 874, duration: 18.303s, episode steps: 895, steps per second:  49, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.013575, mae: 1.634775, mean_q: 1.978321, mean_eps: 0.100000
  627529/1200000: episode: 875, duration: 33.429s, episode steps: 921, steps per second:  28, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.013032, mae: 1.625023, mean_q: 1.966649, mean_eps: 0.100000
  628237/1200000: episode: 876, duration: 15.283s, episode steps: 708, steps per second:  46, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.012579, mae: 1.638249, mean_q: 1.983472, mean_eps: 0.100000
  628880/1200000: episode: 877, duration: 13.325s, episode steps: 643, steps per second:  48, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.309 [0.000, 5.000],  loss: 0.012777, mae: 1.638239, mean_q: 1.983086, mean_eps: 0.100000
  629711/1200000: episode: 878, duration: 16.643s, episode steps: 831, steps per second:  50, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.271 [0.000, 5.000],  loss: 0.014365, mae: 1.649949, mean_q: 1.997055, mean_eps: 0.100000
  630801/1200000: episode: 879, duration: 21.864s, episode steps: 1090, steps per second:  50, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.012605, mae: 1.643975, mean_q: 1.990841, mean_eps: 0.100000
  631556/1200000: episode: 880, duration: 14.789s, episode steps: 755, steps per second:  51, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.014237, mae: 1.638262, mean_q: 1.983389, mean_eps: 0.100000
  632191/1200000: episode: 881, duration: 12.127s, episode steps: 635, steps per second:  52, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.980 [0.000, 5.000],  loss: 0.013005, mae: 1.667360, mean_q: 2.018934, mean_eps: 0.100000
  633157/1200000: episode: 882, duration: 18.344s, episode steps: 966, steps per second:  53, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.903 [0.000, 5.000],  loss: 0.012900, mae: 1.659328, mean_q: 2.009227, mean_eps: 0.100000
  634075/1200000: episode: 883, duration: 18.346s, episode steps: 918, steps per second:  50, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.015942, mae: 1.670290, mean_q: 2.020375, mean_eps: 0.100000
  634574/1200000: episode: 884, duration: 10.338s, episode steps: 499, steps per second:  48, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.014550, mae: 1.678528, mean_q: 2.029607, mean_eps: 0.100000
  635715/1200000: episode: 885, duration: 22.445s, episode steps: 1141, steps per second:  51, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.082 [0.000, 5.000],  loss: 0.013002, mae: 1.659930, mean_q: 2.007652, mean_eps: 0.100000
  636360/1200000: episode: 886, duration: 13.140s, episode steps: 645, steps per second:  49, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.209 [0.000, 5.000],  loss: 0.014971, mae: 1.644624, mean_q: 1.988458, mean_eps: 0.100000
  637158/1200000: episode: 887, duration: 17.093s, episode steps: 798, steps per second:  47, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.012772, mae: 1.666837, mean_q: 2.016280, mean_eps: 0.100000
  637955/1200000: episode: 888, duration: 16.223s, episode steps: 797, steps per second:  49, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.854 [0.000, 5.000],  loss: 0.012008, mae: 1.658000, mean_q: 2.006434, mean_eps: 0.100000
  638594/1200000: episode: 889, duration: 12.497s, episode steps: 639, steps per second:  51, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.854 [0.000, 5.000],  loss: 0.013308, mae: 1.674597, mean_q: 2.026557, mean_eps: 0.100000
  639276/1200000: episode: 890, duration: 13.008s, episode steps: 682, steps per second:  52, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.214 [0.000, 5.000],  loss: 0.012007, mae: 1.662574, mean_q: 2.011730, mean_eps: 0.100000
  640030/1200000: episode: 891, duration: 15.689s, episode steps: 754, steps per second:  48, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.013201, mae: 1.672178, mean_q: 2.022596, mean_eps: 0.100000
  640829/1200000: episode: 892, duration: 16.011s, episode steps: 799, steps per second:  50, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.150 [0.000, 5.000],  loss: 0.013220, mae: 1.691639, mean_q: 2.044264, mean_eps: 0.100000
  641555/1200000: episode: 893, duration: 14.258s, episode steps: 726, steps per second:  51, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.013927, mae: 1.682446, mean_q: 2.033053, mean_eps: 0.100000
  642431/1200000: episode: 894, duration: 17.239s, episode steps: 876, steps per second:  51, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.704 [0.000, 5.000],  loss: 0.013341, mae: 1.683809, mean_q: 2.035639, mean_eps: 0.100000
  643551/1200000: episode: 895, duration: 22.246s, episode steps: 1120, steps per second:  50, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.187 [0.000, 5.000],  loss: 0.013124, mae: 1.686694, mean_q: 2.038635, mean_eps: 0.100000
  644349/1200000: episode: 896, duration: 16.747s, episode steps: 798, steps per second:  48, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.010 [0.000, 5.000],  loss: 0.012777, mae: 1.669348, mean_q: 2.017914, mean_eps: 0.100000
  645603/1200000: episode: 897, duration: 26.058s, episode steps: 1254, steps per second:  48, episode reward: 25.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.175 [0.000, 5.000],  loss: 0.013321, mae: 1.690521, mean_q: 2.043995, mean_eps: 0.100000
  646122/1200000: episode: 898, duration: 11.119s, episode steps: 519, steps per second:  47, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.033 [0.000, 5.000],  loss: 0.013983, mae: 1.663765, mean_q: 2.011550, mean_eps: 0.100000
  646594/1200000: episode: 899, duration: 9.702s, episode steps: 472, steps per second:  49, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.157 [0.000, 5.000],  loss: 0.015228, mae: 1.677168, mean_q: 2.026180, mean_eps: 0.100000
  647427/1200000: episode: 900, duration: 17.518s, episode steps: 833, steps per second:  48, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.014210, mae: 1.691171, mean_q: 2.045286, mean_eps: 0.100000
  648312/1200000: episode: 901, duration: 18.230s, episode steps: 885, steps per second:  49, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.014408, mae: 1.684867, mean_q: 2.038006, mean_eps: 0.100000
  648962/1200000: episode: 902, duration: 13.846s, episode steps: 650, steps per second:  47, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.257 [0.000, 5.000],  loss: 0.012828, mae: 1.685001, mean_q: 2.039393, mean_eps: 0.100000
  649703/1200000: episode: 903, duration: 15.140s, episode steps: 741, steps per second:  49, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.821 [0.000, 5.000],  loss: 0.016138, mae: 1.674252, mean_q: 2.025895, mean_eps: 0.100000
  650360/1200000: episode: 904, duration: 13.498s, episode steps: 657, steps per second:  49, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.014441, mae: 1.675795, mean_q: 2.029408, mean_eps: 0.100000
  651234/1200000: episode: 905, duration: 18.575s, episode steps: 874, steps per second:  47, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.636 [0.000, 5.000],  loss: 0.012761, mae: 1.694586, mean_q: 2.051366, mean_eps: 0.100000
  651987/1200000: episode: 906, duration: 15.980s, episode steps: 753, steps per second:  47, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.264 [0.000, 5.000],  loss: 0.013378, mae: 1.692928, mean_q: 2.048342, mean_eps: 0.100000
  652690/1200000: episode: 907, duration: 15.231s, episode steps: 703, steps per second:  46, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.013409, mae: 1.678086, mean_q: 2.029613, mean_eps: 0.100000
  653362/1200000: episode: 908, duration: 14.170s, episode steps: 672, steps per second:  47, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.012472, mae: 1.698488, mean_q: 2.054842, mean_eps: 0.100000
  654444/1200000: episode: 909, duration: 23.220s, episode steps: 1082, steps per second:  47, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.012397, mae: 1.688838, mean_q: 2.042921, mean_eps: 0.100000
  655372/1200000: episode: 910, duration: 19.383s, episode steps: 928, steps per second:  48, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.200 [0.000, 5.000],  loss: 0.013627, mae: 1.700235, mean_q: 2.055875, mean_eps: 0.100000
  655985/1200000: episode: 911, duration: 12.319s, episode steps: 613, steps per second:  50, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.013334, mae: 1.696257, mean_q: 2.051650, mean_eps: 0.100000
  656859/1200000: episode: 912, duration: 17.349s, episode steps: 874, steps per second:  50, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.088 [0.000, 5.000],  loss: 0.013781, mae: 1.672980, mean_q: 2.022314, mean_eps: 0.100000
  657981/1200000: episode: 913, duration: 24.810s, episode steps: 1122, steps per second:  45, episode reward: 31.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.136 [0.000, 5.000],  loss: 0.012570, mae: 1.686542, mean_q: 2.037627, mean_eps: 0.100000
  658692/1200000: episode: 914, duration: 14.274s, episode steps: 711, steps per second:  50, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.256 [0.000, 5.000],  loss: 0.012888, mae: 1.698975, mean_q: 2.054057, mean_eps: 0.100000
  659505/1200000: episode: 915, duration: 16.160s, episode steps: 813, steps per second:  50, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.256 [0.000, 5.000],  loss: 0.015189, mae: 1.690409, mean_q: 2.042639, mean_eps: 0.100000
  660241/1200000: episode: 916, duration: 14.446s, episode steps: 736, steps per second:  51, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.610 [0.000, 5.000],  loss: 0.011069, mae: 1.680785, mean_q: 2.030911, mean_eps: 0.100000
  661277/1200000: episode: 917, duration: 20.319s, episode steps: 1036, steps per second:  51, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.162 [0.000, 5.000],  loss: 0.014072, mae: 1.698701, mean_q: 2.050111, mean_eps: 0.100000
  661775/1200000: episode: 918, duration: 10.215s, episode steps: 498, steps per second:  49, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.679 [0.000, 5.000],  loss: 0.013499, mae: 1.693028, mean_q: 2.045062, mean_eps: 0.100000
  662297/1200000: episode: 919, duration: 10.965s, episode steps: 522, steps per second:  48, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.117 [0.000, 5.000],  loss: 0.013829, mae: 1.703154, mean_q: 2.058622, mean_eps: 0.100000
  662961/1200000: episode: 920, duration: 15.275s, episode steps: 664, steps per second:  43, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.744 [0.000, 5.000],  loss: 0.012243, mae: 1.706591, mean_q: 2.063176, mean_eps: 0.100000
  663692/1200000: episode: 921, duration: 17.327s, episode steps: 731, steps per second:  42, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.067 [0.000, 5.000],  loss: 0.013116, mae: 1.671259, mean_q: 2.018570, mean_eps: 0.100000
  664783/1200000: episode: 922, duration: 23.857s, episode steps: 1091, steps per second:  46, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.936 [0.000, 5.000],  loss: 0.013823, mae: 1.684010, mean_q: 2.035171, mean_eps: 0.100000
  665299/1200000: episode: 923, duration: 11.122s, episode steps: 516, steps per second:  46, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: 0.015103, mae: 1.680493, mean_q: 2.030165, mean_eps: 0.100000
  666797/1200000: episode: 924, duration: 33.127s, episode steps: 1498, steps per second:  45, episode reward: 27.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.790 [0.000, 5.000],  loss: 0.014312, mae: 1.689981, mean_q: 2.041802, mean_eps: 0.100000
  667562/1200000: episode: 925, duration: 15.996s, episode steps: 765, steps per second:  48, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.014139, mae: 1.683397, mean_q: 2.032621, mean_eps: 0.100000
  668211/1200000: episode: 926, duration: 14.380s, episode steps: 649, steps per second:  45, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.015015, mae: 1.668526, mean_q: 2.014483, mean_eps: 0.100000
  669104/1200000: episode: 927, duration: 29.615s, episode steps: 893, steps per second:  30, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.946 [0.000, 5.000],  loss: 0.012772, mae: 1.672182, mean_q: 2.020494, mean_eps: 0.100000
  669656/1200000: episode: 928, duration: 23.100s, episode steps: 552, steps per second:  24, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.793 [0.000, 5.000],  loss: 0.012685, mae: 1.674766, mean_q: 2.022942, mean_eps: 0.100000
  670286/1200000: episode: 929, duration: 14.481s, episode steps: 630, steps per second:  44, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.010917, mae: 1.698293, mean_q: 2.052755, mean_eps: 0.100000
  671014/1200000: episode: 930, duration: 15.702s, episode steps: 728, steps per second:  46, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.783 [0.000, 5.000],  loss: 0.012731, mae: 1.715159, mean_q: 2.072460, mean_eps: 0.100000
  671833/1200000: episode: 931, duration: 16.623s, episode steps: 819, steps per second:  49, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.103 [0.000, 5.000],  loss: 0.015422, mae: 1.700875, mean_q: 2.053486, mean_eps: 0.100000
  672546/1200000: episode: 932, duration: 14.129s, episode steps: 713, steps per second:  50, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.013355, mae: 1.705700, mean_q: 2.059614, mean_eps: 0.100000
  673113/1200000: episode: 933, duration: 12.010s, episode steps: 567, steps per second:  47, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.206 [0.000, 5.000],  loss: 0.016472, mae: 1.700854, mean_q: 2.052603, mean_eps: 0.100000
  673810/1200000: episode: 934, duration: 15.014s, episode steps: 697, steps per second:  46, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.012835, mae: 1.711536, mean_q: 2.066484, mean_eps: 0.100000
  674558/1200000: episode: 935, duration: 15.531s, episode steps: 748, steps per second:  48, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.016940, mae: 1.715987, mean_q: 2.069485, mean_eps: 0.100000
  675483/1200000: episode: 936, duration: 18.886s, episode steps: 925, steps per second:  49, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.866 [0.000, 5.000],  loss: 0.012973, mae: 1.721649, mean_q: 2.078356, mean_eps: 0.100000
  676502/1200000: episode: 937, duration: 23.437s, episode steps: 1019, steps per second:  43, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.044 [0.000, 5.000],  loss: 0.014275, mae: 1.712788, mean_q: 2.068810, mean_eps: 0.100000
  677253/1200000: episode: 938, duration: 16.248s, episode steps: 751, steps per second:  46, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.306 [0.000, 5.000],  loss: 0.014772, mae: 1.710239, mean_q: 2.064832, mean_eps: 0.100000
  678015/1200000: episode: 939, duration: 16.797s, episode steps: 762, steps per second:  45, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.165 [0.000, 5.000],  loss: 0.013259, mae: 1.713240, mean_q: 2.069983, mean_eps: 0.100000
  678964/1200000: episode: 940, duration: 22.364s, episode steps: 949, steps per second:  42, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.184 [0.000, 5.000],  loss: 0.013767, mae: 1.704643, mean_q: 2.058785, mean_eps: 0.100000
  679795/1200000: episode: 941, duration: 18.188s, episode steps: 831, steps per second:  46, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.012784, mae: 1.719962, mean_q: 2.078167, mean_eps: 0.100000
  680528/1200000: episode: 942, duration: 15.527s, episode steps: 733, steps per second:  47, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.844 [0.000, 5.000],  loss: 0.013474, mae: 1.726212, mean_q: 2.086718, mean_eps: 0.100000
  681246/1200000: episode: 943, duration: 15.183s, episode steps: 718, steps per second:  47, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.016061, mae: 1.740977, mean_q: 2.103456, mean_eps: 0.100000
  682019/1200000: episode: 944, duration: 16.467s, episode steps: 773, steps per second:  47, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.177 [0.000, 5.000],  loss: 0.013363, mae: 1.748230, mean_q: 2.112135, mean_eps: 0.100000
  682767/1200000: episode: 945, duration: 17.321s, episode steps: 748, steps per second:  43, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.698 [0.000, 5.000],  loss: 0.014937, mae: 1.746870, mean_q: 2.109534, mean_eps: 0.100000
  683497/1200000: episode: 946, duration: 15.631s, episode steps: 730, steps per second:  47, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.275 [0.000, 5.000],  loss: 0.014098, mae: 1.718977, mean_q: 2.076716, mean_eps: 0.100000
  684210/1200000: episode: 947, duration: 16.219s, episode steps: 713, steps per second:  44, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.919 [0.000, 5.000],  loss: 0.012673, mae: 1.733758, mean_q: 2.095385, mean_eps: 0.100000
  684816/1200000: episode: 948, duration: 14.034s, episode steps: 606, steps per second:  43, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.011568, mae: 1.708610, mean_q: 2.064794, mean_eps: 0.100000
  685452/1200000: episode: 949, duration: 14.001s, episode steps: 636, steps per second:  45, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.954 [0.000, 5.000],  loss: 0.014118, mae: 1.736284, mean_q: 2.097815, mean_eps: 0.100000
  685907/1200000: episode: 950, duration: 10.654s, episode steps: 455, steps per second:  43, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.077 [0.000, 5.000],  loss: 0.012849, mae: 1.771961, mean_q: 2.140702, mean_eps: 0.100000
  686774/1200000: episode: 951, duration: 20.291s, episode steps: 867, steps per second:  43, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.601 [0.000, 5.000],  loss: 0.013321, mae: 1.734293, mean_q: 2.096085, mean_eps: 0.100000
  687614/1200000: episode: 952, duration: 18.593s, episode steps: 840, steps per second:  45, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.898 [0.000, 5.000],  loss: 0.014859, mae: 1.721909, mean_q: 2.079562, mean_eps: 0.100000
  688192/1200000: episode: 953, duration: 11.610s, episode steps: 578, steps per second:  50, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.803 [0.000, 5.000],  loss: 0.013596, mae: 1.747227, mean_q: 2.110862, mean_eps: 0.100000
  689170/1200000: episode: 954, duration: 21.375s, episode steps: 978, steps per second:  46, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.885 [0.000, 5.000],  loss: 0.013195, mae: 1.711841, mean_q: 2.067239, mean_eps: 0.100000
  689844/1200000: episode: 955, duration: 16.311s, episode steps: 674, steps per second:  41, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.013261, mae: 1.725354, mean_q: 2.084011, mean_eps: 0.100000
  690473/1200000: episode: 956, duration: 14.522s, episode steps: 629, steps per second:  43, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.012288, mae: 1.739153, mean_q: 2.100540, mean_eps: 0.100000
  691183/1200000: episode: 957, duration: 14.804s, episode steps: 710, steps per second:  48, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.154 [0.000, 5.000],  loss: 0.013540, mae: 1.735058, mean_q: 2.095176, mean_eps: 0.100000
  692073/1200000: episode: 958, duration: 18.039s, episode steps: 890, steps per second:  49, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.014315, mae: 1.761182, mean_q: 2.126280, mean_eps: 0.100000
  692835/1200000: episode: 959, duration: 16.147s, episode steps: 762, steps per second:  47, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.138 [0.000, 5.000],  loss: 0.012966, mae: 1.736741, mean_q: 2.098574, mean_eps: 0.100000
  693839/1200000: episode: 960, duration: 21.743s, episode steps: 1004, steps per second:  46, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.198 [0.000, 5.000],  loss: 0.013994, mae: 1.752381, mean_q: 2.115325, mean_eps: 0.100000
  694483/1200000: episode: 961, duration: 14.422s, episode steps: 644, steps per second:  45, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.734 [0.000, 5.000],  loss: 0.015763, mae: 1.749272, mean_q: 2.111912, mean_eps: 0.100000
  695334/1200000: episode: 962, duration: 20.623s, episode steps: 851, steps per second:  41, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.931 [0.000, 5.000],  loss: 0.013930, mae: 1.743958, mean_q: 2.105245, mean_eps: 0.100000
  696054/1200000: episode: 963, duration: 15.415s, episode steps: 720, steps per second:  47, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.257 [0.000, 5.000],  loss: 0.013636, mae: 1.763007, mean_q: 2.128116, mean_eps: 0.100000
  696636/1200000: episode: 964, duration: 12.913s, episode steps: 582, steps per second:  45, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.013277, mae: 1.741420, mean_q: 2.103923, mean_eps: 0.100000
  697561/1200000: episode: 965, duration: 20.348s, episode steps: 925, steps per second:  45, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.014051, mae: 1.742109, mean_q: 2.103661, mean_eps: 0.100000
  698388/1200000: episode: 966, duration: 17.146s, episode steps: 827, steps per second:  48, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.015383, mae: 1.729067, mean_q: 2.087881, mean_eps: 0.100000
  699043/1200000: episode: 967, duration: 13.637s, episode steps: 655, steps per second:  48, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.876 [0.000, 5.000],  loss: 0.013370, mae: 1.754715, mean_q: 2.118419, mean_eps: 0.100000
  699676/1200000: episode: 968, duration: 13.364s, episode steps: 633, steps per second:  47, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.716 [0.000, 5.000],  loss: 0.012166, mae: 1.768719, mean_q: 2.135041, mean_eps: 0.100000
  700577/1200000: episode: 969, duration: 22.433s, episode steps: 901, steps per second:  40, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.011999, mae: 1.763576, mean_q: 2.130970, mean_eps: 0.100000
  701209/1200000: episode: 970, duration: 15.027s, episode steps: 632, steps per second:  42, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.066 [0.000, 5.000],  loss: 0.017450, mae: 1.752378, mean_q: 2.116495, mean_eps: 0.100000
  702009/1200000: episode: 971, duration: 18.301s, episode steps: 800, steps per second:  44, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.014 [0.000, 5.000],  loss: 0.016872, mae: 1.787388, mean_q: 2.158232, mean_eps: 0.100000
  703010/1200000: episode: 972, duration: 23.552s, episode steps: 1001, steps per second:  43, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.017 [0.000, 5.000],  loss: 0.013802, mae: 1.763469, mean_q: 2.130931, mean_eps: 0.100000
  703803/1200000: episode: 973, duration: 17.874s, episode steps: 793, steps per second:  44, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.017219, mae: 1.761176, mean_q: 2.125898, mean_eps: 0.100000
  704377/1200000: episode: 974, duration: 12.573s, episode steps: 574, steps per second:  46, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.775 [0.000, 5.000],  loss: 0.015374, mae: 1.754228, mean_q: 2.117779, mean_eps: 0.100000
  705229/1200000: episode: 975, duration: 18.737s, episode steps: 852, steps per second:  45, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.015426, mae: 1.786797, mean_q: 2.159156, mean_eps: 0.100000
  706016/1200000: episode: 976, duration: 19.335s, episode steps: 787, steps per second:  41, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.186 [0.000, 5.000],  loss: 0.014090, mae: 1.763776, mean_q: 2.131979, mean_eps: 0.100000
  706829/1200000: episode: 977, duration: 17.674s, episode steps: 813, steps per second:  46, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.013410, mae: 1.766577, mean_q: 2.134105, mean_eps: 0.100000
  707562/1200000: episode: 978, duration: 15.155s, episode steps: 733, steps per second:  48, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.013941, mae: 1.771549, mean_q: 2.138740, mean_eps: 0.100000
  708008/1200000: episode: 979, duration: 9.459s, episode steps: 446, steps per second:  47, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.002 [0.000, 5.000],  loss: 0.015945, mae: 1.749485, mean_q: 2.112120, mean_eps: 0.100000
  708649/1200000: episode: 980, duration: 13.458s, episode steps: 641, steps per second:  48, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.015789, mae: 1.772011, mean_q: 2.140767, mean_eps: 0.100000
  709281/1200000: episode: 981, duration: 13.053s, episode steps: 632, steps per second:  48, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.076 [0.000, 5.000],  loss: 0.012246, mae: 1.739620, mean_q: 2.102257, mean_eps: 0.100000
  710208/1200000: episode: 982, duration: 20.043s, episode steps: 927, steps per second:  46, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.014565, mae: 1.764475, mean_q: 2.131964, mean_eps: 0.100000
  711055/1200000: episode: 983, duration: 18.224s, episode steps: 847, steps per second:  46, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.103 [0.000, 5.000],  loss: 0.013340, mae: 1.782269, mean_q: 2.152667, mean_eps: 0.100000
  711455/1200000: episode: 984, duration: 9.115s, episode steps: 400, steps per second:  44, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.016466, mae: 1.805049, mean_q: 2.180369, mean_eps: 0.100000
  712198/1200000: episode: 985, duration: 16.362s, episode steps: 743, steps per second:  45, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.003 [0.000, 5.000],  loss: 0.013996, mae: 1.806495, mean_q: 2.181955, mean_eps: 0.100000
  712780/1200000: episode: 986, duration: 12.808s, episode steps: 582, steps per second:  45, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.921 [0.000, 5.000],  loss: 0.013899, mae: 1.803841, mean_q: 2.177987, mean_eps: 0.100000
  713415/1200000: episode: 987, duration: 14.094s, episode steps: 635, steps per second:  45, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.235 [0.000, 5.000],  loss: 0.016002, mae: 1.805002, mean_q: 2.179733, mean_eps: 0.100000
  714040/1200000: episode: 988, duration: 14.416s, episode steps: 625, steps per second:  43, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.107 [0.000, 5.000],  loss: 0.016461, mae: 1.786559, mean_q: 2.158136, mean_eps: 0.100000
  714730/1200000: episode: 989, duration: 15.527s, episode steps: 690, steps per second:  44, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.129 [0.000, 5.000],  loss: 0.014305, mae: 1.806126, mean_q: 2.182236, mean_eps: 0.100000
  715464/1200000: episode: 990, duration: 15.792s, episode steps: 734, steps per second:  46, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.013432, mae: 1.798046, mean_q: 2.172245, mean_eps: 0.100000
  716334/1200000: episode: 991, duration: 19.410s, episode steps: 870, steps per second:  45, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.215 [0.000, 5.000],  loss: 0.015658, mae: 1.793165, mean_q: 2.165384, mean_eps: 0.100000
  717095/1200000: episode: 992, duration: 17.686s, episode steps: 761, steps per second:  43, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.669 [0.000, 5.000],  loss: 0.014681, mae: 1.782800, mean_q: 2.152640, mean_eps: 0.100000
  718046/1200000: episode: 993, duration: 22.513s, episode steps: 951, steps per second:  42, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.907 [0.000, 5.000],  loss: 0.014895, mae: 1.786222, mean_q: 2.156250, mean_eps: 0.100000
  718694/1200000: episode: 994, duration: 15.149s, episode steps: 648, steps per second:  43, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.017230, mae: 1.772694, mean_q: 2.138618, mean_eps: 0.100000
  719678/1200000: episode: 995, duration: 24.010s, episode steps: 984, steps per second:  41, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.762 [0.000, 5.000],  loss: 0.013206, mae: 1.782528, mean_q: 2.151793, mean_eps: 0.100000
  720959/1200000: episode: 996, duration: 26.612s, episode steps: 1281, steps per second:  48, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.222 [0.000, 5.000],  loss: 0.012699, mae: 1.786535, mean_q: 2.156167, mean_eps: 0.100000
  721610/1200000: episode: 997, duration: 15.010s, episode steps: 651, steps per second:  43, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.014591, mae: 1.787811, mean_q: 2.156144, mean_eps: 0.100000
  722264/1200000: episode: 998, duration: 16.100s, episode steps: 654, steps per second:  41, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.121 [0.000, 5.000],  loss: 0.013049, mae: 1.778294, mean_q: 2.146656, mean_eps: 0.100000
  723432/1200000: episode: 999, duration: 24.633s, episode steps: 1168, steps per second:  47, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.323 [0.000, 5.000],  loss: 0.014299, mae: 1.785932, mean_q: 2.155835, mean_eps: 0.100000
  724400/1200000: episode: 1000, duration: 20.712s, episode steps: 968, steps per second:  47, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.954 [0.000, 5.000],  loss: 0.014063, mae: 1.782420, mean_q: 2.152098, mean_eps: 0.100000
  725437/1200000: episode: 1001, duration: 22.293s, episode steps: 1037, steps per second:  47, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.012 [0.000, 5.000],  loss: 0.012919, mae: 1.776929, mean_q: 2.144560, mean_eps: 0.100000
  726549/1200000: episode: 1002, duration: 26.101s, episode steps: 1112, steps per second:  43, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.012916, mae: 1.774410, mean_q: 2.140827, mean_eps: 0.100000
  727495/1200000: episode: 1003, duration: 23.745s, episode steps: 946, steps per second:  40, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.013526, mae: 1.805350, mean_q: 2.178784, mean_eps: 0.100000
  728050/1200000: episode: 1004, duration: 13.949s, episode steps: 555, steps per second:  40, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.029 [0.000, 5.000],  loss: 0.013892, mae: 1.770641, mean_q: 2.136661, mean_eps: 0.100000
  728860/1200000: episode: 1005, duration: 18.667s, episode steps: 810, steps per second:  43, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.807 [0.000, 5.000],  loss: 0.015598, mae: 1.792452, mean_q: 2.162739, mean_eps: 0.100000
  729465/1200000: episode: 1006, duration: 13.344s, episode steps: 605, steps per second:  45, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.013242, mae: 1.783937, mean_q: 2.153291, mean_eps: 0.100000
  730089/1200000: episode: 1007, duration: 14.347s, episode steps: 624, steps per second:  43, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.013385, mae: 1.777606, mean_q: 2.146028, mean_eps: 0.100000
  731084/1200000: episode: 1008, duration: 21.705s, episode steps: 995, steps per second:  46, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.127 [0.000, 5.000],  loss: 0.012882, mae: 1.795520, mean_q: 2.168926, mean_eps: 0.100000
  731845/1200000: episode: 1009, duration: 16.970s, episode steps: 761, steps per second:  45, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.011957, mae: 1.799769, mean_q: 2.172832, mean_eps: 0.100000
  733079/1200000: episode: 1010, duration: 29.265s, episode steps: 1234, steps per second:  42, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.055 [0.000, 5.000],  loss: 0.014041, mae: 1.792592, mean_q: 2.163744, mean_eps: 0.100000
  734204/1200000: episode: 1011, duration: 26.195s, episode steps: 1125, steps per second:  43, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.057 [0.000, 5.000],  loss: 0.013925, mae: 1.788946, mean_q: 2.160335, mean_eps: 0.100000
  734937/1200000: episode: 1012, duration: 16.852s, episode steps: 733, steps per second:  43, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.318 [0.000, 5.000],  loss: 0.013326, mae: 1.804077, mean_q: 2.178863, mean_eps: 0.100000
  735614/1200000: episode: 1013, duration: 15.884s, episode steps: 677, steps per second:  43, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.087 [0.000, 5.000],  loss: 0.012704, mae: 1.781245, mean_q: 2.150460, mean_eps: 0.100000
  736624/1200000: episode: 1014, duration: 20.879s, episode steps: 1010, steps per second:  48, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.013675, mae: 1.798498, mean_q: 2.170720, mean_eps: 0.100000
  737302/1200000: episode: 1015, duration: 15.267s, episode steps: 678, steps per second:  44, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.319 [0.000, 5.000],  loss: 0.013287, mae: 1.795017, mean_q: 2.165472, mean_eps: 0.100000
  738380/1200000: episode: 1016, duration: 25.050s, episode steps: 1078, steps per second:  43, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.116 [0.000, 5.000],  loss: 0.013803, mae: 1.798403, mean_q: 2.169189, mean_eps: 0.100000
  739335/1200000: episode: 1017, duration: 20.532s, episode steps: 955, steps per second:  47, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.015013, mae: 1.785515, mean_q: 2.154321, mean_eps: 0.100000
  740388/1200000: episode: 1018, duration: 21.593s, episode steps: 1053, steps per second:  49, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.228 [0.000, 5.000],  loss: 0.015345, mae: 1.790229, mean_q: 2.162369, mean_eps: 0.100000
  741050/1200000: episode: 1019, duration: 13.879s, episode steps: 662, steps per second:  48, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.026 [0.000, 5.000],  loss: 0.014111, mae: 1.802900, mean_q: 2.177488, mean_eps: 0.100000
  741854/1200000: episode: 1020, duration: 18.895s, episode steps: 804, steps per second:  43, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.013590, mae: 1.800361, mean_q: 2.173212, mean_eps: 0.100000
  742598/1200000: episode: 1021, duration: 16.626s, episode steps: 744, steps per second:  45, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.012835, mae: 1.818571, mean_q: 2.195534, mean_eps: 0.100000
  743148/1200000: episode: 1022, duration: 13.011s, episode steps: 550, steps per second:  42, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.013 [0.000, 5.000],  loss: 0.016051, mae: 1.802327, mean_q: 2.175719, mean_eps: 0.100000
  744097/1200000: episode: 1023, duration: 22.334s, episode steps: 949, steps per second:  42, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.985 [0.000, 5.000],  loss: 0.014268, mae: 1.812277, mean_q: 2.188702, mean_eps: 0.100000
  744643/1200000: episode: 1024, duration: 11.864s, episode steps: 546, steps per second:  46, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.899 [0.000, 5.000],  loss: 0.015587, mae: 1.791171, mean_q: 2.161941, mean_eps: 0.100000
  745315/1200000: episode: 1025, duration: 14.607s, episode steps: 672, steps per second:  46, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.150 [0.000, 5.000],  loss: 0.017499, mae: 1.820641, mean_q: 2.195923, mean_eps: 0.100000
  745964/1200000: episode: 1026, duration: 13.610s, episode steps: 649, steps per second:  48, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.006 [0.000, 5.000],  loss: 0.013836, mae: 1.807639, mean_q: 2.182465, mean_eps: 0.100000
  746900/1200000: episode: 1027, duration: 21.719s, episode steps: 936, steps per second:  43, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.018 [0.000, 5.000],  loss: 0.013947, mae: 1.799093, mean_q: 2.171972, mean_eps: 0.100000
  747709/1200000: episode: 1028, duration: 17.880s, episode steps: 809, steps per second:  45, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.938 [0.000, 5.000],  loss: 0.016246, mae: 1.812537, mean_q: 2.187845, mean_eps: 0.100000
  748340/1200000: episode: 1029, duration: 14.999s, episode steps: 631, steps per second:  42, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.014112, mae: 1.803680, mean_q: 2.176878, mean_eps: 0.100000
  749383/1200000: episode: 1030, duration: 23.641s, episode steps: 1043, steps per second:  44, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.252 [0.000, 5.000],  loss: 0.013481, mae: 1.809371, mean_q: 2.184380, mean_eps: 0.100000
  750265/1200000: episode: 1031, duration: 18.779s, episode steps: 882, steps per second:  47, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.728 [0.000, 5.000],  loss: 0.015627, mae: 1.792027, mean_q: 2.163321, mean_eps: 0.100000
  751553/1200000: episode: 1032, duration: 30.039s, episode steps: 1288, steps per second:  43, episode reward: 26.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.319 [0.000, 5.000],  loss: 0.014808, mae: 1.837166, mean_q: 2.216993, mean_eps: 0.100000
  752506/1200000: episode: 1033, duration: 19.205s, episode steps: 953, steps per second:  50, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.820 [0.000, 5.000],  loss: 0.015801, mae: 1.824273, mean_q: 2.200640, mean_eps: 0.100000
  753140/1200000: episode: 1034, duration: 12.864s, episode steps: 634, steps per second:  49, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.830 [0.000, 5.000],  loss: 0.014274, mae: 1.821936, mean_q: 2.198447, mean_eps: 0.100000
  753515/1200000: episode: 1035, duration: 8.674s, episode steps: 375, steps per second:  43, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.704 [0.000, 5.000],  loss: 0.013869, mae: 1.801755, mean_q: 2.172361, mean_eps: 0.100000
  754371/1200000: episode: 1036, duration: 18.948s, episode steps: 856, steps per second:  45, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.213 [0.000, 5.000],  loss: 0.014659, mae: 1.834177, mean_q: 2.212053, mean_eps: 0.100000
  754815/1200000: episode: 1037, duration: 9.102s, episode steps: 444, steps per second:  49, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.014517, mae: 1.814806, mean_q: 2.189768, mean_eps: 0.100000
  755591/1200000: episode: 1038, duration: 16.011s, episode steps: 776, steps per second:  48, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.777 [0.000, 5.000],  loss: 0.013783, mae: 1.811871, mean_q: 2.186867, mean_eps: 0.100000
  756279/1200000: episode: 1039, duration: 13.657s, episode steps: 688, steps per second:  50, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.294 [0.000, 5.000],  loss: 0.013232, mae: 1.830040, mean_q: 2.208707, mean_eps: 0.100000
  756727/1200000: episode: 1040, duration: 9.659s, episode steps: 448, steps per second:  46, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.761 [0.000, 5.000],  loss: 0.014197, mae: 1.821290, mean_q: 2.197342, mean_eps: 0.100000
  757463/1200000: episode: 1041, duration: 15.307s, episode steps: 736, steps per second:  48, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.247 [0.000, 5.000],  loss: 0.014471, mae: 1.799158, mean_q: 2.171734, mean_eps: 0.100000
  758595/1200000: episode: 1042, duration: 25.088s, episode steps: 1132, steps per second:  45, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.725 [0.000, 5.000],  loss: 0.014386, mae: 1.816376, mean_q: 2.192202, mean_eps: 0.100000
  759450/1200000: episode: 1043, duration: 20.510s, episode steps: 855, steps per second:  42, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: 0.012113, mae: 1.835518, mean_q: 2.216260, mean_eps: 0.100000
  760379/1200000: episode: 1044, duration: 21.388s, episode steps: 929, steps per second:  43, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.014543, mae: 1.824789, mean_q: 2.203918, mean_eps: 0.100000
  761283/1200000: episode: 1045, duration: 19.359s, episode steps: 904, steps per second:  47, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.016915, mae: 1.852452, mean_q: 2.236849, mean_eps: 0.100000
  762596/1200000: episode: 1046, duration: 28.563s, episode steps: 1313, steps per second:  46, episode reward: 21.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.101 [0.000, 5.000],  loss: 0.013173, mae: 1.854746, mean_q: 2.240129, mean_eps: 0.100000
  763331/1200000: episode: 1047, duration: 15.438s, episode steps: 735, steps per second:  48, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.170 [0.000, 5.000],  loss: 0.015129, mae: 1.831301, mean_q: 2.211199, mean_eps: 0.100000
  763884/1200000: episode: 1048, duration: 11.660s, episode steps: 553, steps per second:  47, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.015547, mae: 1.841457, mean_q: 2.222866, mean_eps: 0.100000
  764502/1200000: episode: 1049, duration: 13.395s, episode steps: 618, steps per second:  46, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 0.015246, mae: 1.865944, mean_q: 2.252146, mean_eps: 0.100000
  765398/1200000: episode: 1050, duration: 21.347s, episode steps: 896, steps per second:  42, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.961 [0.000, 5.000],  loss: 0.014283, mae: 1.849246, mean_q: 2.231740, mean_eps: 0.100000
  767140/1200000: episode: 1051, duration: 40.429s, episode steps: 1742, steps per second:  43, episode reward: 30.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.014181, mae: 1.835972, mean_q: 2.214626, mean_eps: 0.100000
  767536/1200000: episode: 1052, duration: 9.774s, episode steps: 396, steps per second:  41, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.016192, mae: 1.852490, mean_q: 2.235531, mean_eps: 0.100000
  768431/1200000: episode: 1053, duration: 19.716s, episode steps: 895, steps per second:  45, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.015655, mae: 1.841211, mean_q: 2.221370, mean_eps: 0.100000
  768922/1200000: episode: 1054, duration: 10.043s, episode steps: 491, steps per second:  49, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.898 [0.000, 5.000],  loss: 0.013597, mae: 1.834339, mean_q: 2.213180, mean_eps: 0.100000
  770085/1200000: episode: 1055, duration: 23.937s, episode steps: 1163, steps per second:  49, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.014786, mae: 1.822907, mean_q: 2.199396, mean_eps: 0.100000
  770631/1200000: episode: 1056, duration: 11.979s, episode steps: 546, steps per second:  46, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.013777, mae: 1.826013, mean_q: 2.204844, mean_eps: 0.100000
  771765/1200000: episode: 1057, duration: 23.264s, episode steps: 1134, steps per second:  49, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.014971, mae: 1.826768, mean_q: 2.205724, mean_eps: 0.100000
  772408/1200000: episode: 1058, duration: 13.009s, episode steps: 643, steps per second:  49, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.016217, mae: 1.812745, mean_q: 2.186994, mean_eps: 0.100000
  773221/1200000: episode: 1059, duration: 16.368s, episode steps: 813, steps per second:  50, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.015981, mae: 1.829863, mean_q: 2.208074, mean_eps: 0.100000
  773886/1200000: episode: 1060, duration: 13.278s, episode steps: 665, steps per second:  50, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.122 [0.000, 5.000],  loss: 0.014233, mae: 1.829023, mean_q: 2.207719, mean_eps: 0.100000
  774739/1200000: episode: 1061, duration: 17.436s, episode steps: 853, steps per second:  49, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.014086, mae: 1.821393, mean_q: 2.199192, mean_eps: 0.100000
  775584/1200000: episode: 1062, duration: 17.417s, episode steps: 845, steps per second:  49, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.016568, mae: 1.826481, mean_q: 2.203975, mean_eps: 0.100000
  775970/1200000: episode: 1063, duration: 8.317s, episode steps: 386, steps per second:  46, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.855 [0.000, 5.000],  loss: 0.015461, mae: 1.822824, mean_q: 2.198249, mean_eps: 0.100000
  776518/1200000: episode: 1064, duration: 12.436s, episode steps: 548, steps per second:  44, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.662 [0.000, 5.000],  loss: 0.014347, mae: 1.794081, mean_q: 2.164186, mean_eps: 0.100000
  777325/1200000: episode: 1065, duration: 17.100s, episode steps: 807, steps per second:  47, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.013340, mae: 1.815668, mean_q: 2.190431, mean_eps: 0.100000
  778129/1200000: episode: 1066, duration: 17.444s, episode steps: 804, steps per second:  46, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.014613, mae: 1.822339, mean_q: 2.197855, mean_eps: 0.100000
  779095/1200000: episode: 1067, duration: 21.931s, episode steps: 966, steps per second:  44, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.015029, mae: 1.828032, mean_q: 2.204531, mean_eps: 0.100000
  779564/1200000: episode: 1068, duration: 10.841s, episode steps: 469, steps per second:  43, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.011798, mae: 1.828774, mean_q: 2.206198, mean_eps: 0.100000
  780413/1200000: episode: 1069, duration: 17.773s, episode steps: 849, steps per second:  48, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.145 [0.000, 5.000],  loss: 0.012975, mae: 1.804799, mean_q: 2.176762, mean_eps: 0.100000
  780806/1200000: episode: 1070, duration: 8.742s, episode steps: 393, steps per second:  45, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.132 [0.000, 5.000],  loss: 0.011234, mae: 1.796765, mean_q: 2.167722, mean_eps: 0.100000
  781327/1200000: episode: 1071, duration: 12.084s, episode steps: 521, steps per second:  43, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.228 [0.000, 5.000],  loss: 0.014844, mae: 1.840368, mean_q: 2.219895, mean_eps: 0.100000
  782179/1200000: episode: 1072, duration: 20.847s, episode steps: 852, steps per second:  41, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.938 [0.000, 5.000],  loss: 0.011515, mae: 1.791082, mean_q: 2.161972, mean_eps: 0.100000
  782869/1200000: episode: 1073, duration: 15.919s, episode steps: 690, steps per second:  43, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.932 [0.000, 5.000],  loss: 0.013751, mae: 1.801719, mean_q: 2.175275, mean_eps: 0.100000
  783513/1200000: episode: 1074, duration: 15.047s, episode steps: 644, steps per second:  43, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.012038, mae: 1.793967, mean_q: 2.165034, mean_eps: 0.100000
  783894/1200000: episode: 1075, duration: 8.506s, episode steps: 381, steps per second:  45, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.012154, mae: 1.798624, mean_q: 2.170534, mean_eps: 0.100000
  784783/1200000: episode: 1076, duration: 20.563s, episode steps: 889, steps per second:  43, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.139 [0.000, 5.000],  loss: 0.013435, mae: 1.804533, mean_q: 2.177708, mean_eps: 0.100000
  785433/1200000: episode: 1077, duration: 13.161s, episode steps: 650, steps per second:  49, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.214 [0.000, 5.000],  loss: 0.015879, mae: 1.814036, mean_q: 2.187492, mean_eps: 0.100000
  786085/1200000: episode: 1078, duration: 13.125s, episode steps: 652, steps per second:  50, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.923 [0.000, 5.000],  loss: 0.015567, mae: 1.812832, mean_q: 2.186515, mean_eps: 0.100000
  786716/1200000: episode: 1079, duration: 13.813s, episode steps: 631, steps per second:  46, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.003 [0.000, 5.000],  loss: 0.015278, mae: 1.804543, mean_q: 2.176565, mean_eps: 0.100000
  787415/1200000: episode: 1080, duration: 15.200s, episode steps: 699, steps per second:  46, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.213 [0.000, 5.000],  loss: 0.013987, mae: 1.820420, mean_q: 2.196651, mean_eps: 0.100000
  788236/1200000: episode: 1081, duration: 17.201s, episode steps: 821, steps per second:  48, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.981 [0.000, 5.000],  loss: 0.012957, mae: 1.792638, mean_q: 2.162304, mean_eps: 0.100000
  788600/1200000: episode: 1082, duration: 7.555s, episode steps: 364, steps per second:  48, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.904 [0.000, 5.000],  loss: 0.013611, mae: 1.816968, mean_q: 2.192412, mean_eps: 0.100000
  789420/1200000: episode: 1083, duration: 17.185s, episode steps: 820, steps per second:  48, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.261 [0.000, 5.000],  loss: 0.014230, mae: 1.804178, mean_q: 2.175912, mean_eps: 0.100000
  790156/1200000: episode: 1084, duration: 15.667s, episode steps: 736, steps per second:  47, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.954 [0.000, 5.000],  loss: 0.015556, mae: 1.809431, mean_q: 2.182230, mean_eps: 0.100000
  791123/1200000: episode: 1085, duration: 19.539s, episode steps: 967, steps per second:  49, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.011851, mae: 1.788966, mean_q: 2.159016, mean_eps: 0.100000
  791859/1200000: episode: 1086, duration: 15.695s, episode steps: 736, steps per second:  47, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.014407, mae: 1.803996, mean_q: 2.175859, mean_eps: 0.100000
  792999/1200000: episode: 1087, duration: 26.468s, episode steps: 1140, steps per second:  43, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.014900, mae: 1.800645, mean_q: 2.171001, mean_eps: 0.100000
  793538/1200000: episode: 1088, duration: 11.660s, episode steps: 539, steps per second:  46, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.014720, mae: 1.810214, mean_q: 2.182480, mean_eps: 0.100000
  794898/1200000: episode: 1089, duration: 29.648s, episode steps: 1360, steps per second:  46, episode reward: 33.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.197 [0.000, 5.000],  loss: 0.016514, mae: 1.799375, mean_q: 2.170806, mean_eps: 0.100000
  795685/1200000: episode: 1090, duration: 16.119s, episode steps: 787, steps per second:  49, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.014648, mae: 1.795215, mean_q: 2.167618, mean_eps: 0.100000
  796757/1200000: episode: 1091, duration: 21.899s, episode steps: 1072, steps per second:  49, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.975 [0.000, 5.000],  loss: 0.014275, mae: 1.806865, mean_q: 2.180694, mean_eps: 0.100000
  797697/1200000: episode: 1092, duration: 18.840s, episode steps: 940, steps per second:  50, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.014505, mae: 1.803855, mean_q: 2.176731, mean_eps: 0.100000
  798437/1200000: episode: 1093, duration: 16.781s, episode steps: 740, steps per second:  44, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.981 [0.000, 5.000],  loss: 0.013771, mae: 1.813537, mean_q: 2.188394, mean_eps: 0.100000
  799129/1200000: episode: 1094, duration: 15.340s, episode steps: 692, steps per second:  45, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.014801, mae: 1.812864, mean_q: 2.187388, mean_eps: 0.100000
  800168/1200000: episode: 1095, duration: 22.896s, episode steps: 1039, steps per second:  45, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.623 [0.000, 5.000],  loss: 0.014059, mae: 1.799900, mean_q: 2.172730, mean_eps: 0.100000
  801056/1200000: episode: 1096, duration: 20.404s, episode steps: 888, steps per second:  44, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.014183, mae: 1.809460, mean_q: 2.184527, mean_eps: 0.100000
  801805/1200000: episode: 1097, duration: 16.906s, episode steps: 749, steps per second:  44, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.830 [0.000, 5.000],  loss: 0.015134, mae: 1.804329, mean_q: 2.178289, mean_eps: 0.100000
  803288/1200000: episode: 1098, duration: 30.749s, episode steps: 1483, steps per second:  48, episode reward: 33.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.335 [0.000, 5.000],  loss: 0.013639, mae: 1.810844, mean_q: 2.184899, mean_eps: 0.100000
  805040/1200000: episode: 1099, duration: 37.223s, episode steps: 1752, steps per second:  47, episode reward: 36.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.219 [0.000, 5.000],  loss: 0.012916, mae: 1.803548, mean_q: 2.178054, mean_eps: 0.100000
  806791/1200000: episode: 1100, duration: 37.657s, episode steps: 1751, steps per second:  46, episode reward: 26.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.074 [0.000, 5.000],  loss: 0.015123, mae: 1.813190, mean_q: 2.187011, mean_eps: 0.100000
  807566/1200000: episode: 1101, duration: 16.301s, episode steps: 775, steps per second:  48, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.566 [0.000, 5.000],  loss: 0.015496, mae: 1.807015, mean_q: 2.179365, mean_eps: 0.100000
  808482/1200000: episode: 1102, duration: 19.414s, episode steps: 916, steps per second:  47, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.805 [0.000, 5.000],  loss: 0.014038, mae: 1.798874, mean_q: 2.170290, mean_eps: 0.100000
  809368/1200000: episode: 1103, duration: 19.626s, episode steps: 886, steps per second:  45, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.271 [0.000, 5.000],  loss: 0.015850, mae: 1.816966, mean_q: 2.191830, mean_eps: 0.100000
  810355/1200000: episode: 1104, duration: 20.911s, episode steps: 987, steps per second:  47, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.955 [0.000, 5.000],  loss: 0.015649, mae: 1.821726, mean_q: 2.199468, mean_eps: 0.100000
  811244/1200000: episode: 1105, duration: 17.976s, episode steps: 889, steps per second:  49, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.013386, mae: 1.786601, mean_q: 2.158554, mean_eps: 0.100000
  812212/1200000: episode: 1106, duration: 20.255s, episode steps: 968, steps per second:  48, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.013350, mae: 1.810649, mean_q: 2.185910, mean_eps: 0.100000
  812734/1200000: episode: 1107, duration: 11.066s, episode steps: 522, steps per second:  47, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.013902, mae: 1.806370, mean_q: 2.180595, mean_eps: 0.100000
  813363/1200000: episode: 1108, duration: 13.966s, episode steps: 629, steps per second:  45, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.013 [0.000, 5.000],  loss: 0.014781, mae: 1.794924, mean_q: 2.165418, mean_eps: 0.100000
  814140/1200000: episode: 1109, duration: 16.819s, episode steps: 777, steps per second:  46, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.015652, mae: 1.824027, mean_q: 2.198906, mean_eps: 0.100000
  814920/1200000: episode: 1110, duration: 18.374s, episode steps: 780, steps per second:  42, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.015349, mae: 1.831871, mean_q: 2.208781, mean_eps: 0.100000
  815926/1200000: episode: 1111, duration: 23.641s, episode steps: 1006, steps per second:  43, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.017048, mae: 1.833973, mean_q: 2.211822, mean_eps: 0.100000
  816797/1200000: episode: 1112, duration: 19.418s, episode steps: 871, steps per second:  45, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.013941, mae: 1.813463, mean_q: 2.187902, mean_eps: 0.100000
  817357/1200000: episode: 1113, duration: 12.552s, episode steps: 560, steps per second:  45, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.015357, mae: 1.811071, mean_q: 2.184219, mean_eps: 0.100000
  818070/1200000: episode: 1114, duration: 15.868s, episode steps: 713, steps per second:  45, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.317 [0.000, 5.000],  loss: 0.016540, mae: 1.803081, mean_q: 2.174571, mean_eps: 0.100000
  819119/1200000: episode: 1115, duration: 22.000s, episode steps: 1049, steps per second:  48, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.012689, mae: 1.809416, mean_q: 2.183768, mean_eps: 0.100000
  819495/1200000: episode: 1116, duration: 7.349s, episode steps: 376, steps per second:  51, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.011 [0.000, 5.000],  loss: 0.016147, mae: 1.812778, mean_q: 2.188529, mean_eps: 0.100000
  820463/1200000: episode: 1117, duration: 20.399s, episode steps: 968, steps per second:  47, episode reward: 11.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 3.110 [0.000, 5.000],  loss: 0.013876, mae: 1.805707, mean_q: 2.180879, mean_eps: 0.100000
  821249/1200000: episode: 1118, duration: 15.474s, episode steps: 786, steps per second:  51, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.884 [0.000, 5.000],  loss: 0.014220, mae: 1.834881, mean_q: 2.215937, mean_eps: 0.100000
  822142/1200000: episode: 1119, duration: 18.193s, episode steps: 893, steps per second:  49, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.081 [0.000, 5.000],  loss: 0.012238, mae: 1.835830, mean_q: 2.217298, mean_eps: 0.100000
  822998/1200000: episode: 1120, duration: 17.039s, episode steps: 856, steps per second:  50, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.244 [0.000, 5.000],  loss: 0.014996, mae: 1.833304, mean_q: 2.212504, mean_eps: 0.100000
  823578/1200000: episode: 1121, duration: 12.263s, episode steps: 580, steps per second:  47, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.252 [0.000, 5.000],  loss: 0.013647, mae: 1.836295, mean_q: 2.215205, mean_eps: 0.100000
  824363/1200000: episode: 1122, duration: 17.294s, episode steps: 785, steps per second:  45, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.139 [0.000, 5.000],  loss: 0.014269, mae: 1.830079, mean_q: 2.209244, mean_eps: 0.100000
  825096/1200000: episode: 1123, duration: 16.105s, episode steps: 733, steps per second:  46, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.013293, mae: 1.815169, mean_q: 2.190973, mean_eps: 0.100000
  825858/1200000: episode: 1124, duration: 19.352s, episode steps: 762, steps per second:  39, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.194 [0.000, 5.000],  loss: 0.012632, mae: 1.828984, mean_q: 2.206778, mean_eps: 0.100000
  826600/1200000: episode: 1125, duration: 18.179s, episode steps: 742, steps per second:  41, episode reward: 21.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.013642, mae: 1.825407, mean_q: 2.202072, mean_eps: 0.100000
  827530/1200000: episode: 1126, duration: 18.630s, episode steps: 930, steps per second:  50, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.014058, mae: 1.832137, mean_q: 2.211012, mean_eps: 0.100000
  828295/1200000: episode: 1127, duration: 15.755s, episode steps: 765, steps per second:  49, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.012554, mae: 1.821832, mean_q: 2.199232, mean_eps: 0.100000
  829112/1200000: episode: 1128, duration: 16.522s, episode steps: 817, steps per second:  49, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.754 [0.000, 5.000],  loss: 0.013517, mae: 1.808446, mean_q: 2.183006, mean_eps: 0.100000
  829895/1200000: episode: 1129, duration: 16.647s, episode steps: 783, steps per second:  47, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.880 [0.000, 5.000],  loss: 0.011717, mae: 1.806416, mean_q: 2.180278, mean_eps: 0.100000
  830703/1200000: episode: 1130, duration: 15.233s, episode steps: 808, steps per second:  53, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.843 [0.000, 5.000],  loss: 0.013798, mae: 1.836277, mean_q: 2.217023, mean_eps: 0.100000
  832344/1200000: episode: 1131, duration: 35.902s, episode steps: 1641, steps per second:  46, episode reward: 32.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.575 [0.000, 5.000],  loss: 0.013878, mae: 1.836654, mean_q: 2.217005, mean_eps: 0.100000
  833116/1200000: episode: 1132, duration: 15.407s, episode steps: 772, steps per second:  50, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.236 [0.000, 5.000],  loss: 0.014550, mae: 1.827145, mean_q: 2.205533, mean_eps: 0.100000
  834047/1200000: episode: 1133, duration: 18.930s, episode steps: 931, steps per second:  49, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.874 [0.000, 5.000],  loss: 0.014364, mae: 1.844002, mean_q: 2.224867, mean_eps: 0.100000
  834510/1200000: episode: 1134, duration: 9.177s, episode steps: 463, steps per second:  50, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.881 [0.000, 5.000],  loss: 0.015483, mae: 1.844133, mean_q: 2.225353, mean_eps: 0.100000
  835386/1200000: episode: 1135, duration: 17.880s, episode steps: 876, steps per second:  49, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.015111, mae: 1.847143, mean_q: 2.228266, mean_eps: 0.100000
  836361/1200000: episode: 1136, duration: 18.268s, episode steps: 975, steps per second:  53, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.851 [0.000, 5.000],  loss: 0.013159, mae: 1.851094, mean_q: 2.234856, mean_eps: 0.100000
  836751/1200000: episode: 1137, duration: 7.715s, episode steps: 390, steps per second:  51, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.018857, mae: 1.831762, mean_q: 2.210515, mean_eps: 0.100000
  837670/1200000: episode: 1138, duration: 19.519s, episode steps: 919, steps per second:  47, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.168 [0.000, 5.000],  loss: 0.015419, mae: 1.846254, mean_q: 2.227368, mean_eps: 0.100000
  838466/1200000: episode: 1139, duration: 16.521s, episode steps: 796, steps per second:  48, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.991 [0.000, 5.000],  loss: 0.014665, mae: 1.845687, mean_q: 2.227093, mean_eps: 0.100000
  839197/1200000: episode: 1140, duration: 14.257s, episode steps: 731, steps per second:  51, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.012697, mae: 1.841544, mean_q: 2.223374, mean_eps: 0.100000
  839828/1200000: episode: 1141, duration: 12.741s, episode steps: 631, steps per second:  50, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.746 [0.000, 5.000],  loss: 0.012691, mae: 1.829377, mean_q: 2.208195, mean_eps: 0.100000
  840463/1200000: episode: 1142, duration: 12.843s, episode steps: 635, steps per second:  49, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.012161, mae: 1.847282, mean_q: 2.230151, mean_eps: 0.100000
  841093/1200000: episode: 1143, duration: 12.514s, episode steps: 630, steps per second:  50, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.073 [0.000, 5.000],  loss: 0.013900, mae: 1.839982, mean_q: 2.221375, mean_eps: 0.100000
  841745/1200000: episode: 1144, duration: 13.833s, episode steps: 652, steps per second:  47, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.776 [0.000, 5.000],  loss: 0.014415, mae: 1.861868, mean_q: 2.247659, mean_eps: 0.100000
  842525/1200000: episode: 1145, duration: 18.749s, episode steps: 780, steps per second:  42, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.013437, mae: 1.869937, mean_q: 2.258273, mean_eps: 0.100000
  843057/1200000: episode: 1146, duration: 13.294s, episode steps: 532, steps per second:  40, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.137 [0.000, 5.000],  loss: 0.015522, mae: 1.872296, mean_q: 2.260140, mean_eps: 0.100000
  843821/1200000: episode: 1147, duration: 18.660s, episode steps: 764, steps per second:  41, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.920 [0.000, 5.000],  loss: 0.015339, mae: 1.863772, mean_q: 2.249740, mean_eps: 0.100000
  844637/1200000: episode: 1148, duration: 15.864s, episode steps: 816, steps per second:  51, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.665 [0.000, 5.000],  loss: 0.013985, mae: 1.835424, mean_q: 2.216891, mean_eps: 0.100000
  845185/1200000: episode: 1149, duration: 10.711s, episode steps: 548, steps per second:  51, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.664 [0.000, 5.000],  loss: 0.012885, mae: 1.854513, mean_q: 2.239752, mean_eps: 0.100000
  845836/1200000: episode: 1150, duration: 13.745s, episode steps: 651, steps per second:  47, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.227 [0.000, 5.000],  loss: 0.013728, mae: 1.847856, mean_q: 2.232155, mean_eps: 0.100000
  846593/1200000: episode: 1151, duration: 16.191s, episode steps: 757, steps per second:  47, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.148 [0.000, 5.000],  loss: 0.013815, mae: 1.842984, mean_q: 2.224680, mean_eps: 0.100000
  847343/1200000: episode: 1152, duration: 14.442s, episode steps: 750, steps per second:  52, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.696 [0.000, 5.000],  loss: 0.013620, mae: 1.850169, mean_q: 2.234896, mean_eps: 0.100000
  847806/1200000: episode: 1153, duration: 9.120s, episode steps: 463, steps per second:  51, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.091 [0.000, 5.000],  loss: 0.013579, mae: 1.861105, mean_q: 2.248059, mean_eps: 0.100000
  848467/1200000: episode: 1154, duration: 12.541s, episode steps: 661, steps per second:  53, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.016005, mae: 1.852834, mean_q: 2.236808, mean_eps: 0.100000
  849245/1200000: episode: 1155, duration: 16.677s, episode steps: 778, steps per second:  47, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.671 [0.000, 5.000],  loss: 0.015503, mae: 1.844016, mean_q: 2.227239, mean_eps: 0.100000
  850384/1200000: episode: 1156, duration: 23.968s, episode steps: 1139, steps per second:  48, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.971 [0.000, 5.000],  loss: 0.014908, mae: 1.834103, mean_q: 2.215022, mean_eps: 0.100000
  851346/1200000: episode: 1157, duration: 19.403s, episode steps: 962, steps per second:  50, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.013793, mae: 1.856488, mean_q: 2.242394, mean_eps: 0.100000
  851977/1200000: episode: 1158, duration: 12.581s, episode steps: 631, steps per second:  50, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.724 [0.000, 5.000],  loss: 0.013464, mae: 1.866803, mean_q: 2.256064, mean_eps: 0.100000
  852740/1200000: episode: 1159, duration: 15.725s, episode steps: 763, steps per second:  49, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.012989, mae: 1.854284, mean_q: 2.241131, mean_eps: 0.100000
  853515/1200000: episode: 1160, duration: 14.449s, episode steps: 775, steps per second:  54, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.970 [0.000, 5.000],  loss: 0.016113, mae: 1.848076, mean_q: 2.231720, mean_eps: 0.100000
  854309/1200000: episode: 1161, duration: 15.633s, episode steps: 794, steps per second:  51, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.033 [0.000, 5.000],  loss: 0.012795, mae: 1.837707, mean_q: 2.219484, mean_eps: 0.100000
  854912/1200000: episode: 1162, duration: 12.122s, episode steps: 603, steps per second:  50, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.748 [0.000, 5.000],  loss: 0.011920, mae: 1.843927, mean_q: 2.227351, mean_eps: 0.100000
  855778/1200000: episode: 1163, duration: 17.277s, episode steps: 866, steps per second:  50, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.178 [0.000, 5.000],  loss: 0.012782, mae: 1.844336, mean_q: 2.227039, mean_eps: 0.100000
  856542/1200000: episode: 1164, duration: 14.848s, episode steps: 764, steps per second:  51, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.137 [0.000, 5.000],  loss: 0.013084, mae: 1.850670, mean_q: 2.234177, mean_eps: 0.100000
  857031/1200000: episode: 1165, duration: 9.390s, episode steps: 489, steps per second:  52, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.650 [0.000, 5.000],  loss: 0.013772, mae: 1.841866, mean_q: 2.224571, mean_eps: 0.100000
  858012/1200000: episode: 1166, duration: 19.086s, episode steps: 981, steps per second:  51, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.157 [0.000, 5.000],  loss: 0.014288, mae: 1.845753, mean_q: 2.228595, mean_eps: 0.100000
  858952/1200000: episode: 1167, duration: 18.677s, episode steps: 940, steps per second:  50, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.963 [0.000, 5.000],  loss: 0.012740, mae: 1.850634, mean_q: 2.235326, mean_eps: 0.100000
  859854/1200000: episode: 1168, duration: 19.158s, episode steps: 902, steps per second:  47, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.911 [0.000, 5.000],  loss: 0.013216, mae: 1.856915, mean_q: 2.242214, mean_eps: 0.100000
  860501/1200000: episode: 1169, duration: 14.544s, episode steps: 647, steps per second:  44, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.943 [0.000, 5.000],  loss: 0.013401, mae: 1.883815, mean_q: 2.275664, mean_eps: 0.100000
  861309/1200000: episode: 1170, duration: 18.809s, episode steps: 808, steps per second:  43, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.873 [0.000, 5.000],  loss: 0.013020, mae: 1.872259, mean_q: 2.260787, mean_eps: 0.100000
  862016/1200000: episode: 1171, duration: 14.077s, episode steps: 707, steps per second:  50, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.972 [0.000, 5.000],  loss: 0.012549, mae: 1.891390, mean_q: 2.284104, mean_eps: 0.100000
  862783/1200000: episode: 1172, duration: 15.428s, episode steps: 767, steps per second:  50, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.583 [0.000, 5.000],  loss: 0.013374, mae: 1.881918, mean_q: 2.272899, mean_eps: 0.100000
  863749/1200000: episode: 1173, duration: 19.734s, episode steps: 966, steps per second:  49, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.811 [0.000, 5.000],  loss: 0.013843, mae: 1.856856, mean_q: 2.243077, mean_eps: 0.100000
  864507/1200000: episode: 1174, duration: 15.732s, episode steps: 758, steps per second:  48, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.091 [0.000, 5.000],  loss: 0.014792, mae: 1.882323, mean_q: 2.273815, mean_eps: 0.100000
  865430/1200000: episode: 1175, duration: 18.323s, episode steps: 923, steps per second:  50, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.041 [0.000, 5.000],  loss: 0.013711, mae: 1.876436, mean_q: 2.266711, mean_eps: 0.100000
  866248/1200000: episode: 1176, duration: 16.684s, episode steps: 818, steps per second:  49, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.218 [0.000, 5.000],  loss: 0.014837, mae: 1.866030, mean_q: 2.253708, mean_eps: 0.100000
  867064/1200000: episode: 1177, duration: 17.024s, episode steps: 816, steps per second:  48, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.759 [0.000, 5.000],  loss: 0.013682, mae: 1.867726, mean_q: 2.255903, mean_eps: 0.100000
  867994/1200000: episode: 1178, duration: 18.825s, episode steps: 930, steps per second:  49, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.871 [0.000, 5.000],  loss: 0.013008, mae: 1.886958, mean_q: 2.279831, mean_eps: 0.100000
  868431/1200000: episode: 1179, duration: 8.955s, episode steps: 437, steps per second:  49, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.043 [0.000, 5.000],  loss: 0.013001, mae: 1.874004, mean_q: 2.262971, mean_eps: 0.100000
  869313/1200000: episode: 1180, duration: 18.092s, episode steps: 882, steps per second:  49, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.957 [0.000, 5.000],  loss: 0.013232, mae: 1.866772, mean_q: 2.255005, mean_eps: 0.100000
  870317/1200000: episode: 1181, duration: 20.316s, episode steps: 1004, steps per second:  49, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.015730, mae: 1.878256, mean_q: 2.269355, mean_eps: 0.100000
  871294/1200000: episode: 1182, duration: 18.625s, episode steps: 977, steps per second:  52, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.567 [0.000, 5.000],  loss: 0.014527, mae: 1.899652, mean_q: 2.294587, mean_eps: 0.100000
  871739/1200000: episode: 1183, duration: 8.256s, episode steps: 445, steps per second:  54, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.180 [0.000, 5.000],  loss: 0.013476, mae: 1.873823, mean_q: 2.264871, mean_eps: 0.100000
  872702/1200000: episode: 1184, duration: 19.556s, episode steps: 963, steps per second:  49, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.621 [0.000, 5.000],  loss: 0.014387, mae: 1.896566, mean_q: 2.291395, mean_eps: 0.100000
  873158/1200000: episode: 1185, duration: 9.393s, episode steps: 456, steps per second:  49, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.228 [0.000, 5.000],  loss: 0.014316, mae: 1.897858, mean_q: 2.291719, mean_eps: 0.100000
  873778/1200000: episode: 1186, duration: 11.743s, episode steps: 620, steps per second:  53, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.790 [0.000, 5.000],  loss: 0.013336, mae: 1.871686, mean_q: 2.261483, mean_eps: 0.100000
  874670/1200000: episode: 1187, duration: 17.585s, episode steps: 892, steps per second:  51, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.055 [0.000, 5.000],  loss: 0.014532, mae: 1.887141, mean_q: 2.277796, mean_eps: 0.100000
  875207/1200000: episode: 1188, duration: 10.623s, episode steps: 537, steps per second:  51, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.959 [0.000, 5.000],  loss: 0.014774, mae: 1.889309, mean_q: 2.279383, mean_eps: 0.100000
  875696/1200000: episode: 1189, duration: 9.615s, episode steps: 489, steps per second:  51, episode reward: 13.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.726 [0.000, 5.000],  loss: 0.011563, mae: 1.863434, mean_q: 2.249504, mean_eps: 0.100000
  876338/1200000: episode: 1190, duration: 12.698s, episode steps: 642, steps per second:  51, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.738 [0.000, 5.000],  loss: 0.013252, mae: 1.912760, mean_q: 2.309004, mean_eps: 0.100000
  876829/1200000: episode: 1191, duration: 10.137s, episode steps: 491, steps per second:  48, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.013130, mae: 1.900579, mean_q: 2.294640, mean_eps: 0.100000
  877551/1200000: episode: 1192, duration: 15.169s, episode steps: 722, steps per second:  48, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.013928, mae: 1.907747, mean_q: 2.302632, mean_eps: 0.100000
  878543/1200000: episode: 1193, duration: 22.206s, episode steps: 992, steps per second:  45, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.012890, mae: 1.899477, mean_q: 2.293279, mean_eps: 0.100000
  879184/1200000: episode: 1194, duration: 15.549s, episode steps: 641, steps per second:  41, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.885 [0.000, 5.000],  loss: 0.013973, mae: 1.896492, mean_q: 2.289980, mean_eps: 0.100000
  880012/1200000: episode: 1195, duration: 17.510s, episode steps: 828, steps per second:  47, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.897 [0.000, 5.000],  loss: 0.014299, mae: 1.913661, mean_q: 2.310941, mean_eps: 0.100000
  881234/1200000: episode: 1196, duration: 25.251s, episode steps: 1222, steps per second:  48, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.013289, mae: 1.947202, mean_q: 2.352427, mean_eps: 0.100000
  882262/1200000: episode: 1197, duration: 20.827s, episode steps: 1028, steps per second:  49, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.668 [0.000, 5.000],  loss: 0.014767, mae: 1.933915, mean_q: 2.334199, mean_eps: 0.100000
  882698/1200000: episode: 1198, duration: 8.913s, episode steps: 436, steps per second:  49, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.013130, mae: 1.944415, mean_q: 2.346943, mean_eps: 0.100000
  883166/1200000: episode: 1199, duration: 8.952s, episode steps: 468, steps per second:  52, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.910 [0.000, 5.000],  loss: 0.014009, mae: 1.927134, mean_q: 2.324061, mean_eps: 0.100000
  884189/1200000: episode: 1200, duration: 21.207s, episode steps: 1023, steps per second:  48, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.786 [0.000, 5.000],  loss: 0.014508, mae: 1.947913, mean_q: 2.348906, mean_eps: 0.100000
  885227/1200000: episode: 1201, duration: 22.061s, episode steps: 1038, steps per second:  47, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.709 [0.000, 5.000],  loss: 0.013352, mae: 1.946559, mean_q: 2.348415, mean_eps: 0.100000
  886126/1200000: episode: 1202, duration: 18.536s, episode steps: 899, steps per second:  49, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.192 [0.000, 5.000],  loss: 0.017561, mae: 1.918974, mean_q: 2.313608, mean_eps: 0.100000
  887064/1200000: episode: 1203, duration: 19.493s, episode steps: 938, steps per second:  48, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 0.014298, mae: 1.941071, mean_q: 2.341465, mean_eps: 0.100000
  887694/1200000: episode: 1204, duration: 12.617s, episode steps: 630, steps per second:  50, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.816 [0.000, 5.000],  loss: 0.014916, mae: 1.958000, mean_q: 2.362501, mean_eps: 0.100000
  888069/1200000: episode: 1205, duration: 6.827s, episode steps: 375, steps per second:  55, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.979 [0.000, 5.000],  loss: 0.014504, mae: 1.912296, mean_q: 2.307880, mean_eps: 0.100000
  888837/1200000: episode: 1206, duration: 14.749s, episode steps: 768, steps per second:  52, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.565 [0.000, 5.000],  loss: 0.014631, mae: 1.937006, mean_q: 2.335937, mean_eps: 0.100000
  889675/1200000: episode: 1207, duration: 16.291s, episode steps: 838, steps per second:  51, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.351 [0.000, 5.000],  loss: 0.014368, mae: 1.958577, mean_q: 2.362621, mean_eps: 0.100000
  890072/1200000: episode: 1208, duration: 8.710s, episode steps: 397, steps per second:  46, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.360 [0.000, 5.000],  loss: 0.014588, mae: 1.941437, mean_q: 2.342703, mean_eps: 0.100000
  890720/1200000: episode: 1209, duration: 13.599s, episode steps: 648, steps per second:  48, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.025 [0.000, 5.000],  loss: 0.012176, mae: 1.997201, mean_q: 2.412320, mean_eps: 0.100000
  891356/1200000: episode: 1210, duration: 13.002s, episode steps: 636, steps per second:  49, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.972 [0.000, 5.000],  loss: 0.013953, mae: 1.981277, mean_q: 2.390323, mean_eps: 0.100000
  892294/1200000: episode: 1211, duration: 18.172s, episode steps: 938, steps per second:  52, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.900 [0.000, 5.000],  loss: 0.015246, mae: 1.997294, mean_q: 2.410220, mean_eps: 0.100000
  892969/1200000: episode: 1212, duration: 13.485s, episode steps: 675, steps per second:  50, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.014885, mae: 1.994559, mean_q: 2.406299, mean_eps: 0.100000
  893788/1200000: episode: 1213, duration: 16.084s, episode steps: 819, steps per second:  51, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.016197, mae: 1.994805, mean_q: 2.406578, mean_eps: 0.100000
  894527/1200000: episode: 1214, duration: 15.603s, episode steps: 739, steps per second:  47, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.014639, mae: 1.984566, mean_q: 2.394545, mean_eps: 0.100000
  895440/1200000: episode: 1215, duration: 18.674s, episode steps: 913, steps per second:  49, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: 0.013738, mae: 1.975057, mean_q: 2.382985, mean_eps: 0.100000
  896409/1200000: episode: 1216, duration: 21.747s, episode steps: 969, steps per second:  45, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.014141, mae: 1.989099, mean_q: 2.399775, mean_eps: 0.100000
  897148/1200000: episode: 1217, duration: 15.682s, episode steps: 739, steps per second:  47, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.112 [0.000, 5.000],  loss: 0.012658, mae: 2.001438, mean_q: 2.416870, mean_eps: 0.100000
  898195/1200000: episode: 1218, duration: 20.743s, episode steps: 1047, steps per second:  50, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.129 [0.000, 5.000],  loss: 0.014130, mae: 1.985747, mean_q: 2.396975, mean_eps: 0.100000
  899165/1200000: episode: 1219, duration: 19.398s, episode steps: 970, steps per second:  50, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.890 [0.000, 5.000],  loss: 0.013152, mae: 1.999084, mean_q: 2.413726, mean_eps: 0.100000
  899861/1200000: episode: 1220, duration: 13.836s, episode steps: 696, steps per second:  50, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.717 [0.000, 5.000],  loss: 0.013897, mae: 1.995443, mean_q: 2.408968, mean_eps: 0.100000
  900493/1200000: episode: 1221, duration: 12.419s, episode steps: 632, steps per second:  51, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.734 [0.000, 5.000],  loss: 0.013413, mae: 1.996131, mean_q: 2.410732, mean_eps: 0.100000
  901456/1200000: episode: 1222, duration: 19.255s, episode steps: 963, steps per second:  50, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.249 [0.000, 5.000],  loss: 0.013597, mae: 1.985692, mean_q: 2.398810, mean_eps: 0.100000
  902494/1200000: episode: 1223, duration: 21.558s, episode steps: 1038, steps per second:  48, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 0.015382, mae: 2.007939, mean_q: 2.423741, mean_eps: 0.100000
  903253/1200000: episode: 1224, duration: 16.080s, episode steps: 759, steps per second:  47, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.024 [0.000, 5.000],  loss: 0.013316, mae: 2.007470, mean_q: 2.423523, mean_eps: 0.100000
  903576/1200000: episode: 1225, duration: 6.686s, episode steps: 323, steps per second:  48, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.910 [0.000, 5.000],  loss: 0.012839, mae: 1.993887, mean_q: 2.406823, mean_eps: 0.100000
  904691/1200000: episode: 1226, duration: 23.454s, episode steps: 1115, steps per second:  48, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.169 [0.000, 5.000],  loss: 0.014736, mae: 1.987074, mean_q: 2.398628, mean_eps: 0.100000
  905805/1200000: episode: 1227, duration: 23.200s, episode steps: 1114, steps per second:  48, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.109 [0.000, 5.000],  loss: 0.013691, mae: 2.015557, mean_q: 2.435357, mean_eps: 0.100000
  906873/1200000: episode: 1228, duration: 20.428s, episode steps: 1068, steps per second:  52, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.799 [0.000, 5.000],  loss: 0.013529, mae: 2.004771, mean_q: 2.421593, mean_eps: 0.100000
  908186/1200000: episode: 1229, duration: 26.308s, episode steps: 1313, steps per second:  50, episode reward: 35.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.013101, mae: 2.008827, mean_q: 2.425484, mean_eps: 0.100000
  908956/1200000: episode: 1230, duration: 15.016s, episode steps: 770, steps per second:  51, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.022 [0.000, 5.000],  loss: 0.016323, mae: 2.014447, mean_q: 2.431501, mean_eps: 0.100000
  909855/1200000: episode: 1231, duration: 17.509s, episode steps: 899, steps per second:  51, episode reward: 28.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.015200, mae: 2.007054, mean_q: 2.422369, mean_eps: 0.100000
  910311/1200000: episode: 1232, duration: 9.447s, episode steps: 456, steps per second:  48, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.014706, mae: 2.032888, mean_q: 2.454888, mean_eps: 0.100000
  911273/1200000: episode: 1233, duration: 18.862s, episode steps: 962, steps per second:  51, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.187 [0.000, 5.000],  loss: 0.013563, mae: 2.023341, mean_q: 2.443929, mean_eps: 0.100000
  911773/1200000: episode: 1234, duration: 9.977s, episode steps: 500, steps per second:  50, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.762 [0.000, 5.000],  loss: 0.014287, mae: 2.002866, mean_q: 2.418079, mean_eps: 0.100000
  912351/1200000: episode: 1235, duration: 11.623s, episode steps: 578, steps per second:  50, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.014651, mae: 2.015354, mean_q: 2.430990, mean_eps: 0.100000
  913308/1200000: episode: 1236, duration: 19.727s, episode steps: 957, steps per second:  49, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.057 [0.000, 5.000],  loss: 0.015386, mae: 2.029109, mean_q: 2.447723, mean_eps: 0.100000
  914301/1200000: episode: 1237, duration: 22.252s, episode steps: 993, steps per second:  45, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.014456, mae: 2.031382, mean_q: 2.450163, mean_eps: 0.100000
  914876/1200000: episode: 1238, duration: 11.531s, episode steps: 575, steps per second:  50, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.701 [0.000, 5.000],  loss: 0.014994, mae: 2.033722, mean_q: 2.454389, mean_eps: 0.100000
  915676/1200000: episode: 1239, duration: 16.054s, episode steps: 800, steps per second:  50, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.204 [0.000, 5.000],  loss: 0.013088, mae: 2.012227, mean_q: 2.430279, mean_eps: 0.100000
  916305/1200000: episode: 1240, duration: 12.496s, episode steps: 629, steps per second:  50, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.269 [0.000, 5.000],  loss: 0.013602, mae: 2.026870, mean_q: 2.446432, mean_eps: 0.100000
  917490/1200000: episode: 1241, duration: 24.889s, episode steps: 1185, steps per second:  48, episode reward: 17.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.892 [0.000, 5.000],  loss: 0.014640, mae: 2.026395, mean_q: 2.446437, mean_eps: 0.100000
  918122/1200000: episode: 1242, duration: 12.895s, episode steps: 632, steps per second:  49, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.334 [0.000, 5.000],  loss: 0.013978, mae: 2.014300, mean_q: 2.433141, mean_eps: 0.100000
  919212/1200000: episode: 1243, duration: 22.678s, episode steps: 1090, steps per second:  48, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.817 [0.000, 5.000],  loss: 0.015805, mae: 2.040271, mean_q: 2.463158, mean_eps: 0.100000
  920016/1200000: episode: 1244, duration: 17.011s, episode steps: 804, steps per second:  47, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.692 [0.000, 5.000],  loss: 0.014934, mae: 2.016629, mean_q: 2.435168, mean_eps: 0.100000
  921056/1200000: episode: 1245, duration: 21.852s, episode steps: 1040, steps per second:  48, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.295 [0.000, 5.000],  loss: 0.015571, mae: 2.078152, mean_q: 2.509485, mean_eps: 0.100000
  922079/1200000: episode: 1246, duration: 21.265s, episode steps: 1023, steps per second:  48, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.517 [0.000, 5.000],  loss: 0.013411, mae: 2.066963, mean_q: 2.496725, mean_eps: 0.100000
  922871/1200000: episode: 1247, duration: 17.181s, episode steps: 792, steps per second:  46, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.004 [0.000, 5.000],  loss: 0.012404, mae: 2.072116, mean_q: 2.504520, mean_eps: 0.100000
  923237/1200000: episode: 1248, duration: 7.542s, episode steps: 366, steps per second:  49, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.101 [0.000, 5.000],  loss: 0.013396, mae: 2.053181, mean_q: 2.480896, mean_eps: 0.100000
  924169/1200000: episode: 1249, duration: 17.429s, episode steps: 932, steps per second:  53, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.948 [0.000, 5.000],  loss: 0.013532, mae: 2.067345, mean_q: 2.498118, mean_eps: 0.100000
  924959/1200000: episode: 1250, duration: 15.514s, episode steps: 790, steps per second:  51, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.671 [0.000, 5.000],  loss: 0.014055, mae: 2.081306, mean_q: 2.513161, mean_eps: 0.100000
  925890/1200000: episode: 1251, duration: 18.771s, episode steps: 931, steps per second:  50, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.046 [0.000, 5.000],  loss: 0.014084, mae: 2.050918, mean_q: 2.475951, mean_eps: 0.100000
  926586/1200000: episode: 1252, duration: 13.882s, episode steps: 696, steps per second:  50, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.012396, mae: 2.064104, mean_q: 2.493159, mean_eps: 0.100000
  927234/1200000: episode: 1253, duration: 12.477s, episode steps: 648, steps per second:  52, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.827 [0.000, 5.000],  loss: 0.015250, mae: 2.056829, mean_q: 2.482342, mean_eps: 0.100000
  928135/1200000: episode: 1254, duration: 17.534s, episode steps: 901, steps per second:  51, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.222 [0.000, 5.000],  loss: 0.014367, mae: 2.060416, mean_q: 2.487727, mean_eps: 0.100000
  929057/1200000: episode: 1255, duration: 18.701s, episode steps: 922, steps per second:  49, episode reward: 28.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.105 [0.000, 5.000],  loss: 0.014674, mae: 2.077155, mean_q: 2.507967, mean_eps: 0.100000
  929668/1200000: episode: 1256, duration: 12.428s, episode steps: 611, steps per second:  49, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.984 [0.000, 5.000],  loss: 0.014654, mae: 2.080606, mean_q: 2.512382, mean_eps: 0.100000
  930070/1200000: episode: 1257, duration: 8.058s, episode steps: 402, steps per second:  50, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.657 [0.000, 5.000],  loss: 0.012675, mae: 2.103006, mean_q: 2.540812, mean_eps: 0.100000
  930921/1200000: episode: 1258, duration: 18.533s, episode steps: 851, steps per second:  46, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.874 [0.000, 5.000],  loss: 0.014208, mae: 2.078362, mean_q: 2.510602, mean_eps: 0.100000
  931673/1200000: episode: 1259, duration: 17.848s, episode steps: 752, steps per second:  42, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.012461, mae: 2.072117, mean_q: 2.504068, mean_eps: 0.100000
  932375/1200000: episode: 1260, duration: 15.227s, episode steps: 702, steps per second:  46, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.012604, mae: 2.084401, mean_q: 2.518200, mean_eps: 0.100000
  933226/1200000: episode: 1261, duration: 17.819s, episode steps: 851, steps per second:  48, episode reward:  8.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.814 [0.000, 5.000],  loss: 0.013427, mae: 2.069707, mean_q: 2.499195, mean_eps: 0.100000
  933825/1200000: episode: 1262, duration: 12.261s, episode steps: 599, steps per second:  49, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.317 [0.000, 5.000],  loss: 0.013387, mae: 2.068042, mean_q: 2.498358, mean_eps: 0.100000
  934476/1200000: episode: 1263, duration: 13.501s, episode steps: 651, steps per second:  48, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.014137, mae: 2.091784, mean_q: 2.526262, mean_eps: 0.100000
  934905/1200000: episode: 1264, duration: 9.075s, episode steps: 429, steps per second:  47, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.026 [0.000, 5.000],  loss: 0.013379, mae: 2.060942, mean_q: 2.487644, mean_eps: 0.100000
  936700/1200000: episode: 1265, duration: 35.156s, episode steps: 1795, steps per second:  51, episode reward: 33.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.140 [0.000, 5.000],  loss: 0.015422, mae: 2.074884, mean_q: 2.504449, mean_eps: 0.100000
  937362/1200000: episode: 1266, duration: 14.333s, episode steps: 662, steps per second:  46, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.937 [0.000, 5.000],  loss: 0.014926, mae: 2.078645, mean_q: 2.510030, mean_eps: 0.100000
  938287/1200000: episode: 1267, duration: 19.472s, episode steps: 925, steps per second:  48, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.013677, mae: 2.079388, mean_q: 2.509445, mean_eps: 0.100000
  939171/1200000: episode: 1268, duration: 18.023s, episode steps: 884, steps per second:  49, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.258 [0.000, 5.000],  loss: 0.015904, mae: 2.066887, mean_q: 2.493415, mean_eps: 0.100000
  939941/1200000: episode: 1269, duration: 16.140s, episode steps: 770, steps per second:  48, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.475 [0.000, 5.000],  loss: 0.015719, mae: 2.071591, mean_q: 2.500913, mean_eps: 0.100000
  940469/1200000: episode: 1270, duration: 10.932s, episode steps: 528, steps per second:  48, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.970 [0.000, 5.000],  loss: 0.015117, mae: 2.082757, mean_q: 2.514923, mean_eps: 0.100000
  941377/1200000: episode: 1271, duration: 17.313s, episode steps: 908, steps per second:  52, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.014285, mae: 2.082010, mean_q: 2.513723, mean_eps: 0.100000
  942246/1200000: episode: 1272, duration: 16.532s, episode steps: 869, steps per second:  53, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.739 [0.000, 5.000],  loss: 0.013114, mae: 2.085541, mean_q: 2.517704, mean_eps: 0.100000
  943144/1200000: episode: 1273, duration: 17.781s, episode steps: 898, steps per second:  51, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.041 [0.000, 5.000],  loss: 0.013801, mae: 2.094600, mean_q: 2.528409, mean_eps: 0.100000
  943768/1200000: episode: 1274, duration: 12.367s, episode steps: 624, steps per second:  50, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.014185, mae: 2.085393, mean_q: 2.516488, mean_eps: 0.100000
  944787/1200000: episode: 1275, duration: 20.065s, episode steps: 1019, steps per second:  51, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.014880, mae: 2.082112, mean_q: 2.512922, mean_eps: 0.100000
  945702/1200000: episode: 1276, duration: 17.947s, episode steps: 915, steps per second:  51, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.227 [0.000, 5.000],  loss: 0.014964, mae: 2.079119, mean_q: 2.508916, mean_eps: 0.100000
  946439/1200000: episode: 1277, duration: 14.054s, episode steps: 737, steps per second:  52, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.179 [0.000, 5.000],  loss: 0.015600, mae: 2.066791, mean_q: 2.494967, mean_eps: 0.100000
  947358/1200000: episode: 1278, duration: 17.833s, episode steps: 919, steps per second:  52, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.014302, mae: 2.082135, mean_q: 2.513578, mean_eps: 0.100000
  948193/1200000: episode: 1279, duration: 17.462s, episode steps: 835, steps per second:  48, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.129 [0.000, 5.000],  loss: 0.013549, mae: 2.073320, mean_q: 2.503234, mean_eps: 0.100000
  949048/1200000: episode: 1280, duration: 20.633s, episode steps: 855, steps per second:  41, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.015222, mae: 2.075938, mean_q: 2.505618, mean_eps: 0.100000
  950023/1200000: episode: 1281, duration: 23.711s, episode steps: 975, steps per second:  41, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.142 [0.000, 5.000],  loss: 0.015108, mae: 2.089721, mean_q: 2.524328, mean_eps: 0.100000
  951328/1200000: episode: 1282, duration: 29.409s, episode steps: 1305, steps per second:  44, episode reward: 33.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.015404, mae: 2.093804, mean_q: 2.528840, mean_eps: 0.100000
  952281/1200000: episode: 1283, duration: 22.115s, episode steps: 953, steps per second:  43, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.007 [0.000, 5.000],  loss: 0.014770, mae: 2.082792, mean_q: 2.515742, mean_eps: 0.100000
  952805/1200000: episode: 1284, duration: 10.760s, episode steps: 524, steps per second:  49, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.210 [0.000, 5.000],  loss: 0.015072, mae: 2.118976, mean_q: 2.556877, mean_eps: 0.100000
  953555/1200000: episode: 1285, duration: 14.290s, episode steps: 750, steps per second:  52, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.015387, mae: 2.079706, mean_q: 2.510383, mean_eps: 0.100000
  954317/1200000: episode: 1286, duration: 15.765s, episode steps: 762, steps per second:  48, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.790 [0.000, 5.000],  loss: 0.015527, mae: 2.105814, mean_q: 2.541087, mean_eps: 0.100000
  954971/1200000: episode: 1287, duration: 13.877s, episode steps: 654, steps per second:  47, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.563 [0.000, 5.000],  loss: 0.016536, mae: 2.089991, mean_q: 2.520212, mean_eps: 0.100000
  955459/1200000: episode: 1288, duration: 10.375s, episode steps: 488, steps per second:  47, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.139 [0.000, 5.000],  loss: 0.013454, mae: 2.100345, mean_q: 2.535953, mean_eps: 0.100000
  956601/1200000: episode: 1289, duration: 24.444s, episode steps: 1142, steps per second:  47, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.014272, mae: 2.092945, mean_q: 2.525459, mean_eps: 0.100000
  957547/1200000: episode: 1290, duration: 19.205s, episode steps: 946, steps per second:  49, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.015202, mae: 2.084095, mean_q: 2.515075, mean_eps: 0.100000
  958528/1200000: episode: 1291, duration: 19.857s, episode steps: 981, steps per second:  49, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.955 [0.000, 5.000],  loss: 0.014110, mae: 2.088546, mean_q: 2.521278, mean_eps: 0.100000
  959483/1200000: episode: 1292, duration: 18.434s, episode steps: 955, steps per second:  52, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.137 [0.000, 5.000],  loss: 0.015895, mae: 2.090683, mean_q: 2.523199, mean_eps: 0.100000
  960425/1200000: episode: 1293, duration: 20.285s, episode steps: 942, steps per second:  46, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.027 [0.000, 5.000],  loss: 0.013715, mae: 2.108859, mean_q: 2.545699, mean_eps: 0.100000
  961338/1200000: episode: 1294, duration: 18.846s, episode steps: 913, steps per second:  48, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.021 [0.000, 5.000],  loss: 0.015151, mae: 2.119157, mean_q: 2.556786, mean_eps: 0.100000
  961963/1200000: episode: 1295, duration: 12.814s, episode steps: 625, steps per second:  49, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.744 [0.000, 5.000],  loss: 0.015234, mae: 2.120046, mean_q: 2.557799, mean_eps: 0.100000
  962737/1200000: episode: 1296, duration: 15.115s, episode steps: 774, steps per second:  51, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.015869, mae: 2.090650, mean_q: 2.522605, mean_eps: 0.100000
  963498/1200000: episode: 1297, duration: 14.716s, episode steps: 761, steps per second:  52, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.196 [0.000, 5.000],  loss: 0.014546, mae: 2.095687, mean_q: 2.528825, mean_eps: 0.100000
  964170/1200000: episode: 1298, duration: 13.252s, episode steps: 672, steps per second:  51, episode reward: 21.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.015578, mae: 2.101987, mean_q: 2.536160, mean_eps: 0.100000
  965031/1200000: episode: 1299, duration: 17.928s, episode steps: 861, steps per second:  48, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.015721, mae: 2.107609, mean_q: 2.543908, mean_eps: 0.100000
  965727/1200000: episode: 1300, duration: 14.892s, episode steps: 696, steps per second:  47, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.014340, mae: 2.139867, mean_q: 2.584119, mean_eps: 0.100000
  967108/1200000: episode: 1301, duration: 31.252s, episode steps: 1381, steps per second:  44, episode reward: 29.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.014634, mae: 2.106757, mean_q: 2.544381, mean_eps: 0.100000
  967767/1200000: episode: 1302, duration: 14.202s, episode steps: 659, steps per second:  46, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.932 [0.000, 5.000],  loss: 0.015473, mae: 2.099735, mean_q: 2.534947, mean_eps: 0.100000
  968648/1200000: episode: 1303, duration: 18.557s, episode steps: 881, steps per second:  47, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.064 [0.000, 5.000],  loss: 0.015087, mae: 2.107675, mean_q: 2.544729, mean_eps: 0.100000
  969668/1200000: episode: 1304, duration: 21.613s, episode steps: 1020, steps per second:  47, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.013751, mae: 2.118931, mean_q: 2.558324, mean_eps: 0.100000
  970580/1200000: episode: 1305, duration: 18.221s, episode steps: 912, steps per second:  50, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.132 [0.000, 5.000],  loss: 0.015212, mae: 2.099354, mean_q: 2.534774, mean_eps: 0.100000
  971584/1200000: episode: 1306, duration: 21.095s, episode steps: 1004, steps per second:  48, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.013381, mae: 2.077665, mean_q: 2.509148, mean_eps: 0.100000
  972447/1200000: episode: 1307, duration: 18.044s, episode steps: 863, steps per second:  48, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.061 [0.000, 5.000],  loss: 0.013712, mae: 2.095769, mean_q: 2.529888, mean_eps: 0.100000
  973197/1200000: episode: 1308, duration: 15.393s, episode steps: 750, steps per second:  49, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.237 [0.000, 5.000],  loss: 0.014577, mae: 2.101429, mean_q: 2.536765, mean_eps: 0.100000
  973883/1200000: episode: 1309, duration: 14.789s, episode steps: 686, steps per second:  46, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.959 [0.000, 5.000],  loss: 0.014825, mae: 2.107654, mean_q: 2.543338, mean_eps: 0.100000
  974707/1200000: episode: 1310, duration: 17.272s, episode steps: 824, steps per second:  48, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.014581, mae: 2.101744, mean_q: 2.537620, mean_eps: 0.100000
  975535/1200000: episode: 1311, duration: 16.977s, episode steps: 828, steps per second:  49, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.870 [0.000, 5.000],  loss: 0.012910, mae: 2.111178, mean_q: 2.549281, mean_eps: 0.100000
  976370/1200000: episode: 1312, duration: 16.067s, episode steps: 835, steps per second:  52, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.004 [0.000, 5.000],  loss: 0.013700, mae: 2.095100, mean_q: 2.529234, mean_eps: 0.100000
  977328/1200000: episode: 1313, duration: 19.752s, episode steps: 958, steps per second:  49, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.090 [0.000, 5.000],  loss: 0.015766, mae: 2.111795, mean_q: 2.551236, mean_eps: 0.100000
  977970/1200000: episode: 1314, duration: 14.654s, episode steps: 642, steps per second:  44, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.139 [0.000, 5.000],  loss: 0.014815, mae: 2.087017, mean_q: 2.522346, mean_eps: 0.100000
  978662/1200000: episode: 1315, duration: 14.014s, episode steps: 692, steps per second:  49, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.014003, mae: 2.090996, mean_q: 2.527780, mean_eps: 0.100000
  979813/1200000: episode: 1316, duration: 23.066s, episode steps: 1151, steps per second:  50, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.013 [0.000, 5.000],  loss: 0.013910, mae: 2.096392, mean_q: 2.533176, mean_eps: 0.100000
  980415/1200000: episode: 1317, duration: 11.879s, episode steps: 602, steps per second:  51, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.485 [0.000, 5.000],  loss: 0.014490, mae: 2.095218, mean_q: 2.529825, mean_eps: 0.100000
  981436/1200000: episode: 1318, duration: 19.639s, episode steps: 1021, steps per second:  52, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.112 [0.000, 5.000],  loss: 0.014596, mae: 2.106197, mean_q: 2.544048, mean_eps: 0.100000
  982135/1200000: episode: 1319, duration: 13.783s, episode steps: 699, steps per second:  51, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.139 [0.000, 5.000],  loss: 0.013499, mae: 2.119083, mean_q: 2.558645, mean_eps: 0.100000
  983038/1200000: episode: 1320, duration: 18.951s, episode steps: 903, steps per second:  48, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.110 [0.000, 5.000],  loss: 0.015893, mae: 2.130367, mean_q: 2.572194, mean_eps: 0.100000
  983855/1200000: episode: 1321, duration: 19.462s, episode steps: 817, steps per second:  42, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.908 [0.000, 5.000],  loss: 0.014690, mae: 2.126427, mean_q: 2.566523, mean_eps: 0.100000
  984514/1200000: episode: 1322, duration: 13.820s, episode steps: 659, steps per second:  48, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.874 [0.000, 5.000],  loss: 0.013655, mae: 2.103720, mean_q: 2.539509, mean_eps: 0.100000
  985518/1200000: episode: 1323, duration: 19.922s, episode steps: 1004, steps per second:  50, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.015169, mae: 2.111435, mean_q: 2.549201, mean_eps: 0.100000
  986185/1200000: episode: 1324, duration: 13.430s, episode steps: 667, steps per second:  50, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.014228, mae: 2.120325, mean_q: 2.559839, mean_eps: 0.100000
  986644/1200000: episode: 1325, duration: 9.494s, episode steps: 459, steps per second:  48, episode reward: 11.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.220 [0.000, 5.000],  loss: 0.011761, mae: 2.130644, mean_q: 2.574102, mean_eps: 0.100000
  987595/1200000: episode: 1326, duration: 18.962s, episode steps: 951, steps per second:  50, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.014500, mae: 2.083926, mean_q: 2.517333, mean_eps: 0.100000
  988562/1200000: episode: 1327, duration: 18.869s, episode steps: 967, steps per second:  51, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.199 [0.000, 5.000],  loss: 0.013988, mae: 2.115260, mean_q: 2.555219, mean_eps: 0.100000
  989496/1200000: episode: 1328, duration: 19.684s, episode steps: 934, steps per second:  47, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.111 [0.000, 5.000],  loss: 0.014973, mae: 2.102607, mean_q: 2.538946, mean_eps: 0.100000
  990058/1200000: episode: 1329, duration: 11.914s, episode steps: 562, steps per second:  47, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.014338, mae: 2.108049, mean_q: 2.546098, mean_eps: 0.100000
  990593/1200000: episode: 1330, duration: 10.773s, episode steps: 535, steps per second:  50, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.277 [0.000, 5.000],  loss: 0.016809, mae: 2.125781, mean_q: 2.566304, mean_eps: 0.100000
  991045/1200000: episode: 1331, duration: 9.596s, episode steps: 452, steps per second:  47, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.184 [0.000, 5.000],  loss: 0.015624, mae: 2.121091, mean_q: 2.559686, mean_eps: 0.100000
  992016/1200000: episode: 1332, duration: 20.673s, episode steps: 971, steps per second:  47, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.013764, mae: 2.139079, mean_q: 2.581550, mean_eps: 0.100000
  992874/1200000: episode: 1333, duration: 17.394s, episode steps: 858, steps per second:  49, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.176 [0.000, 5.000],  loss: 0.015567, mae: 2.145646, mean_q: 2.589783, mean_eps: 0.100000
  993839/1200000: episode: 1334, duration: 18.044s, episode steps: 965, steps per second:  53, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.263 [0.000, 5.000],  loss: 0.014972, mae: 2.151712, mean_q: 2.597220, mean_eps: 0.100000
  994484/1200000: episode: 1335, duration: 12.434s, episode steps: 645, steps per second:  52, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.014365, mae: 2.130366, mean_q: 2.572378, mean_eps: 0.100000
  995512/1200000: episode: 1336, duration: 21.796s, episode steps: 1028, steps per second:  47, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.014891, mae: 2.138661, mean_q: 2.580941, mean_eps: 0.100000
  996071/1200000: episode: 1337, duration: 11.282s, episode steps: 559, steps per second:  50, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.015502, mae: 2.155785, mean_q: 2.601809, mean_eps: 0.100000
  996947/1200000: episode: 1338, duration: 17.365s, episode steps: 876, steps per second:  50, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.014629, mae: 2.122953, mean_q: 2.562885, mean_eps: 0.100000
  997952/1200000: episode: 1339, duration: 20.326s, episode steps: 1005, steps per second:  49, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.182 [0.000, 5.000],  loss: 0.013791, mae: 2.150268, mean_q: 2.596540, mean_eps: 0.100000
  998963/1200000: episode: 1340, duration: 19.582s, episode steps: 1011, steps per second:  52, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.878 [0.000, 5.000],  loss: 0.013636, mae: 2.131621, mean_q: 2.573594, mean_eps: 0.100000
  999753/1200000: episode: 1341, duration: 16.353s, episode steps: 790, steps per second:  48, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.227 [0.000, 5.000],  loss: 0.014324, mae: 2.145932, mean_q: 2.591375, mean_eps: 0.100000
 1000243/1200000: episode: 1342, duration: 10.127s, episode steps: 490, steps per second:  48, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.014198, mae: 2.139660, mean_q: 2.583160, mean_eps: 0.100000
 1000769/1200000: episode: 1343, duration: 12.219s, episode steps: 526, steps per second:  43, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 0.014267, mae: 2.144623, mean_q: 2.589765, mean_eps: 0.100000
 1001492/1200000: episode: 1344, duration: 16.785s, episode steps: 723, steps per second:  43, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.015699, mae: 2.140660, mean_q: 2.583863, mean_eps: 0.100000
 1002545/1200000: episode: 1345, duration: 21.616s, episode steps: 1053, steps per second:  49, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.013321, mae: 2.162675, mean_q: 2.612608, mean_eps: 0.100000
 1003419/1200000: episode: 1346, duration: 17.908s, episode steps: 874, steps per second:  49, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.299 [0.000, 5.000],  loss: 0.014294, mae: 2.169201, mean_q: 2.618781, mean_eps: 0.100000
 1003924/1200000: episode: 1347, duration: 10.879s, episode steps: 505, steps per second:  46, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.701 [0.000, 5.000],  loss: 0.014432, mae: 2.160490, mean_q: 2.609242, mean_eps: 0.100000
 1004988/1200000: episode: 1348, duration: 21.011s, episode steps: 1064, steps per second:  51, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.901 [0.000, 5.000],  loss: 0.014344, mae: 2.161145, mean_q: 2.610773, mean_eps: 0.100000
 1005760/1200000: episode: 1349, duration: 14.906s, episode steps: 772, steps per second:  52, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.679 [0.000, 5.000],  loss: 0.014590, mae: 2.152298, mean_q: 2.598510, mean_eps: 0.100000
 1006456/1200000: episode: 1350, duration: 14.727s, episode steps: 696, steps per second:  47, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.141 [0.000, 5.000],  loss: 0.015454, mae: 2.172438, mean_q: 2.622023, mean_eps: 0.100000
 1007215/1200000: episode: 1351, duration: 17.969s, episode steps: 759, steps per second:  42, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.121 [0.000, 5.000],  loss: 0.013956, mae: 2.175489, mean_q: 2.626119, mean_eps: 0.100000
 1008153/1200000: episode: 1352, duration: 21.125s, episode steps: 938, steps per second:  44, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.015630, mae: 2.159266, mean_q: 2.606114, mean_eps: 0.100000
 1008891/1200000: episode: 1353, duration: 16.258s, episode steps: 738, steps per second:  45, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.977 [0.000, 5.000],  loss: 0.014704, mae: 2.176227, mean_q: 2.626290, mean_eps: 0.100000
 1009608/1200000: episode: 1354, duration: 16.184s, episode steps: 717, steps per second:  44, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.873 [0.000, 5.000],  loss: 0.013917, mae: 2.162532, mean_q: 2.610529, mean_eps: 0.100000
 1010528/1200000: episode: 1355, duration: 19.790s, episode steps: 920, steps per second:  46, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.903 [0.000, 5.000],  loss: 0.012782, mae: 2.178680, mean_q: 2.631903, mean_eps: 0.100000
 1011417/1200000: episode: 1356, duration: 18.553s, episode steps: 889, steps per second:  48, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.014480, mae: 2.205146, mean_q: 2.662618, mean_eps: 0.100000
 1012364/1200000: episode: 1357, duration: 20.369s, episode steps: 947, steps per second:  46, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.015318, mae: 2.202797, mean_q: 2.658239, mean_eps: 0.100000
 1013244/1200000: episode: 1358, duration: 18.652s, episode steps: 880, steps per second:  47, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.772 [0.000, 5.000],  loss: 0.014340, mae: 2.206084, mean_q: 2.663684, mean_eps: 0.100000
 1013915/1200000: episode: 1359, duration: 13.278s, episode steps: 671, steps per second:  51, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 0.013364, mae: 2.192275, mean_q: 2.645737, mean_eps: 0.100000
 1014834/1200000: episode: 1360, duration: 18.619s, episode steps: 919, steps per second:  49, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.156 [0.000, 5.000],  loss: 0.014654, mae: 2.200300, mean_q: 2.655489, mean_eps: 0.100000
 1015792/1200000: episode: 1361, duration: 19.791s, episode steps: 958, steps per second:  48, episode reward: 29.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.015801, mae: 2.213390, mean_q: 2.670630, mean_eps: 0.100000
 1016606/1200000: episode: 1362, duration: 17.271s, episode steps: 814, steps per second:  47, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.162 [0.000, 5.000],  loss: 0.015414, mae: 2.197692, mean_q: 2.652266, mean_eps: 0.100000
 1017340/1200000: episode: 1363, duration: 15.923s, episode steps: 734, steps per second:  46, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.995 [0.000, 5.000],  loss: 0.014230, mae: 2.203930, mean_q: 2.662201, mean_eps: 0.100000
 1017998/1200000: episode: 1364, duration: 15.982s, episode steps: 658, steps per second:  41, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.942 [0.000, 5.000],  loss: 0.013208, mae: 2.175750, mean_q: 2.626571, mean_eps: 0.100000
 1019182/1200000: episode: 1365, duration: 28.248s, episode steps: 1184, steps per second:  42, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.699 [0.000, 5.000],  loss: 0.017647, mae: 2.198972, mean_q: 2.654946, mean_eps: 0.100000
 1020633/1200000: episode: 1366, duration: 33.934s, episode steps: 1451, steps per second:  43, episode reward: 30.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.720 [0.000, 5.000],  loss: 0.014456, mae: 2.206240, mean_q: 2.665851, mean_eps: 0.100000
 1021317/1200000: episode: 1367, duration: 15.272s, episode steps: 684, steps per second:  45, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.968 [0.000, 5.000],  loss: 0.013046, mae: 2.215524, mean_q: 2.676846, mean_eps: 0.100000
 1021774/1200000: episode: 1368, duration: 10.261s, episode steps: 457, steps per second:  45, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.947 [0.000, 5.000],  loss: 0.015124, mae: 2.228803, mean_q: 2.692106, mean_eps: 0.100000
 1022509/1200000: episode: 1369, duration: 16.201s, episode steps: 735, steps per second:  45, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.052 [0.000, 5.000],  loss: 0.013722, mae: 2.217072, mean_q: 2.678080, mean_eps: 0.100000
 1023196/1200000: episode: 1370, duration: 16.922s, episode steps: 687, steps per second:  41, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.243 [0.000, 5.000],  loss: 0.014856, mae: 2.216902, mean_q: 2.676455, mean_eps: 0.100000
 1024132/1200000: episode: 1371, duration: 21.951s, episode steps: 936, steps per second:  43, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.078 [0.000, 5.000],  loss: 0.014973, mae: 2.239465, mean_q: 2.704081, mean_eps: 0.100000
 1024817/1200000: episode: 1372, duration: 16.034s, episode steps: 685, steps per second:  43, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.155 [0.000, 5.000],  loss: 0.012035, mae: 2.223428, mean_q: 2.685418, mean_eps: 0.100000
 1025906/1200000: episode: 1373, duration: 24.354s, episode steps: 1089, steps per second:  45, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.015168, mae: 2.219151, mean_q: 2.679663, mean_eps: 0.100000
 1026334/1200000: episode: 1374, duration: 9.747s, episode steps: 428, steps per second:  44, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.143 [0.000, 5.000],  loss: 0.014215, mae: 2.220235, mean_q: 2.682397, mean_eps: 0.100000
 1027022/1200000: episode: 1375, duration: 14.374s, episode steps: 688, steps per second:  48, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.014568, mae: 2.221129, mean_q: 2.681366, mean_eps: 0.100000
 1027997/1200000: episode: 1376, duration: 20.983s, episode steps: 975, steps per second:  46, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.015316, mae: 2.214024, mean_q: 2.671503, mean_eps: 0.100000
 1028980/1200000: episode: 1377, duration: 22.122s, episode steps: 983, steps per second:  44, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.189 [0.000, 5.000],  loss: 0.013743, mae: 2.209675, mean_q: 2.666811, mean_eps: 0.100000
 1029738/1200000: episode: 1378, duration: 16.058s, episode steps: 758, steps per second:  47, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.228 [0.000, 5.000],  loss: 0.017558, mae: 2.236914, mean_q: 2.698392, mean_eps: 0.100000
 1030453/1200000: episode: 1379, duration: 14.914s, episode steps: 715, steps per second:  48, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.015388, mae: 2.222301, mean_q: 2.684187, mean_eps: 0.100000
 1031358/1200000: episode: 1380, duration: 18.730s, episode steps: 905, steps per second:  48, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.746 [0.000, 5.000],  loss: 0.015038, mae: 2.262991, mean_q: 2.732219, mean_eps: 0.100000
 1032224/1200000: episode: 1381, duration: 18.709s, episode steps: 866, steps per second:  46, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.013116, mae: 2.247114, mean_q: 2.713719, mean_eps: 0.100000
 1032727/1200000: episode: 1382, duration: 10.807s, episode steps: 503, steps per second:  47, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.014744, mae: 2.243075, mean_q: 2.708406, mean_eps: 0.100000
 1033562/1200000: episode: 1383, duration: 18.093s, episode steps: 835, steps per second:  46, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.099 [0.000, 5.000],  loss: 0.014119, mae: 2.264558, mean_q: 2.733495, mean_eps: 0.100000
 1034378/1200000: episode: 1384, duration: 18.535s, episode steps: 816, steps per second:  44, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.088 [0.000, 5.000],  loss: 0.015391, mae: 2.246307, mean_q: 2.711637, mean_eps: 0.100000
 1035370/1200000: episode: 1385, duration: 21.015s, episode steps: 992, steps per second:  47, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.193 [0.000, 5.000],  loss: 0.014672, mae: 2.273283, mean_q: 2.744045, mean_eps: 0.100000
 1036221/1200000: episode: 1386, duration: 17.464s, episode steps: 851, steps per second:  49, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.015215, mae: 2.247797, mean_q: 2.713651, mean_eps: 0.100000
 1036892/1200000: episode: 1387, duration: 14.841s, episode steps: 671, steps per second:  45, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.037 [0.000, 5.000],  loss: 0.014342, mae: 2.256312, mean_q: 2.723820, mean_eps: 0.100000
 1037676/1200000: episode: 1388, duration: 17.173s, episode steps: 784, steps per second:  46, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.742 [0.000, 5.000],  loss: 0.014965, mae: 2.277539, mean_q: 2.748828, mean_eps: 0.100000
 1038628/1200000: episode: 1389, duration: 20.458s, episode steps: 952, steps per second:  47, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.639 [0.000, 5.000],  loss: 0.013121, mae: 2.248262, mean_q: 2.714485, mean_eps: 0.100000
 1039750/1200000: episode: 1390, duration: 26.466s, episode steps: 1122, steps per second:  42, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.039 [0.000, 5.000],  loss: 0.014920, mae: 2.282095, mean_q: 2.755732, mean_eps: 0.100000
 1040660/1200000: episode: 1391, duration: 20.891s, episode steps: 910, steps per second:  44, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.014331, mae: 2.243448, mean_q: 2.708259, mean_eps: 0.100000
 1041410/1200000: episode: 1392, duration: 17.311s, episode steps: 750, steps per second:  43, episode reward: 21.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.969 [0.000, 5.000],  loss: 0.013586, mae: 2.240071, mean_q: 2.705403, mean_eps: 0.100000
 1043078/1200000: episode: 1393, duration: 38.158s, episode steps: 1668, steps per second:  44, episode reward: 32.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.015778, mae: 2.266184, mean_q: 2.736407, mean_eps: 0.100000
 1044033/1200000: episode: 1394, duration: 19.782s, episode steps: 955, steps per second:  48, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.016022, mae: 2.261753, mean_q: 2.730007, mean_eps: 0.100000
 1044945/1200000: episode: 1395, duration: 21.091s, episode steps: 912, steps per second:  43, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.246 [0.000, 5.000],  loss: 0.015020, mae: 2.251154, mean_q: 2.717940, mean_eps: 0.100000
 1045927/1200000: episode: 1396, duration: 21.603s, episode steps: 982, steps per second:  45, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.145 [0.000, 5.000],  loss: 0.015129, mae: 2.267971, mean_q: 2.736019, mean_eps: 0.100000
 1046895/1200000: episode: 1397, duration: 20.279s, episode steps: 968, steps per second:  48, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.806 [0.000, 5.000],  loss: 0.016284, mae: 2.273739, mean_q: 2.743074, mean_eps: 0.100000
 1047830/1200000: episode: 1398, duration: 18.962s, episode steps: 935, steps per second:  49, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.172 [0.000, 5.000],  loss: 0.016675, mae: 2.270488, mean_q: 2.739683, mean_eps: 0.100000
 1048625/1200000: episode: 1399, duration: 16.337s, episode steps: 795, steps per second:  49, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.809 [0.000, 5.000],  loss: 0.014300, mae: 2.265472, mean_q: 2.733862, mean_eps: 0.100000
 1049714/1200000: episode: 1400, duration: 23.379s, episode steps: 1089, steps per second:  47, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.085 [0.000, 5.000],  loss: 0.014916, mae: 2.249123, mean_q: 2.715596, mean_eps: 0.100000
 1050388/1200000: episode: 1401, duration: 19.251s, episode steps: 674, steps per second:  35, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.016915, mae: 2.286763, mean_q: 2.762213, mean_eps: 0.100000
 1051343/1200000: episode: 1402, duration: 24.759s, episode steps: 955, steps per second:  39, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.172 [0.000, 5.000],  loss: 0.015138, mae: 2.285031, mean_q: 2.758946, mean_eps: 0.100000
 1052034/1200000: episode: 1403, duration: 17.060s, episode steps: 691, steps per second:  41, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.162 [0.000, 5.000],  loss: 0.013558, mae: 2.274587, mean_q: 2.746571, mean_eps: 0.100000
 1052489/1200000: episode: 1404, duration: 11.562s, episode steps: 455, steps per second:  39, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.765 [0.000, 5.000],  loss: 0.016629, mae: 2.272989, mean_q: 2.743221, mean_eps: 0.100000
 1053495/1200000: episode: 1405, duration: 23.286s, episode steps: 1006, steps per second:  43, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.056 [0.000, 5.000],  loss: 0.014521, mae: 2.275924, mean_q: 2.747604, mean_eps: 0.100000
 1054468/1200000: episode: 1406, duration: 20.986s, episode steps: 973, steps per second:  46, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.016299, mae: 2.293835, mean_q: 2.767895, mean_eps: 0.100000
 1055542/1200000: episode: 1407, duration: 24.662s, episode steps: 1074, steps per second:  44, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.138 [0.000, 5.000],  loss: 0.013490, mae: 2.294342, mean_q: 2.770472, mean_eps: 0.100000
 1056603/1200000: episode: 1408, duration: 24.599s, episode steps: 1061, steps per second:  43, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.988 [0.000, 5.000],  loss: 0.016961, mae: 2.276196, mean_q: 2.747155, mean_eps: 0.100000
 1057214/1200000: episode: 1409, duration: 14.621s, episode steps: 611, steps per second:  42, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.016078, mae: 2.253007, mean_q: 2.720085, mean_eps: 0.100000
 1057876/1200000: episode: 1410, duration: 15.176s, episode steps: 662, steps per second:  44, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.967 [0.000, 5.000],  loss: 0.014664, mae: 2.264387, mean_q: 2.733238, mean_eps: 0.100000
 1058617/1200000: episode: 1411, duration: 16.635s, episode steps: 741, steps per second:  45, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.816 [0.000, 5.000],  loss: 0.013937, mae: 2.289885, mean_q: 2.766052, mean_eps: 0.100000
 1059233/1200000: episode: 1412, duration: 13.213s, episode steps: 616, steps per second:  47, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.955 [0.000, 5.000],  loss: 0.014599, mae: 2.281020, mean_q: 2.753754, mean_eps: 0.100000
 1060353/1200000: episode: 1413, duration: 23.466s, episode steps: 1120, steps per second:  48, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.807 [0.000, 5.000],  loss: 0.014950, mae: 2.278062, mean_q: 2.751920, mean_eps: 0.100000
 1061471/1200000: episode: 1414, duration: 23.636s, episode steps: 1118, steps per second:  47, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.148 [0.000, 5.000],  loss: 0.013309, mae: 2.289338, mean_q: 2.765783, mean_eps: 0.100000
 1062401/1200000: episode: 1415, duration: 19.210s, episode steps: 930, steps per second:  48, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.902 [0.000, 5.000],  loss: 0.016212, mae: 2.309443, mean_q: 2.788533, mean_eps: 0.100000
 1063289/1200000: episode: 1416, duration: 18.479s, episode steps: 888, steps per second:  48, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.816 [0.000, 5.000],  loss: 0.015852, mae: 2.301488, mean_q: 2.778092, mean_eps: 0.100000
 1064078/1200000: episode: 1417, duration: 16.312s, episode steps: 789, steps per second:  48, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.894 [0.000, 5.000],  loss: 0.015821, mae: 2.302784, mean_q: 2.781770, mean_eps: 0.100000
 1064978/1200000: episode: 1418, duration: 21.058s, episode steps: 900, steps per second:  43, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.602 [0.000, 5.000],  loss: 0.015746, mae: 2.302475, mean_q: 2.781072, mean_eps: 0.100000
 1065754/1200000: episode: 1419, duration: 17.866s, episode steps: 776, steps per second:  43, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.258 [0.000, 5.000],  loss: 0.015006, mae: 2.283629, mean_q: 2.758149, mean_eps: 0.100000
 1066627/1200000: episode: 1420, duration: 22.188s, episode steps: 873, steps per second:  39, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.632 [0.000, 5.000],  loss: 0.014452, mae: 2.302466, mean_q: 2.780472, mean_eps: 0.100000
 1067331/1200000: episode: 1421, duration: 17.005s, episode steps: 704, steps per second:  41, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.091 [0.000, 5.000],  loss: 0.016781, mae: 2.290123, mean_q: 2.764781, mean_eps: 0.100000
 1068255/1200000: episode: 1422, duration: 21.702s, episode steps: 924, steps per second:  43, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.983 [0.000, 5.000],  loss: 0.014206, mae: 2.296256, mean_q: 2.773783, mean_eps: 0.100000
 1068808/1200000: episode: 1423, duration: 12.237s, episode steps: 553, steps per second:  45, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.942 [0.000, 5.000],  loss: 0.013456, mae: 2.311585, mean_q: 2.792452, mean_eps: 0.100000
 1069657/1200000: episode: 1424, duration: 19.047s, episode steps: 849, steps per second:  45, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.014835, mae: 2.302044, mean_q: 2.780479, mean_eps: 0.100000
 1070403/1200000: episode: 1425, duration: 17.034s, episode steps: 746, steps per second:  44, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.966 [0.000, 5.000],  loss: 0.017018, mae: 2.322567, mean_q: 2.804747, mean_eps: 0.100000
 1071106/1200000: episode: 1426, duration: 16.692s, episode steps: 703, steps per second:  42, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.191 [0.000, 5.000],  loss: 0.013712, mae: 2.332652, mean_q: 2.819655, mean_eps: 0.100000
 1072009/1200000: episode: 1427, duration: 20.278s, episode steps: 903, steps per second:  45, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.948 [0.000, 5.000],  loss: 0.016306, mae: 2.335556, mean_q: 2.822154, mean_eps: 0.100000
 1072572/1200000: episode: 1428, duration: 12.861s, episode steps: 563, steps per second:  44, episode reward: 15.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.826 [0.000, 5.000],  loss: 0.014699, mae: 2.311852, mean_q: 2.794286, mean_eps: 0.100000
 1073673/1200000: episode: 1429, duration: 24.734s, episode steps: 1101, steps per second:  45, episode reward: 32.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 0.015306, mae: 2.334816, mean_q: 2.820947, mean_eps: 0.100000
 1074685/1200000: episode: 1430, duration: 23.314s, episode steps: 1012, steps per second:  43, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.015557, mae: 2.343729, mean_q: 2.829971, mean_eps: 0.100000
 1075576/1200000: episode: 1431, duration: 18.544s, episode steps: 891, steps per second:  48, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.957 [0.000, 5.000],  loss: 0.014811, mae: 2.334910, mean_q: 2.820424, mean_eps: 0.100000
 1076573/1200000: episode: 1432, duration: 20.737s, episode steps: 997, steps per second:  48, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.069 [0.000, 5.000],  loss: 0.015495, mae: 2.323033, mean_q: 2.807205, mean_eps: 0.100000
 1077299/1200000: episode: 1433, duration: 16.322s, episode steps: 726, steps per second:  44, episode reward: 20.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.013685, mae: 2.342749, mean_q: 2.828117, mean_eps: 0.100000
 1078262/1200000: episode: 1434, duration: 21.779s, episode steps: 963, steps per second:  44, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.013535, mae: 2.332663, mean_q: 2.817259, mean_eps: 0.100000
 1079229/1200000: episode: 1435, duration: 20.920s, episode steps: 967, steps per second:  46, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.294 [0.000, 5.000],  loss: 0.015928, mae: 2.330193, mean_q: 2.815067, mean_eps: 0.100000
 1079847/1200000: episode: 1436, duration: 13.246s, episode steps: 618, steps per second:  47, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.749 [0.000, 5.000],  loss: 0.014625, mae: 2.341967, mean_q: 2.828838, mean_eps: 0.100000
 1080345/1200000: episode: 1437, duration: 10.760s, episode steps: 498, steps per second:  46, episode reward: 15.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.015936, mae: 2.319538, mean_q: 2.800546, mean_eps: 0.100000
 1081196/1200000: episode: 1438, duration: 20.599s, episode steps: 851, steps per second:  41, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.291 [0.000, 5.000],  loss: 0.014272, mae: 2.339168, mean_q: 2.824911, mean_eps: 0.100000
 1082242/1200000: episode: 1439, duration: 27.164s, episode steps: 1046, steps per second:  39, episode reward: 29.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.015880, mae: 2.346425, mean_q: 2.832853, mean_eps: 0.100000
 1083007/1200000: episode: 1440, duration: 17.770s, episode steps: 765, steps per second:  43, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.015935, mae: 2.362989, mean_q: 2.853767, mean_eps: 0.100000
 1084086/1200000: episode: 1441, duration: 23.588s, episode steps: 1079, steps per second:  46, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.015788, mae: 2.331921, mean_q: 2.814933, mean_eps: 0.100000
 1085064/1200000: episode: 1442, duration: 20.623s, episode steps: 978, steps per second:  47, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.014939, mae: 2.328738, mean_q: 2.811282, mean_eps: 0.100000
 1085689/1200000: episode: 1443, duration: 13.353s, episode steps: 625, steps per second:  47, episode reward: 18.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.699 [0.000, 5.000],  loss: 0.014607, mae: 2.325894, mean_q: 2.807620, mean_eps: 0.100000
 1086704/1200000: episode: 1444, duration: 21.635s, episode steps: 1015, steps per second:  47, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.214 [0.000, 5.000],  loss: 0.016195, mae: 2.348458, mean_q: 2.834677, mean_eps: 0.100000
 1087769/1200000: episode: 1445, duration: 24.567s, episode steps: 1065, steps per second:  43, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.014899, mae: 2.338965, mean_q: 2.824904, mean_eps: 0.100000
 1088768/1200000: episode: 1446, duration: 22.129s, episode steps: 999, steps per second:  45, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.906 [0.000, 5.000],  loss: 0.014406, mae: 2.361675, mean_q: 2.852318, mean_eps: 0.100000
 1089708/1200000: episode: 1447, duration: 21.509s, episode steps: 940, steps per second:  44, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.846 [0.000, 5.000],  loss: 0.016175, mae: 2.337050, mean_q: 2.821296, mean_eps: 0.100000
 1090756/1200000: episode: 1448, duration: 23.668s, episode steps: 1048, steps per second:  44, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.014706, mae: 2.352962, mean_q: 2.839282, mean_eps: 0.100000
 1091739/1200000: episode: 1449, duration: 20.509s, episode steps: 983, steps per second:  48, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.015493, mae: 2.342369, mean_q: 2.828906, mean_eps: 0.100000
 1092215/1200000: episode: 1450, duration: 9.536s, episode steps: 476, steps per second:  50, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.267 [0.000, 5.000],  loss: 0.016332, mae: 2.360811, mean_q: 2.850178, mean_eps: 0.100000
 1093145/1200000: episode: 1451, duration: 19.775s, episode steps: 930, steps per second:  47, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.014029, mae: 2.348804, mean_q: 2.836158, mean_eps: 0.100000
 1093877/1200000: episode: 1452, duration: 15.661s, episode steps: 732, steps per second:  47, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.016315, mae: 2.354627, mean_q: 2.843614, mean_eps: 0.100000
 1094816/1200000: episode: 1453, duration: 19.373s, episode steps: 939, steps per second:  48, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.014602, mae: 2.348116, mean_q: 2.834169, mean_eps: 0.100000
 1095960/1200000: episode: 1454, duration: 23.751s, episode steps: 1144, steps per second:  48, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.015938, mae: 2.355344, mean_q: 2.845023, mean_eps: 0.100000
 1096576/1200000: episode: 1455, duration: 12.854s, episode steps: 616, steps per second:  48, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.166 [0.000, 5.000],  loss: 0.016395, mae: 2.333181, mean_q: 2.819850, mean_eps: 0.100000
 1097672/1200000: episode: 1456, duration: 22.600s, episode steps: 1096, steps per second:  48, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.175 [0.000, 5.000],  loss: 0.016572, mae: 2.345831, mean_q: 2.835238, mean_eps: 0.100000
 1098350/1200000: episode: 1457, duration: 15.023s, episode steps: 678, steps per second:  45, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.111 [0.000, 5.000],  loss: 0.015494, mae: 2.349250, mean_q: 2.839430, mean_eps: 0.100000
 1099254/1200000: episode: 1458, duration: 20.512s, episode steps: 904, steps per second:  44, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.013863, mae: 2.361914, mean_q: 2.854035, mean_eps: 0.100000
 1100047/1200000: episode: 1459, duration: 16.381s, episode steps: 793, steps per second:  48, episode reward: 25.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.115 [0.000, 5.000],  loss: 0.015664, mae: 2.358607, mean_q: 2.849488, mean_eps: 0.100000
 1100794/1200000: episode: 1460, duration: 15.346s, episode steps: 747, steps per second:  49, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.940 [0.000, 5.000],  loss: 0.015936, mae: 2.392255, mean_q: 2.888725, mean_eps: 0.100000
 1102012/1200000: episode: 1461, duration: 25.682s, episode steps: 1218, steps per second:  47, episode reward: 32.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.015457, mae: 2.375013, mean_q: 2.868645, mean_eps: 0.100000
 1103040/1200000: episode: 1462, duration: 21.238s, episode steps: 1028, steps per second:  48, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.299 [0.000, 5.000],  loss: 0.014604, mae: 2.377165, mean_q: 2.872069, mean_eps: 0.100000
 1103789/1200000: episode: 1463, duration: 17.011s, episode steps: 749, steps per second:  44, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.967 [0.000, 5.000],  loss: 0.016123, mae: 2.390744, mean_q: 2.885841, mean_eps: 0.100000
 1105392/1200000: episode: 1464, duration: 36.675s, episode steps: 1603, steps per second:  44, episode reward: 36.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.152 [0.000, 5.000],  loss: 0.015946, mae: 2.394010, mean_q: 2.890984, mean_eps: 0.100000
 1105977/1200000: episode: 1465, duration: 13.408s, episode steps: 585, steps per second:  44, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.065 [0.000, 5.000],  loss: 0.014979, mae: 2.384598, mean_q: 2.879108, mean_eps: 0.100000
 1106571/1200000: episode: 1466, duration: 13.057s, episode steps: 594, steps per second:  45, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.936 [0.000, 5.000],  loss: 0.017168, mae: 2.404170, mean_q: 2.902023, mean_eps: 0.100000
 1107180/1200000: episode: 1467, duration: 14.421s, episode steps: 609, steps per second:  42, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.015876, mae: 2.391952, mean_q: 2.888523, mean_eps: 0.100000
 1107752/1200000: episode: 1468, duration: 12.848s, episode steps: 572, steps per second:  45, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.009 [0.000, 5.000],  loss: 0.018827, mae: 2.397271, mean_q: 2.894124, mean_eps: 0.100000
 1108676/1200000: episode: 1469, duration: 18.827s, episode steps: 924, steps per second:  49, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.273 [0.000, 5.000],  loss: 0.015989, mae: 2.409990, mean_q: 2.910203, mean_eps: 0.100000
 1109583/1200000: episode: 1470, duration: 19.498s, episode steps: 907, steps per second:  47, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.179 [0.000, 5.000],  loss: 0.014812, mae: 2.392202, mean_q: 2.888591, mean_eps: 0.100000
 1110508/1200000: episode: 1471, duration: 20.544s, episode steps: 925, steps per second:  45, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.107 [0.000, 5.000],  loss: 0.014620, mae: 2.409379, mean_q: 2.911588, mean_eps: 0.100000
 1111226/1200000: episode: 1472, duration: 14.978s, episode steps: 718, steps per second:  48, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.092 [0.000, 5.000],  loss: 0.015462, mae: 2.413545, mean_q: 2.914789, mean_eps: 0.100000
 1111730/1200000: episode: 1473, duration: 10.210s, episode steps: 504, steps per second:  49, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.169 [0.000, 5.000],  loss: 0.018836, mae: 2.441916, mean_q: 2.949585, mean_eps: 0.100000
 1112190/1200000: episode: 1474, duration: 9.589s, episode steps: 460, steps per second:  48, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.020772, mae: 2.446792, mean_q: 2.955283, mean_eps: 0.100000
 1112797/1200000: episode: 1475, duration: 12.830s, episode steps: 607, steps per second:  47, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: 0.017039, mae: 2.420853, mean_q: 2.923998, mean_eps: 0.100000
 1113175/1200000: episode: 1476, duration: 7.461s, episode steps: 378, steps per second:  51, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.770 [0.000, 5.000],  loss: 0.020098, mae: 2.428245, mean_q: 2.930622, mean_eps: 0.100000
 1114306/1200000: episode: 1477, duration: 23.429s, episode steps: 1131, steps per second:  48, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.014868, mae: 2.419872, mean_q: 2.922767, mean_eps: 0.100000
 1115128/1200000: episode: 1478, duration: 18.878s, episode steps: 822, steps per second:  44, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.768 [0.000, 5.000],  loss: 0.014135, mae: 2.427664, mean_q: 2.933239, mean_eps: 0.100000
 1116086/1200000: episode: 1479, duration: 21.398s, episode steps: 958, steps per second:  45, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.287 [0.000, 5.000],  loss: 0.019763, mae: 2.432262, mean_q: 2.935409, mean_eps: 0.100000
 1117559/1200000: episode: 1480, duration: 32.063s, episode steps: 1473, steps per second:  46, episode reward: 31.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.247 [0.000, 5.000],  loss: 0.014929, mae: 2.430742, mean_q: 2.934586, mean_eps: 0.100000
 1118562/1200000: episode: 1481, duration: 23.535s, episode steps: 1003, steps per second:  43, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.178 [0.000, 5.000],  loss: 0.015847, mae: 2.454403, mean_q: 2.964532, mean_eps: 0.100000
 1119045/1200000: episode: 1482, duration: 10.642s, episode steps: 483, steps per second:  45, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.673 [0.000, 5.000],  loss: 0.017038, mae: 2.450800, mean_q: 2.958858, mean_eps: 0.100000
 1119927/1200000: episode: 1483, duration: 19.259s, episode steps: 882, steps per second:  46, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.376 [0.000, 5.000],  loss: 0.014623, mae: 2.435741, mean_q: 2.941165, mean_eps: 0.100000
 1120516/1200000: episode: 1484, duration: 14.646s, episode steps: 589, steps per second:  40, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.757 [0.000, 5.000],  loss: 0.015220, mae: 2.426914, mean_q: 2.929837, mean_eps: 0.100000
 1121229/1200000: episode: 1485, duration: 16.506s, episode steps: 713, steps per second:  43, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.971 [0.000, 5.000],  loss: 0.014681, mae: 2.421621, mean_q: 2.924892, mean_eps: 0.100000
 1121939/1200000: episode: 1486, duration: 16.706s, episode steps: 710, steps per second:  42, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.015851, mae: 2.452889, mean_q: 2.961022, mean_eps: 0.100000
 1122473/1200000: episode: 1487, duration: 12.372s, episode steps: 534, steps per second:  43, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.596 [0.000, 5.000],  loss: 0.016236, mae: 2.441500, mean_q: 2.946665, mean_eps: 0.100000
 1123448/1200000: episode: 1488, duration: 23.472s, episode steps: 975, steps per second:  42, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.730 [0.000, 5.000],  loss: 0.014977, mae: 2.438227, mean_q: 2.943390, mean_eps: 0.100000
 1124372/1200000: episode: 1489, duration: 20.039s, episode steps: 924, steps per second:  46, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.017076, mae: 2.432183, mean_q: 2.933697, mean_eps: 0.100000
 1125469/1200000: episode: 1490, duration: 22.736s, episode steps: 1097, steps per second:  48, episode reward: 33.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.990 [0.000, 5.000],  loss: 0.015689, mae: 2.438448, mean_q: 2.942796, mean_eps: 0.100000
 1126166/1200000: episode: 1491, duration: 15.020s, episode steps: 697, steps per second:  46, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.674 [0.000, 5.000],  loss: 0.015692, mae: 2.434720, mean_q: 2.938603, mean_eps: 0.100000
 1127126/1200000: episode: 1492, duration: 20.021s, episode steps: 960, steps per second:  48, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.077 [0.000, 5.000],  loss: 0.015403, mae: 2.446009, mean_q: 2.952165, mean_eps: 0.100000
 1128089/1200000: episode: 1493, duration: 20.099s, episode steps: 963, steps per second:  48, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.273 [0.000, 5.000],  loss: 0.015890, mae: 2.441386, mean_q: 2.946393, mean_eps: 0.100000
 1128995/1200000: episode: 1494, duration: 19.767s, episode steps: 906, steps per second:  46, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.015126, mae: 2.439960, mean_q: 2.944291, mean_eps: 0.100000
 1129532/1200000: episode: 1495, duration: 11.100s, episode steps: 537, steps per second:  48, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.972 [0.000, 5.000],  loss: 0.018011, mae: 2.440958, mean_q: 2.945409, mean_eps: 0.100000
 1130133/1200000: episode: 1496, duration: 13.357s, episode steps: 601, steps per second:  45, episode reward: 15.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 0.016532, mae: 2.447911, mean_q: 2.953529, mean_eps: 0.100000
 1130824/1200000: episode: 1497, duration: 14.756s, episode steps: 691, steps per second:  47, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.166 [0.000, 5.000],  loss: 0.013165, mae: 2.434887, mean_q: 2.940215, mean_eps: 0.100000
 1131479/1200000: episode: 1498, duration: 15.860s, episode steps: 655, steps per second:  41, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.634 [0.000, 5.000],  loss: 0.018005, mae: 2.458147, mean_q: 2.964980, mean_eps: 0.100000
 1132578/1200000: episode: 1499, duration: 26.270s, episode steps: 1099, steps per second:  42, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.097 [0.000, 5.000],  loss: 0.016621, mae: 2.462883, mean_q: 2.971462, mean_eps: 0.100000
 1134248/1200000: episode: 1500, duration: 39.284s, episode steps: 1670, steps per second:  43, episode reward: 38.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.865 [0.000, 5.000],  loss: 0.017771, mae: 2.458382, mean_q: 2.965455, mean_eps: 0.100000
 1134840/1200000: episode: 1501, duration: 14.034s, episode steps: 592, steps per second:  42, episode reward: 15.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.787 [0.000, 5.000],  loss: 0.017095, mae: 2.469728, mean_q: 2.979549, mean_eps: 0.100000
 1135655/1200000: episode: 1502, duration: 18.446s, episode steps: 815, steps per second:  44, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.854 [0.000, 5.000],  loss: 0.017680, mae: 2.456564, mean_q: 2.963508, mean_eps: 0.100000
 1136104/1200000: episode: 1503, duration: 11.481s, episode steps: 449, steps per second:  39, episode reward: 11.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.016746, mae: 2.437403, mean_q: 2.939862, mean_eps: 0.100000
 1136622/1200000: episode: 1504, duration: 12.954s, episode steps: 518, steps per second:  40, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.840 [0.000, 5.000],  loss: 0.014983, mae: 2.460437, mean_q: 2.967416, mean_eps: 0.100000
 1137515/1200000: episode: 1505, duration: 21.511s, episode steps: 893, steps per second:  42, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.586 [0.000, 5.000],  loss: 0.015104, mae: 2.461421, mean_q: 2.968891, mean_eps: 0.100000
 1138240/1200000: episode: 1506, duration: 16.075s, episode steps: 725, steps per second:  45, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.012 [0.000, 5.000],  loss: 0.017037, mae: 2.430919, mean_q: 2.931731, mean_eps: 0.100000
 1138875/1200000: episode: 1507, duration: 14.103s, episode steps: 635, steps per second:  45, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.877 [0.000, 5.000],  loss: 0.018546, mae: 2.440939, mean_q: 2.942922, mean_eps: 0.100000
 1139725/1200000: episode: 1508, duration: 19.302s, episode steps: 850, steps per second:  44, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.184 [0.000, 5.000],  loss: 0.014887, mae: 2.459508, mean_q: 2.967417, mean_eps: 0.100000
 1140971/1200000: episode: 1509, duration: 25.195s, episode steps: 1246, steps per second:  49, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.956 [0.000, 5.000],  loss: 0.017421, mae: 2.473712, mean_q: 2.984300, mean_eps: 0.100000
 1141786/1200000: episode: 1510, duration: 16.731s, episode steps: 815, steps per second:  49, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.168 [0.000, 5.000],  loss: 0.017346, mae: 2.442710, mean_q: 2.947549, mean_eps: 0.100000
 1142466/1200000: episode: 1511, duration: 14.465s, episode steps: 680, steps per second:  47, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.009 [0.000, 5.000],  loss: 0.016990, mae: 2.466646, mean_q: 2.976710, mean_eps: 0.100000
 1143093/1200000: episode: 1512, duration: 12.551s, episode steps: 627, steps per second:  50, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.378 [0.000, 5.000],  loss: 0.017226, mae: 2.478753, mean_q: 2.992593, mean_eps: 0.100000
 1144062/1200000: episode: 1513, duration: 20.008s, episode steps: 969, steps per second:  48, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.693 [0.000, 5.000],  loss: 0.018549, mae: 2.477248, mean_q: 2.989212, mean_eps: 0.100000
 1144634/1200000: episode: 1514, duration: 11.458s, episode steps: 572, steps per second:  50, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.351 [0.000, 5.000],  loss: 0.016258, mae: 2.483720, mean_q: 2.997516, mean_eps: 0.100000
 1145341/1200000: episode: 1515, duration: 14.254s, episode steps: 707, steps per second:  50, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.765 [0.000, 5.000],  loss: 0.016146, mae: 2.484739, mean_q: 2.998998, mean_eps: 0.100000
 1145694/1200000: episode: 1516, duration: 7.494s, episode steps: 353, steps per second:  47, episode reward:  7.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.578 [0.000, 5.000],  loss: 0.014964, mae: 2.474010, mean_q: 2.983087, mean_eps: 0.100000
 1146367/1200000: episode: 1517, duration: 14.747s, episode steps: 673, steps per second:  46, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.691 [0.000, 5.000],  loss: 0.016002, mae: 2.488031, mean_q: 3.001558, mean_eps: 0.100000
 1146924/1200000: episode: 1518, duration: 11.571s, episode steps: 557, steps per second:  48, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.551 [0.000, 5.000],  loss: 0.015983, mae: 2.504406, mean_q: 3.022280, mean_eps: 0.100000
 1147794/1200000: episode: 1519, duration: 19.684s, episode steps: 870, steps per second:  44, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.699 [0.000, 5.000],  loss: 0.017615, mae: 2.472092, mean_q: 2.982631, mean_eps: 0.100000
 1148522/1200000: episode: 1520, duration: 16.516s, episode steps: 728, steps per second:  44, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.765 [0.000, 5.000],  loss: 0.015666, mae: 2.484791, mean_q: 2.998397, mean_eps: 0.100000
 1149279/1200000: episode: 1521, duration: 15.931s, episode steps: 757, steps per second:  48, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.646 [0.000, 5.000],  loss: 0.017273, mae: 2.496927, mean_q: 3.013309, mean_eps: 0.100000
 1150328/1200000: episode: 1522, duration: 21.334s, episode steps: 1049, steps per second:  49, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.877 [0.000, 5.000],  loss: 0.015945, mae: 2.480215, mean_q: 2.994520, mean_eps: 0.100000
 1150951/1200000: episode: 1523, duration: 13.193s, episode steps: 623, steps per second:  47, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.632 [0.000, 5.000],  loss: 0.013951, mae: 2.490300, mean_q: 3.007315, mean_eps: 0.100000
 1151619/1200000: episode: 1524, duration: 14.942s, episode steps: 668, steps per second:  45, episode reward: 20.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.558 [0.000, 5.000],  loss: 0.015832, mae: 2.483793, mean_q: 2.998081, mean_eps: 0.100000
 1152484/1200000: episode: 1525, duration: 18.672s, episode steps: 865, steps per second:  46, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.015493, mae: 2.469014, mean_q: 2.981775, mean_eps: 0.100000
 1153432/1200000: episode: 1526, duration: 21.126s, episode steps: 948, steps per second:  45, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.015740, mae: 2.483470, mean_q: 2.998798, mean_eps: 0.100000
 1153949/1200000: episode: 1527, duration: 12.077s, episode steps: 517, steps per second:  43, episode reward: 13.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.950 [0.000, 5.000],  loss: 0.017556, mae: 2.495878, mean_q: 3.011767, mean_eps: 0.100000
 1154664/1200000: episode: 1528, duration: 16.210s, episode steps: 715, steps per second:  44, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.105 [0.000, 5.000],  loss: 0.014830, mae: 2.479122, mean_q: 2.995724, mean_eps: 0.100000
 1155228/1200000: episode: 1529, duration: 12.961s, episode steps: 564, steps per second:  44, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.895 [0.000, 5.000],  loss: 0.017774, mae: 2.456935, mean_q: 2.965967, mean_eps: 0.100000
 1155868/1200000: episode: 1530, duration: 15.588s, episode steps: 640, steps per second:  41, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.163 [0.000, 5.000],  loss: 0.017223, mae: 2.475161, mean_q: 2.987554, mean_eps: 0.100000
 1156916/1200000: episode: 1531, duration: 21.279s, episode steps: 1048, steps per second:  49, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.287 [0.000, 5.000],  loss: 0.016076, mae: 2.476345, mean_q: 2.989895, mean_eps: 0.100000
 1157474/1200000: episode: 1532, duration: 10.766s, episode steps: 558, steps per second:  52, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.012720, mae: 2.487993, mean_q: 3.004710, mean_eps: 0.100000
 1158003/1200000: episode: 1533, duration: 10.132s, episode steps: 529, steps per second:  52, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.819 [0.000, 5.000],  loss: 0.015200, mae: 2.466425, mean_q: 2.977318, mean_eps: 0.100000
 1158663/1200000: episode: 1534, duration: 13.848s, episode steps: 660, steps per second:  48, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.016051, mae: 2.476715, mean_q: 2.989490, mean_eps: 0.100000
 1159236/1200000: episode: 1535, duration: 12.145s, episode steps: 573, steps per second:  47, episode reward: 15.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.154 [0.000, 5.000],  loss: 0.015399, mae: 2.477806, mean_q: 2.991303, mean_eps: 0.100000
 1160320/1200000: episode: 1536, duration: 20.735s, episode steps: 1084, steps per second:  52, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.015957, mae: 2.484000, mean_q: 2.998951, mean_eps: 0.100000
 1161240/1200000: episode: 1537, duration: 17.740s, episode steps: 920, steps per second:  52, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.077 [0.000, 5.000],  loss: 0.016774, mae: 2.498207, mean_q: 3.017805, mean_eps: 0.100000
 1162190/1200000: episode: 1538, duration: 19.086s, episode steps: 950, steps per second:  50, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.995 [0.000, 5.000],  loss: 0.015661, mae: 2.492123, mean_q: 3.011200, mean_eps: 0.100000
 1162919/1200000: episode: 1539, duration: 15.020s, episode steps: 729, steps per second:  49, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.936 [0.000, 5.000],  loss: 0.016213, mae: 2.472934, mean_q: 2.987757, mean_eps: 0.100000
 1163544/1200000: episode: 1540, duration: 13.627s, episode steps: 625, steps per second:  46, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.017945, mae: 2.494883, mean_q: 3.012261, mean_eps: 0.100000
 1164490/1200000: episode: 1541, duration: 21.039s, episode steps: 946, steps per second:  45, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.017393, mae: 2.494411, mean_q: 3.011134, mean_eps: 0.100000
 1165698/1200000: episode: 1542, duration: 26.272s, episode steps: 1208, steps per second:  46, episode reward: 34.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.146 [0.000, 5.000],  loss: 0.016907, mae: 2.500520, mean_q: 3.019569, mean_eps: 0.100000
 1166346/1200000: episode: 1543, duration: 12.981s, episode steps: 648, steps per second:  50, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.846 [0.000, 5.000],  loss: 0.014976, mae: 2.487192, mean_q: 3.004553, mean_eps: 0.100000
 1167119/1200000: episode: 1544, duration: 16.332s, episode steps: 773, steps per second:  47, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.019126, mae: 2.501409, mean_q: 3.020078, mean_eps: 0.100000
 1167855/1200000: episode: 1545, duration: 15.486s, episode steps: 736, steps per second:  48, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 0.017023, mae: 2.494073, mean_q: 3.010826, mean_eps: 0.100000
 1168730/1200000: episode: 1546, duration: 18.121s, episode steps: 875, steps per second:  48, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.255 [0.000, 5.000],  loss: 0.016094, mae: 2.488585, mean_q: 3.004476, mean_eps: 0.100000
 1169587/1200000: episode: 1547, duration: 17.084s, episode steps: 857, steps per second:  50, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.975 [0.000, 5.000],  loss: 0.015872, mae: 2.486551, mean_q: 3.002846, mean_eps: 0.100000
 1170064/1200000: episode: 1548, duration: 9.991s, episode steps: 477, steps per second:  48, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.260 [0.000, 5.000],  loss: 0.014501, mae: 2.493023, mean_q: 3.011596, mean_eps: 0.100000
 1170954/1200000: episode: 1549, duration: 18.872s, episode steps: 890, steps per second:  47, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.071 [0.000, 5.000],  loss: 0.013489, mae: 2.518442, mean_q: 3.042847, mean_eps: 0.100000
 1171737/1200000: episode: 1550, duration: 16.176s, episode steps: 783, steps per second:  48, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.015515, mae: 2.535329, mean_q: 3.062592, mean_eps: 0.100000
 1172647/1200000: episode: 1551, duration: 18.244s, episode steps: 910, steps per second:  50, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.103 [0.000, 5.000],  loss: 0.016198, mae: 2.494127, mean_q: 3.012993, mean_eps: 0.100000
 1173246/1200000: episode: 1552, duration: 12.694s, episode steps: 599, steps per second:  47, episode reward: 15.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.918 [0.000, 5.000],  loss: 0.016486, mae: 2.527413, mean_q: 3.052403, mean_eps: 0.100000
 1173907/1200000: episode: 1553, duration: 13.683s, episode steps: 661, steps per second:  48, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.016544, mae: 2.502529, mean_q: 3.021626, mean_eps: 0.100000
 1174745/1200000: episode: 1554, duration: 15.801s, episode steps: 838, steps per second:  53, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.792 [0.000, 5.000],  loss: 0.014455, mae: 2.518850, mean_q: 3.042979, mean_eps: 0.100000
 1175465/1200000: episode: 1555, duration: 14.402s, episode steps: 720, steps per second:  50, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.131 [0.000, 5.000],  loss: 0.017793, mae: 2.495410, mean_q: 3.015218, mean_eps: 0.100000
 1176066/1200000: episode: 1556, duration: 12.096s, episode steps: 601, steps per second:  50, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.116 [0.000, 5.000],  loss: 0.016874, mae: 2.513176, mean_q: 3.035333, mean_eps: 0.100000
 1177049/1200000: episode: 1557, duration: 20.173s, episode steps: 983, steps per second:  49, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.187 [0.000, 5.000],  loss: 0.015558, mae: 2.513751, mean_q: 3.035931, mean_eps: 0.100000
 1177902/1200000: episode: 1558, duration: 18.449s, episode steps: 853, steps per second:  46, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.751 [0.000, 5.000],  loss: 0.014182, mae: 2.520815, mean_q: 3.044356, mean_eps: 0.100000
 1178571/1200000: episode: 1559, duration: 13.964s, episode steps: 669, steps per second:  48, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.018056, mae: 2.496857, mean_q: 3.016154, mean_eps: 0.100000
 1179319/1200000: episode: 1560, duration: 15.642s, episode steps: 748, steps per second:  48, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.678 [0.000, 5.000],  loss: 0.016948, mae: 2.494611, mean_q: 3.013868, mean_eps: 0.100000
 1179961/1200000: episode: 1561, duration: 13.470s, episode steps: 642, steps per second:  48, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.980 [0.000, 5.000],  loss: 0.016707, mae: 2.517727, mean_q: 3.043532, mean_eps: 0.100000
 1180672/1200000: episode: 1562, duration: 15.613s, episode steps: 711, steps per second:  46, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.592 [0.000, 5.000],  loss: 0.015248, mae: 2.536459, mean_q: 3.065486, mean_eps: 0.100000
 1181449/1200000: episode: 1563, duration: 16.598s, episode steps: 777, steps per second:  47, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.921 [0.000, 5.000],  loss: 0.015970, mae: 2.511010, mean_q: 3.034308, mean_eps: 0.100000
 1182460/1200000: episode: 1564, duration: 23.089s, episode steps: 1011, steps per second:  44, episode reward: 29.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.016452, mae: 2.523691, mean_q: 3.049836, mean_eps: 0.100000
 1183079/1200000: episode: 1565, duration: 13.083s, episode steps: 619, steps per second:  47, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.110 [0.000, 5.000],  loss: 0.015527, mae: 2.551118, mean_q: 3.083650, mean_eps: 0.100000
 1183729/1200000: episode: 1566, duration: 13.125s, episode steps: 650, steps per second:  50, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.288 [0.000, 5.000],  loss: 0.014374, mae: 2.538637, mean_q: 3.067165, mean_eps: 0.100000
 1184372/1200000: episode: 1567, duration: 13.167s, episode steps: 643, steps per second:  49, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.885 [0.000, 5.000],  loss: 0.016060, mae: 2.541862, mean_q: 3.070692, mean_eps: 0.100000
 1185360/1200000: episode: 1568, duration: 20.265s, episode steps: 988, steps per second:  49, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.070 [0.000, 5.000],  loss: 0.016786, mae: 2.524023, mean_q: 3.050248, mean_eps: 0.100000
 1186251/1200000: episode: 1569, duration: 17.102s, episode steps: 891, steps per second:  52, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.059 [0.000, 5.000],  loss: 0.018084, mae: 2.522701, mean_q: 3.047459, mean_eps: 0.100000
 1187339/1200000: episode: 1570, duration: 22.304s, episode steps: 1088, steps per second:  49, episode reward: 30.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.154 [0.000, 5.000],  loss: 0.016651, mae: 2.549142, mean_q: 3.079660, mean_eps: 0.100000
 1188242/1200000: episode: 1571, duration: 18.457s, episode steps: 903, steps per second:  49, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.869 [0.000, 5.000],  loss: 0.018586, mae: 2.527773, mean_q: 3.053422, mean_eps: 0.100000
 1188669/1200000: episode: 1572, duration: 8.761s, episode steps: 427, steps per second:  49, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.017815, mae: 2.539723, mean_q: 3.069651, mean_eps: 0.100000
 1189630/1200000: episode: 1573, duration: 20.280s, episode steps: 961, steps per second:  47, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.171 [0.000, 5.000],  loss: 0.017066, mae: 2.548360, mean_q: 3.079035, mean_eps: 0.100000
 1190301/1200000: episode: 1574, duration: 13.672s, episode steps: 671, steps per second:  49, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.999 [0.000, 5.000],  loss: 0.019606, mae: 2.555182, mean_q: 3.086975, mean_eps: 0.100000
 1191287/1200000: episode: 1575, duration: 20.274s, episode steps: 986, steps per second:  49, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.956 [0.000, 5.000],  loss: 0.016097, mae: 2.555486, mean_q: 3.088968, mean_eps: 0.100000
 1191932/1200000: episode: 1576, duration: 12.598s, episode steps: 645, steps per second:  51, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.291 [0.000, 5.000],  loss: 0.015333, mae: 2.571703, mean_q: 3.108232, mean_eps: 0.100000
 1192853/1200000: episode: 1577, duration: 17.464s, episode steps: 921, steps per second:  53, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.849 [0.000, 5.000],  loss: 0.015926, mae: 2.555832, mean_q: 3.089113, mean_eps: 0.100000
 1193600/1200000: episode: 1578, duration: 14.745s, episode steps: 747, steps per second:  51, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.080 [0.000, 5.000],  loss: 0.015540, mae: 2.554133, mean_q: 3.086835, mean_eps: 0.100000
 1194379/1200000: episode: 1579, duration: 16.387s, episode steps: 779, steps per second:  48, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.019607, mae: 2.551090, mean_q: 3.081484, mean_eps: 0.100000
 1195249/1200000: episode: 1580, duration: 17.225s, episode steps: 870, steps per second:  51, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.985 [0.000, 5.000],  loss: 0.016004, mae: 2.544615, mean_q: 3.073711, mean_eps: 0.100000
 1196151/1200000: episode: 1581, duration: 17.443s, episode steps: 902, steps per second:  52, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.214 [0.000, 5.000],  loss: 0.016124, mae: 2.558253, mean_q: 3.091761, mean_eps: 0.100000
 1197004/1200000: episode: 1582, duration: 16.386s, episode steps: 853, steps per second:  52, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.070 [0.000, 5.000],  loss: 0.015077, mae: 2.573045, mean_q: 3.110132, mean_eps: 0.100000
 1197982/1200000: episode: 1583, duration: 19.630s, episode steps: 978, steps per second:  50, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.037 [0.000, 5.000],  loss: 0.016873, mae: 2.564992, mean_q: 3.099209, mean_eps: 0.100000
 1198446/1200000: episode: 1584, duration: 9.320s, episode steps: 464, steps per second:  50, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.862 [0.000, 5.000],  loss: 0.016411, mae: 2.546143, mean_q: 3.076272, mean_eps: 0.100000
 1198859/1200000: episode: 1585, duration: 8.166s, episode steps: 413, steps per second:  51, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.523 [0.000, 5.000],  loss: 0.015979, mae: 2.581017, mean_q: 3.118247, mean_eps: 0.100000
 1199790/1200000: episode: 1586, duration: 20.773s, episode steps: 931, steps per second:  45, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.901 [0.000, 5.000],  loss: 0.014797, mae: 2.555668, mean_q: 3.086585, mean_eps: 0.100000
done, took 24106.903 seconds