Training for 6000000 steps ...
c:\Users\jgilg\anaconda3\envs\miar_rl\lib\site-packages\tensorflow\python\keras\engine\training.py:2424: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
     420/6000000: episode: 1, duration: 1.822s, episode steps: 420, steps per second: 230, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    1131/6000000: episode: 2, duration: 2.623s, episode steps: 711, steps per second: 271, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    1941/6000000: episode: 3, duration: 3.284s, episode steps: 810, steps per second: 247, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    2809/6000000: episode: 4, duration: 3.755s, episode steps: 868, steps per second: 231, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    3730/6000000: episode: 5, duration: 3.943s, episode steps: 921, steps per second: 234, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    4382/6000000: episode: 6, duration: 2.622s, episode steps: 652, steps per second: 249, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    4891/6000000: episode: 7, duration: 2.003s, episode steps: 509, steps per second: 254, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    5296/6000000: episode: 8, duration: 1.508s, episode steps: 405, steps per second: 269, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    6258/6000000: episode: 9, duration: 3.395s, episode steps: 962, steps per second: 283, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    6895/6000000: episode: 10, duration: 2.126s, episode steps: 637, steps per second: 300, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    7291/6000000: episode: 11, duration: 1.502s, episode steps: 396, steps per second: 264, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    7854/6000000: episode: 12, duration: 2.554s, episode steps: 563, steps per second: 220, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    8879/6000000: episode: 13, duration: 8.267s, episode steps: 1025, steps per second: 124, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    9454/6000000: episode: 14, duration: 4.300s, episode steps: 575, steps per second: 134, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    9856/6000000: episode: 15, duration: 3.237s, episode steps: 402, steps per second: 124, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   10529/6000000: episode: 16, duration: 4.861s, episode steps: 673, steps per second: 138, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   11441/6000000: episode: 17, duration: 6.827s, episode steps: 912, steps per second: 134, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   11840/6000000: episode: 18, duration: 2.757s, episode steps: 399, steps per second: 145, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.351 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   12667/6000000: episode: 19, duration: 6.304s, episode steps: 827, steps per second: 131, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   13678/6000000: episode: 20, duration: 7.390s, episode steps: 1011, steps per second: 137, episode reward: 11.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   14456/6000000: episode: 21, duration: 5.866s, episode steps: 778, steps per second: 133, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   15366/6000000: episode: 22, duration: 7.360s, episode steps: 910, steps per second: 124, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   16139/6000000: episode: 23, duration: 5.066s, episode steps: 773, steps per second: 153, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   16529/6000000: episode: 24, duration: 1.637s, episode steps: 390, steps per second: 238, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   17013/6000000: episode: 25, duration: 1.836s, episode steps: 484, steps per second: 264, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   17980/6000000: episode: 26, duration: 3.448s, episode steps: 967, steps per second: 280, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   18659/6000000: episode: 27, duration: 2.203s, episode steps: 679, steps per second: 308, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   19182/6000000: episode: 28, duration: 1.590s, episode steps: 523, steps per second: 329, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   19872/6000000: episode: 29, duration: 2.282s, episode steps: 690, steps per second: 302, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   20439/6000000: episode: 30, duration: 2.016s, episode steps: 567, steps per second: 281, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   21282/6000000: episode: 31, duration: 3.199s, episode steps: 843, steps per second: 264, episode reward:  7.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   21829/6000000: episode: 32, duration: 1.799s, episode steps: 547, steps per second: 304, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   22881/6000000: episode: 33, duration: 3.302s, episode steps: 1052, steps per second: 319, episode reward:  9.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   23398/6000000: episode: 34, duration: 2.180s, episode steps: 517, steps per second: 237, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   23951/6000000: episode: 35, duration: 1.964s, episode steps: 553, steps per second: 282, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   24800/6000000: episode: 36, duration: 3.170s, episode steps: 849, steps per second: 268, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   25318/6000000: episode: 37, duration: 2.030s, episode steps: 518, steps per second: 255, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   25781/6000000: episode: 38, duration: 1.445s, episode steps: 463, steps per second: 321, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   26294/6000000: episode: 39, duration: 1.627s, episode steps: 513, steps per second: 315, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   26867/6000000: episode: 40, duration: 2.088s, episode steps: 573, steps per second: 274, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   27521/6000000: episode: 41, duration: 2.299s, episode steps: 654, steps per second: 284, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   28228/6000000: episode: 42, duration: 2.591s, episode steps: 707, steps per second: 273, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   29165/6000000: episode: 43, duration: 3.437s, episode steps: 937, steps per second: 273, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   29986/6000000: episode: 44, duration: 2.749s, episode steps: 821, steps per second: 299, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   30602/6000000: episode: 45, duration: 2.242s, episode steps: 616, steps per second: 275, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   31418/6000000: episode: 46, duration: 2.768s, episode steps: 816, steps per second: 295, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   32226/6000000: episode: 47, duration: 2.335s, episode steps: 808, steps per second: 346, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   32814/6000000: episode: 48, duration: 2.069s, episode steps: 588, steps per second: 284, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   33342/6000000: episode: 49, duration: 2.309s, episode steps: 528, steps per second: 229, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   34274/6000000: episode: 50, duration: 3.716s, episode steps: 932, steps per second: 251, episode reward:  9.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   35100/6000000: episode: 51, duration: 2.866s, episode steps: 826, steps per second: 288, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   35921/6000000: episode: 52, duration: 2.953s, episode steps: 821, steps per second: 278, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   36428/6000000: episode: 53, duration: 1.637s, episode steps: 507, steps per second: 310, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   37141/6000000: episode: 54, duration: 2.383s, episode steps: 713, steps per second: 299, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   37501/6000000: episode: 55, duration: 1.384s, episode steps: 360, steps per second: 260, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   38264/6000000: episode: 56, duration: 3.041s, episode steps: 763, steps per second: 251, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.651 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   38921/6000000: episode: 57, duration: 2.693s, episode steps: 657, steps per second: 244, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   39505/6000000: episode: 58, duration: 1.950s, episode steps: 584, steps per second: 299, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   40154/6000000: episode: 59, duration: 2.700s, episode steps: 649, steps per second: 240, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   40972/6000000: episode: 60, duration: 3.119s, episode steps: 818, steps per second: 262, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   41707/6000000: episode: 61, duration: 3.086s, episode steps: 735, steps per second: 238, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   42319/6000000: episode: 62, duration: 2.778s, episode steps: 612, steps per second: 220, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   42800/6000000: episode: 63, duration: 1.738s, episode steps: 481, steps per second: 277, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   43743/6000000: episode: 64, duration: 3.964s, episode steps: 943, steps per second: 238, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   44674/6000000: episode: 65, duration: 3.388s, episode steps: 931, steps per second: 275, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   45565/6000000: episode: 66, duration: 3.501s, episode steps: 891, steps per second: 254, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   46125/6000000: episode: 67, duration: 2.262s, episode steps: 560, steps per second: 248, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   46797/6000000: episode: 68, duration: 2.605s, episode steps: 672, steps per second: 258, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   47622/6000000: episode: 69, duration: 3.603s, episode steps: 825, steps per second: 229, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   48238/6000000: episode: 70, duration: 2.529s, episode steps: 616, steps per second: 244, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   48859/6000000: episode: 71, duration: 2.561s, episode steps: 621, steps per second: 242, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   49663/6000000: episode: 72, duration: 3.448s, episode steps: 804, steps per second: 233, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   50800/6000000: episode: 73, duration: 4.919s, episode steps: 1137, steps per second: 231, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   51343/6000000: episode: 74, duration: 1.844s, episode steps: 543, steps per second: 294, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   51837/6000000: episode: 75, duration: 1.751s, episode steps: 494, steps per second: 282, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   52240/6000000: episode: 76, duration: 1.655s, episode steps: 403, steps per second: 244, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.635 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   52831/6000000: episode: 77, duration: 2.607s, episode steps: 591, steps per second: 227, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   53469/6000000: episode: 78, duration: 1.976s, episode steps: 638, steps per second: 323, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   53921/6000000: episode: 79, duration: 1.875s, episode steps: 452, steps per second: 241, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   54543/6000000: episode: 80, duration: 2.471s, episode steps: 622, steps per second: 252, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   55454/6000000: episode: 81, duration: 4.043s, episode steps: 911, steps per second: 225, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   56204/6000000: episode: 82, duration: 3.529s, episode steps: 750, steps per second: 213, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   56953/6000000: episode: 83, duration: 2.908s, episode steps: 749, steps per second: 258, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   57586/6000000: episode: 84, duration: 2.965s, episode steps: 633, steps per second: 214, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   57997/6000000: episode: 85, duration: 1.574s, episode steps: 411, steps per second: 261, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   58396/6000000: episode: 86, duration: 1.633s, episode steps: 399, steps per second: 244, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   59207/6000000: episode: 87, duration: 3.490s, episode steps: 811, steps per second: 232, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   60338/6000000: episode: 88, duration: 4.595s, episode steps: 1131, steps per second: 246, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   61289/6000000: episode: 89, duration: 3.828s, episode steps: 951, steps per second: 248, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   62069/6000000: episode: 90, duration: 2.711s, episode steps: 780, steps per second: 288, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   62530/6000000: episode: 91, duration: 1.522s, episode steps: 461, steps per second: 303, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   63290/6000000: episode: 92, duration: 3.169s, episode steps: 760, steps per second: 240, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   63670/6000000: episode: 93, duration: 1.583s, episode steps: 380, steps per second: 240, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   64440/6000000: episode: 94, duration: 3.082s, episode steps: 770, steps per second: 250, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   64958/6000000: episode: 95, duration: 2.149s, episode steps: 518, steps per second: 241, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   65763/6000000: episode: 96, duration: 3.037s, episode steps: 805, steps per second: 265, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   66403/6000000: episode: 97, duration: 2.536s, episode steps: 640, steps per second: 252, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   67057/6000000: episode: 98, duration: 2.454s, episode steps: 654, steps per second: 267, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   67987/6000000: episode: 99, duration: 3.805s, episode steps: 930, steps per second: 244, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   68530/6000000: episode: 100, duration: 1.968s, episode steps: 543, steps per second: 276, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   69439/6000000: episode: 101, duration: 3.773s, episode steps: 909, steps per second: 241, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   70571/6000000: episode: 102, duration: 3.943s, episode steps: 1132, steps per second: 287, episode reward: 15.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   71216/6000000: episode: 103, duration: 2.699s, episode steps: 645, steps per second: 239, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   71727/6000000: episode: 104, duration: 1.817s, episode steps: 511, steps per second: 281, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   72327/6000000: episode: 105, duration: 2.528s, episode steps: 600, steps per second: 237, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   72756/6000000: episode: 106, duration: 1.558s, episode steps: 429, steps per second: 275, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   73437/6000000: episode: 107, duration: 2.520s, episode steps: 681, steps per second: 270, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   74078/6000000: episode: 108, duration: 2.419s, episode steps: 641, steps per second: 265, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   74866/6000000: episode: 109, duration: 2.841s, episode steps: 788, steps per second: 277, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   75574/6000000: episode: 110, duration: 2.481s, episode steps: 708, steps per second: 285, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   76111/6000000: episode: 111, duration: 2.179s, episode steps: 537, steps per second: 246, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   76809/6000000: episode: 112, duration: 2.574s, episode steps: 698, steps per second: 271, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   77594/6000000: episode: 113, duration: 3.167s, episode steps: 785, steps per second: 248, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   78141/6000000: episode: 114, duration: 1.837s, episode steps: 547, steps per second: 298, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   79090/6000000: episode: 115, duration: 3.493s, episode steps: 949, steps per second: 272, episode reward: 10.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   79594/6000000: episode: 116, duration: 2.170s, episode steps: 504, steps per second: 232, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   80401/6000000: episode: 117, duration: 3.386s, episode steps: 807, steps per second: 238, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   81235/6000000: episode: 118, duration: 3.275s, episode steps: 834, steps per second: 255, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   82047/6000000: episode: 119, duration: 2.930s, episode steps: 812, steps per second: 277, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   82665/6000000: episode: 120, duration: 2.429s, episode steps: 618, steps per second: 254, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   83227/6000000: episode: 121, duration: 2.730s, episode steps: 562, steps per second: 206, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   83697/6000000: episode: 122, duration: 1.759s, episode steps: 470, steps per second: 267, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   84428/6000000: episode: 123, duration: 3.209s, episode steps: 731, steps per second: 228, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   85387/6000000: episode: 124, duration: 4.039s, episode steps: 959, steps per second: 237, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   85935/6000000: episode: 125, duration: 2.086s, episode steps: 548, steps per second: 263, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   86548/6000000: episode: 126, duration: 2.713s, episode steps: 613, steps per second: 226, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   87157/6000000: episode: 127, duration: 2.694s, episode steps: 609, steps per second: 226, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   87512/6000000: episode: 128, duration: 1.460s, episode steps: 355, steps per second: 243, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   88379/6000000: episode: 129, duration: 4.017s, episode steps: 867, steps per second: 216, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   89076/6000000: episode: 130, duration: 2.866s, episode steps: 697, steps per second: 243, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   89890/6000000: episode: 131, duration: 3.507s, episode steps: 814, steps per second: 232, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   90544/6000000: episode: 132, duration: 2.798s, episode steps: 654, steps per second: 234, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   91069/6000000: episode: 133, duration: 2.032s, episode steps: 525, steps per second: 258, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   91558/6000000: episode: 134, duration: 1.892s, episode steps: 489, steps per second: 258, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   92272/6000000: episode: 135, duration: 2.834s, episode steps: 714, steps per second: 252, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   92758/6000000: episode: 136, duration: 1.803s, episode steps: 486, steps per second: 270, episode reward:  1.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   93572/6000000: episode: 137, duration: 2.820s, episode steps: 814, steps per second: 289, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   93956/6000000: episode: 138, duration: 1.331s, episode steps: 384, steps per second: 289, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   94607/6000000: episode: 139, duration: 2.754s, episode steps: 651, steps per second: 236, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   95486/6000000: episode: 140, duration: 3.439s, episode steps: 879, steps per second: 256, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   95999/6000000: episode: 141, duration: 2.099s, episode steps: 513, steps per second: 244, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   96541/6000000: episode: 142, duration: 1.872s, episode steps: 542, steps per second: 290, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   97047/6000000: episode: 143, duration: 2.160s, episode steps: 506, steps per second: 234, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   97879/6000000: episode: 144, duration: 3.348s, episode steps: 832, steps per second: 249, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   98501/6000000: episode: 145, duration: 2.619s, episode steps: 622, steps per second: 237, episode reward:  4.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   99614/6000000: episode: 146, duration: 4.524s, episode steps: 1113, steps per second: 246, episode reward: 10.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
c:\Users\jgilg\anaconda3\envs\miar_rl\lib\site-packages\tensorflow\python\keras\engine\training.py:2424: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
...
 1644217/6000000: episode: 2327, duration: 11.468s, episode steps: 510, steps per second:  44, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.210 [0.000, 5.000],  loss: 0.013481, mae: 2.155100, mean_q: 2.592414, mean_eps: 0.671208
 1644710/6000000: episode: 2328, duration: 10.541s, episode steps: 493, steps per second:  47, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.010907, mae: 2.144647, mean_q: 2.580228, mean_eps: 0.671107
 1645424/6000000: episode: 2329, duration: 15.881s, episode steps: 714, steps per second:  45, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.010304, mae: 2.139012, mean_q: 2.574633, mean_eps: 0.670987
 1646330/6000000: episode: 2330, duration: 19.635s, episode steps: 906, steps per second:  46, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.012989, mae: 2.137610, mean_q: 2.573585, mean_eps: 0.670825
 1647298/6000000: episode: 2331, duration: 19.804s, episode steps: 968, steps per second:  49, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.014496, mae: 2.147997, mean_q: 2.586544, mean_eps: 0.670637
 1647866/6000000: episode: 2332, duration: 12.017s, episode steps: 568, steps per second:  47, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.011804, mae: 2.099363, mean_q: 2.526223, mean_eps: 0.670484
 1648853/6000000: episode: 2333, duration: 21.109s, episode steps: 987, steps per second:  47, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.012311, mae: 2.140753, mean_q: 2.577522, mean_eps: 0.670328
 1649602/6000000: episode: 2334, duration: 15.409s, episode steps: 749, steps per second:  49, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.012051, mae: 2.144692, mean_q: 2.583609, mean_eps: 0.670154
 1650674/6000000: episode: 2335, duration: 22.590s, episode steps: 1072, steps per second:  47, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.262 [0.000, 5.000],  loss: 0.011463, mae: 2.157615, mean_q: 2.598907, mean_eps: 0.669972
 1651054/6000000: episode: 2336, duration: 8.216s, episode steps: 380, steps per second:  46, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.013108, mae: 2.147131, mean_q: 2.582824, mean_eps: 0.669827
 1651463/6000000: episode: 2337, duration: 8.531s, episode steps: 409, steps per second:  48, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.014268, mae: 2.166813, mean_q: 2.608276, mean_eps: 0.669748
 1651970/6000000: episode: 2338, duration: 10.598s, episode steps: 507, steps per second:  48, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.300 [0.000, 5.000],  loss: 0.014011, mae: 2.144238, mean_q: 2.580842, mean_eps: 0.669657
 1653499/6000000: episode: 2339, duration: 37.123s, episode steps: 1529, steps per second:  41, episode reward: 33.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.013022, mae: 2.146915, mean_q: 2.584898, mean_eps: 0.669453
 1654008/6000000: episode: 2340, duration: 14.397s, episode steps: 509, steps per second:  35, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.013080, mae: 2.153574, mean_q: 2.591608, mean_eps: 0.669250
 1654965/6000000: episode: 2341, duration: 23.977s, episode steps: 957, steps per second:  40, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.011726, mae: 2.150534, mean_q: 2.590515, mean_eps: 0.669103
 1655912/6000000: episode: 2342, duration: 22.348s, episode steps: 947, steps per second:  42, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.012909, mae: 2.147535, mean_q: 2.584969, mean_eps: 0.668912
 1656354/6000000: episode: 2343, duration: 11.312s, episode steps: 442, steps per second:  39, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.010867, mae: 2.143665, mean_q: 2.581294, mean_eps: 0.668774
 1656937/6000000: episode: 2344, duration: 14.926s, episode steps: 583, steps per second:  39, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.012456, mae: 2.149532, mean_q: 2.587258, mean_eps: 0.668671
 1657738/6000000: episode: 2345, duration: 18.854s, episode steps: 801, steps per second:  42, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.011098, mae: 2.136162, mean_q: 2.572369, mean_eps: 0.668532
 1658268/6000000: episode: 2346, duration: 12.387s, episode steps: 530, steps per second:  43, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.013534, mae: 2.146323, mean_q: 2.583650, mean_eps: 0.668400
 1659064/6000000: episode: 2347, duration: 18.818s, episode steps: 796, steps per second:  42, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.012148, mae: 2.132760, mean_q: 2.567337, mean_eps: 0.668267
 1659908/6000000: episode: 2348, duration: 19.375s, episode steps: 844, steps per second:  44, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 0.013184, mae: 2.128567, mean_q: 2.563044, mean_eps: 0.668103
 1660303/6000000: episode: 2349, duration: 9.266s, episode steps: 395, steps per second:  43, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.775 [0.000, 5.000],  loss: 0.013407, mae: 2.131227, mean_q: 2.567335, mean_eps: 0.667979
 1660692/6000000: episode: 2350, duration: 9.100s, episode steps: 389, steps per second:  43, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.781 [0.000, 5.000],  loss: 0.013130, mae: 2.149873, mean_q: 2.587109, mean_eps: 0.667901
 1661282/6000000: episode: 2351, duration: 14.632s, episode steps: 590, steps per second:  40, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.639 [0.000, 5.000],  loss: 0.013028, mae: 2.157140, mean_q: 2.596815, mean_eps: 0.667803
 1661869/6000000: episode: 2352, duration: 14.173s, episode steps: 587, steps per second:  41, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.704 [0.000, 5.000],  loss: 0.010368, mae: 2.148430, mean_q: 2.587616, mean_eps: 0.667685
 1662554/6000000: episode: 2353, duration: 16.282s, episode steps: 685, steps per second:  42, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.012214, mae: 2.138819, mean_q: 2.573254, mean_eps: 0.667558
 1662942/6000000: episode: 2354, duration: 9.256s, episode steps: 388, steps per second:  42, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.802 [0.000, 5.000],  loss: 0.011054, mae: 2.114386, mean_q: 2.544690, mean_eps: 0.667450
 1663572/6000000: episode: 2355, duration: 15.101s, episode steps: 630, steps per second:  42, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.011259, mae: 2.128795, mean_q: 2.563650, mean_eps: 0.667349
 1664420/6000000: episode: 2356, duration: 19.932s, episode steps: 848, steps per second:  43, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.177 [0.000, 5.000],  loss: 0.009925, mae: 2.115309, mean_q: 2.546044, mean_eps: 0.667201
 1664806/6000000: episode: 2357, duration: 9.164s, episode steps: 386, steps per second:  42, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.699 [0.000, 5.000],  loss: 0.010231, mae: 2.131156, mean_q: 2.565792, mean_eps: 0.667078
 1665429/6000000: episode: 2358, duration: 15.203s, episode steps: 623, steps per second:  41, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.012099, mae: 2.163435, mean_q: 2.603613, mean_eps: 0.666976
 1666264/6000000: episode: 2359, duration: 20.067s, episode steps: 835, steps per second:  42, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.012024, mae: 2.147481, mean_q: 2.584934, mean_eps: 0.666831
 1666932/6000000: episode: 2360, duration: 16.151s, episode steps: 668, steps per second:  41, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.222 [0.000, 5.000],  loss: 0.014856, mae: 2.131294, mean_q: 2.564709, mean_eps: 0.666681
 1667588/6000000: episode: 2361, duration: 15.744s, episode steps: 656, steps per second:  42, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.319 [0.000, 5.000],  loss: 0.011533, mae: 2.145273, mean_q: 2.582720, mean_eps: 0.666548
 1668242/6000000: episode: 2362, duration: 16.060s, episode steps: 654, steps per second:  41, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.011589, mae: 2.162155, mean_q: 2.601656, mean_eps: 0.666417
 1669288/6000000: episode: 2363, duration: 25.631s, episode steps: 1046, steps per second:  41, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.011795, mae: 2.161425, mean_q: 2.601133, mean_eps: 0.666247
 1670271/6000000: episode: 2364, duration: 23.887s, episode steps: 983, steps per second:  41, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.189 [0.000, 5.000],  loss: 0.012583, mae: 2.167762, mean_q: 2.608177, mean_eps: 0.666044
 1671125/6000000: episode: 2365, duration: 20.781s, episode steps: 854, steps per second:  41, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.275 [0.000, 5.000],  loss: 0.013039, mae: 2.181797, mean_q: 2.626729, mean_eps: 0.665860
 1671822/6000000: episode: 2366, duration: 17.276s, episode steps: 697, steps per second:  40, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.228 [0.000, 5.000],  loss: 0.012152, mae: 2.165108, mean_q: 2.605756, mean_eps: 0.665705
 1672696/6000000: episode: 2367, duration: 20.266s, episode steps: 874, steps per second:  43, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.011578, mae: 2.170924, mean_q: 2.613277, mean_eps: 0.665548
 1673269/6000000: episode: 2368, duration: 13.567s, episode steps: 573, steps per second:  42, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.010824, mae: 2.162758, mean_q: 2.601748, mean_eps: 0.665404
 1674093/6000000: episode: 2369, duration: 20.210s, episode steps: 824, steps per second:  41, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.011130, mae: 2.170752, mean_q: 2.611993, mean_eps: 0.665264
 1674603/6000000: episode: 2370, duration: 13.202s, episode steps: 510, steps per second:  39, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.012123, mae: 2.161967, mean_q: 2.600359, mean_eps: 0.665130
 1674938/6000000: episode: 2371, duration: 8.387s, episode steps: 335, steps per second:  40, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.791 [0.000, 5.000],  loss: 0.009940, mae: 2.162222, mean_q: 2.602106, mean_eps: 0.665046
 1675780/6000000: episode: 2372, duration: 21.735s, episode steps: 842, steps per second:  39, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.010998, mae: 2.186695, mean_q: 2.632856, mean_eps: 0.664928
 1676778/6000000: episode: 2373, duration: 26.087s, episode steps: 998, steps per second:  38, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.012029, mae: 2.174121, mean_q: 2.617417, mean_eps: 0.664744
 1677868/6000000: episode: 2374, duration: 28.158s, episode steps: 1090, steps per second:  39, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.011285, mae: 2.185081, mean_q: 2.630458, mean_eps: 0.664536
 1678761/6000000: episode: 2375, duration: 22.486s, episode steps: 893, steps per second:  40, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.674 [0.000, 5.000],  loss: 0.012049, mae: 2.186966, mean_q: 2.632874, mean_eps: 0.664337
 1679555/6000000: episode: 2376, duration: 17.715s, episode steps: 794, steps per second:  45, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.012214, mae: 2.187165, mean_q: 2.633217, mean_eps: 0.664168
 1680876/6000000: episode: 2377, duration: 28.858s, episode steps: 1321, steps per second:  46, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.012597, mae: 2.180335, mean_q: 2.623110, mean_eps: 0.663957
 1681802/6000000: episode: 2378, duration: 19.921s, episode steps: 926, steps per second:  46, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.011906, mae: 2.161399, mean_q: 2.604189, mean_eps: 0.663732
 1682551/6000000: episode: 2379, duration: 16.532s, episode steps: 749, steps per second:  45, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.011043, mae: 2.167520, mean_q: 2.611271, mean_eps: 0.663565
 1683291/6000000: episode: 2380, duration: 17.072s, episode steps: 740, steps per second:  43, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.976 [0.000, 5.000],  loss: 0.011945, mae: 2.143871, mean_q: 2.580988, mean_eps: 0.663416
 1684285/6000000: episode: 2381, duration: 24.124s, episode steps: 994, steps per second:  41, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.013520, mae: 2.169477, mean_q: 2.610377, mean_eps: 0.663242
 1684916/6000000: episode: 2382, duration: 13.942s, episode steps: 631, steps per second:  45, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.011305, mae: 2.159206, mean_q: 2.599352, mean_eps: 0.663080
 1685511/6000000: episode: 2383, duration: 12.851s, episode steps: 595, steps per second:  46, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.012442, mae: 2.126317, mean_q: 2.558503, mean_eps: 0.662958
 1686224/6000000: episode: 2384, duration: 16.966s, episode steps: 713, steps per second:  42, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.012515, mae: 2.164161, mean_q: 2.606121, mean_eps: 0.662827
 1686741/6000000: episode: 2385, duration: 12.308s, episode steps: 517, steps per second:  42, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.012173, mae: 2.163567, mean_q: 2.605352, mean_eps: 0.662704
 1687217/6000000: episode: 2386, duration: 11.106s, episode steps: 476, steps per second:  43, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.162 [0.000, 5.000],  loss: 0.014929, mae: 2.158936, mean_q: 2.600190, mean_eps: 0.662604
 1687804/6000000: episode: 2387, duration: 13.780s, episode steps: 587, steps per second:  43, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.009931, mae: 2.152170, mean_q: 2.592396, mean_eps: 0.662498
 1688602/6000000: episode: 2388, duration: 19.783s, episode steps: 798, steps per second:  40, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.011886, mae: 2.151128, mean_q: 2.589246, mean_eps: 0.662360
 1689404/6000000: episode: 2389, duration: 20.209s, episode steps: 802, steps per second:  40, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.013047, mae: 2.136710, mean_q: 2.571999, mean_eps: 0.662200
 1689774/6000000: episode: 2390, duration: 8.181s, episode steps: 370, steps per second:  45, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.754 [0.000, 5.000],  loss: 0.011222, mae: 2.126692, mean_q: 2.562593, mean_eps: 0.662082
 1690321/6000000: episode: 2391, duration: 12.097s, episode steps: 547, steps per second:  45, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.012264, mae: 2.179342, mean_q: 2.623251, mean_eps: 0.661990
 1690971/6000000: episode: 2392, duration: 14.204s, episode steps: 650, steps per second:  46, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.013701, mae: 2.165784, mean_q: 2.606214, mean_eps: 0.661871
 1691678/6000000: episode: 2393, duration: 15.811s, episode steps: 707, steps per second:  45, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.015069, mae: 2.143236, mean_q: 2.580787, mean_eps: 0.661735
 1692722/6000000: episode: 2394, duration: 22.522s, episode steps: 1044, steps per second:  46, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.176 [0.000, 5.000],  loss: 0.012069, mae: 2.164483, mean_q: 2.605530, mean_eps: 0.661560
 1693519/6000000: episode: 2395, duration: 16.274s, episode steps: 797, steps per second:  49, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.158 [0.000, 5.000],  loss: 0.013252, mae: 2.148906, mean_q: 2.585550, mean_eps: 0.661376
 1694159/6000000: episode: 2396, duration: 13.342s, episode steps: 640, steps per second:  48, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.011807, mae: 2.160731, mean_q: 2.600544, mean_eps: 0.661232
 1695083/6000000: episode: 2397, duration: 20.022s, episode steps: 924, steps per second:  46, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.011326, mae: 2.164078, mean_q: 2.604625, mean_eps: 0.661076
 1695637/6000000: episode: 2398, duration: 11.413s, episode steps: 554, steps per second:  49, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.011727, mae: 2.194915, mean_q: 2.640392, mean_eps: 0.660928
 1696282/6000000: episode: 2399, duration: 13.648s, episode steps: 645, steps per second:  47, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.101 [0.000, 5.000],  loss: 0.012927, mae: 2.162203, mean_q: 2.602961, mean_eps: 0.660808
 1696795/6000000: episode: 2400, duration: 11.282s, episode steps: 513, steps per second:  45, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.012885, mae: 2.157798, mean_q: 2.600331, mean_eps: 0.660692
 1697750/6000000: episode: 2401, duration: 20.384s, episode steps: 955, steps per second:  47, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.012644, mae: 2.168945, mean_q: 2.610159, mean_eps: 0.660546
 1698564/6000000: episode: 2402, duration: 18.724s, episode steps: 814, steps per second:  43, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.012663, mae: 2.155143, mean_q: 2.594665, mean_eps: 0.660369
 1699505/6000000: episode: 2403, duration: 22.031s, episode steps: 941, steps per second:  43, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.013152, mae: 2.169471, mean_q: 2.610134, mean_eps: 0.660193
 1700015/6000000: episode: 2404, duration: 11.694s, episode steps: 510, steps per second:  44, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.163 [0.000, 5.000],  loss: 0.013759, mae: 2.182020, mean_q: 2.625313, mean_eps: 0.660048
 1700715/6000000: episode: 2405, duration: 15.885s, episode steps: 700, steps per second:  44, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.013139, mae: 2.178218, mean_q: 2.620500, mean_eps: 0.659927
 1701374/6000000: episode: 2406, duration: 14.769s, episode steps: 659, steps per second:  45, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.313 [0.000, 5.000],  loss: 0.011699, mae: 2.177720, mean_q: 2.621537, mean_eps: 0.659791
 1702160/6000000: episode: 2407, duration: 17.242s, episode steps: 786, steps per second:  46, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.291 [0.000, 5.000],  loss: 0.011587, mae: 2.176426, mean_q: 2.620625, mean_eps: 0.659647
 1702769/6000000: episode: 2408, duration: 13.617s, episode steps: 609, steps per second:  45, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.011699, mae: 2.170342, mean_q: 2.611147, mean_eps: 0.659507
 1703841/6000000: episode: 2409, duration: 22.949s, episode steps: 1072, steps per second:  47, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.011364, mae: 2.157508, mean_q: 2.596689, mean_eps: 0.659339
 1704353/6000000: episode: 2410, duration: 10.808s, episode steps: 512, steps per second:  47, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.701 [0.000, 5.000],  loss: 0.010435, mae: 2.189349, mean_q: 2.634272, mean_eps: 0.659180
 1704876/6000000: episode: 2411, duration: 12.084s, episode steps: 523, steps per second:  43, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.012851, mae: 2.186948, mean_q: 2.633557, mean_eps: 0.659077
 1705521/6000000: episode: 2412, duration: 15.490s, episode steps: 645, steps per second:  42, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.010545, mae: 2.210171, mean_q: 2.658763, mean_eps: 0.658960
 1706330/6000000: episode: 2413, duration: 18.102s, episode steps: 809, steps per second:  45, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.011046, mae: 2.165443, mean_q: 2.606341, mean_eps: 0.658815
 1706965/6000000: episode: 2414, duration: 13.950s, episode steps: 635, steps per second:  46, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.013294, mae: 2.195846, mean_q: 2.641367, mean_eps: 0.658670
 1707605/6000000: episode: 2415, duration: 14.680s, episode steps: 640, steps per second:  44, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.012669, mae: 2.209094, mean_q: 2.657583, mean_eps: 0.658543
 1707945/6000000: episode: 2416, duration: 8.010s, episode steps: 340, steps per second:  42, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.171 [0.000, 5.000],  loss: 0.012317, mae: 2.194527, mean_q: 2.641323, mean_eps: 0.658445
 1708606/6000000: episode: 2417, duration: 15.328s, episode steps: 661, steps per second:  43, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.828 [0.000, 5.000],  loss: 0.010765, mae: 2.202143, mean_q: 2.650826, mean_eps: 0.658345
 1709247/6000000: episode: 2418, duration: 13.190s, episode steps: 641, steps per second:  49, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.010856, mae: 2.191694, mean_q: 2.639874, mean_eps: 0.658215
 1709636/6000000: episode: 2419, duration: 8.168s, episode steps: 389, steps per second:  48, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.012799, mae: 2.191190, mean_q: 2.638006, mean_eps: 0.658112
 1710108/6000000: episode: 2420, duration: 10.005s, episode steps: 472, steps per second:  47, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.627 [0.000, 5.000],  loss: 0.009922, mae: 2.199691, mean_q: 2.648322, mean_eps: 0.658026
 1710746/6000000: episode: 2421, duration: 13.988s, episode steps: 638, steps per second:  46, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.013091, mae: 2.164637, mean_q: 2.604008, mean_eps: 0.657915
 1711862/6000000: episode: 2422, duration: 23.153s, episode steps: 1116, steps per second:  48, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.011852, mae: 2.158152, mean_q: 2.597366, mean_eps: 0.657739
 1712343/6000000: episode: 2423, duration: 10.345s, episode steps: 481, steps per second:  46, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 0.011656, mae: 2.159465, mean_q: 2.597348, mean_eps: 0.657580
 1712718/6000000: episode: 2424, duration: 8.113s, episode steps: 375, steps per second:  46, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.013183, mae: 2.159001, mean_q: 2.596908, mean_eps: 0.657494
 1713947/6000000: episode: 2425, duration: 25.989s, episode steps: 1229, steps per second:  47, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.010996, mae: 2.161600, mean_q: 2.601751, mean_eps: 0.657334
 1714543/6000000: episode: 2426, duration: 13.406s, episode steps: 596, steps per second:  44, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.773 [0.000, 5.000],  loss: 0.012979, mae: 2.165828, mean_q: 2.605769, mean_eps: 0.657151
 1715432/6000000: episode: 2427, duration: 19.837s, episode steps: 889, steps per second:  45, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.011037, mae: 2.148739, mean_q: 2.584603, mean_eps: 0.657003
 1715915/6000000: episode: 2428, duration: 12.088s, episode steps: 483, steps per second:  40, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.013742, mae: 2.139999, mean_q: 2.575651, mean_eps: 0.656866
 1716873/6000000: episode: 2429, duration: 23.761s, episode steps: 958, steps per second:  40, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.011774, mae: 2.142752, mean_q: 2.580863, mean_eps: 0.656721
 1717514/6000000: episode: 2430, duration: 14.441s, episode steps: 641, steps per second:  44, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.012917, mae: 2.128819, mean_q: 2.564258, mean_eps: 0.656561
 1718207/6000000: episode: 2431, duration: 15.482s, episode steps: 693, steps per second:  45, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.014479, mae: 2.149576, mean_q: 2.585243, mean_eps: 0.656428
 1718752/6000000: episode: 2432, duration: 12.389s, episode steps: 545, steps per second:  44, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.012642, mae: 2.155109, mean_q: 2.592489, mean_eps: 0.656304
 1719134/6000000: episode: 2433, duration: 8.251s, episode steps: 382, steps per second:  46, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.728 [0.000, 5.000],  loss: 0.011451, mae: 2.167410, mean_q: 2.606422, mean_eps: 0.656212
 1719564/6000000: episode: 2434, duration: 9.237s, episode steps: 430, steps per second:  47, episode reward: 10.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.013535, mae: 2.165133, mean_q: 2.604292, mean_eps: 0.656130
 1721045/6000000: episode: 2435, duration: 33.666s, episode steps: 1481, steps per second:  44, episode reward: 30.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.012485, mae: 2.132828, mean_q: 2.567737, mean_eps: 0.655939
 1721568/6000000: episode: 2436, duration: 13.253s, episode steps: 523, steps per second:  39, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.013377, mae: 2.146658, mean_q: 2.582472, mean_eps: 0.655739
 1721937/6000000: episode: 2437, duration: 8.703s, episode steps: 369, steps per second:  42, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.672 [0.000, 5.000],  loss: 0.013173, mae: 2.136362, mean_q: 2.572055, mean_eps: 0.655650
 1722450/6000000: episode: 2438, duration: 11.535s, episode steps: 513, steps per second:  44, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.011800, mae: 2.155306, mean_q: 2.595688, mean_eps: 0.655561
 1723711/6000000: episode: 2439, duration: 28.521s, episode steps: 1261, steps per second:  44, episode reward: 32.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.011997, mae: 2.138183, mean_q: 2.572304, mean_eps: 0.655384
 1724560/6000000: episode: 2440, duration: 19.943s, episode steps: 849, steps per second:  43, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.249 [0.000, 5.000],  loss: 0.010008, mae: 2.130626, mean_q: 2.563951, mean_eps: 0.655173
 1725067/6000000: episode: 2441, duration: 10.939s, episode steps: 507, steps per second:  46, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.012208, mae: 2.149263, mean_q: 2.586591, mean_eps: 0.655038
 1725952/6000000: episode: 2442, duration: 18.009s, episode steps: 885, steps per second:  49, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.011285, mae: 2.144333, mean_q: 2.580329, mean_eps: 0.654898
 1726687/6000000: episode: 2443, duration: 15.632s, episode steps: 735, steps per second:  47, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.011781, mae: 2.144243, mean_q: 2.579716, mean_eps: 0.654736
 1727260/6000000: episode: 2444, duration: 12.745s, episode steps: 573, steps per second:  45, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.013751, mae: 2.150883, mean_q: 2.588561, mean_eps: 0.654606
 1728100/6000000: episode: 2445, duration: 17.658s, episode steps: 840, steps per second:  48, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.013110, mae: 2.146073, mean_q: 2.582690, mean_eps: 0.654464
 1729250/6000000: episode: 2446, duration: 25.364s, episode steps: 1150, steps per second:  45, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.011429, mae: 2.144578, mean_q: 2.580471, mean_eps: 0.654265
 1730176/6000000: episode: 2447, duration: 21.209s, episode steps: 926, steps per second:  44, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.012416, mae: 2.137881, mean_q: 2.572184, mean_eps: 0.654058
 1730640/6000000: episode: 2448, duration: 11.530s, episode steps: 464, steps per second:  40, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.155 [0.000, 5.000],  loss: 0.013264, mae: 2.168471, mean_q: 2.611821, mean_eps: 0.653919
 1731729/6000000: episode: 2449, duration: 26.196s, episode steps: 1089, steps per second:  42, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.012667, mae: 2.154611, mean_q: 2.593527, mean_eps: 0.653763
 1732446/6000000: episode: 2450, duration: 18.494s, episode steps: 717, steps per second:  39, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.011086, mae: 2.125261, mean_q: 2.558547, mean_eps: 0.653582
 1732955/6000000: episode: 2451, duration: 12.335s, episode steps: 509, steps per second:  41, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.010401, mae: 2.163205, mean_q: 2.602805, mean_eps: 0.653460
 1733584/6000000: episode: 2452, duration: 14.327s, episode steps: 629, steps per second:  44, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.234 [0.000, 5.000],  loss: 0.012683, mae: 2.148323, mean_q: 2.585345, mean_eps: 0.653346
 1734245/6000000: episode: 2453, duration: 14.749s, episode steps: 661, steps per second:  45, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.011362, mae: 2.135424, mean_q: 2.570031, mean_eps: 0.653217
 1734993/6000000: episode: 2454, duration: 16.016s, episode steps: 748, steps per second:  47, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: 0.012904, mae: 2.153737, mean_q: 2.592653, mean_eps: 0.653076
 1735559/6000000: episode: 2455, duration: 12.923s, episode steps: 566, steps per second:  44, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.705 [0.000, 5.000],  loss: 0.013285, mae: 2.155723, mean_q: 2.594929, mean_eps: 0.652945
 1735965/6000000: episode: 2456, duration: 9.363s, episode steps: 406, steps per second:  43, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.012816, mae: 2.161312, mean_q: 2.602844, mean_eps: 0.652848
 1736643/6000000: episode: 2457, duration: 15.300s, episode steps: 678, steps per second:  44, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.686 [0.000, 5.000],  loss: 0.012285, mae: 2.140735, mean_q: 2.575656, mean_eps: 0.652739
 1737223/6000000: episode: 2458, duration: 13.196s, episode steps: 580, steps per second:  44, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.009914, mae: 2.157468, mean_q: 2.595900, mean_eps: 0.652614
 1737851/6000000: episode: 2459, duration: 13.838s, episode steps: 628, steps per second:  45, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.012129, mae: 2.146343, mean_q: 2.583796, mean_eps: 0.652493
 1739062/6000000: episode: 2460, duration: 26.419s, episode steps: 1211, steps per second:  46, episode reward: 32.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.013448, mae: 2.147060, mean_q: 2.583662, mean_eps: 0.652309
 1739972/6000000: episode: 2461, duration: 20.056s, episode steps: 910, steps per second:  45, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.012437, mae: 2.154818, mean_q: 2.593216, mean_eps: 0.652097
 1741054/6000000: episode: 2462, duration: 23.009s, episode steps: 1082, steps per second:  47, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.012427, mae: 2.132281, mean_q: 2.566803, mean_eps: 0.651898
 1741723/6000000: episode: 2463, duration: 13.975s, episode steps: 669, steps per second:  48, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.139 [0.000, 5.000],  loss: 0.011640, mae: 2.134647, mean_q: 2.569779, mean_eps: 0.651722
 1742277/6000000: episode: 2464, duration: 11.931s, episode steps: 554, steps per second:  46, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.012213, mae: 2.124293, mean_q: 2.556717, mean_eps: 0.651600
 1742910/6000000: episode: 2465, duration: 14.699s, episode steps: 633, steps per second:  43, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.012045, mae: 2.147099, mean_q: 2.583422, mean_eps: 0.651481
 1743643/6000000: episode: 2466, duration: 15.765s, episode steps: 733, steps per second:  46, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.012153, mae: 2.144734, mean_q: 2.580976, mean_eps: 0.651345
 1744705/6000000: episode: 2467, duration: 23.038s, episode steps: 1062, steps per second:  46, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.190 [0.000, 5.000],  loss: 0.014039, mae: 2.143792, mean_q: 2.579520, mean_eps: 0.651165
 1745195/6000000: episode: 2468, duration: 11.008s, episode steps: 490, steps per second:  45, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.286 [0.000, 5.000],  loss: 0.012556, mae: 2.151060, mean_q: 2.588239, mean_eps: 0.651010
 1746430/6000000: episode: 2469, duration: 26.930s, episode steps: 1235, steps per second:  46, episode reward: 19.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.011386, mae: 2.141969, mean_q: 2.578295, mean_eps: 0.650838
 1747154/6000000: episode: 2470, duration: 16.303s, episode steps: 724, steps per second:  44, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.264 [0.000, 5.000],  loss: 0.012278, mae: 2.139253, mean_q: 2.575154, mean_eps: 0.650642
 1748292/6000000: episode: 2471, duration: 27.465s, episode steps: 1138, steps per second:  41, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.281 [0.000, 5.000],  loss: 0.012034, mae: 2.133880, mean_q: 2.569106, mean_eps: 0.650456
 1749130/6000000: episode: 2472, duration: 20.019s, episode steps: 838, steps per second:  42, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.236 [0.000, 5.000],  loss: 0.012212, mae: 2.141115, mean_q: 2.577081, mean_eps: 0.650258
 1749822/6000000: episode: 2473, duration: 15.848s, episode steps: 692, steps per second:  44, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.012860, mae: 2.156815, mean_q: 2.596009, mean_eps: 0.650105
 1750530/6000000: episode: 2474, duration: 15.952s, episode steps: 708, steps per second:  44, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.011982, mae: 2.129646, mean_q: 2.564953, mean_eps: 0.649965
 1751207/6000000: episode: 2475, duration: 14.878s, episode steps: 677, steps per second:  46, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.014142, mae: 2.152787, mean_q: 2.592167, mean_eps: 0.649826
 1752299/6000000: episode: 2476, duration: 23.632s, episode steps: 1092, steps per second:  46, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.012348, mae: 2.142213, mean_q: 2.578425, mean_eps: 0.649650
 1753133/6000000: episode: 2477, duration: 19.099s, episode steps: 834, steps per second:  44, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.191 [0.000, 5.000],  loss: 0.011428, mae: 2.137904, mean_q: 2.574185, mean_eps: 0.649457
 1754106/6000000: episode: 2478, duration: 22.969s, episode steps: 973, steps per second:  42, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.012326, mae: 2.141351, mean_q: 2.576194, mean_eps: 0.649276
 1754776/6000000: episode: 2479, duration: 14.559s, episode steps: 670, steps per second:  46, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.011315, mae: 2.154533, mean_q: 2.594045, mean_eps: 0.649112
 1755419/6000000: episode: 2480, duration: 14.160s, episode steps: 643, steps per second:  45, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.756 [0.000, 5.000],  loss: 0.011207, mae: 2.146236, mean_q: 2.582021, mean_eps: 0.648981
 1756233/6000000: episode: 2481, duration: 17.999s, episode steps: 814, steps per second:  45, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.193 [0.000, 5.000],  loss: 0.013807, mae: 2.119513, mean_q: 2.547972, mean_eps: 0.648835
 1756825/6000000: episode: 2482, duration: 12.719s, episode steps: 592, steps per second:  47, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.012801, mae: 2.131547, mean_q: 2.564503, mean_eps: 0.648694
 1757629/6000000: episode: 2483, duration: 16.359s, episode steps: 804, steps per second:  49, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.013686, mae: 2.123876, mean_q: 2.554175, mean_eps: 0.648554
 1758234/6000000: episode: 2484, duration: 12.847s, episode steps: 605, steps per second:  47, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.012020, mae: 2.130734, mean_q: 2.564323, mean_eps: 0.648414
 1758817/6000000: episode: 2485, duration: 12.925s, episode steps: 583, steps per second:  45, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.013445, mae: 2.140637, mean_q: 2.575063, mean_eps: 0.648295
 1759440/6000000: episode: 2486, duration: 13.439s, episode steps: 623, steps per second:  46, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.698 [0.000, 5.000],  loss: 0.011738, mae: 2.131101, mean_q: 2.563978, mean_eps: 0.648174
 1760938/6000000: episode: 2487, duration: 30.823s, episode steps: 1498, steps per second:  49, episode reward: 28.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.011997, mae: 2.132524, mean_q: 2.567200, mean_eps: 0.647962
 1762052/6000000: episode: 2488, duration: 23.646s, episode steps: 1114, steps per second:  47, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.012830, mae: 2.137352, mean_q: 2.571042, mean_eps: 0.647701
 1762849/6000000: episode: 2489, duration: 17.266s, episode steps: 797, steps per second:  46, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.012612, mae: 2.116285, mean_q: 2.546103, mean_eps: 0.647510
 1763237/6000000: episode: 2490, duration: 8.984s, episode steps: 388, steps per second:  43, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.012526, mae: 2.133990, mean_q: 2.567316, mean_eps: 0.647391
 1763963/6000000: episode: 2491, duration: 17.437s, episode steps: 726, steps per second:  42, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.012291, mae: 2.140535, mean_q: 2.574832, mean_eps: 0.647280
 1764501/6000000: episode: 2492, duration: 12.185s, episode steps: 538, steps per second:  44, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.013468, mae: 2.146270, mean_q: 2.581224, mean_eps: 0.647154
 1764875/6000000: episode: 2493, duration: 9.089s, episode steps: 374, steps per second:  41, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.647 [0.000, 5.000],  loss: 0.011565, mae: 2.159639, mean_q: 2.598705, mean_eps: 0.647062
 1765580/6000000: episode: 2494, duration: 15.431s, episode steps: 705, steps per second:  46, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.166 [0.000, 5.000],  loss: 0.011190, mae: 2.118629, mean_q: 2.550721, mean_eps: 0.646955
 1766096/6000000: episode: 2495, duration: 12.052s, episode steps: 516, steps per second:  43, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.011930, mae: 2.102140, mean_q: 2.531268, mean_eps: 0.646833
 1766898/6000000: episode: 2496, duration: 18.472s, episode steps: 802, steps per second:  43, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.213 [0.000, 5.000],  loss: 0.013711, mae: 2.120555, mean_q: 2.552735, mean_eps: 0.646701
 1767351/6000000: episode: 2497, duration: 10.996s, episode steps: 453, steps per second:  41, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.278 [0.000, 5.000],  loss: 0.012847, mae: 2.120556, mean_q: 2.553126, mean_eps: 0.646575
 1767777/6000000: episode: 2498, duration: 10.186s, episode steps: 426, steps per second:  42, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.192 [0.000, 5.000],  loss: 0.014725, mae: 2.114704, mean_q: 2.545761, mean_eps: 0.646487
 1768602/6000000: episode: 2499, duration: 18.545s, episode steps: 825, steps per second:  44, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.216 [0.000, 5.000],  loss: 0.013886, mae: 2.113123, mean_q: 2.542125, mean_eps: 0.646362
 1769855/6000000: episode: 2500, duration: 28.498s, episode steps: 1253, steps per second:  44, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.011659, mae: 2.128284, mean_q: 2.560734, mean_eps: 0.646154
 1770250/6000000: episode: 2501, duration: 9.013s, episode steps: 395, steps per second:  44, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.013790, mae: 2.112455, mean_q: 2.540942, mean_eps: 0.645990
 1770790/6000000: episode: 2502, duration: 11.873s, episode steps: 540, steps per second:  45, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.013719, mae: 2.142742, mean_q: 2.578839, mean_eps: 0.645896
 1771764/6000000: episode: 2503, duration: 21.970s, episode steps: 974, steps per second:  44, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.013323, mae: 2.112712, mean_q: 2.543179, mean_eps: 0.645745
 1772473/6000000: episode: 2504, duration: 16.067s, episode steps: 709, steps per second:  44, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.137 [0.000, 5.000],  loss: 0.012223, mae: 2.131999, mean_q: 2.566972, mean_eps: 0.645576
 1773184/6000000: episode: 2505, duration: 15.844s, episode steps: 711, steps per second:  45, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.013737, mae: 2.127027, mean_q: 2.560244, mean_eps: 0.645434
 1774013/6000000: episode: 2506, duration: 17.290s, episode steps: 829, steps per second:  48, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.012109, mae: 2.128433, mean_q: 2.561282, mean_eps: 0.645280
 1774660/6000000: episode: 2507, duration: 13.682s, episode steps: 647, steps per second:  47, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.716 [0.000, 5.000],  loss: 0.013273, mae: 2.091729, mean_q: 2.517704, mean_eps: 0.645133
 1775429/6000000: episode: 2508, duration: 16.260s, episode steps: 769, steps per second:  47, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.012141, mae: 2.106656, mean_q: 2.535229, mean_eps: 0.644991
 1776496/6000000: episode: 2509, duration: 22.959s, episode steps: 1067, steps per second:  46, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.012745, mae: 2.104861, mean_q: 2.534844, mean_eps: 0.644808
 1777534/6000000: episode: 2510, duration: 22.461s, episode steps: 1038, steps per second:  46, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.012582, mae: 2.124343, mean_q: 2.556594, mean_eps: 0.644597
 1778537/6000000: episode: 2511, duration: 21.614s, episode steps: 1003, steps per second:  46, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.928 [0.000, 5.000],  loss: 0.013237, mae: 2.103392, mean_q: 2.531799, mean_eps: 0.644393
 1779182/6000000: episode: 2512, duration: 14.458s, episode steps: 645, steps per second:  45, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.013155, mae: 2.099649, mean_q: 2.526948, mean_eps: 0.644228
 1779806/6000000: episode: 2513, duration: 14.395s, episode steps: 624, steps per second:  43, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.210 [0.000, 5.000],  loss: 0.012173, mae: 2.115033, mean_q: 2.546707, mean_eps: 0.644101
 1780397/6000000: episode: 2514, duration: 16.156s, episode steps: 591, steps per second:  37, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.252 [0.000, 5.000],  loss: 0.013173, mae: 2.099479, mean_q: 2.528384, mean_eps: 0.643980
 1781020/6000000: episode: 2515, duration: 16.399s, episode steps: 623, steps per second:  38, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.014318, mae: 2.098695, mean_q: 2.526061, mean_eps: 0.643858
 1782016/6000000: episode: 2516, duration: 23.459s, episode steps: 996, steps per second:  42, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.013565, mae: 2.111409, mean_q: 2.540627, mean_eps: 0.643697
 1782904/6000000: episode: 2517, duration: 21.579s, episode steps: 888, steps per second:  41, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.013662, mae: 2.119806, mean_q: 2.550608, mean_eps: 0.643508
 1783395/6000000: episode: 2518, duration: 11.101s, episode steps: 491, steps per second:  44, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: 0.011759, mae: 2.096789, mean_q: 2.523428, mean_eps: 0.643370
 1784501/6000000: episode: 2519, duration: 26.272s, episode steps: 1106, steps per second:  42, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.013044, mae: 2.102447, mean_q: 2.529392, mean_eps: 0.643210
 1784970/6000000: episode: 2520, duration: 10.972s, episode steps: 469, steps per second:  43, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.808 [0.000, 5.000],  loss: 0.013466, mae: 2.089279, mean_q: 2.512907, mean_eps: 0.643053
 1785624/6000000: episode: 2521, duration: 15.203s, episode steps: 654, steps per second:  43, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.013536, mae: 2.097650, mean_q: 2.525058, mean_eps: 0.642941
 1786983/6000000: episode: 2522, duration: 30.265s, episode steps: 1359, steps per second:  45, episode reward: 27.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.012592, mae: 2.109968, mean_q: 2.540055, mean_eps: 0.642740
 1787457/6000000: episode: 2523, duration: 10.363s, episode steps: 474, steps per second:  46, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.078 [0.000, 5.000],  loss: 0.012343, mae: 2.115324, mean_q: 2.545122, mean_eps: 0.642556
 1788001/6000000: episode: 2524, duration: 12.277s, episode steps: 544, steps per second:  44, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.114 [0.000, 5.000],  loss: 0.014333, mae: 2.120361, mean_q: 2.549860, mean_eps: 0.642454
 1788445/6000000: episode: 2525, duration: 9.895s, episode steps: 444, steps per second:  45, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.013506, mae: 2.131045, mean_q: 2.562841, mean_eps: 0.642355
 1789069/6000000: episode: 2526, duration: 12.820s, episode steps: 624, steps per second:  49, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.013439, mae: 2.118248, mean_q: 2.547650, mean_eps: 0.642248
 1789925/6000000: episode: 2527, duration: 17.440s, episode steps: 856, steps per second:  49, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.012051, mae: 2.116910, mean_q: 2.546869, mean_eps: 0.642100
 1790900/6000000: episode: 2528, duration: 20.731s, episode steps: 975, steps per second:  47, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.012493, mae: 2.119145, mean_q: 2.552822, mean_eps: 0.641918
 1791889/6000000: episode: 2529, duration: 21.357s, episode steps: 989, steps per second:  46, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.012394, mae: 2.116432, mean_q: 2.547873, mean_eps: 0.641721
 1792717/6000000: episode: 2530, duration: 17.688s, episode steps: 828, steps per second:  47, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.011697, mae: 2.124552, mean_q: 2.556572, mean_eps: 0.641539
 1793403/6000000: episode: 2531, duration: 14.483s, episode steps: 686, steps per second:  47, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.013994, mae: 2.130610, mean_q: 2.563533, mean_eps: 0.641388
 1793980/6000000: episode: 2532, duration: 11.956s, episode steps: 577, steps per second:  48, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.236 [0.000, 5.000],  loss: 0.014902, mae: 2.134820, mean_q: 2.568290, mean_eps: 0.641262
 1794355/6000000: episode: 2533, duration: 7.806s, episode steps: 375, steps per second:  48, episode reward:  9.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.012244, mae: 2.119477, mean_q: 2.551202, mean_eps: 0.641167
 1795003/6000000: episode: 2534, duration: 14.397s, episode steps: 648, steps per second:  45, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.011646, mae: 2.138378, mean_q: 2.573011, mean_eps: 0.641064
 1795571/6000000: episode: 2535, duration: 12.501s, episode steps: 568, steps per second:  45, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.197 [0.000, 5.000],  loss: 0.013040, mae: 2.105921, mean_q: 2.534299, mean_eps: 0.640943
 1795965/6000000: episode: 2536, duration: 9.163s, episode steps: 394, steps per second:  43, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.685 [0.000, 5.000],  loss: 0.012879, mae: 2.097728, mean_q: 2.524149, mean_eps: 0.640846
 1796875/6000000: episode: 2537, duration: 24.058s, episode steps: 910, steps per second:  38, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.205 [0.000, 5.000],  loss: 0.012910, mae: 2.106752, mean_q: 2.535889, mean_eps: 0.640716
 1797637/6000000: episode: 2538, duration: 16.879s, episode steps: 762, steps per second:  45, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.226 [0.000, 5.000],  loss: 0.013302, mae: 2.103333, mean_q: 2.531122, mean_eps: 0.640549
 1797978/6000000: episode: 2539, duration: 7.127s, episode steps: 341, steps per second:  48, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.848 [0.000, 5.000],  loss: 0.010075, mae: 2.083018, mean_q: 2.506555, mean_eps: 0.640438
 1798739/6000000: episode: 2540, duration: 16.621s, episode steps: 761, steps per second:  46, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.185 [0.000, 5.000],  loss: 0.013126, mae: 2.114768, mean_q: 2.545195, mean_eps: 0.640328
 1799806/6000000: episode: 2541, duration: 23.515s, episode steps: 1067, steps per second:  45, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.291 [0.000, 5.000],  loss: 0.012325, mae: 2.092527, mean_q: 2.519242, mean_eps: 0.640146
 1800790/6000000: episode: 2542, duration: 20.866s, episode steps: 984, steps per second:  47, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.271 [0.000, 5.000],  loss: 0.012877, mae: 2.121876, mean_q: 2.554808, mean_eps: 0.639940
 1801485/6000000: episode: 2543, duration: 15.266s, episode steps: 695, steps per second:  46, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.013032, mae: 2.113442, mean_q: 2.542619, mean_eps: 0.639772
 1802261/6000000: episode: 2544, duration: 17.331s, episode steps: 776, steps per second:  45, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.045 [0.000, 5.000],  loss: 0.012940, mae: 2.127679, mean_q: 2.563789, mean_eps: 0.639625
 1803021/6000000: episode: 2545, duration: 17.200s, episode steps: 760, steps per second:  44, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.150 [0.000, 5.000],  loss: 0.012495, mae: 2.119770, mean_q: 2.552881, mean_eps: 0.639472
 1803400/6000000: episode: 2546, duration: 8.755s, episode steps: 379, steps per second:  43, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.937 [0.000, 5.000],  loss: 0.013596, mae: 2.121884, mean_q: 2.555939, mean_eps: 0.639358
 1803953/6000000: episode: 2547, duration: 13.066s, episode steps: 553, steps per second:  42, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.010598, mae: 2.137381, mean_q: 2.574103, mean_eps: 0.639265
 1804782/6000000: episode: 2548, duration: 18.768s, episode steps: 829, steps per second:  44, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.012392, mae: 2.121019, mean_q: 2.554529, mean_eps: 0.639126
 1805189/6000000: episode: 2549, duration: 9.155s, episode steps: 407, steps per second:  44, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.012304, mae: 2.137956, mean_q: 2.575008, mean_eps: 0.639003
 1805730/6000000: episode: 2550, duration: 11.430s, episode steps: 541, steps per second:  47, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.011433, mae: 2.132730, mean_q: 2.567993, mean_eps: 0.638908
 1806438/6000000: episode: 2551, duration: 14.402s, episode steps: 708, steps per second:  49, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.012397, mae: 2.111513, mean_q: 2.540692, mean_eps: 0.638783
 1807481/6000000: episode: 2552, duration: 22.731s, episode steps: 1043, steps per second:  46, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.312 [0.000, 5.000],  loss: 0.012443, mae: 2.119367, mean_q: 2.551716, mean_eps: 0.638608
 1808205/6000000: episode: 2553, duration: 15.903s, episode steps: 724, steps per second:  46, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.011428, mae: 2.113970, mean_q: 2.546017, mean_eps: 0.638431
 1808819/6000000: episode: 2554, duration: 12.751s, episode steps: 614, steps per second:  48, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.011498, mae: 2.139846, mean_q: 2.574667, mean_eps: 0.638298
 1809806/6000000: episode: 2555, duration: 20.299s, episode steps: 987, steps per second:  49, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.012611, mae: 2.130618, mean_q: 2.564295, mean_eps: 0.638138
 1810432/6000000: episode: 2556, duration: 13.484s, episode steps: 626, steps per second:  46, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.013422, mae: 2.140856, mean_q: 2.576264, mean_eps: 0.637976
 1811073/6000000: episode: 2557, duration: 14.200s, episode steps: 641, steps per second:  45, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.011506, mae: 2.124739, mean_q: 2.556269, mean_eps: 0.637850
 1812075/6000000: episode: 2558, duration: 21.554s, episode steps: 1002, steps per second:  46, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.137 [0.000, 5.000],  loss: 0.011580, mae: 2.120038, mean_q: 2.553524, mean_eps: 0.637685
 1812854/6000000: episode: 2559, duration: 18.541s, episode steps: 779, steps per second:  42, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.196 [0.000, 5.000],  loss: 0.011942, mae: 2.125442, mean_q: 2.558587, mean_eps: 0.637507
 1813365/6000000: episode: 2560, duration: 11.433s, episode steps: 511, steps per second:  45, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.011857, mae: 2.143135, mean_q: 2.580177, mean_eps: 0.637378
 1814120/6000000: episode: 2561, duration: 16.241s, episode steps: 755, steps per second:  46, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.013068, mae: 2.131817, mean_q: 2.564422, mean_eps: 0.637252
 1814830/6000000: episode: 2562, duration: 15.713s, episode steps: 710, steps per second:  45, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.011912, mae: 2.137666, mean_q: 2.571515, mean_eps: 0.637105
 1815448/6000000: episode: 2563, duration: 13.749s, episode steps: 618, steps per second:  45, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.236 [0.000, 5.000],  loss: 0.014287, mae: 2.137649, mean_q: 2.571164, mean_eps: 0.636972
 1816508/6000000: episode: 2564, duration: 22.669s, episode steps: 1060, steps per second:  47, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.012024, mae: 2.108279, mean_q: 2.537024, mean_eps: 0.636805
 1817152/6000000: episode: 2565, duration: 14.137s, episode steps: 644, steps per second:  46, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 0.011205, mae: 2.108141, mean_q: 2.537414, mean_eps: 0.636634
 1818400/6000000: episode: 2566, duration: 28.519s, episode steps: 1248, steps per second:  44, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.012734, mae: 2.130545, mean_q: 2.562872, mean_eps: 0.636445
 1818947/6000000: episode: 2567, duration: 12.279s, episode steps: 547, steps per second:  45, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.832 [0.000, 5.000],  loss: 0.013979, mae: 2.114714, mean_q: 2.545458, mean_eps: 0.636266
 1819497/6000000: episode: 2568, duration: 11.789s, episode steps: 550, steps per second:  47, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.012621, mae: 2.116495, mean_q: 2.546095, mean_eps: 0.636156
 1820100/6000000: episode: 2569, duration: 13.181s, episode steps: 603, steps per second:  46, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.012378, mae: 2.108915, mean_q: 2.537027, mean_eps: 0.636040
 1820486/6000000: episode: 2570, duration: 8.482s, episode steps: 386, steps per second:  46, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.012581, mae: 2.111272, mean_q: 2.543440, mean_eps: 0.635942
 1821439/6000000: episode: 2571, duration: 21.291s, episode steps: 953, steps per second:  45, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.201 [0.000, 5.000],  loss: 0.012649, mae: 2.123496, mean_q: 2.557267, mean_eps: 0.635808
 1822319/6000000: episode: 2572, duration: 17.766s, episode steps: 880, steps per second:  50, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.010 [0.000, 5.000],  loss: 0.012778, mae: 2.127295, mean_q: 2.560833, mean_eps: 0.635624
 1823363/6000000: episode: 2573, duration: 22.040s, episode steps: 1044, steps per second:  47, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.011130, mae: 2.119893, mean_q: 2.551072, mean_eps: 0.635432
 1823732/6000000: episode: 2574, duration: 8.045s, episode steps: 369, steps per second:  46, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.106 [0.000, 5.000],  loss: 0.010911, mae: 2.127551, mean_q: 2.561474, mean_eps: 0.635291
 1824400/6000000: episode: 2575, duration: 14.262s, episode steps: 668, steps per second:  47, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.012850, mae: 2.114619, mean_q: 2.543884, mean_eps: 0.635187
 1824882/6000000: episode: 2576, duration: 9.679s, episode steps: 482, steps per second:  50, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.147 [0.000, 5.000],  loss: 0.012035, mae: 2.124317, mean_q: 2.554163, mean_eps: 0.635072
 1825644/6000000: episode: 2577, duration: 16.357s, episode steps: 762, steps per second:  47, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.013337, mae: 2.145108, mean_q: 2.582113, mean_eps: 0.634948
 1826060/6000000: episode: 2578, duration: 9.171s, episode steps: 416, steps per second:  45, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.013249, mae: 2.176179, mean_q: 2.625807, mean_eps: 0.634830
 1827002/6000000: episode: 2579, duration: 19.841s, episode steps: 942, steps per second:  47, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.013304, mae: 2.149787, mean_q: 2.587165, mean_eps: 0.634694
 1827847/6000000: episode: 2580, duration: 18.202s, episode steps: 845, steps per second:  46, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.170 [0.000, 5.000],  loss: 0.012598, mae: 2.136033, mean_q: 2.571289, mean_eps: 0.634515
 1828519/6000000: episode: 2581, duration: 14.786s, episode steps: 672, steps per second:  45, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.054 [0.000, 5.000],  loss: 0.011189, mae: 2.135409, mean_q: 2.570346, mean_eps: 0.634364
 1829269/6000000: episode: 2582, duration: 17.193s, episode steps: 750, steps per second:  44, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.117 [0.000, 5.000],  loss: 0.012087, mae: 2.134143, mean_q: 2.567862, mean_eps: 0.634221
 1829771/6000000: episode: 2583, duration: 11.528s, episode steps: 502, steps per second:  44, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.011986, mae: 2.145083, mean_q: 2.581121, mean_eps: 0.634096
 1830564/6000000: episode: 2584, duration: 18.107s, episode steps: 793, steps per second:  44, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.013785, mae: 2.166556, mean_q: 2.606836, mean_eps: 0.633967
 1831353/6000000: episode: 2585, duration: 18.135s, episode steps: 789, steps per second:  44, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.013637, mae: 2.183725, mean_q: 2.626576, mean_eps: 0.633808
 1832121/6000000: episode: 2586, duration: 16.828s, episode steps: 768, steps per second:  46, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.132 [0.000, 5.000],  loss: 0.011606, mae: 2.161921, mean_q: 2.601387, mean_eps: 0.633652
 1832644/6000000: episode: 2587, duration: 11.561s, episode steps: 523, steps per second:  45, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.902 [0.000, 5.000],  loss: 0.013226, mae: 2.173020, mean_q: 2.614043, mean_eps: 0.633524
 1833202/6000000: episode: 2588, duration: 11.994s, episode steps: 558, steps per second:  47, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.013481, mae: 2.173641, mean_q: 2.616251, mean_eps: 0.633416
 1834067/6000000: episode: 2589, duration: 18.648s, episode steps: 865, steps per second:  46, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.012072, mae: 2.151788, mean_q: 2.589096, mean_eps: 0.633273
 1834978/6000000: episode: 2590, duration: 20.913s, episode steps: 911, steps per second:  44, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.014869, mae: 2.171169, mean_q: 2.611581, mean_eps: 0.633096
 1835504/6000000: episode: 2591, duration: 12.142s, episode steps: 526, steps per second:  43, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.188 [0.000, 5.000],  loss: 0.012310, mae: 2.154654, mean_q: 2.592446, mean_eps: 0.632952
 1836634/6000000: episode: 2592, duration: 24.870s, episode steps: 1130, steps per second:  45, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.013033, mae: 2.150186, mean_q: 2.585595, mean_eps: 0.632786
 1837633/6000000: episode: 2593, duration: 22.904s, episode steps: 999, steps per second:  44, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.014382, mae: 2.154455, mean_q: 2.592775, mean_eps: 0.632573
 1838415/6000000: episode: 2594, duration: 17.082s, episode steps: 782, steps per second:  46, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.239 [0.000, 5.000],  loss: 0.012073, mae: 2.140201, mean_q: 2.577073, mean_eps: 0.632395
 1839092/6000000: episode: 2595, duration: 13.877s, episode steps: 677, steps per second:  49, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.121 [0.000, 5.000],  loss: 0.013020, mae: 2.152023, mean_q: 2.589508, mean_eps: 0.632250
 1840028/6000000: episode: 2596, duration: 20.224s, episode steps: 936, steps per second:  46, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.012060, mae: 2.126494, mean_q: 2.558959, mean_eps: 0.632088
 1841116/6000000: episode: 2597, duration: 23.195s, episode steps: 1088, steps per second:  47, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.011828, mae: 2.171246, mean_q: 2.612975, mean_eps: 0.631886
 1841674/6000000: episode: 2598, duration: 11.625s, episode steps: 558, steps per second:  48, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.247 [0.000, 5.000],  loss: 0.011467, mae: 2.173043, mean_q: 2.615839, mean_eps: 0.631721
 1842173/6000000: episode: 2599, duration: 10.475s, episode steps: 499, steps per second:  48, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.011966, mae: 2.164063, mean_q: 2.603483, mean_eps: 0.631615
 1842835/6000000: episode: 2600, duration: 14.052s, episode steps: 662, steps per second:  47, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.189 [0.000, 5.000],  loss: 0.013746, mae: 2.173636, mean_q: 2.615038, mean_eps: 0.631499
 1843850/6000000: episode: 2601, duration: 22.776s, episode steps: 1015, steps per second:  45, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.294 [0.000, 5.000],  loss: 0.013928, mae: 2.162359, mean_q: 2.603311, mean_eps: 0.631332
 1844453/6000000: episode: 2602, duration: 13.698s, episode steps: 603, steps per second:  44, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.014777, mae: 2.177005, mean_q: 2.620719, mean_eps: 0.631170
 1846298/6000000: episode: 2603, duration: 42.563s, episode steps: 1845, steps per second:  43, episode reward: 39.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 0.012610, mae: 2.164721, mean_q: 2.604374, mean_eps: 0.630925
 1846928/6000000: episode: 2604, duration: 14.160s, episode steps: 630, steps per second:  44, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.011429, mae: 2.144140, mean_q: 2.581024, mean_eps: 0.630678
 1848358/6000000: episode: 2605, duration: 31.936s, episode steps: 1430, steps per second:  45, episode reward: 32.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.011807, mae: 2.161874, mean_q: 2.601598, mean_eps: 0.630472
 1849283/6000000: episode: 2606, duration: 20.390s, episode steps: 925, steps per second:  45, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.312 [0.000, 5.000],  loss: 0.011327, mae: 2.148526, mean_q: 2.586222, mean_eps: 0.630236
 1849983/6000000: episode: 2607, duration: 15.381s, episode steps: 700, steps per second:  46, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.013283, mae: 2.157871, mean_q: 2.596075, mean_eps: 0.630074
 1850762/6000000: episode: 2608, duration: 17.547s, episode steps: 779, steps per second:  44, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.012842, mae: 2.154732, mean_q: 2.593047, mean_eps: 0.629926
 1851834/6000000: episode: 2609, duration: 24.271s, episode steps: 1072, steps per second:  44, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.013226, mae: 2.159103, mean_q: 2.598031, mean_eps: 0.629740
 1852392/6000000: episode: 2610, duration: 12.839s, episode steps: 558, steps per second:  43, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.012200, mae: 2.148851, mean_q: 2.585910, mean_eps: 0.629578
 1852985/6000000: episode: 2611, duration: 13.583s, episode steps: 593, steps per second:  44, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.239 [0.000, 5.000],  loss: 0.013713, mae: 2.159684, mean_q: 2.598576, mean_eps: 0.629462
 1853926/6000000: episode: 2612, duration: 21.444s, episode steps: 941, steps per second:  44, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.014261, mae: 2.141213, mean_q: 2.576804, mean_eps: 0.629309
 1854714/6000000: episode: 2613, duration: 16.562s, episode steps: 788, steps per second:  48, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.012813, mae: 2.147696, mean_q: 2.582887, mean_eps: 0.629136
 1855237/6000000: episode: 2614, duration: 11.005s, episode steps: 523, steps per second:  48, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.178 [0.000, 5.000],  loss: 0.013626, mae: 2.129931, mean_q: 2.563851, mean_eps: 0.629005
 1855816/6000000: episode: 2615, duration: 12.571s, episode steps: 579, steps per second:  46, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.012114, mae: 2.159095, mean_q: 2.598099, mean_eps: 0.628895
 1856887/6000000: episode: 2616, duration: 24.028s, episode steps: 1071, steps per second:  45, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.012438, mae: 2.156157, mean_q: 2.595514, mean_eps: 0.628730
 1857961/6000000: episode: 2617, duration: 22.576s, episode steps: 1074, steps per second:  48, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.013829, mae: 2.136029, mean_q: 2.570702, mean_eps: 0.628515
 1859417/6000000: episode: 2618, duration: 30.779s, episode steps: 1456, steps per second:  47, episode reward: 29.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.011859, mae: 2.146090, mean_q: 2.582034, mean_eps: 0.628262
 1859817/6000000: episode: 2619, duration: 8.637s, episode steps: 400, steps per second:  46, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.012321, mae: 2.135120, mean_q: 2.570645, mean_eps: 0.628076
 1860561/6000000: episode: 2620, duration: 16.089s, episode steps: 744, steps per second:  46, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.011087, mae: 2.150941, mean_q: 2.588000, mean_eps: 0.627962
 1861455/6000000: episode: 2621, duration: 19.506s, episode steps: 894, steps per second:  46, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.012236, mae: 2.138513, mean_q: 2.572960, mean_eps: 0.627798
 1862022/6000000: episode: 2622, duration: 13.638s, episode steps: 567, steps per second:  42, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.012430, mae: 2.142141, mean_q: 2.577701, mean_eps: 0.627652
 1863077/6000000: episode: 2623, duration: 24.154s, episode steps: 1055, steps per second:  44, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.191 [0.000, 5.000],  loss: 0.013227, mae: 2.158157, mean_q: 2.596778, mean_eps: 0.627490
 1863960/6000000: episode: 2624, duration: 19.395s, episode steps: 883, steps per second:  46, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.011672, mae: 2.159419, mean_q: 2.598382, mean_eps: 0.627296
 1865002/6000000: episode: 2625, duration: 23.606s, episode steps: 1042, steps per second:  44, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.012227, mae: 2.144945, mean_q: 2.581729, mean_eps: 0.627104
 1865770/6000000: episode: 2626, duration: 16.141s, episode steps: 768, steps per second:  48, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.902 [0.000, 5.000],  loss: 0.011915, mae: 2.151471, mean_q: 2.589346, mean_eps: 0.626923
 1866169/6000000: episode: 2627, duration: 8.318s, episode steps: 399, steps per second:  48, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.233 [0.000, 5.000],  loss: 0.013654, mae: 2.139994, mean_q: 2.576089, mean_eps: 0.626806
 1866860/6000000: episode: 2628, duration: 15.414s, episode steps: 691, steps per second:  45, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.014741, mae: 2.120683, mean_q: 2.551184, mean_eps: 0.626697
 1867678/6000000: episode: 2629, duration: 18.812s, episode steps: 818, steps per second:  43, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.012618, mae: 2.129793, mean_q: 2.562543, mean_eps: 0.626546
 1868910/6000000: episode: 2630, duration: 27.458s, episode steps: 1232, steps per second:  45, episode reward: 19.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.183 [0.000, 5.000],  loss: 0.011805, mae: 2.138221, mean_q: 2.574734, mean_eps: 0.626341
 1869648/6000000: episode: 2631, duration: 16.253s, episode steps: 738, steps per second:  45, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.205 [0.000, 5.000],  loss: 0.013506, mae: 2.131485, mean_q: 2.564736, mean_eps: 0.626144
 1870775/6000000: episode: 2632, duration: 25.040s, episode steps: 1127, steps per second:  45, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.012535, mae: 2.146517, mean_q: 2.582978, mean_eps: 0.625958
 1871425/6000000: episode: 2633, duration: 13.419s, episode steps: 650, steps per second:  48, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: 0.012898, mae: 2.132971, mean_q: 2.567222, mean_eps: 0.625780
 1872155/6000000: episode: 2634, duration: 15.361s, episode steps: 730, steps per second:  48, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.011069, mae: 2.151780, mean_q: 2.589241, mean_eps: 0.625642
 1872819/6000000: episode: 2635, duration: 14.208s, episode steps: 664, steps per second:  47, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.122 [0.000, 5.000],  loss: 0.013035, mae: 2.143725, mean_q: 2.579762, mean_eps: 0.625503
 1873748/6000000: episode: 2636, duration: 20.065s, episode steps: 929, steps per second:  46, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.248 [0.000, 5.000],  loss: 0.013101, mae: 2.160968, mean_q: 2.602146, mean_eps: 0.625344
 1874429/6000000: episode: 2637, duration: 14.763s, episode steps: 681, steps per second:  46, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.245 [0.000, 5.000],  loss: 0.011154, mae: 2.151382, mean_q: 2.589605, mean_eps: 0.625182
 1875089/6000000: episode: 2638, duration: 14.385s, episode steps: 660, steps per second:  46, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.012561, mae: 2.126653, mean_q: 2.559048, mean_eps: 0.625048
 1876308/6000000: episode: 2639, duration: 26.254s, episode steps: 1219, steps per second:  46, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.071 [0.000, 5.000],  loss: 0.012327, mae: 2.141588, mean_q: 2.578688, mean_eps: 0.624860
 1876862/6000000: episode: 2640, duration: 12.513s, episode steps: 554, steps per second:  44, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.269 [0.000, 5.000],  loss: 0.013011, mae: 2.133762, mean_q: 2.568208, mean_eps: 0.624683
 1877868/6000000: episode: 2641, duration: 22.794s, episode steps: 1006, steps per second:  44, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.013060, mae: 2.149155, mean_q: 2.586533, mean_eps: 0.624527
 1878865/6000000: episode: 2642, duration: 23.002s, episode steps: 997, steps per second:  43, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.681 [0.000, 5.000],  loss: 0.012842, mae: 2.154016, mean_q: 2.591610, mean_eps: 0.624327
 1879693/6000000: episode: 2643, duration: 18.059s, episode steps: 828, steps per second:  46, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.010439, mae: 2.135477, mean_q: 2.571447, mean_eps: 0.624144
 1880375/6000000: episode: 2644, duration: 15.240s, episode steps: 682, steps per second:  45, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.012813, mae: 2.140754, mean_q: 2.576696, mean_eps: 0.623993
 1881021/6000000: episode: 2645, duration: 14.586s, episode steps: 646, steps per second:  44, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.014625, mae: 2.145390, mean_q: 2.581852, mean_eps: 0.623860
 1881673/6000000: episode: 2646, duration: 14.387s, episode steps: 652, steps per second:  45, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.011748, mae: 2.170953, mean_q: 2.611561, mean_eps: 0.623730
 1882305/6000000: episode: 2647, duration: 14.100s, episode steps: 632, steps per second:  45, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.012897, mae: 2.169336, mean_q: 2.610375, mean_eps: 0.623602
 1883032/6000000: episode: 2648, duration: 15.575s, episode steps: 727, steps per second:  47, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.012428, mae: 2.165782, mean_q: 2.605424, mean_eps: 0.623466
 1883594/6000000: episode: 2649, duration: 13.185s, episode steps: 562, steps per second:  43, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.066 [0.000, 5.000],  loss: 0.012758, mae: 2.160364, mean_q: 2.601051, mean_eps: 0.623338
 1884407/6000000: episode: 2650, duration: 18.736s, episode steps: 813, steps per second:  43, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.246 [0.000, 5.000],  loss: 0.011502, mae: 2.156920, mean_q: 2.596551, mean_eps: 0.623200
 1885221/6000000: episode: 2651, duration: 18.873s, episode steps: 814, steps per second:  43, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.011635, mae: 2.144055, mean_q: 2.579784, mean_eps: 0.623037
 1886459/6000000: episode: 2652, duration: 28.413s, episode steps: 1238, steps per second:  44, episode reward: 25.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.012835, mae: 2.176647, mean_q: 2.618994, mean_eps: 0.622832
 1886961/6000000: episode: 2653, duration: 11.107s, episode steps: 502, steps per second:  45, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.906 [0.000, 5.000],  loss: 0.012612, mae: 2.175659, mean_q: 2.619489, mean_eps: 0.622658
 1888806/6000000: episode: 2654, duration: 38.714s, episode steps: 1845, steps per second:  48, episode reward: 39.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.012782, mae: 2.183649, mean_q: 2.628103, mean_eps: 0.622423
 1889306/6000000: episode: 2655, duration: 11.316s, episode steps: 500, steps per second:  44, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.228 [0.000, 5.000],  loss: 0.012217, mae: 2.192871, mean_q: 2.639525, mean_eps: 0.622189
 1890021/6000000: episode: 2656, duration: 15.170s, episode steps: 715, steps per second:  47, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.062 [0.000, 5.000],  loss: 0.010783, mae: 2.172833, mean_q: 2.614630, mean_eps: 0.622067
 1890822/6000000: episode: 2657, duration: 16.458s, episode steps: 801, steps per second:  49, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.011937, mae: 2.185837, mean_q: 2.628849, mean_eps: 0.621916
 1891644/6000000: episode: 2658, duration: 16.980s, episode steps: 822, steps per second:  48, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.012719, mae: 2.187469, mean_q: 2.631579, mean_eps: 0.621754
 1892688/6000000: episode: 2659, duration: 22.608s, episode steps: 1044, steps per second:  46, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.061 [0.000, 5.000],  loss: 0.012550, mae: 2.188841, mean_q: 2.634016, mean_eps: 0.621567
 1893138/6000000: episode: 2660, duration: 10.206s, episode steps: 450, steps per second:  44, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.012009, mae: 2.184568, mean_q: 2.628095, mean_eps: 0.621418
 1894067/6000000: episode: 2661, duration: 20.631s, episode steps: 929, steps per second:  45, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.012534, mae: 2.192168, mean_q: 2.637995, mean_eps: 0.621280
 1894579/6000000: episode: 2662, duration: 12.192s, episode steps: 512, steps per second:  42, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.011498, mae: 2.191109, mean_q: 2.636919, mean_eps: 0.621136
 1895830/6000000: episode: 2663, duration: 28.799s, episode steps: 1251, steps per second:  43, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 0.012727, mae: 2.202563, mean_q: 2.650723, mean_eps: 0.620959
 1896971/6000000: episode: 2664, duration: 24.428s, episode steps: 1141, steps per second:  47, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.216 [0.000, 5.000],  loss: 0.014098, mae: 2.216734, mean_q: 2.668121, mean_eps: 0.620720
 1897497/6000000: episode: 2665, duration: 11.841s, episode steps: 526, steps per second:  44, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.759 [0.000, 5.000],  loss: 0.012813, mae: 2.215720, mean_q: 2.665517, mean_eps: 0.620553
 1898213/6000000: episode: 2666, duration: 15.299s, episode steps: 716, steps per second:  47, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.179 [0.000, 5.000],  loss: 0.013204, mae: 2.215969, mean_q: 2.666296, mean_eps: 0.620429
 1898888/6000000: episode: 2667, duration: 14.527s, episode steps: 675, steps per second:  46, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.014919, mae: 2.193375, mean_q: 2.638292, mean_eps: 0.620290
 1899449/6000000: episode: 2668, duration: 12.921s, episode steps: 561, steps per second:  43, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 0.012314, mae: 2.229620, mean_q: 2.682601, mean_eps: 0.620166
 1900187/6000000: episode: 2669, duration: 17.714s, episode steps: 738, steps per second:  42, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.011812, mae: 2.206403, mean_q: 2.654829, mean_eps: 0.620036
 1900969/6000000: episode: 2670, duration: 17.469s, episode steps: 782, steps per second:  45, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.012484, mae: 2.236513, mean_q: 2.692083, mean_eps: 0.619884
 1901673/6000000: episode: 2671, duration: 15.708s, episode steps: 704, steps per second:  45, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.216 [0.000, 5.000],  loss: 0.013028, mae: 2.220063, mean_q: 2.671905, mean_eps: 0.619736
 1902744/6000000: episode: 2672, duration: 23.784s, episode steps: 1071, steps per second:  45, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.265 [0.000, 5.000],  loss: 0.012445, mae: 2.218451, mean_q: 2.667874, mean_eps: 0.619558
 1903810/6000000: episode: 2673, duration: 22.764s, episode steps: 1066, steps per second:  47, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.288 [0.000, 5.000],  loss: 0.014268, mae: 2.217187, mean_q: 2.667981, mean_eps: 0.619345
 1904484/6000000: episode: 2674, duration: 13.909s, episode steps: 674, steps per second:  48, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.011662, mae: 2.231093, mean_q: 2.683511, mean_eps: 0.619171
 1905147/6000000: episode: 2675, duration: 14.857s, episode steps: 663, steps per second:  45, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.189 [0.000, 5.000],  loss: 0.013728, mae: 2.192061, mean_q: 2.636774, mean_eps: 0.619037
 1905464/6000000: episode: 2676, duration: 7.758s, episode steps: 317, steps per second:  41, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.019 [0.000, 5.000],  loss: 0.010619, mae: 2.229769, mean_q: 2.683089, mean_eps: 0.618939
 1906078/6000000: episode: 2677, duration: 13.559s, episode steps: 614, steps per second:  45, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.013957, mae: 2.222940, mean_q: 2.675446, mean_eps: 0.618846
 1906745/6000000: episode: 2678, duration: 13.985s, episode steps: 667, steps per second:  48, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.126 [0.000, 5.000],  loss: 0.014089, mae: 2.208121, mean_q: 2.657576, mean_eps: 0.618718
 1907370/6000000: episode: 2679, duration: 13.847s, episode steps: 625, steps per second:  45, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.011776, mae: 2.207122, mean_q: 2.655269, mean_eps: 0.618588
 1908487/6000000: episode: 2680, duration: 22.718s, episode steps: 1117, steps per second:  49, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.014151, mae: 2.214986, mean_q: 2.664724, mean_eps: 0.618414
 1909333/6000000: episode: 2681, duration: 18.278s, episode steps: 846, steps per second:  46, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.091 [0.000, 5.000],  loss: 0.013115, mae: 2.219952, mean_q: 2.670435, mean_eps: 0.618218
 1909842/6000000: episode: 2682, duration: 10.712s, episode steps: 509, steps per second:  48, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.250 [0.000, 5.000],  loss: 0.013394, mae: 2.197693, mean_q: 2.645095, mean_eps: 0.618082
 1910239/6000000: episode: 2683, duration: 8.576s, episode steps: 397, steps per second:  46, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.012150, mae: 2.206385, mean_q: 2.657038, mean_eps: 0.617992
 1911323/6000000: episode: 2684, duration: 29.189s, episode steps: 1084, steps per second:  37, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.188 [0.000, 5.000],  loss: 0.012569, mae: 2.182687, mean_q: 2.627219, mean_eps: 0.617844
 1911957/6000000: episode: 2685, duration: 14.480s, episode steps: 634, steps per second:  44, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.011721, mae: 2.176322, mean_q: 2.619556, mean_eps: 0.617672
 1912460/6000000: episode: 2686, duration: 10.799s, episode steps: 503, steps per second:  47, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.014139, mae: 2.192853, mean_q: 2.637465, mean_eps: 0.617558
 1913177/6000000: episode: 2687, duration: 15.005s, episode steps: 717, steps per second:  48, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: 0.012951, mae: 2.192187, mean_q: 2.639039, mean_eps: 0.617436
 1914074/6000000: episode: 2688, duration: 19.662s, episode steps: 897, steps per second:  46, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.263 [0.000, 5.000],  loss: 0.012159, mae: 2.197588, mean_q: 2.645393, mean_eps: 0.617275
 1914731/6000000: episode: 2689, duration: 14.338s, episode steps: 657, steps per second:  46, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.189 [0.000, 5.000],  loss: 0.012471, mae: 2.186694, mean_q: 2.631164, mean_eps: 0.617120
 1915377/6000000: episode: 2690, duration: 13.489s, episode steps: 646, steps per second:  48, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.013489, mae: 2.196463, mean_q: 2.644136, mean_eps: 0.616989
 1916034/6000000: episode: 2691, duration: 14.521s, episode steps: 657, steps per second:  45, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.011908, mae: 2.215268, mean_q: 2.666316, mean_eps: 0.616859
 1916435/6000000: episode: 2692, duration: 9.174s, episode steps: 401, steps per second:  44, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.237 [0.000, 5.000],  loss: 0.012493, mae: 2.205299, mean_q: 2.653185, mean_eps: 0.616753
 1917032/6000000: episode: 2693, duration: 12.678s, episode steps: 597, steps per second:  47, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.011526, mae: 2.223448, mean_q: 2.675731, mean_eps: 0.616654
 1917684/6000000: episode: 2694, duration: 14.242s, episode steps: 652, steps per second:  46, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.013895, mae: 2.196119, mean_q: 2.642991, mean_eps: 0.616529
 1918320/6000000: episode: 2695, duration: 13.664s, episode steps: 636, steps per second:  47, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.010825, mae: 2.192538, mean_q: 2.640823, mean_eps: 0.616400
 1919085/6000000: episode: 2696, duration: 16.505s, episode steps: 765, steps per second:  46, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.012207, mae: 2.201231, mean_q: 2.648357, mean_eps: 0.616260
 1919818/6000000: episode: 2697, duration: 15.533s, episode steps: 733, steps per second:  47, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.014749, mae: 2.204996, mean_q: 2.652636, mean_eps: 0.616110
 1920526/6000000: episode: 2698, duration: 14.445s, episode steps: 708, steps per second:  49, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.658 [0.000, 5.000],  loss: 0.012717, mae: 2.217823, mean_q: 2.668049, mean_eps: 0.615966
 1921041/6000000: episode: 2699, duration: 10.469s, episode steps: 515, steps per second:  49, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.011673, mae: 2.221664, mean_q: 2.674692, mean_eps: 0.615843
 1921600/6000000: episode: 2700, duration: 11.394s, episode steps: 559, steps per second:  49, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.011634, mae: 2.224892, mean_q: 2.677055, mean_eps: 0.615736
 1922287/6000000: episode: 2701, duration: 14.477s, episode steps: 687, steps per second:  47, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.010897, mae: 2.215443, mean_q: 2.666187, mean_eps: 0.615612
 1923021/6000000: episode: 2702, duration: 15.787s, episode steps: 734, steps per second:  46, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.706 [0.000, 5.000],  loss: 0.012290, mae: 2.218839, mean_q: 2.670573, mean_eps: 0.615469
 1923848/6000000: episode: 2703, duration: 18.089s, episode steps: 827, steps per second:  46, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.013502, mae: 2.234171, mean_q: 2.688773, mean_eps: 0.615313
 1924843/6000000: episode: 2704, duration: 21.300s, episode steps: 995, steps per second:  47, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.224 [0.000, 5.000],  loss: 0.014010, mae: 2.234929, mean_q: 2.690036, mean_eps: 0.615131
 1925820/6000000: episode: 2705, duration: 21.860s, episode steps: 977, steps per second:  45, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.656 [0.000, 5.000],  loss: 0.012284, mae: 2.212943, mean_q: 2.662476, mean_eps: 0.614934
 1927115/6000000: episode: 2706, duration: 29.678s, episode steps: 1295, steps per second:  44, episode reward: 27.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.012307, mae: 2.187706, mean_q: 2.632780, mean_eps: 0.614707
 1927496/6000000: episode: 2707, duration: 9.030s, episode steps: 381, steps per second:  42, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.015048, mae: 2.214699, mean_q: 2.666534, mean_eps: 0.614539
 1928454/6000000: episode: 2708, duration: 21.507s, episode steps: 958, steps per second:  45, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.334 [0.000, 5.000],  loss: 0.013812, mae: 2.204652, mean_q: 2.653092, mean_eps: 0.614405
 1928861/6000000: episode: 2709, duration: 8.911s, episode steps: 407, steps per second:  46, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.011523, mae: 2.194695, mean_q: 2.641311, mean_eps: 0.614268
 1929306/6000000: episode: 2710, duration: 9.790s, episode steps: 445, steps per second:  45, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.267 [0.000, 5.000],  loss: 0.010922, mae: 2.190791, mean_q: 2.635104, mean_eps: 0.614183
 1929692/6000000: episode: 2711, duration: 8.812s, episode steps: 386, steps per second:  44, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.013274, mae: 2.211666, mean_q: 2.661120, mean_eps: 0.614100
 1930468/6000000: episode: 2712, duration: 17.312s, episode steps: 776, steps per second:  45, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.012916, mae: 2.194250, mean_q: 2.639156, mean_eps: 0.613984
 1931146/6000000: episode: 2713, duration: 15.620s, episode steps: 678, steps per second:  43, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.012557, mae: 2.202414, mean_q: 2.650596, mean_eps: 0.613839
 1931827/6000000: episode: 2714, duration: 13.828s, episode steps: 681, steps per second:  49, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.012117, mae: 2.194052, mean_q: 2.639609, mean_eps: 0.613703
 1932417/6000000: episode: 2715, duration: 13.124s, episode steps: 590, steps per second:  45, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.190 [0.000, 5.000],  loss: 0.012560, mae: 2.200030, mean_q: 2.649099, mean_eps: 0.613576
 1933483/6000000: episode: 2716, duration: 24.461s, episode steps: 1066, steps per second:  44, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.275 [0.000, 5.000],  loss: 0.012850, mae: 2.196364, mean_q: 2.643126, mean_eps: 0.613410
 1934081/6000000: episode: 2717, duration: 14.635s, episode steps: 598, steps per second:  41, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.011733, mae: 2.185134, mean_q: 2.630198, mean_eps: 0.613244
 1935392/6000000: episode: 2718, duration: 36.909s, episode steps: 1311, steps per second:  36, episode reward: 25.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.145 [0.000, 5.000],  loss: 0.012612, mae: 2.195681, mean_q: 2.643404, mean_eps: 0.613053
 1936338/6000000: episode: 2719, duration: 23.621s, episode steps: 946, steps per second:  40, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.012519, mae: 2.182607, mean_q: 2.627055, mean_eps: 0.612827
 1937162/6000000: episode: 2720, duration: 21.734s, episode steps: 824, steps per second:  38, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.319 [0.000, 5.000],  loss: 0.013209, mae: 2.195906, mean_q: 2.641345, mean_eps: 0.612650
 1938119/6000000: episode: 2721, duration: 25.378s, episode steps: 957, steps per second:  38, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.014260, mae: 2.180222, mean_q: 2.622265, mean_eps: 0.612472
 1938916/6000000: episode: 2722, duration: 17.224s, episode steps: 797, steps per second:  46, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.014315, mae: 2.195004, mean_q: 2.640833, mean_eps: 0.612297
 1939868/6000000: episode: 2723, duration: 20.143s, episode steps: 952, steps per second:  47, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.237 [0.000, 5.000],  loss: 0.012791, mae: 2.200191, mean_q: 2.647562, mean_eps: 0.612122
 1940413/6000000: episode: 2724, duration: 12.281s, episode steps: 545, steps per second:  44, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.013904, mae: 2.217772, mean_q: 2.667514, mean_eps: 0.611972
 1941342/6000000: episode: 2725, duration: 20.706s, episode steps: 929, steps per second:  45, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.013565, mae: 2.223488, mean_q: 2.673484, mean_eps: 0.611824
 1941983/6000000: episode: 2726, duration: 13.992s, episode steps: 641, steps per second:  46, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.153 [0.000, 5.000],  loss: 0.012189, mae: 2.212742, mean_q: 2.662371, mean_eps: 0.611668
 1942751/6000000: episode: 2727, duration: 17.683s, episode steps: 768, steps per second:  43, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.143 [0.000, 5.000],  loss: 0.011899, mae: 2.212637, mean_q: 2.663741, mean_eps: 0.611527
 1943414/6000000: episode: 2728, duration: 15.242s, episode steps: 663, steps per second:  43, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.235 [0.000, 5.000],  loss: 0.011874, mae: 2.206246, mean_q: 2.655728, mean_eps: 0.611384
 1943925/6000000: episode: 2729, duration: 11.443s, episode steps: 511, steps per second:  45, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.706 [0.000, 5.000],  loss: 0.012151, mae: 2.205477, mean_q: 2.654075, mean_eps: 0.611266
 1944562/6000000: episode: 2730, duration: 13.428s, episode steps: 637, steps per second:  47, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.013519, mae: 2.200901, mean_q: 2.647458, mean_eps: 0.611151
 1945547/6000000: episode: 2731, duration: 21.224s, episode steps: 985, steps per second:  46, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.012069, mae: 2.237025, mean_q: 2.692253, mean_eps: 0.610989
 1946118/6000000: episode: 2732, duration: 12.651s, episode steps: 571, steps per second:  45, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.011741, mae: 2.230104, mean_q: 2.683389, mean_eps: 0.610834
 1946961/6000000: episode: 2733, duration: 18.437s, episode steps: 843, steps per second:  46, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.139 [0.000, 5.000],  loss: 0.013472, mae: 2.234295, mean_q: 2.689077, mean_eps: 0.610692
 1947962/6000000: episode: 2734, duration: 22.177s, episode steps: 1001, steps per second:  45, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.012092, mae: 2.217016, mean_q: 2.669037, mean_eps: 0.610508
 1948535/6000000: episode: 2735, duration: 13.899s, episode steps: 573, steps per second:  41, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.012214, mae: 2.246628, mean_q: 2.703120, mean_eps: 0.610350
 1949178/6000000: episode: 2736, duration: 13.942s, episode steps: 643, steps per second:  46, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.013733, mae: 2.246123, mean_q: 2.701620, mean_eps: 0.610229
 1950083/6000000: episode: 2737, duration: 19.507s, episode steps: 905, steps per second:  46, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.014678, mae: 2.238234, mean_q: 2.693505, mean_eps: 0.610074
 1950957/6000000: episode: 2738, duration: 18.756s, episode steps: 874, steps per second:  47, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.012607, mae: 2.261768, mean_q: 2.721709, mean_eps: 0.609896
 1951751/6000000: episode: 2739, duration: 16.831s, episode steps: 794, steps per second:  47, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.012546, mae: 2.254601, mean_q: 2.714167, mean_eps: 0.609729
 1952930/6000000: episode: 2740, duration: 23.766s, episode steps: 1179, steps per second:  50, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.128 [0.000, 5.000],  loss: 0.013163, mae: 2.241089, mean_q: 2.695857, mean_eps: 0.609532
 1953933/6000000: episode: 2741, duration: 21.139s, episode steps: 1003, steps per second:  47, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.011669, mae: 2.251456, mean_q: 2.709094, mean_eps: 0.609314
 1955071/6000000: episode: 2742, duration: 23.455s, episode steps: 1138, steps per second:  49, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.012487, mae: 2.237741, mean_q: 2.693311, mean_eps: 0.609100
 1955732/6000000: episode: 2743, duration: 14.184s, episode steps: 661, steps per second:  47, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.012308, mae: 2.253901, mean_q: 2.711962, mean_eps: 0.608920
 1956116/6000000: episode: 2744, duration: 8.225s, episode steps: 384, steps per second:  47, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.260 [0.000, 5.000],  loss: 0.012540, mae: 2.254119, mean_q: 2.713222, mean_eps: 0.608816
 1956866/6000000: episode: 2745, duration: 16.442s, episode steps: 750, steps per second:  46, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.010624, mae: 2.260055, mean_q: 2.721077, mean_eps: 0.608702
 1957845/6000000: episode: 2746, duration: 21.263s, episode steps: 979, steps per second:  46, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.013387, mae: 2.241409, mean_q: 2.698417, mean_eps: 0.608529
 1958222/6000000: episode: 2747, duration: 8.314s, episode steps: 377, steps per second:  45, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.907 [0.000, 5.000],  loss: 0.015646, mae: 2.254269, mean_q: 2.714250, mean_eps: 0.608393
 1959056/6000000: episode: 2748, duration: 42.529s, episode steps: 834, steps per second:  20, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: 0.012714, mae: 2.240212, mean_q: 2.697029, mean_eps: 0.608272
 1959564/6000000: episode: 2749, duration: 30.813s, episode steps: 508, steps per second:  16, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.010954, mae: 2.248175, mean_q: 2.708349, mean_eps: 0.608138
 1960284/6000000: episode: 2750, duration: 40.662s, episode steps: 720, steps per second:  18, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.300 [0.000, 5.000],  loss: 0.013317, mae: 2.268711, mean_q: 2.729606, mean_eps: 0.608016
 1961578/6000000: episode: 2751, duration: 32.399s, episode steps: 1294, steps per second:  40, episode reward: 27.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.013887, mae: 2.299223, mean_q: 2.765083, mean_eps: 0.607814
 1962718/6000000: episode: 2752, duration: 25.173s, episode steps: 1140, steps per second:  45, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.020 [0.000, 5.000],  loss: 0.013683, mae: 2.294710, mean_q: 2.760424, mean_eps: 0.607570
 1963547/6000000: episode: 2753, duration: 18.142s, episode steps: 829, steps per second:  46, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.014615, mae: 2.293498, mean_q: 2.758466, mean_eps: 0.607374
 1964163/6000000: episode: 2754, duration: 13.746s, episode steps: 616, steps per second:  45, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.258 [0.000, 5.000],  loss: 0.012688, mae: 2.277510, mean_q: 2.739590, mean_eps: 0.607229
 1964572/6000000: episode: 2755, duration: 8.894s, episode steps: 409, steps per second:  46, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.013015, mae: 2.285249, mean_q: 2.750516, mean_eps: 0.607127
 1965147/6000000: episode: 2756, duration: 12.058s, episode steps: 575, steps per second:  48, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.012951, mae: 2.280617, mean_q: 2.744183, mean_eps: 0.607028
 1965763/6000000: episode: 2757, duration: 12.340s, episode steps: 616, steps per second:  50, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.141 [0.000, 5.000],  loss: 0.013835, mae: 2.277514, mean_q: 2.739768, mean_eps: 0.606909
 1966733/6000000: episode: 2758, duration: 19.771s, episode steps: 970, steps per second:  49, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.013697, mae: 2.272073, mean_q: 2.734349, mean_eps: 0.606750
 1967658/6000000: episode: 2759, duration: 19.520s, episode steps: 925, steps per second:  47, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.720 [0.000, 5.000],  loss: 0.014311, mae: 2.286515, mean_q: 2.749948, mean_eps: 0.606561
 1968161/6000000: episode: 2760, duration: 10.626s, episode steps: 503, steps per second:  47, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.011803, mae: 2.278143, mean_q: 2.742255, mean_eps: 0.606418
 1968923/6000000: episode: 2761, duration: 16.038s, episode steps: 762, steps per second:  48, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.012891, mae: 2.269311, mean_q: 2.729787, mean_eps: 0.606292
 1969922/6000000: episode: 2762, duration: 21.202s, episode steps: 999, steps per second:  47, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.012660, mae: 2.282167, mean_q: 2.746760, mean_eps: 0.606116
 1970927/6000000: episode: 2763, duration: 21.751s, episode steps: 1005, steps per second:  46, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.014049, mae: 2.279402, mean_q: 2.741712, mean_eps: 0.605915
 1971745/6000000: episode: 2764, duration: 17.858s, episode steps: 818, steps per second:  46, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.689 [0.000, 5.000],  loss: 0.014217, mae: 2.287271, mean_q: 2.752507, mean_eps: 0.605733
 1972335/6000000: episode: 2765, duration: 13.523s, episode steps: 590, steps per second:  44, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.012876, mae: 2.264162, mean_q: 2.722821, mean_eps: 0.605592
 1972865/6000000: episode: 2766, duration: 12.036s, episode steps: 530, steps per second:  44, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.014257, mae: 2.280292, mean_q: 2.742920, mean_eps: 0.605480
 1973808/6000000: episode: 2767, duration: 21.663s, episode steps: 943, steps per second:  44, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.264 [0.000, 5.000],  loss: 0.013387, mae: 2.281911, mean_q: 2.746369, mean_eps: 0.605333
 1974491/6000000: episode: 2768, duration: 16.636s, episode steps: 683, steps per second:  41, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.249 [0.000, 5.000],  loss: 0.014072, mae: 2.293965, mean_q: 2.761130, mean_eps: 0.605170
 1975147/6000000: episode: 2769, duration: 14.745s, episode steps: 656, steps per second:  44, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 0.012631, mae: 2.282057, mean_q: 2.745242, mean_eps: 0.605036
 1976002/6000000: episode: 2770, duration: 18.478s, episode steps: 855, steps per second:  46, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.082 [0.000, 5.000],  loss: 0.012015, mae: 2.286106, mean_q: 2.751360, mean_eps: 0.604885
 1976669/6000000: episode: 2771, duration: 14.300s, episode steps: 667, steps per second:  47, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.011226, mae: 2.251045, mean_q: 2.710645, mean_eps: 0.604733
 1977579/6000000: episode: 2772, duration: 19.698s, episode steps: 910, steps per second:  46, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.012387, mae: 2.275329, mean_q: 2.737211, mean_eps: 0.604575
 1978583/6000000: episode: 2773, duration: 22.737s, episode steps: 1004, steps per second:  44, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.266 [0.000, 5.000],  loss: 0.013080, mae: 2.268082, mean_q: 2.728158, mean_eps: 0.604384
 1978965/6000000: episode: 2774, duration: 8.412s, episode steps: 382, steps per second:  45, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.186 [0.000, 5.000],  loss: 0.011892, mae: 2.267433, mean_q: 2.728864, mean_eps: 0.604245
 1980257/6000000: episode: 2775, duration: 28.263s, episode steps: 1292, steps per second:  46, episode reward: 26.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.262 [0.000, 5.000],  loss: 0.012704, mae: 2.274434, mean_q: 2.736931, mean_eps: 0.604078
 1980977/6000000: episode: 2776, duration: 15.757s, episode steps: 720, steps per second:  46, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.013614, mae: 2.276615, mean_q: 2.738102, mean_eps: 0.603876
 1981831/6000000: episode: 2777, duration: 18.000s, episode steps: 854, steps per second:  47, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.014383, mae: 2.292702, mean_q: 2.758238, mean_eps: 0.603719
 1982453/6000000: episode: 2778, duration: 12.828s, episode steps: 622, steps per second:  48, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.013599, mae: 2.268639, mean_q: 2.728296, mean_eps: 0.603572
 1983360/6000000: episode: 2779, duration: 19.708s, episode steps: 907, steps per second:  46, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.236 [0.000, 5.000],  loss: 0.014355, mae: 2.287695, mean_q: 2.752487, mean_eps: 0.603419
 1983943/6000000: episode: 2780, duration: 12.372s, episode steps: 583, steps per second:  47, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.208 [0.000, 5.000],  loss: 0.012528, mae: 2.251943, mean_q: 2.714258, mean_eps: 0.603270
 1984639/6000000: episode: 2781, duration: 14.823s, episode steps: 696, steps per second:  47, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.014496, mae: 2.260742, mean_q: 2.719475, mean_eps: 0.603142
 1985333/6000000: episode: 2782, duration: 14.609s, episode steps: 694, steps per second:  48, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.072 [0.000, 5.000],  loss: 0.013409, mae: 2.271898, mean_q: 2.733223, mean_eps: 0.603003
 1985999/6000000: episode: 2783, duration: 14.186s, episode steps: 666, steps per second:  47, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.012354, mae: 2.274756, mean_q: 2.735992, mean_eps: 0.602867
 1986587/6000000: episode: 2784, duration: 12.424s, episode steps: 588, steps per second:  47, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.014435, mae: 2.300859, mean_q: 2.767968, mean_eps: 0.602742
 1987285/6000000: episode: 2785, duration: 15.197s, episode steps: 698, steps per second:  46, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.133 [0.000, 5.000],  loss: 0.011639, mae: 2.249555, mean_q: 2.706345, mean_eps: 0.602613
 1988172/6000000: episode: 2786, duration: 19.465s, episode steps: 887, steps per second:  46, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.032 [0.000, 5.000],  loss: 0.012932, mae: 2.289757, mean_q: 2.756304, mean_eps: 0.602454
 1988988/6000000: episode: 2787, duration: 19.945s, episode steps: 816, steps per second:  41, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.012241, mae: 2.266470, mean_q: 2.728665, mean_eps: 0.602284
 1989371/6000000: episode: 2788, duration: 9.386s, episode steps: 383, steps per second:  41, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.015168, mae: 2.276336, mean_q: 2.740283, mean_eps: 0.602164
 1990115/6000000: episode: 2789, duration: 16.968s, episode steps: 744, steps per second:  44, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.237 [0.000, 5.000],  loss: 0.014400, mae: 2.286158, mean_q: 2.751022, mean_eps: 0.602052
 1990822/6000000: episode: 2790, duration: 16.595s, episode steps: 707, steps per second:  43, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.269 [0.000, 5.000],  loss: 0.012288, mae: 2.268880, mean_q: 2.731338, mean_eps: 0.601906
 1991738/6000000: episode: 2791, duration: 20.196s, episode steps: 916, steps per second:  45, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.012004, mae: 2.274599, mean_q: 2.737267, mean_eps: 0.601744
 1993146/6000000: episode: 2792, duration: 30.429s, episode steps: 1408, steps per second:  46, episode reward: 34.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.013143, mae: 2.272745, mean_q: 2.735079, mean_eps: 0.601512
 1993916/6000000: episode: 2793, duration: 16.795s, episode steps: 770, steps per second:  46, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.012661, mae: 2.255569, mean_q: 2.714447, mean_eps: 0.601294
 1994726/6000000: episode: 2794, duration: 18.484s, episode steps: 810, steps per second:  44, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.012646, mae: 2.259298, mean_q: 2.718561, mean_eps: 0.601136
 1995419/6000000: episode: 2795, duration: 15.126s, episode steps: 693, steps per second:  46, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.015301, mae: 2.261966, mean_q: 2.722179, mean_eps: 0.600986
 1996056/6000000: episode: 2796, duration: 14.813s, episode steps: 637, steps per second:  43, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.012936, mae: 2.245007, mean_q: 2.701337, mean_eps: 0.600853
 1996916/6000000: episode: 2797, duration: 18.702s, episode steps: 860, steps per second:  46, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.015 [0.000, 5.000],  loss: 0.012467, mae: 2.236056, mean_q: 2.690842, mean_eps: 0.600703
 1997891/6000000: episode: 2798, duration: 20.753s, episode steps: 975, steps per second:  47, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.011574, mae: 2.256721, mean_q: 2.716897, mean_eps: 0.600520
 1998406/6000000: episode: 2799, duration: 10.399s, episode steps: 515, steps per second:  50, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.015373, mae: 2.274237, mean_q: 2.736090, mean_eps: 0.600370
 1999248/6000000: episode: 2800, duration: 17.118s, episode steps: 842, steps per second:  49, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.011484, mae: 2.252038, mean_q: 2.709230, mean_eps: 0.600235
 1999930/6000000: episode: 2801, duration: 14.064s, episode steps: 682, steps per second:  48, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.014172, mae: 2.257843, mean_q: 2.716035, mean_eps: 0.600082
 2000441/6000000: episode: 2802, duration: 10.931s, episode steps: 511, steps per second:  47, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: 0.012653, mae: 2.246553, mean_q: 2.703363, mean_eps: 0.599963
 2001052/6000000: episode: 2803, duration: 12.801s, episode steps: 611, steps per second:  48, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.332 [0.000, 5.000],  loss: 0.014246, mae: 2.237732, mean_q: 2.691264, mean_eps: 0.599851
 2001601/6000000: episode: 2804, duration: 11.697s, episode steps: 549, steps per second:  47, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.013337, mae: 2.223486, mean_q: 2.674378, mean_eps: 0.599735
 2002138/6000000: episode: 2805, duration: 11.815s, episode steps: 537, steps per second:  45, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.780 [0.000, 5.000],  loss: 0.012535, mae: 2.243098, mean_q: 2.698499, mean_eps: 0.599626
 2003092/6000000: episode: 2806, duration: 21.524s, episode steps: 954, steps per second:  44, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.012780, mae: 2.230921, mean_q: 2.687105, mean_eps: 0.599477
 2003986/6000000: episode: 2807, duration: 19.474s, episode steps: 894, steps per second:  46, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.013595, mae: 2.237676, mean_q: 2.693014, mean_eps: 0.599292
 2004493/6000000: episode: 2808, duration: 11.228s, episode steps: 507, steps per second:  45, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.013308, mae: 2.209199, mean_q: 2.657210, mean_eps: 0.599152
 2005025/6000000: episode: 2809, duration: 11.680s, episode steps: 532, steps per second:  46, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.718 [0.000, 5.000],  loss: 0.012641, mae: 2.225540, mean_q: 2.677358, mean_eps: 0.599048
 2005375/6000000: episode: 2810, duration: 7.744s, episode steps: 350, steps per second:  45, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.786 [0.000, 5.000],  loss: 0.013154, mae: 2.267018, mean_q: 2.726576, mean_eps: 0.598960
 2006376/6000000: episode: 2811, duration: 22.070s, episode steps: 1001, steps per second:  45, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.236 [0.000, 5.000],  loss: 0.013206, mae: 2.258168, mean_q: 2.718479, mean_eps: 0.598825
 2007158/6000000: episode: 2812, duration: 16.536s, episode steps: 782, steps per second:  47, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.013059, mae: 2.255926, mean_q: 2.714590, mean_eps: 0.598647
 2007834/6000000: episode: 2813, duration: 15.141s, episode steps: 676, steps per second:  45, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.013011, mae: 2.242830, mean_q: 2.698124, mean_eps: 0.598501
 2008742/6000000: episode: 2814, duration: 19.327s, episode steps: 908, steps per second:  47, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.013860, mae: 2.246160, mean_q: 2.703564, mean_eps: 0.598342
 2009971/6000000: episode: 2815, duration: 25.578s, episode steps: 1229, steps per second:  48, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.011316, mae: 2.250965, mean_q: 2.709839, mean_eps: 0.598129
 2010781/6000000: episode: 2816, duration: 17.828s, episode steps: 810, steps per second:  45, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.286 [0.000, 5.000],  loss: 0.011741, mae: 2.266254, mean_q: 2.728320, mean_eps: 0.597925
 2011322/6000000: episode: 2817, duration: 12.238s, episode steps: 541, steps per second:  44, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.013794, mae: 2.286316, mean_q: 2.751395, mean_eps: 0.597790
 2011858/6000000: episode: 2818, duration: 12.194s, episode steps: 536, steps per second:  44, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.015021, mae: 2.280765, mean_q: 2.744604, mean_eps: 0.597682
 2012198/6000000: episode: 2819, duration: 7.475s, episode steps: 340, steps per second:  45, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.011424, mae: 2.277286, mean_q: 2.742033, mean_eps: 0.597594
 2012953/6000000: episode: 2820, duration: 16.380s, episode steps: 755, steps per second:  46, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.013374, mae: 2.295338, mean_q: 2.763093, mean_eps: 0.597485
 2013905/6000000: episode: 2821, duration: 21.607s, episode steps: 952, steps per second:  44, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.013986, mae: 2.279578, mean_q: 2.743180, mean_eps: 0.597314
 2014553/6000000: episode: 2822, duration: 13.809s, episode steps: 648, steps per second:  47, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.014945, mae: 2.264247, mean_q: 2.726856, mean_eps: 0.597154
 2015025/6000000: episode: 2823, duration: 9.470s, episode steps: 472, steps per second:  50, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.011131, mae: 2.245663, mean_q: 2.703221, mean_eps: 0.597042
 2015749/6000000: episode: 2824, duration: 14.853s, episode steps: 724, steps per second:  49, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.970 [0.000, 5.000],  loss: 0.013585, mae: 2.237493, mean_q: 2.694245, mean_eps: 0.596922
 2016691/6000000: episode: 2825, duration: 19.940s, episode steps: 942, steps per second:  47, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.012940, mae: 2.241713, mean_q: 2.698322, mean_eps: 0.596756
 2017733/6000000: episode: 2826, duration: 21.187s, episode steps: 1042, steps per second:  49, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.013647, mae: 2.241719, mean_q: 2.698331, mean_eps: 0.596558
 2018671/6000000: episode: 2827, duration: 19.478s, episode steps: 938, steps per second:  48, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.013572, mae: 2.242946, mean_q: 2.699581, mean_eps: 0.596360
 2019612/6000000: episode: 2828, duration: 19.832s, episode steps: 941, steps per second:  47, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.013665, mae: 2.238717, mean_q: 2.695123, mean_eps: 0.596172
 2020144/6000000: episode: 2829, duration: 11.148s, episode steps: 532, steps per second:  48, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.013009, mae: 2.238882, mean_q: 2.695357, mean_eps: 0.596025
 2021007/6000000: episode: 2830, duration: 18.200s, episode steps: 863, steps per second:  47, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.137 [0.000, 5.000],  loss: 0.012636, mae: 2.243728, mean_q: 2.700559, mean_eps: 0.595885
 2021632/6000000: episode: 2831, duration: 13.572s, episode steps: 625, steps per second:  46, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.950 [0.000, 5.000],  loss: 0.011901, mae: 2.239268, mean_q: 2.697844, mean_eps: 0.595736
 2022451/6000000: episode: 2832, duration: 18.446s, episode steps: 819, steps per second:  44, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.014487, mae: 2.234082, mean_q: 2.688141, mean_eps: 0.595592
 2023123/6000000: episode: 2833, duration: 15.656s, episode steps: 672, steps per second:  43, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.013164, mae: 2.238739, mean_q: 2.694352, mean_eps: 0.595443
 2024152/6000000: episode: 2834, duration: 21.771s, episode steps: 1029, steps per second:  47, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.263 [0.000, 5.000],  loss: 0.012762, mae: 2.229746, mean_q: 2.682518, mean_eps: 0.595273
 2025023/6000000: episode: 2835, duration: 18.888s, episode steps: 871, steps per second:  46, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.013336, mae: 2.244348, mean_q: 2.699479, mean_eps: 0.595083
 2025704/6000000: episode: 2836, duration: 14.988s, episode steps: 681, steps per second:  45, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.013797, mae: 2.272928, mean_q: 2.735930, mean_eps: 0.594928
 2026670/6000000: episode: 2837, duration: 20.071s, episode steps: 966, steps per second:  48, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.013394, mae: 2.259333, mean_q: 2.718609, mean_eps: 0.594763
 2027584/6000000: episode: 2838, duration: 20.196s, episode steps: 914, steps per second:  45, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.014340, mae: 2.278081, mean_q: 2.740371, mean_eps: 0.594575
 2027971/6000000: episode: 2839, duration: 9.126s, episode steps: 387, steps per second:  42, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.659 [0.000, 5.000],  loss: 0.013280, mae: 2.271473, mean_q: 2.733504, mean_eps: 0.594445
 2028958/6000000: episode: 2840, duration: 22.305s, episode steps: 987, steps per second:  44, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.159 [0.000, 5.000],  loss: 0.013549, mae: 2.262844, mean_q: 2.722503, mean_eps: 0.594307
 2029665/6000000: episode: 2841, duration: 15.592s, episode steps: 707, steps per second:  45, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.012612, mae: 2.274877, mean_q: 2.737164, mean_eps: 0.594138
 2030428/6000000: episode: 2842, duration: 17.173s, episode steps: 763, steps per second:  44, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.013702, mae: 2.291282, mean_q: 2.757545, mean_eps: 0.593991
 2031091/6000000: episode: 2843, duration: 14.746s, episode steps: 663, steps per second:  45, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.014614, mae: 2.274691, mean_q: 2.736476, mean_eps: 0.593848
 2032034/6000000: episode: 2844, duration: 19.486s, episode steps: 943, steps per second:  48, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.013611, mae: 2.294941, mean_q: 2.761489, mean_eps: 0.593688
 2032668/6000000: episode: 2845, duration: 13.049s, episode steps: 634, steps per second:  49, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.312 [0.000, 5.000],  loss: 0.012874, mae: 2.283153, mean_q: 2.747289, mean_eps: 0.593530
 2033345/6000000: episode: 2846, duration: 14.905s, episode steps: 677, steps per second:  45, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.013222, mae: 2.267292, mean_q: 2.731024, mean_eps: 0.593399
 2034190/6000000: episode: 2847, duration: 18.021s, episode steps: 845, steps per second:  47, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.012465, mae: 2.275320, mean_q: 2.739188, mean_eps: 0.593246
 2035053/6000000: episode: 2848, duration: 18.391s, episode steps: 863, steps per second:  47, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.013884, mae: 2.272463, mean_q: 2.735254, mean_eps: 0.593076
 2035828/6000000: episode: 2849, duration: 15.995s, episode steps: 775, steps per second:  48, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.014602, mae: 2.238259, mean_q: 2.692743, mean_eps: 0.592912
 2036637/6000000: episode: 2850, duration: 17.536s, episode steps: 809, steps per second:  46, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.286 [0.000, 5.000],  loss: 0.013607, mae: 2.262912, mean_q: 2.725761, mean_eps: 0.592754
 2037341/6000000: episode: 2851, duration: 15.155s, episode steps: 704, steps per second:  46, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.014121, mae: 2.254636, mean_q: 2.714100, mean_eps: 0.592602
 2038467/6000000: episode: 2852, duration: 26.305s, episode steps: 1126, steps per second:  43, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.261 [0.000, 5.000],  loss: 0.013136, mae: 2.265243, mean_q: 2.727741, mean_eps: 0.592419
 2039393/6000000: episode: 2853, duration: 22.104s, episode steps: 926, steps per second:  42, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.012796, mae: 2.252541, mean_q: 2.712943, mean_eps: 0.592214
 2040072/6000000: episode: 2854, duration: 14.508s, episode steps: 679, steps per second:  47, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.011311, mae: 2.266739, mean_q: 2.729419, mean_eps: 0.592054
 2041193/6000000: episode: 2855, duration: 24.254s, episode steps: 1121, steps per second:  46, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.226 [0.000, 5.000],  loss: 0.013067, mae: 2.254906, mean_q: 2.713397, mean_eps: 0.591874
 2042269/6000000: episode: 2856, duration: 23.131s, episode steps: 1076, steps per second:  47, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.012331, mae: 2.255272, mean_q: 2.715135, mean_eps: 0.591654
 2042937/6000000: episode: 2857, duration: 14.504s, episode steps: 668, steps per second:  46, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.222 [0.000, 5.000],  loss: 0.012232, mae: 2.236922, mean_q: 2.693700, mean_eps: 0.591479
 2043608/6000000: episode: 2858, duration: 14.877s, episode steps: 671, steps per second:  45, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.332 [0.000, 5.000],  loss: 0.012299, mae: 2.254539, mean_q: 2.712943, mean_eps: 0.591346
 2043994/6000000: episode: 2859, duration: 9.090s, episode steps: 386, steps per second:  42, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.014345, mae: 2.236957, mean_q: 2.690657, mean_eps: 0.591240
 2044783/6000000: episode: 2860, duration: 17.870s, episode steps: 789, steps per second:  44, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.166 [0.000, 5.000],  loss: 0.013606, mae: 2.239316, mean_q: 2.695532, mean_eps: 0.591122
 2045504/6000000: episode: 2861, duration: 16.074s, episode steps: 721, steps per second:  45, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.013374, mae: 2.230760, mean_q: 2.684871, mean_eps: 0.590972
 2045988/6000000: episode: 2862, duration: 10.600s, episode steps: 484, steps per second:  46, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.839 [0.000, 5.000],  loss: 0.014507, mae: 2.269354, mean_q: 2.729257, mean_eps: 0.590851
 2046546/6000000: episode: 2863, duration: 12.329s, episode steps: 558, steps per second:  45, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.012843, mae: 2.260956, mean_q: 2.720749, mean_eps: 0.590747
 2047060/6000000: episode: 2864, duration: 11.749s, episode steps: 514, steps per second:  44, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.014351, mae: 2.246330, mean_q: 2.703010, mean_eps: 0.590640
 2047476/6000000: episode: 2865, duration: 9.251s, episode steps: 416, steps per second:  45, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.865 [0.000, 5.000],  loss: 0.012237, mae: 2.246654, mean_q: 2.705265, mean_eps: 0.590547
 2048456/6000000: episode: 2866, duration: 20.232s, episode steps: 980, steps per second:  48, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.011699, mae: 2.262423, mean_q: 2.723068, mean_eps: 0.590407
 2049159/6000000: episode: 2867, duration: 15.415s, episode steps: 703, steps per second:  46, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.312 [0.000, 5.000],  loss: 0.014074, mae: 2.257174, mean_q: 2.715491, mean_eps: 0.590239
 2050012/6000000: episode: 2868, duration: 19.145s, episode steps: 853, steps per second:  45, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.013296, mae: 2.240052, mean_q: 2.694446, mean_eps: 0.590083
 2051408/6000000: episode: 2869, duration: 28.986s, episode steps: 1396, steps per second:  48, episode reward: 27.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.012456, mae: 2.274809, mean_q: 2.736551, mean_eps: 0.589858
 2052011/6000000: episode: 2870, duration: 12.308s, episode steps: 603, steps per second:  49, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.186 [0.000, 5.000],  loss: 0.013077, mae: 2.284005, mean_q: 2.747503, mean_eps: 0.589658
 2052980/6000000: episode: 2871, duration: 20.724s, episode steps: 969, steps per second:  47, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.013039, mae: 2.281825, mean_q: 2.745381, mean_eps: 0.589501
 2053746/6000000: episode: 2872, duration: 17.020s, episode steps: 766, steps per second:  45, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.014413, mae: 2.276984, mean_q: 2.738144, mean_eps: 0.589328
 2054234/6000000: episode: 2873, duration: 10.830s, episode steps: 488, steps per second:  45, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.117 [0.000, 5.000],  loss: 0.013220, mae: 2.270205, mean_q: 2.730769, mean_eps: 0.589202
 2055870/6000000: episode: 2874, duration: 37.326s, episode steps: 1636, steps per second:  44, episode reward: 38.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.659 [0.000, 5.000],  loss: 0.012820, mae: 2.290657, mean_q: 2.755963, mean_eps: 0.588990
 2056698/6000000: episode: 2875, duration: 18.192s, episode steps: 828, steps per second:  46, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.013490, mae: 2.316579, mean_q: 2.786608, mean_eps: 0.588743
 2057661/6000000: episode: 2876, duration: 21.260s, episode steps: 963, steps per second:  45, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.013509, mae: 2.326610, mean_q: 2.798033, mean_eps: 0.588564
 2058037/6000000: episode: 2877, duration: 8.453s, episode steps: 376, steps per second:  44, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.774 [0.000, 5.000],  loss: 0.014021, mae: 2.298173, mean_q: 2.764550, mean_eps: 0.588430
 2058674/6000000: episode: 2878, duration: 13.671s, episode steps: 637, steps per second:  47, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.989 [0.000, 5.000],  loss: 0.013043, mae: 2.303231, mean_q: 2.771559, mean_eps: 0.588329
 2059675/6000000: episode: 2879, duration: 21.451s, episode steps: 1001, steps per second:  47, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.326 [0.000, 5.000],  loss: 0.013035, mae: 2.296763, mean_q: 2.762997, mean_eps: 0.588165
 2060892/6000000: episode: 2880, duration: 27.403s, episode steps: 1217, steps per second:  44, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.012589, mae: 2.292101, mean_q: 2.758945, mean_eps: 0.587944
 2061265/6000000: episode: 2881, duration: 8.505s, episode steps: 373, steps per second:  44, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.737 [0.000, 5.000],  loss: 0.013768, mae: 2.273142, mean_q: 2.735195, mean_eps: 0.587784
 2061793/6000000: episode: 2882, duration: 11.508s, episode steps: 528, steps per second:  46, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.097 [0.000, 5.000],  loss: 0.011337, mae: 2.266592, mean_q: 2.728784, mean_eps: 0.587694
 2062494/6000000: episode: 2883, duration: 14.927s, episode steps: 701, steps per second:  47, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.015045, mae: 2.262114, mean_q: 2.721965, mean_eps: 0.587571
 2063305/6000000: episode: 2884, duration: 17.831s, episode steps: 811, steps per second:  45, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.086 [0.000, 5.000],  loss: 0.012844, mae: 2.269827, mean_q: 2.729321, mean_eps: 0.587420
 2064186/6000000: episode: 2885, duration: 18.904s, episode steps: 881, steps per second:  47, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.243 [0.000, 5.000],  loss: 0.011695, mae: 2.261871, mean_q: 2.722044, mean_eps: 0.587251
 2064965/6000000: episode: 2886, duration: 16.075s, episode steps: 779, steps per second:  48, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.281 [0.000, 5.000],  loss: 0.012936, mae: 2.269296, mean_q: 2.730696, mean_eps: 0.587085
 2065646/6000000: episode: 2887, duration: 14.192s, episode steps: 681, steps per second:  48, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.012934, mae: 2.293403, mean_q: 2.758355, mean_eps: 0.586939
 2066037/6000000: episode: 2888, duration: 8.566s, episode steps: 391, steps per second:  46, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.012204, mae: 2.289976, mean_q: 2.758234, mean_eps: 0.586832
 2066888/6000000: episode: 2889, duration: 18.025s, episode steps: 851, steps per second:  47, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.014225, mae: 2.270397, mean_q: 2.732094, mean_eps: 0.586708
 2067306/6000000: episode: 2890, duration: 8.630s, episode steps: 418, steps per second:  48, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.844 [0.000, 5.000],  loss: 0.013734, mae: 2.265292, mean_q: 2.724048, mean_eps: 0.586581
 2068454/6000000: episode: 2891, duration: 24.212s, episode steps: 1148, steps per second:  47, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.014071, mae: 2.277621, mean_q: 2.739334, mean_eps: 0.586424
 2069371/6000000: episode: 2892, duration: 19.790s, episode steps: 917, steps per second:  46, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.245 [0.000, 5.000],  loss: 0.014370, mae: 2.268168, mean_q: 2.728842, mean_eps: 0.586218
 2070338/6000000: episode: 2893, duration: 21.394s, episode steps: 967, steps per second:  45, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.013028, mae: 2.272110, mean_q: 2.733990, mean_eps: 0.586029
 2070754/6000000: episode: 2894, duration: 9.348s, episode steps: 416, steps per second:  45, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.212 [0.000, 5.000],  loss: 0.013585, mae: 2.300062, mean_q: 2.766828, mean_eps: 0.585891
 2071395/6000000: episode: 2895, duration: 14.382s, episode steps: 641, steps per second:  45, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.014262, mae: 2.292559, mean_q: 2.759214, mean_eps: 0.585785
 2071891/6000000: episode: 2896, duration: 11.804s, episode steps: 496, steps per second:  42, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.012007, mae: 2.294151, mean_q: 2.760493, mean_eps: 0.585672
 2072602/6000000: episode: 2897, duration: 16.351s, episode steps: 711, steps per second:  43, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.013911, mae: 2.269605, mean_q: 2.730556, mean_eps: 0.585551
 2073378/6000000: episode: 2898, duration: 16.534s, episode steps: 776, steps per second:  47, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.093 [0.000, 5.000],  loss: 0.011778, mae: 2.289875, mean_q: 2.753708, mean_eps: 0.585402
 2073884/6000000: episode: 2899, duration: 10.750s, episode steps: 506, steps per second:  47, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.011958, mae: 2.276338, mean_q: 2.736786, mean_eps: 0.585274
 2074686/6000000: episode: 2900, duration: 17.821s, episode steps: 802, steps per second:  45, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.269 [0.000, 5.000],  loss: 0.013778, mae: 2.271698, mean_q: 2.731444, mean_eps: 0.585143
 2075229/6000000: episode: 2901, duration: 12.072s, episode steps: 543, steps per second:  45, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.122 [0.000, 5.000],  loss: 0.013941, mae: 2.273723, mean_q: 2.734529, mean_eps: 0.585008
 2076321/6000000: episode: 2902, duration: 23.266s, episode steps: 1092, steps per second:  47, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.151 [0.000, 5.000],  loss: 0.013670, mae: 2.281330, mean_q: 2.746586, mean_eps: 0.584845
 2077084/6000000: episode: 2903, duration: 17.649s, episode steps: 763, steps per second:  43, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.208 [0.000, 5.000],  loss: 0.014031, mae: 2.282520, mean_q: 2.746606, mean_eps: 0.584660
 2077837/6000000: episode: 2904, duration: 16.894s, episode steps: 753, steps per second:  45, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.208 [0.000, 5.000],  loss: 0.014287, mae: 2.286471, mean_q: 2.751009, mean_eps: 0.584508
 2078525/6000000: episode: 2905, duration: 14.782s, episode steps: 688, steps per second:  47, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.012936, mae: 2.287119, mean_q: 2.750399, mean_eps: 0.584364
 2078954/6000000: episode: 2906, duration: 9.128s, episode steps: 429, steps per second:  47, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.655 [0.000, 5.000],  loss: 0.012856, mae: 2.286691, mean_q: 2.750516, mean_eps: 0.584252
 2079929/6000000: episode: 2907, duration: 22.023s, episode steps: 975, steps per second:  44, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.014290, mae: 2.288232, mean_q: 2.752965, mean_eps: 0.584112
 2080685/6000000: episode: 2908, duration: 16.254s, episode steps: 756, steps per second:  47, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.988 [0.000, 5.000],  loss: 0.013836, mae: 2.286468, mean_q: 2.750476, mean_eps: 0.583938
 2081358/6000000: episode: 2909, duration: 13.684s, episode steps: 673, steps per second:  49, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.013168, mae: 2.285458, mean_q: 2.750054, mean_eps: 0.583796
 2081921/6000000: episode: 2910, duration: 11.526s, episode steps: 563, steps per second:  49, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.013727, mae: 2.257846, mean_q: 2.716241, mean_eps: 0.583672
 2082771/6000000: episode: 2911, duration: 18.546s, episode steps: 850, steps per second:  46, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.013036, mae: 2.278242, mean_q: 2.741504, mean_eps: 0.583531
 2083640/6000000: episode: 2912, duration: 18.681s, episode steps: 869, steps per second:  47, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.011775, mae: 2.298644, mean_q: 2.766263, mean_eps: 0.583359
 2084028/6000000: episode: 2913, duration: 8.464s, episode steps: 388, steps per second:  46, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.013127, mae: 2.271381, mean_q: 2.733474, mean_eps: 0.583234
 2084734/6000000: episode: 2914, duration: 14.818s, episode steps: 706, steps per second:  48, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.127 [0.000, 5.000],  loss: 0.013662, mae: 2.293257, mean_q: 2.760698, mean_eps: 0.583124
 2085110/6000000: episode: 2915, duration: 8.224s, episode steps: 376, steps per second:  46, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.016201, mae: 2.271209, mean_q: 2.730177, mean_eps: 0.583016
 2086237/6000000: episode: 2916, duration: 24.991s, episode steps: 1127, steps per second:  45, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.161 [0.000, 5.000],  loss: 0.013491, mae: 2.289262, mean_q: 2.753646, mean_eps: 0.582865
 2086737/6000000: episode: 2917, duration: 10.915s, episode steps: 500, steps per second:  46, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.012441, mae: 2.296173, mean_q: 2.762376, mean_eps: 0.582702
 2087908/6000000: episode: 2918, duration: 27.200s, episode steps: 1171, steps per second:  43, episode reward: 34.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.106 [0.000, 5.000],  loss: 0.014126, mae: 2.285759, mean_q: 2.750795, mean_eps: 0.582536
 2088303/6000000: episode: 2919, duration: 9.481s, episode steps: 395, steps per second:  42, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.012005, mae: 2.275172, mean_q: 2.737538, mean_eps: 0.582379
 2088953/6000000: episode: 2920, duration: 15.280s, episode steps: 650, steps per second:  43, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.013006, mae: 2.263436, mean_q: 2.722032, mean_eps: 0.582274
 2089761/6000000: episode: 2921, duration: 16.979s, episode steps: 808, steps per second:  48, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.167 [0.000, 5.000],  loss: 0.013291, mae: 2.288706, mean_q: 2.752733, mean_eps: 0.582128
 2090647/6000000: episode: 2922, duration: 19.421s, episode steps: 886, steps per second:  46, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.901 [0.000, 5.000],  loss: 0.015002, mae: 2.273946, mean_q: 2.735542, mean_eps: 0.581959
 2091291/6000000: episode: 2923, duration: 15.190s, episode steps: 644, steps per second:  42, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.014755, mae: 2.256526, mean_q: 2.714099, mean_eps: 0.581806
 2091910/6000000: episode: 2924, duration: 13.377s, episode steps: 619, steps per second:  46, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.012268, mae: 2.272228, mean_q: 2.734371, mean_eps: 0.581680
 2092856/6000000: episode: 2925, duration: 20.629s, episode steps: 946, steps per second:  46, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.012374, mae: 2.257005, mean_q: 2.714919, mean_eps: 0.581524
 2093637/6000000: episode: 2926, duration: 18.777s, episode steps: 781, steps per second:  42, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.205 [0.000, 5.000],  loss: 0.011882, mae: 2.268578, mean_q: 2.729531, mean_eps: 0.581351
 2094011/6000000: episode: 2927, duration: 8.360s, episode steps: 374, steps per second:  45, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.040 [0.000, 5.000],  loss: 0.012971, mae: 2.286869, mean_q: 2.750551, mean_eps: 0.581235
 2094842/6000000: episode: 2928, duration: 17.911s, episode steps: 831, steps per second:  46, episode reward: 24.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.013121, mae: 2.260228, mean_q: 2.720163, mean_eps: 0.581115
 2095220/6000000: episode: 2929, duration: 8.240s, episode steps: 378, steps per second:  46, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.696 [0.000, 5.000],  loss: 0.012015, mae: 2.273290, mean_q: 2.735984, mean_eps: 0.580994
 2095946/6000000: episode: 2930, duration: 15.514s, episode steps: 726, steps per second:  47, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.014172, mae: 2.263524, mean_q: 2.723589, mean_eps: 0.580884
 2096641/6000000: episode: 2931, duration: 14.921s, episode steps: 695, steps per second:  47, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.015458, mae: 2.285046, mean_q: 2.749037, mean_eps: 0.580741
 2097536/6000000: episode: 2932, duration: 18.289s, episode steps: 895, steps per second:  49, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.012904, mae: 2.267128, mean_q: 2.727026, mean_eps: 0.580582
 2098276/6000000: episode: 2933, duration: 15.194s, episode steps: 740, steps per second:  49, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.014097, mae: 2.275539, mean_q: 2.738561, mean_eps: 0.580419
 2098872/6000000: episode: 2934, duration: 12.748s, episode steps: 596, steps per second:  47, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.012265, mae: 2.263359, mean_q: 2.722733, mean_eps: 0.580286
 2099650/6000000: episode: 2935, duration: 16.698s, episode steps: 778, steps per second:  47, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.108 [0.000, 5.000],  loss: 0.012097, mae: 2.275041, mean_q: 2.737584, mean_eps: 0.580148
 2100255/6000000: episode: 2936, duration: 12.858s, episode steps: 605, steps per second:  47, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.273 [0.000, 5.000],  loss: 0.012692, mae: 2.248849, mean_q: 2.706459, mean_eps: 0.580010
 2100782/6000000: episode: 2937, duration: 11.436s, episode steps: 527, steps per second:  46, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.848 [0.000, 5.000],  loss: 0.013635, mae: 2.247867, mean_q: 2.703468, mean_eps: 0.579896
 2101474/6000000: episode: 2938, duration: 15.151s, episode steps: 692, steps per second:  46, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.669 [0.000, 5.000],  loss: 0.011708, mae: 2.254621, mean_q: 2.714388, mean_eps: 0.579774
 2102488/6000000: episode: 2939, duration: 21.040s, episode steps: 1014, steps per second:  48, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.013491, mae: 2.242208, mean_q: 2.697568, mean_eps: 0.579604
 2103212/6000000: episode: 2940, duration: 15.965s, episode steps: 724, steps per second:  45, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.012269, mae: 2.238111, mean_q: 2.693060, mean_eps: 0.579430
 2103984/6000000: episode: 2941, duration: 17.331s, episode steps: 772, steps per second:  45, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.013540, mae: 2.249417, mean_q: 2.705304, mean_eps: 0.579281
 2104887/6000000: episode: 2942, duration: 20.862s, episode steps: 903, steps per second:  43, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.013152, mae: 2.240780, mean_q: 2.695847, mean_eps: 0.579113
 2105863/6000000: episode: 2943, duration: 20.458s, episode steps: 976, steps per second:  48, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.014101, mae: 2.237338, mean_q: 2.691093, mean_eps: 0.578925
 2106368/6000000: episode: 2944, duration: 10.763s, episode steps: 505, steps per second:  47, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.011783, mae: 2.234132, mean_q: 2.687012, mean_eps: 0.578777
 2107942/6000000: episode: 2945, duration: 33.620s, episode steps: 1574, steps per second:  47, episode reward: 35.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.013617, mae: 2.227438, mean_q: 2.679805, mean_eps: 0.578569
 2109028/6000000: episode: 2946, duration: 23.252s, episode steps: 1086, steps per second:  47, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.014027, mae: 2.233247, mean_q: 2.687622, mean_eps: 0.578303
 2109784/6000000: episode: 2947, duration: 16.458s, episode steps: 756, steps per second:  46, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.011809, mae: 2.239558, mean_q: 2.696826, mean_eps: 0.578119
 2110308/6000000: episode: 2948, duration: 12.108s, episode steps: 524, steps per second:  43, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.149 [0.000, 5.000],  loss: 0.011589, mae: 2.245378, mean_q: 2.703190, mean_eps: 0.577991
 2110926/6000000: episode: 2949, duration: 13.697s, episode steps: 618, steps per second:  45, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.073 [0.000, 5.000],  loss: 0.013773, mae: 2.227107, mean_q: 2.678954, mean_eps: 0.577877
 2111410/6000000: episode: 2950, duration: 10.837s, episode steps: 484, steps per second:  45, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.016341, mae: 2.213200, mean_q: 2.661440, mean_eps: 0.577766
 2112563/6000000: episode: 2951, duration: 25.129s, episode steps: 1153, steps per second:  46, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.012870, mae: 2.223745, mean_q: 2.674648, mean_eps: 0.577603
 2114151/6000000: episode: 2952, duration: 34.203s, episode steps: 1588, steps per second:  46, episode reward: 28.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.013101, mae: 2.230269, mean_q: 2.684154, mean_eps: 0.577329
 2114625/6000000: episode: 2953, duration: 9.521s, episode steps: 474, steps per second:  50, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.019 [0.000, 5.000],  loss: 0.013850, mae: 2.238219, mean_q: 2.694438, mean_eps: 0.577122
 2115584/6000000: episode: 2954, duration: 20.739s, episode steps: 959, steps per second:  46, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.015138, mae: 2.245043, mean_q: 2.701300, mean_eps: 0.576979
 2116212/6000000: episode: 2955, duration: 13.697s, episode steps: 628, steps per second:  46, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.710 [0.000, 5.000],  loss: 0.013086, mae: 2.262095, mean_q: 2.720758, mean_eps: 0.576821
 2117167/6000000: episode: 2956, duration: 19.636s, episode steps: 955, steps per second:  49, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.676 [0.000, 5.000],  loss: 0.012770, mae: 2.243821, mean_q: 2.699586, mean_eps: 0.576662
 2117802/6000000: episode: 2957, duration: 13.446s, episode steps: 635, steps per second:  47, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.740 [0.000, 5.000],  loss: 0.013677, mae: 2.258734, mean_q: 2.717352, mean_eps: 0.576503
 2118291/6000000: episode: 2958, duration: 10.510s, episode steps: 489, steps per second:  47, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.014548, mae: 2.273321, mean_q: 2.734010, mean_eps: 0.576391
 2118947/6000000: episode: 2959, duration: 13.737s, episode steps: 656, steps per second:  48, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.157 [0.000, 5.000],  loss: 0.014924, mae: 2.267739, mean_q: 2.727665, mean_eps: 0.576276
 2119797/6000000: episode: 2960, duration: 18.234s, episode steps: 850, steps per second:  47, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.013588, mae: 2.273382, mean_q: 2.734976, mean_eps: 0.576126
 2120606/6000000: episode: 2961, duration: 17.539s, episode steps: 809, steps per second:  46, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.012895, mae: 2.255728, mean_q: 2.713914, mean_eps: 0.575960
 2121270/6000000: episode: 2962, duration: 16.131s, episode steps: 664, steps per second:  41, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.013652, mae: 2.238786, mean_q: 2.692520, mean_eps: 0.575812
 2121746/6000000: episode: 2963, duration: 11.431s, episode steps: 476, steps per second:  42, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.239 [0.000, 5.000],  loss: 0.013071, mae: 2.225959, mean_q: 2.677685, mean_eps: 0.575698
 2122262/6000000: episode: 2964, duration: 10.858s, episode steps: 516, steps per second:  48, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 0.012045, mae: 2.256158, mean_q: 2.715254, mean_eps: 0.575599
 2123049/6000000: episode: 2965, duration: 16.899s, episode steps: 787, steps per second:  47, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.271 [0.000, 5.000],  loss: 0.012264, mae: 2.263377, mean_q: 2.722541, mean_eps: 0.575469
 2123947/6000000: episode: 2966, duration: 18.718s, episode steps: 898, steps per second:  48, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.306 [0.000, 5.000],  loss: 0.014488, mae: 2.235006, mean_q: 2.687118, mean_eps: 0.575300
 2124702/6000000: episode: 2967, duration: 16.507s, episode steps: 755, steps per second:  46, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.013089, mae: 2.230555, mean_q: 2.682191, mean_eps: 0.575135
 2125611/6000000: episode: 2968, duration: 20.252s, episode steps: 909, steps per second:  45, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.351 [0.000, 5.000],  loss: 0.013889, mae: 2.259308, mean_q: 2.719177, mean_eps: 0.574969
 2126495/6000000: episode: 2969, duration: 20.040s, episode steps: 884, steps per second:  44, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.012541, mae: 2.233922, mean_q: 2.687622, mean_eps: 0.574790
 2127219/6000000: episode: 2970, duration: 16.771s, episode steps: 724, steps per second:  43, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.012039, mae: 2.245752, mean_q: 2.701018, mean_eps: 0.574629
 2127806/6000000: episode: 2971, duration: 12.720s, episode steps: 587, steps per second:  46, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.324 [0.000, 5.000],  loss: 0.013524, mae: 2.251217, mean_q: 2.707347, mean_eps: 0.574498
 2128818/6000000: episode: 2972, duration: 21.916s, episode steps: 1012, steps per second:  46, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.912 [0.000, 5.000],  loss: 0.014140, mae: 2.239280, mean_q: 2.696987, mean_eps: 0.574338
 2129556/6000000: episode: 2973, duration: 16.040s, episode steps: 738, steps per second:  46, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.108 [0.000, 5.000],  loss: 0.012779, mae: 2.235786, mean_q: 2.690292, mean_eps: 0.574163
 2130081/6000000: episode: 2974, duration: 11.628s, episode steps: 525, steps per second:  45, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.012335, mae: 2.221419, mean_q: 2.672901, mean_eps: 0.574036
 2130734/6000000: episode: 2975, duration: 13.072s, episode steps: 653, steps per second:  50, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.013041, mae: 2.204850, mean_q: 2.651716, mean_eps: 0.573918
 2131567/6000000: episode: 2976, duration: 17.250s, episode steps: 833, steps per second:  48, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.012470, mae: 2.215561, mean_q: 2.666743, mean_eps: 0.573770
 2131942/6000000: episode: 2977, duration: 8.335s, episode steps: 375, steps per second:  45, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.012683, mae: 2.213822, mean_q: 2.663935, mean_eps: 0.573649
 2132823/6000000: episode: 2978, duration: 19.716s, episode steps: 881, steps per second:  45, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.013186, mae: 2.202493, mean_q: 2.648553, mean_eps: 0.573524
 2133824/6000000: episode: 2979, duration: 20.671s, episode steps: 1001, steps per second:  48, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.013083, mae: 2.220071, mean_q: 2.670184, mean_eps: 0.573336
 2134759/6000000: episode: 2980, duration: 19.614s, episode steps: 935, steps per second:  48, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.227 [0.000, 5.000],  loss: 0.012659, mae: 2.209322, mean_q: 2.657645, mean_eps: 0.573142
 2135542/6000000: episode: 2981, duration: 16.867s, episode steps: 783, steps per second:  46, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.950 [0.000, 5.000],  loss: 0.013444, mae: 2.205532, mean_q: 2.653282, mean_eps: 0.572970
 2136418/6000000: episode: 2982, duration: 19.630s, episode steps: 876, steps per second:  45, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.128 [0.000, 5.000],  loss: 0.012894, mae: 2.197195, mean_q: 2.644233, mean_eps: 0.572804
 2137002/6000000: episode: 2983, duration: 12.835s, episode steps: 584, steps per second:  46, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.012439, mae: 2.191423, mean_q: 2.636178, mean_eps: 0.572658
 2137443/6000000: episode: 2984, duration: 10.813s, episode steps: 441, steps per second:  41, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.104 [0.000, 5.000],  loss: 0.011088, mae: 2.206979, mean_q: 2.654516, mean_eps: 0.572556
 2138204/6000000: episode: 2985, duration: 19.086s, episode steps: 761, steps per second:  40, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.013161, mae: 2.173701, mean_q: 2.616434, mean_eps: 0.572436
 2139122/6000000: episode: 2986, duration: 21.627s, episode steps: 918, steps per second:  42, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.012947, mae: 2.180821, mean_q: 2.625145, mean_eps: 0.572268
 2139943/6000000: episode: 2987, duration: 18.619s, episode steps: 821, steps per second:  44, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.012473, mae: 2.201332, mean_q: 2.648508, mean_eps: 0.572094
 2140498/6000000: episode: 2988, duration: 12.582s, episode steps: 555, steps per second:  44, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.095 [0.000, 5.000],  loss: 0.012217, mae: 2.182304, mean_q: 2.627126, mean_eps: 0.571956
 2141167/6000000: episode: 2989, duration: 15.156s, episode steps: 669, steps per second:  44, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.013816, mae: 2.207333, mean_q: 2.655234, mean_eps: 0.571834
 2142056/6000000: episode: 2990, duration: 19.085s, episode steps: 889, steps per second:  47, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.011922, mae: 2.189221, mean_q: 2.634613, mean_eps: 0.571678
 2142724/6000000: episode: 2991, duration: 14.828s, episode steps: 668, steps per second:  45, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.015269, mae: 2.182269, mean_q: 2.625795, mean_eps: 0.571522
 2143470/6000000: episode: 2992, duration: 16.543s, episode steps: 746, steps per second:  45, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.013954, mae: 2.191070, mean_q: 2.637138, mean_eps: 0.571381
 2144185/6000000: episode: 2993, duration: 15.335s, episode steps: 715, steps per second:  47, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.266 [0.000, 5.000],  loss: 0.012153, mae: 2.187765, mean_q: 2.632968, mean_eps: 0.571234
 2145172/6000000: episode: 2994, duration: 21.866s, episode steps: 987, steps per second:  45, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.013403, mae: 2.179760, mean_q: 2.622487, mean_eps: 0.571064
 2145838/6000000: episode: 2995, duration: 14.850s, episode steps: 666, steps per second:  45, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.014193, mae: 2.150142, mean_q: 2.586227, mean_eps: 0.570899
 2146813/6000000: episode: 2996, duration: 20.758s, episode steps: 975, steps per second:  47, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.013335, mae: 2.153118, mean_q: 2.590637, mean_eps: 0.570735
 2147695/6000000: episode: 2997, duration: 17.709s, episode steps: 882, steps per second:  50, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.013828, mae: 2.158389, mean_q: 2.596425, mean_eps: 0.570549
 2148310/6000000: episode: 2998, duration: 13.099s, episode steps: 615, steps per second:  47, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.911 [0.000, 5.000],  loss: 0.012011, mae: 2.140946, mean_q: 2.577618, mean_eps: 0.570400
 2149301/6000000: episode: 2999, duration: 21.799s, episode steps: 991, steps per second:  45, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.013321, mae: 2.145824, mean_q: 2.582419, mean_eps: 0.570239
 2150595/6000000: episode: 3000, duration: 26.442s, episode steps: 1294, steps per second:  49, episode reward: 17.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.013329, mae: 2.162120, mean_q: 2.602097, mean_eps: 0.570010
 2151885/6000000: episode: 3001, duration: 28.558s, episode steps: 1290, steps per second:  45, episode reward: 26.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.136 [0.000, 5.000],  loss: 0.013217, mae: 2.195330, mean_q: 2.641409, mean_eps: 0.569752
 2153021/6000000: episode: 3002, duration: 25.131s, episode steps: 1136, steps per second:  45, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.012904, mae: 2.181465, mean_q: 2.623562, mean_eps: 0.569509
 2154070/6000000: episode: 3003, duration: 24.763s, episode steps: 1049, steps per second:  42, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.097 [0.000, 5.000],  loss: 0.014002, mae: 2.189594, mean_q: 2.635154, mean_eps: 0.569291
 2154808/6000000: episode: 3004, duration: 16.590s, episode steps: 738, steps per second:  44, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.013489, mae: 2.177264, mean_q: 2.619801, mean_eps: 0.569112
 2155848/6000000: episode: 3005, duration: 21.656s, episode steps: 1040, steps per second:  48, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.012668, mae: 2.207661, mean_q: 2.654844, mean_eps: 0.568935
 2156719/6000000: episode: 3006, duration: 18.302s, episode steps: 871, steps per second:  48, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.273 [0.000, 5.000],  loss: 0.013145, mae: 2.200875, mean_q: 2.649416, mean_eps: 0.568744
 2157377/6000000: episode: 3007, duration: 14.332s, episode steps: 658, steps per second:  46, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.150 [0.000, 5.000],  loss: 0.014358, mae: 2.218405, mean_q: 2.668287, mean_eps: 0.568590
 2158253/6000000: episode: 3008, duration: 19.028s, episode steps: 876, steps per second:  46, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.965 [0.000, 5.000],  loss: 0.012516, mae: 2.194746, mean_q: 2.641896, mean_eps: 0.568437
 2159372/6000000: episode: 3009, duration: 25.063s, episode steps: 1119, steps per second:  45, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.014127, mae: 2.193307, mean_q: 2.640954, mean_eps: 0.568238
 2160015/6000000: episode: 3010, duration: 14.578s, episode steps: 643, steps per second:  44, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.011306, mae: 2.188423, mean_q: 2.635027, mean_eps: 0.568062
 2160669/6000000: episode: 3011, duration: 14.261s, episode steps: 654, steps per second:  46, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.012980, mae: 2.206551, mean_q: 2.655330, mean_eps: 0.567932
 2161466/6000000: episode: 3012, duration: 17.282s, episode steps: 797, steps per second:  46, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.013919, mae: 2.176427, mean_q: 2.618529, mean_eps: 0.567786
 2162020/6000000: episode: 3013, duration: 12.151s, episode steps: 554, steps per second:  46, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.012318, mae: 2.183930, mean_q: 2.629237, mean_eps: 0.567652
 2162727/6000000: episode: 3014, duration: 15.788s, episode steps: 707, steps per second:  45, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: 0.013377, mae: 2.188955, mean_q: 2.634712, mean_eps: 0.567526
 2163734/6000000: episode: 3015, duration: 20.201s, episode steps: 1007, steps per second:  50, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.011916, mae: 2.188733, mean_q: 2.634234, mean_eps: 0.567354
 2164715/6000000: episode: 3016, duration: 20.223s, episode steps: 981, steps per second:  49, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.013528, mae: 2.193235, mean_q: 2.638628, mean_eps: 0.567155
 2165406/6000000: episode: 3017, duration: 14.802s, episode steps: 691, steps per second:  47, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.674 [0.000, 5.000],  loss: 0.013875, mae: 2.182219, mean_q: 2.626142, mean_eps: 0.566988
 2166046/6000000: episode: 3018, duration: 13.581s, episode steps: 640, steps per second:  47, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.012240, mae: 2.196630, mean_q: 2.643486, mean_eps: 0.566855
 2166582/6000000: episode: 3019, duration: 11.507s, episode steps: 536, steps per second:  47, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.185 [0.000, 5.000],  loss: 0.013773, mae: 2.196207, mean_q: 2.642104, mean_eps: 0.566737
 2167101/6000000: episode: 3020, duration: 10.813s, episode steps: 519, steps per second:  48, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.102 [0.000, 5.000],  loss: 0.011281, mae: 2.201126, mean_q: 2.647886, mean_eps: 0.566632
 2168059/6000000: episode: 3021, duration: 21.346s, episode steps: 958, steps per second:  45, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.013070, mae: 2.185009, mean_q: 2.627843, mean_eps: 0.566484
 2168600/6000000: episode: 3022, duration: 12.251s, episode steps: 541, steps per second:  44, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.013029, mae: 2.183955, mean_q: 2.627036, mean_eps: 0.566334
 2169495/6000000: episode: 3023, duration: 20.408s, episode steps: 895, steps per second:  44, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.012957, mae: 2.171619, mean_q: 2.611631, mean_eps: 0.566191
 2170159/6000000: episode: 3024, duration: 16.713s, episode steps: 664, steps per second:  40, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.012672, mae: 2.201389, mean_q: 2.647827, mean_eps: 0.566035
 2170964/6000000: episode: 3025, duration: 21.637s, episode steps: 805, steps per second:  37, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.014020, mae: 2.200639, mean_q: 2.646683, mean_eps: 0.565888
 2171793/6000000: episode: 3026, duration: 18.031s, episode steps: 829, steps per second:  46, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.215 [0.000, 5.000],  loss: 0.015181, mae: 2.209800, mean_q: 2.659579, mean_eps: 0.565724
 2172747/6000000: episode: 3027, duration: 20.672s, episode steps: 954, steps per second:  46, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.682 [0.000, 5.000],  loss: 0.012495, mae: 2.197242, mean_q: 2.642890, mean_eps: 0.565546
 2173340/6000000: episode: 3028, duration: 13.042s, episode steps: 593, steps per second:  45, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.012866, mae: 2.205573, mean_q: 2.653762, mean_eps: 0.565392
 2173977/6000000: episode: 3029, duration: 13.287s, episode steps: 637, steps per second:  48, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.207 [0.000, 5.000],  loss: 0.012980, mae: 2.181999, mean_q: 2.623773, mean_eps: 0.565268
 2174838/6000000: episode: 3030, duration: 18.121s, episode steps: 861, steps per second:  48, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.013273, mae: 2.201057, mean_q: 2.648317, mean_eps: 0.565118
 2176015/6000000: episode: 3031, duration: 26.076s, episode steps: 1177, steps per second:  45, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.015004, mae: 2.212838, mean_q: 2.663634, mean_eps: 0.564915
 2176927/6000000: episode: 3032, duration: 20.497s, episode steps: 912, steps per second:  44, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.839 [0.000, 5.000],  loss: 0.013238, mae: 2.205996, mean_q: 2.655592, mean_eps: 0.564706
 2177908/6000000: episode: 3033, duration: 21.441s, episode steps: 981, steps per second:  46, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.155 [0.000, 5.000],  loss: 0.013982, mae: 2.214036, mean_q: 2.664871, mean_eps: 0.564517
 2178558/6000000: episode: 3034, duration: 14.376s, episode steps: 650, steps per second:  45, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.012554, mae: 2.208740, mean_q: 2.657937, mean_eps: 0.564354
 2179510/6000000: episode: 3035, duration: 20.375s, episode steps: 952, steps per second:  47, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.300 [0.000, 5.000],  loss: 0.013222, mae: 2.213763, mean_q: 2.665122, mean_eps: 0.564193
 2180266/6000000: episode: 3036, duration: 15.266s, episode steps: 756, steps per second:  50, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.262 [0.000, 5.000],  loss: 0.012535, mae: 2.215686, mean_q: 2.668217, mean_eps: 0.564022
 2180752/6000000: episode: 3037, duration: 10.129s, episode steps: 486, steps per second:  48, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.041 [0.000, 5.000],  loss: 0.012021, mae: 2.223925, mean_q: 2.677479, mean_eps: 0.563898
 2181247/6000000: episode: 3038, duration: 10.633s, episode steps: 495, steps per second:  47, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.012717, mae: 2.224996, mean_q: 2.676687, mean_eps: 0.563800
 2181612/6000000: episode: 3039, duration: 7.987s, episode steps: 365, steps per second:  46, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.236 [0.000, 5.000],  loss: 0.013191, mae: 2.167112, mean_q: 2.609006, mean_eps: 0.563714
 2182392/6000000: episode: 3040, duration: 16.991s, episode steps: 780, steps per second:  46, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.014335, mae: 2.220760, mean_q: 2.672957, mean_eps: 0.563600
 2183550/6000000: episode: 3041, duration: 24.706s, episode steps: 1158, steps per second:  47, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.013099, mae: 2.216982, mean_q: 2.667254, mean_eps: 0.563406
 2184725/6000000: episode: 3042, duration: 25.231s, episode steps: 1175, steps per second:  47, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.013827, mae: 2.214885, mean_q: 2.663511, mean_eps: 0.563172
 2185689/6000000: episode: 3043, duration: 21.280s, episode steps: 964, steps per second:  45, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.014191, mae: 2.219948, mean_q: 2.669530, mean_eps: 0.562958
 2186482/6000000: episode: 3044, duration: 18.247s, episode steps: 793, steps per second:  43, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.012165, mae: 2.215681, mean_q: 2.665799, mean_eps: 0.562783
 2187395/6000000: episode: 3045, duration: 21.889s, episode steps: 913, steps per second:  42, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.012022, mae: 2.223402, mean_q: 2.674765, mean_eps: 0.562612
 2188325/6000000: episode: 3046, duration: 19.615s, episode steps: 930, steps per second:  47, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.263 [0.000, 5.000],  loss: 0.013740, mae: 2.227703, mean_q: 2.681358, mean_eps: 0.562428
 2189234/6000000: episode: 3047, duration: 19.666s, episode steps: 909, steps per second:  46, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.013068, mae: 2.211750, mean_q: 2.661948, mean_eps: 0.562244
 2189890/6000000: episode: 3048, duration: 14.326s, episode steps: 656, steps per second:  46, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.104 [0.000, 5.000],  loss: 0.014737, mae: 2.235091, mean_q: 2.688782, mean_eps: 0.562088
 2190771/6000000: episode: 3049, duration: 18.752s, episode steps: 881, steps per second:  47, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.013954, mae: 2.221417, mean_q: 2.671463, mean_eps: 0.561934
 2191189/6000000: episode: 3050, duration: 9.217s, episode steps: 418, steps per second:  45, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.011695, mae: 2.217418, mean_q: 2.669014, mean_eps: 0.561804
 2192013/6000000: episode: 3051, duration: 18.079s, episode steps: 824, steps per second:  46, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.014351, mae: 2.209878, mean_q: 2.657774, mean_eps: 0.561680
 2192885/6000000: episode: 3052, duration: 19.287s, episode steps: 872, steps per second:  45, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.188 [0.000, 5.000],  loss: 0.013817, mae: 2.204040, mean_q: 2.651319, mean_eps: 0.561510
 2193879/6000000: episode: 3053, duration: 21.280s, episode steps: 994, steps per second:  47, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.011910, mae: 2.209014, mean_q: 2.657682, mean_eps: 0.561324
 2194435/6000000: episode: 3054, duration: 12.070s, episode steps: 556, steps per second:  46, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.209 [0.000, 5.000],  loss: 0.014796, mae: 2.216118, mean_q: 2.665622, mean_eps: 0.561169
 2195170/6000000: episode: 3055, duration: 16.357s, episode steps: 735, steps per second:  45, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.226 [0.000, 5.000],  loss: 0.013329, mae: 2.222983, mean_q: 2.673497, mean_eps: 0.561040
 2195974/6000000: episode: 3056, duration: 17.787s, episode steps: 804, steps per second:  45, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.184 [0.000, 5.000],  loss: 0.014333, mae: 2.209041, mean_q: 2.658305, mean_eps: 0.560886
 2196634/6000000: episode: 3057, duration: 13.616s, episode steps: 660, steps per second:  48, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.013631, mae: 2.205249, mean_q: 2.652906, mean_eps: 0.560739
 2197472/6000000: episode: 3058, duration: 17.558s, episode steps: 838, steps per second:  48, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 0.012585, mae: 2.196326, mean_q: 2.643134, mean_eps: 0.560590
 2198020/6000000: episode: 3059, duration: 11.664s, episode steps: 548, steps per second:  47, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.014975, mae: 2.192226, mean_q: 2.637454, mean_eps: 0.560451
 2198443/6000000: episode: 3060, duration: 9.423s, episode steps: 423, steps per second:  45, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.015871, mae: 2.186455, mean_q: 2.629438, mean_eps: 0.560354
 2199365/6000000: episode: 3061, duration: 19.097s, episode steps: 922, steps per second:  48, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.275 [0.000, 5.000],  loss: 0.013042, mae: 2.198904, mean_q: 2.645614, mean_eps: 0.560219
 2200177/6000000: episode: 3062, duration: 16.980s, episode steps: 812, steps per second:  48, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.243 [0.000, 5.000],  loss: 0.010872, mae: 2.203129, mean_q: 2.653422, mean_eps: 0.560046
 2200597/6000000: episode: 3063, duration: 8.697s, episode steps: 420, steps per second:  48, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.014292, mae: 2.207418, mean_q: 2.653856, mean_eps: 0.559922
 2201310/6000000: episode: 3064, duration: 15.353s, episode steps: 713, steps per second:  46, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.013946, mae: 2.185540, mean_q: 2.629531, mean_eps: 0.559809
 2201927/6000000: episode: 3065, duration: 13.114s, episode steps: 617, steps per second:  47, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.013261, mae: 2.185499, mean_q: 2.628815, mean_eps: 0.559676
 2202794/6000000: episode: 3066, duration: 19.390s, episode steps: 867, steps per second:  45, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.306 [0.000, 5.000],  loss: 0.012667, mae: 2.194642, mean_q: 2.641411, mean_eps: 0.559528
 2203547/6000000: episode: 3067, duration: 17.921s, episode steps: 753, steps per second:  42, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.224 [0.000, 5.000],  loss: 0.014183, mae: 2.186496, mean_q: 2.630454, mean_eps: 0.559366
 2204197/6000000: episode: 3068, duration: 15.158s, episode steps: 650, steps per second:  43, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.012 [0.000, 5.000],  loss: 0.012691, mae: 2.170557, mean_q: 2.611030, mean_eps: 0.559226
 2204836/6000000: episode: 3069, duration: 13.848s, episode steps: 639, steps per second:  46, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.012825, mae: 2.192219, mean_q: 2.636986, mean_eps: 0.559097
 2205647/6000000: episode: 3070, duration: 17.467s, episode steps: 811, steps per second:  46, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.703 [0.000, 5.000],  loss: 0.013979, mae: 2.225040, mean_q: 2.676289, mean_eps: 0.558952
 2206030/6000000: episode: 3071, duration: 8.517s, episode steps: 383, steps per second:  45, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.013074, mae: 2.202637, mean_q: 2.649492, mean_eps: 0.558832
 2206918/6000000: episode: 3072, duration: 20.123s, episode steps: 888, steps per second:  44, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.012894, mae: 2.214867, mean_q: 2.664925, mean_eps: 0.558705
 2207910/6000000: episode: 3073, duration: 21.492s, episode steps: 992, steps per second:  46, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.158 [0.000, 5.000],  loss: 0.013910, mae: 2.213854, mean_q: 2.666441, mean_eps: 0.558517
 2208941/6000000: episode: 3074, duration: 23.813s, episode steps: 1031, steps per second:  43, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.012972, mae: 2.207781, mean_q: 2.656472, mean_eps: 0.558315
 2209584/6000000: episode: 3075, duration: 14.737s, episode steps: 643, steps per second:  44, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.300 [0.000, 5.000],  loss: 0.012700, mae: 2.200374, mean_q: 2.648894, mean_eps: 0.558148
 2210245/6000000: episode: 3076, duration: 14.296s, episode steps: 661, steps per second:  46, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.206 [0.000, 5.000],  loss: 0.013313, mae: 2.191302, mean_q: 2.638924, mean_eps: 0.558017
 2210604/6000000: episode: 3077, duration: 7.610s, episode steps: 359, steps per second:  47, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.635 [0.000, 5.000],  loss: 0.012273, mae: 2.160017, mean_q: 2.599162, mean_eps: 0.557915
 2211171/6000000: episode: 3078, duration: 12.562s, episode steps: 567, steps per second:  45, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.915 [0.000, 5.000],  loss: 0.012867, mae: 2.174829, mean_q: 2.616940, mean_eps: 0.557823
 2212125/6000000: episode: 3079, duration: 21.364s, episode steps: 954, steps per second:  45, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.013684, mae: 2.182196, mean_q: 2.625838, mean_eps: 0.557670
 2212889/6000000: episode: 3080, duration: 15.880s, episode steps: 764, steps per second:  48, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.012451, mae: 2.164407, mean_q: 2.604084, mean_eps: 0.557498
 2213794/6000000: episode: 3081, duration: 18.816s, episode steps: 905, steps per second:  48, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.116 [0.000, 5.000],  loss: 0.011914, mae: 2.187142, mean_q: 2.633087, mean_eps: 0.557332
 2214798/6000000: episode: 3082, duration: 21.365s, episode steps: 1004, steps per second:  47, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.278 [0.000, 5.000],  loss: 0.011541, mae: 2.177680, mean_q: 2.621681, mean_eps: 0.557141
 2215506/6000000: episode: 3083, duration: 14.830s, episode steps: 708, steps per second:  48, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.278 [0.000, 5.000],  loss: 0.012868, mae: 2.175973, mean_q: 2.619680, mean_eps: 0.556970
 2216611/6000000: episode: 3084, duration: 23.200s, episode steps: 1105, steps per second:  48, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.013310, mae: 2.180178, mean_q: 2.623942, mean_eps: 0.556788
 2217429/6000000: episode: 3085, duration: 17.447s, episode steps: 818, steps per second:  47, episode reward: 25.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.214 [0.000, 5.000],  loss: 0.013104, mae: 2.184746, mean_q: 2.631923, mean_eps: 0.556596
 2218109/6000000: episode: 3086, duration: 14.130s, episode steps: 680, steps per second:  48, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.013528, mae: 2.171536, mean_q: 2.614355, mean_eps: 0.556446
 2219403/6000000: episode: 3087, duration: 29.292s, episode steps: 1294, steps per second:  44, episode reward: 32.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.233 [0.000, 5.000],  loss: 0.012254, mae: 2.186224, mean_q: 2.633006, mean_eps: 0.556249
 2220018/6000000: episode: 3088, duration: 15.526s, episode steps: 615, steps per second:  40, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.013560, mae: 2.174710, mean_q: 2.619116, mean_eps: 0.556058
 2220651/6000000: episode: 3089, duration: 15.045s, episode steps: 633, steps per second:  42, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.273 [0.000, 5.000],  loss: 0.012272, mae: 2.166179, mean_q: 2.609209, mean_eps: 0.555933
 2221308/6000000: episode: 3090, duration: 14.319s, episode steps: 657, steps per second:  46, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.764 [0.000, 5.000],  loss: 0.012389, mae: 2.181705, mean_q: 2.625220, mean_eps: 0.555804
 2222159/6000000: episode: 3091, duration: 18.254s, episode steps: 851, steps per second:  47, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 0.012344, mae: 2.176302, mean_q: 2.620299, mean_eps: 0.555654
 2223362/6000000: episode: 3092, duration: 26.084s, episode steps: 1203, steps per second:  46, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.318 [0.000, 5.000],  loss: 0.012436, mae: 2.180300, mean_q: 2.622692, mean_eps: 0.555448
 2224295/6000000: episode: 3093, duration: 20.158s, episode steps: 933, steps per second:  46, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.013250, mae: 2.174955, mean_q: 2.617827, mean_eps: 0.555234
 2225361/6000000: episode: 3094, duration: 23.864s, episode steps: 1066, steps per second:  45, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.011950, mae: 2.191045, mean_q: 2.638804, mean_eps: 0.555034
 2225760/6000000: episode: 3095, duration: 9.137s, episode steps: 399, steps per second:  44, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.900 [0.000, 5.000],  loss: 0.014371, mae: 2.201675, mean_q: 2.648058, mean_eps: 0.554888
 2226957/6000000: episode: 3096, duration: 26.460s, episode steps: 1197, steps per second:  45, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.013478, mae: 2.193042, mean_q: 2.638610, mean_eps: 0.554728
 2227503/6000000: episode: 3097, duration: 12.126s, episode steps: 546, steps per second:  45, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.014123, mae: 2.201732, mean_q: 2.648738, mean_eps: 0.554554
 2227905/6000000: episode: 3098, duration: 9.282s, episode steps: 402, steps per second:  43, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.062 [0.000, 5.000],  loss: 0.010436, mae: 2.184485, mean_q: 2.627393, mean_eps: 0.554459
 2228558/6000000: episode: 3099, duration: 14.863s, episode steps: 653, steps per second:  44, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.012494, mae: 2.185750, mean_q: 2.631661, mean_eps: 0.554354
 2229423/6000000: episode: 3100, duration: 17.610s, episode steps: 865, steps per second:  49, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.012204, mae: 2.177381, mean_q: 2.620925, mean_eps: 0.554202
 2230230/6000000: episode: 3101, duration: 16.829s, episode steps: 807, steps per second:  48, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 0.013368, mae: 2.155791, mean_q: 2.594573, mean_eps: 0.554035
 2231344/6000000: episode: 3102, duration: 23.909s, episode steps: 1114, steps per second:  47, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.727 [0.000, 5.000],  loss: 0.012847, mae: 2.141305, mean_q: 2.577727, mean_eps: 0.553843
 2232356/6000000: episode: 3103, duration: 21.267s, episode steps: 1012, steps per second:  48, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.012129, mae: 2.145902, mean_q: 2.583171, mean_eps: 0.553630
 2233338/6000000: episode: 3104, duration: 20.882s, episode steps: 982, steps per second:  47, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.012941, mae: 2.141291, mean_q: 2.577025, mean_eps: 0.553431
 2234511/6000000: episode: 3105, duration: 25.622s, episode steps: 1173, steps per second:  46, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.013144, mae: 2.150668, mean_q: 2.586880, mean_eps: 0.553215
 2234926/6000000: episode: 3106, duration: 9.977s, episode steps: 415, steps per second:  42, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.013389, mae: 2.147108, mean_q: 2.583162, mean_eps: 0.553056
 2235455/6000000: episode: 3107, duration: 11.396s, episode steps: 529, steps per second:  46, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.012373, mae: 2.176663, mean_q: 2.618541, mean_eps: 0.552962
 2236219/6000000: episode: 3108, duration: 17.644s, episode steps: 764, steps per second:  43, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.144 [0.000, 5.000],  loss: 0.012567, mae: 2.176631, mean_q: 2.621084, mean_eps: 0.552833
 2237189/6000000: episode: 3109, duration: 22.435s, episode steps: 970, steps per second:  43, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: 0.012576, mae: 2.173083, mean_q: 2.616063, mean_eps: 0.552659
 2237823/6000000: episode: 3110, duration: 13.348s, episode steps: 634, steps per second:  47, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.010452, mae: 2.180520, mean_q: 2.625882, mean_eps: 0.552499
 2239017/6000000: episode: 3111, duration: 27.902s, episode steps: 1194, steps per second:  43, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.299 [0.000, 5.000],  loss: 0.013064, mae: 2.163407, mean_q: 2.605070, mean_eps: 0.552316
 2239985/6000000: episode: 3112, duration: 22.749s, episode steps: 968, steps per second:  43, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.215 [0.000, 5.000],  loss: 0.011400, mae: 2.184238, mean_q: 2.630678, mean_eps: 0.552100
 2240496/6000000: episode: 3113, duration: 10.824s, episode steps: 511, steps per second:  47, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.810 [0.000, 5.000],  loss: 0.012179, mae: 2.186635, mean_q: 2.634219, mean_eps: 0.551952
 2241537/6000000: episode: 3114, duration: 22.968s, episode steps: 1041, steps per second:  45, episode reward: 30.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.086 [0.000, 5.000],  loss: 0.013011, mae: 2.177412, mean_q: 2.622265, mean_eps: 0.551797
 2242096/6000000: episode: 3115, duration: 13.582s, episode steps: 559, steps per second:  41, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.832 [0.000, 5.000],  loss: 0.012725, mae: 2.189526, mean_q: 2.636640, mean_eps: 0.551637
 2242817/6000000: episode: 3116, duration: 16.509s, episode steps: 721, steps per second:  44, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.011801, mae: 2.167828, mean_q: 2.611511, mean_eps: 0.551509
 2243550/6000000: episode: 3117, duration: 16.718s, episode steps: 733, steps per second:  44, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.011 [0.000, 5.000],  loss: 0.013129, mae: 2.210243, mean_q: 2.661012, mean_eps: 0.551363
 2244703/6000000: episode: 3118, duration: 26.274s, episode steps: 1153, steps per second:  44, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.251 [0.000, 5.000],  loss: 0.012690, mae: 2.191722, mean_q: 2.638600, mean_eps: 0.551175
 2245389/6000000: episode: 3119, duration: 14.074s, episode steps: 686, steps per second:  49, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.014508, mae: 2.197812, mean_q: 2.644251, mean_eps: 0.550991
 2246269/6000000: episode: 3120, duration: 17.868s, episode steps: 880, steps per second:  49, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.013080, mae: 2.186931, mean_q: 2.636168, mean_eps: 0.550834
 2247193/6000000: episode: 3121, duration: 19.672s, episode steps: 924, steps per second:  47, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.012341, mae: 2.192717, mean_q: 2.638809, mean_eps: 0.550654
 2248300/6000000: episode: 3122, duration: 24.006s, episode steps: 1107, steps per second:  46, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.013874, mae: 2.178055, mean_q: 2.622413, mean_eps: 0.550451
 2249407/6000000: episode: 3123, duration: 24.115s, episode steps: 1107, steps per second:  46, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.012208, mae: 2.197688, mean_q: 2.644666, mean_eps: 0.550230
 2250371/6000000: episode: 3124, duration: 21.114s, episode steps: 964, steps per second:  46, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.013309, mae: 2.212390, mean_q: 2.662446, mean_eps: 0.550022
 2251379/6000000: episode: 3125, duration: 23.006s, episode steps: 1008, steps per second:  44, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.094 [0.000, 5.000],  loss: 0.011809, mae: 2.187815, mean_q: 2.634259, mean_eps: 0.549825
 2252377/6000000: episode: 3126, duration: 23.329s, episode steps: 998, steps per second:  43, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: 0.014064, mae: 2.185004, mean_q: 2.630590, mean_eps: 0.549624
 2253758/6000000: episode: 3127, duration: 30.967s, episode steps: 1381, steps per second:  45, episode reward: 35.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.190 [0.000, 5.000],  loss: 0.013908, mae: 2.201385, mean_q: 2.648280, mean_eps: 0.549386
 2254479/6000000: episode: 3128, duration: 15.582s, episode steps: 721, steps per second:  46, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.012224, mae: 2.195551, mean_q: 2.642187, mean_eps: 0.549176
 2255421/6000000: episode: 3129, duration: 20.080s, episode steps: 942, steps per second:  47, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.142 [0.000, 5.000],  loss: 0.014363, mae: 2.203627, mean_q: 2.652638, mean_eps: 0.549010
 2256070/6000000: episode: 3130, duration: 13.804s, episode steps: 649, steps per second:  47, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.012501, mae: 2.232847, mean_q: 2.687626, mean_eps: 0.548851
 2256935/6000000: episode: 3131, duration: 18.189s, episode steps: 865, steps per second:  48, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.011784, mae: 2.223636, mean_q: 2.676170, mean_eps: 0.548700
 2257911/6000000: episode: 3132, duration: 21.150s, episode steps: 976, steps per second:  46, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.147 [0.000, 5.000],  loss: 0.013143, mae: 2.221724, mean_q: 2.673138, mean_eps: 0.548516
 2258548/6000000: episode: 3133, duration: 14.605s, episode steps: 637, steps per second:  44, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.662 [0.000, 5.000],  loss: 0.012896, mae: 2.223927, mean_q: 2.675704, mean_eps: 0.548354
 2260321/6000000: episode: 3134, duration: 39.588s, episode steps: 1773, steps per second:  45, episode reward: 33.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.232 [0.000, 5.000],  loss: 0.012443, mae: 2.224565, mean_q: 2.677854, mean_eps: 0.548113
 2261179/6000000: episode: 3135, duration: 18.925s, episode steps: 858, steps per second:  45, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.013920, mae: 2.231322, mean_q: 2.685173, mean_eps: 0.547850
 2261974/6000000: episode: 3136, duration: 16.280s, episode steps: 795, steps per second:  49, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.013843, mae: 2.216886, mean_q: 2.666595, mean_eps: 0.547685
 2262691/6000000: episode: 3137, duration: 14.589s, episode steps: 717, steps per second:  49, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.174 [0.000, 5.000],  loss: 0.013087, mae: 2.233255, mean_q: 2.686862, mean_eps: 0.547534
 2263462/6000000: episode: 3138, duration: 16.452s, episode steps: 771, steps per second:  47, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.243 [0.000, 5.000],  loss: 0.012835, mae: 2.213197, mean_q: 2.664922, mean_eps: 0.547385
 2264095/6000000: episode: 3139, duration: 13.478s, episode steps: 633, steps per second:  47, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.651 [0.000, 5.000],  loss: 0.013076, mae: 2.227124, mean_q: 2.679307, mean_eps: 0.547244
 2264724/6000000: episode: 3140, duration: 13.299s, episode steps: 629, steps per second:  47, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.014240, mae: 2.227462, mean_q: 2.680269, mean_eps: 0.547118
 2265304/6000000: episode: 3141, duration: 12.061s, episode steps: 580, steps per second:  48, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.783 [0.000, 5.000],  loss: 0.015282, mae: 2.241537, mean_q: 2.696765, mean_eps: 0.546998
 2265677/6000000: episode: 3142, duration: 7.981s, episode steps: 373, steps per second:  47, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.737 [0.000, 5.000],  loss: 0.013852, mae: 2.254181, mean_q: 2.712660, mean_eps: 0.546902
 2266454/6000000: episode: 3143, duration: 16.651s, episode steps: 777, steps per second:  47, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.013095, mae: 2.261376, mean_q: 2.721043, mean_eps: 0.546787
 2267258/6000000: episode: 3144, duration: 17.576s, episode steps: 804, steps per second:  46, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.955 [0.000, 5.000],  loss: 0.012880, mae: 2.231668, mean_q: 2.685360, mean_eps: 0.546629
 2268536/6000000: episode: 3145, duration: 28.058s, episode steps: 1278, steps per second:  46, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.013278, mae: 2.247211, mean_q: 2.704112, mean_eps: 0.546421
 2269361/6000000: episode: 3146, duration: 19.462s, episode steps: 825, steps per second:  42, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.015248, mae: 2.240717, mean_q: 2.695924, mean_eps: 0.546210
 2270017/6000000: episode: 3147, duration: 14.550s, episode steps: 656, steps per second:  45, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.291 [0.000, 5.000],  loss: 0.012734, mae: 2.238733, mean_q: 2.695729, mean_eps: 0.546062
 2271202/6000000: episode: 3148, duration: 26.648s, episode steps: 1185, steps per second:  44, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.126 [0.000, 5.000],  loss: 0.012631, mae: 2.210572, mean_q: 2.660817, mean_eps: 0.545878
 2271601/6000000: episode: 3149, duration: 10.044s, episode steps: 399, steps per second:  40, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.011574, mae: 2.199230, mean_q: 2.647557, mean_eps: 0.545720
 2272642/6000000: episode: 3150, duration: 24.090s, episode steps: 1041, steps per second:  43, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.012654, mae: 2.206217, mean_q: 2.655923, mean_eps: 0.545576
 2273461/6000000: episode: 3151, duration: 18.034s, episode steps: 819, steps per second:  45, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.013218, mae: 2.212157, mean_q: 2.662782, mean_eps: 0.545390
 2274179/6000000: episode: 3152, duration: 16.693s, episode steps: 718, steps per second:  43, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.312 [0.000, 5.000],  loss: 0.011863, mae: 2.206068, mean_q: 2.654774, mean_eps: 0.545236
 2275073/6000000: episode: 3153, duration: 21.092s, episode steps: 894, steps per second:  42, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.012792, mae: 2.203242, mean_q: 2.651956, mean_eps: 0.545075
 2275947/6000000: episode: 3154, duration: 19.576s, episode steps: 874, steps per second:  45, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.058 [0.000, 5.000],  loss: 0.012736, mae: 2.239247, mean_q: 2.694613, mean_eps: 0.544898
 2276526/6000000: episode: 3155, duration: 13.174s, episode steps: 579, steps per second:  44, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.130 [0.000, 5.000],  loss: 0.013668, mae: 2.243109, mean_q: 2.700651, mean_eps: 0.544753
 2277712/6000000: episode: 3156, duration: 26.375s, episode steps: 1186, steps per second:  45, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.151 [0.000, 5.000],  loss: 0.013500, mae: 2.227441, mean_q: 2.681032, mean_eps: 0.544576
 2278396/6000000: episode: 3157, duration: 13.958s, episode steps: 684, steps per second:  49, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.013027, mae: 2.239955, mean_q: 2.695952, mean_eps: 0.544390
 2279521/6000000: episode: 3158, duration: 23.613s, episode steps: 1125, steps per second:  48, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.898 [0.000, 5.000],  loss: 0.012589, mae: 2.235802, mean_q: 2.689711, mean_eps: 0.544208
 2280082/6000000: episode: 3159, duration: 11.626s, episode steps: 561, steps per second:  48, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.014889, mae: 2.235020, mean_q: 2.689151, mean_eps: 0.544040
 2280582/6000000: episode: 3160, duration: 10.915s, episode steps: 500, steps per second:  46, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.012976, mae: 2.211266, mean_q: 2.662134, mean_eps: 0.543934
 2281216/6000000: episode: 3161, duration: 13.365s, episode steps: 634, steps per second:  47, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.120 [0.000, 5.000],  loss: 0.013276, mae: 2.224535, mean_q: 2.677566, mean_eps: 0.543820
 2281918/6000000: episode: 3162, duration: 14.691s, episode steps: 702, steps per second:  48, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.014283, mae: 2.228343, mean_q: 2.680147, mean_eps: 0.543687
 2282567/6000000: episode: 3163, duration: 13.926s, episode steps: 649, steps per second:  47, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.978 [0.000, 5.000],  loss: 0.013286, mae: 2.236452, mean_q: 2.691072, mean_eps: 0.543552
 2283358/6000000: episode: 3164, duration: 17.010s, episode steps: 791, steps per second:  47, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.012843, mae: 2.224044, mean_q: 2.675011, mean_eps: 0.543408
 2284372/6000000: episode: 3165, duration: 22.621s, episode steps: 1014, steps per second:  45, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.160 [0.000, 5.000],  loss: 0.012585, mae: 2.226334, mean_q: 2.679363, mean_eps: 0.543227
 2285003/6000000: episode: 3166, duration: 14.541s, episode steps: 631, steps per second:  43, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.013621, mae: 2.226992, mean_q: 2.677826, mean_eps: 0.543063
 2285996/6000000: episode: 3167, duration: 23.782s, episode steps: 993, steps per second:  42, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.013408, mae: 2.214671, mean_q: 2.666841, mean_eps: 0.542900
 2287143/6000000: episode: 3168, duration: 24.776s, episode steps: 1147, steps per second:  46, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.252 [0.000, 5.000],  loss: 0.013050, mae: 2.217466, mean_q: 2.667585, mean_eps: 0.542686
 2288143/6000000: episode: 3169, duration: 23.145s, episode steps: 1000, steps per second:  43, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.013189, mae: 2.217124, mean_q: 2.669267, mean_eps: 0.542472
 2289142/6000000: episode: 3170, duration: 22.223s, episode steps: 999, steps per second:  45, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.013117, mae: 2.220586, mean_q: 2.673501, mean_eps: 0.542272
 2289948/6000000: episode: 3171, duration: 17.143s, episode steps: 806, steps per second:  47, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.012064, mae: 2.198691, mean_q: 2.646567, mean_eps: 0.542091
 2290771/6000000: episode: 3172, duration: 18.643s, episode steps: 823, steps per second:  44, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.012039, mae: 2.201480, mean_q: 2.649199, mean_eps: 0.541928
 2291562/6000000: episode: 3173, duration: 17.765s, episode steps: 791, steps per second:  45, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.300 [0.000, 5.000],  loss: 0.012953, mae: 2.207374, mean_q: 2.654734, mean_eps: 0.541767
 2291909/6000000: episode: 3174, duration: 7.649s, episode steps: 347, steps per second:  45, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.205 [0.000, 5.000],  loss: 0.014248, mae: 2.220246, mean_q: 2.670612, mean_eps: 0.541653
 2292696/6000000: episode: 3175, duration: 17.676s, episode steps: 787, steps per second:  45, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.032 [0.000, 5.000],  loss: 0.012674, mae: 2.189513, mean_q: 2.636278, mean_eps: 0.541540
 2293496/6000000: episode: 3176, duration: 18.271s, episode steps: 800, steps per second:  44, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.015278, mae: 2.196877, mean_q: 2.643275, mean_eps: 0.541381
 2295060/6000000: episode: 3177, duration: 33.351s, episode steps: 1564, steps per second:  47, episode reward: 32.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.130 [0.000, 5.000],  loss: 0.013140, mae: 2.203502, mean_q: 2.652079, mean_eps: 0.541145
 2296051/6000000: episode: 3178, duration: 20.913s, episode steps: 991, steps per second:  47, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.013208, mae: 2.179388, mean_q: 2.623158, mean_eps: 0.540889
 2296964/6000000: episode: 3179, duration: 19.256s, episode steps: 913, steps per second:  47, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.219 [0.000, 5.000],  loss: 0.013942, mae: 2.177258, mean_q: 2.620150, mean_eps: 0.540699
 2297812/6000000: episode: 3180, duration: 17.661s, episode steps: 848, steps per second:  48, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.926 [0.000, 5.000],  loss: 0.013419, mae: 2.162429, mean_q: 2.602773, mean_eps: 0.540523
 2298617/6000000: episode: 3181, duration: 16.887s, episode steps: 805, steps per second:  48, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.011795, mae: 2.171034, mean_q: 2.612675, mean_eps: 0.540357
 2299118/6000000: episode: 3182, duration: 10.777s, episode steps: 501, steps per second:  46, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.222 [0.000, 5.000],  loss: 0.014120, mae: 2.181333, mean_q: 2.623505, mean_eps: 0.540226
 2299697/6000000: episode: 3183, duration: 12.489s, episode steps: 579, steps per second:  46, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.012400, mae: 2.166835, mean_q: 2.606700, mean_eps: 0.540118
 2300355/6000000: episode: 3184, duration: 14.144s, episode steps: 658, steps per second:  47, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.059 [0.000, 5.000],  loss: 0.013445, mae: 2.193879, mean_q: 2.640498, mean_eps: 0.539995
 2300854/6000000: episode: 3185, duration: 10.480s, episode steps: 499, steps per second:  48, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.826 [0.000, 5.000],  loss: 0.013361, mae: 2.171409, mean_q: 2.612727, mean_eps: 0.539879
 2301442/6000000: episode: 3186, duration: 12.905s, episode steps: 588, steps per second:  46, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.013307, mae: 2.154228, mean_q: 2.591351, mean_eps: 0.539770
 2302855/6000000: episode: 3187, duration: 32.464s, episode steps: 1413, steps per second:  44, episode reward: 29.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.180 [0.000, 5.000],  loss: 0.012989, mae: 2.160580, mean_q: 2.600517, mean_eps: 0.539570
 2303498/6000000: episode: 3188, duration: 13.273s, episode steps: 643, steps per second:  48, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.012571, mae: 2.162674, mean_q: 2.604867, mean_eps: 0.539365
 2304619/6000000: episode: 3189, duration: 23.816s, episode steps: 1121, steps per second:  47, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.014020, mae: 2.160667, mean_q: 2.601526, mean_eps: 0.539188
 2305628/6000000: episode: 3190, duration: 21.503s, episode steps: 1009, steps per second:  47, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.746 [0.000, 5.000],  loss: 0.013182, mae: 2.180552, mean_q: 2.624500, mean_eps: 0.538976
 2306636/6000000: episode: 3191, duration: 21.373s, episode steps: 1008, steps per second:  47, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.012343, mae: 2.191210, mean_q: 2.637907, mean_eps: 0.538774
 2307257/6000000: episode: 3192, duration: 14.169s, episode steps: 621, steps per second:  44, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.012870, mae: 2.169315, mean_q: 2.608633, mean_eps: 0.538611
 2308245/6000000: episode: 3193, duration: 22.180s, episode steps: 988, steps per second:  45, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.177 [0.000, 5.000],  loss: 0.013491, mae: 2.188339, mean_q: 2.632921, mean_eps: 0.538450
 2308872/6000000: episode: 3194, duration: 14.119s, episode steps: 627, steps per second:  44, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.222 [0.000, 5.000],  loss: 0.011825, mae: 2.187998, mean_q: 2.635673, mean_eps: 0.538288
 2309365/6000000: episode: 3195, duration: 11.015s, episode steps: 493, steps per second:  45, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.011055, mae: 2.159426, mean_q: 2.599112, mean_eps: 0.538176
 2309747/6000000: episode: 3196, duration: 8.850s, episode steps: 382, steps per second:  43, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.864 [0.000, 5.000],  loss: 0.013056, mae: 2.189988, mean_q: 2.636538, mean_eps: 0.538089
 2310716/6000000: episode: 3197, duration: 21.273s, episode steps: 969, steps per second:  46, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.037 [0.000, 5.000],  loss: 0.014321, mae: 2.169922, mean_q: 2.610754, mean_eps: 0.537954
 2311554/6000000: episode: 3198, duration: 17.254s, episode steps: 838, steps per second:  49, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.202 [0.000, 5.000],  loss: 0.014257, mae: 2.169858, mean_q: 2.611467, mean_eps: 0.537773
 2312488/6000000: episode: 3199, duration: 20.149s, episode steps: 934, steps per second:  46, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.011597, mae: 2.181902, mean_q: 2.626629, mean_eps: 0.537596
 2313355/6000000: episode: 3200, duration: 19.022s, episode steps: 867, steps per second:  46, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.023 [0.000, 5.000],  loss: 0.012995, mae: 2.170346, mean_q: 2.612567, mean_eps: 0.537416
 2314192/6000000: episode: 3201, duration: 17.406s, episode steps: 837, steps per second:  48, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.878 [0.000, 5.000],  loss: 0.013639, mae: 2.187565, mean_q: 2.632451, mean_eps: 0.537246
 2314764/6000000: episode: 3202, duration: 11.884s, episode steps: 572, steps per second:  48, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.015773, mae: 2.188831, mean_q: 2.631857, mean_eps: 0.537105
 2316000/6000000: episode: 3203, duration: 26.845s, episode steps: 1236, steps per second:  46, episode reward: 30.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.239 [0.000, 5.000],  loss: 0.013843, mae: 2.180116, mean_q: 2.624142, mean_eps: 0.536924
 2316644/6000000: episode: 3204, duration: 14.992s, episode steps: 644, steps per second:  43, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.199 [0.000, 5.000],  loss: 0.013050, mae: 2.153335, mean_q: 2.593807, mean_eps: 0.536736
 2317050/6000000: episode: 3205, duration: 10.157s, episode steps: 406, steps per second:  40, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.995 [0.000, 5.000],  loss: 0.012538, mae: 2.168053, mean_q: 2.608554, mean_eps: 0.536631
 2317857/6000000: episode: 3206, duration: 20.625s, episode steps: 807, steps per second:  39, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.278 [0.000, 5.000],  loss: 0.012595, mae: 2.185022, mean_q: 2.628952, mean_eps: 0.536509
 2318485/6000000: episode: 3207, duration: 16.489s, episode steps: 628, steps per second:  38, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.013411, mae: 2.197493, mean_q: 2.643629, mean_eps: 0.536366
 2319396/6000000: episode: 3208, duration: 19.836s, episode steps: 911, steps per second:  46, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.216 [0.000, 5.000],  loss: 0.013813, mae: 2.177651, mean_q: 2.621642, mean_eps: 0.536212
 2320086/6000000: episode: 3209, duration: 15.557s, episode steps: 690, steps per second:  44, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.013856, mae: 2.169841, mean_q: 2.611057, mean_eps: 0.536052
 2321117/6000000: episode: 3210, duration: 22.932s, episode steps: 1031, steps per second:  45, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.111 [0.000, 5.000],  loss: 0.013695, mae: 2.199417, mean_q: 2.645888, mean_eps: 0.535880
 2322124/6000000: episode: 3211, duration: 21.939s, episode steps: 1007, steps per second:  46, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.013869, mae: 2.205023, mean_q: 2.653993, mean_eps: 0.535676
 2322845/6000000: episode: 3212, duration: 15.708s, episode steps: 721, steps per second:  46, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.011276, mae: 2.194884, mean_q: 2.641985, mean_eps: 0.535503
 2323903/6000000: episode: 3213, duration: 24.344s, episode steps: 1058, steps per second:  43, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.013606, mae: 2.175881, mean_q: 2.619039, mean_eps: 0.535325
 2324811/6000000: episode: 3214, duration: 19.336s, episode steps: 908, steps per second:  47, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.012652, mae: 2.192959, mean_q: 2.639361, mean_eps: 0.535129
 2325448/6000000: episode: 3215, duration: 14.230s, episode steps: 637, steps per second:  45, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.012811, mae: 2.177721, mean_q: 2.621469, mean_eps: 0.534974
 2326071/6000000: episode: 3216, duration: 14.420s, episode steps: 623, steps per second:  43, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.207 [0.000, 5.000],  loss: 0.015035, mae: 2.180217, mean_q: 2.622530, mean_eps: 0.534848
 2327094/6000000: episode: 3217, duration: 22.488s, episode steps: 1023, steps per second:  45, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.012265, mae: 2.186123, mean_q: 2.630447, mean_eps: 0.534684
 2328041/6000000: episode: 3218, duration: 19.600s, episode steps: 947, steps per second:  48, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.014196, mae: 2.214260, mean_q: 2.662647, mean_eps: 0.534486
 2328431/6000000: episode: 3219, duration: 8.437s, episode steps: 390, steps per second:  46, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.008 [0.000, 5.000],  loss: 0.013649, mae: 2.169066, mean_q: 2.612038, mean_eps: 0.534353
 2329493/6000000: episode: 3220, duration: 22.518s, episode steps: 1062, steps per second:  47, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.013428, mae: 2.181733, mean_q: 2.625208, mean_eps: 0.534208
 2330045/6000000: episode: 3221, duration: 11.842s, episode steps: 552, steps per second:  47, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.012941, mae: 2.198499, mean_q: 2.646219, mean_eps: 0.534046
 2331080/6000000: episode: 3222, duration: 22.246s, episode steps: 1035, steps per second:  47, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.013363, mae: 2.157107, mean_q: 2.593815, mean_eps: 0.533888
 2331736/6000000: episode: 3223, duration: 14.073s, episode steps: 656, steps per second:  47, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.013752, mae: 2.145710, mean_q: 2.580130, mean_eps: 0.533719
 2332831/6000000: episode: 3224, duration: 23.328s, episode steps: 1095, steps per second:  47, episode reward: 31.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.013513, mae: 2.158158, mean_q: 2.596252, mean_eps: 0.533544
 2334110/6000000: episode: 3225, duration: 29.668s, episode steps: 1279, steps per second:  43, episode reward: 28.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.012535, mae: 2.151469, mean_q: 2.589324, mean_eps: 0.533306
 2335104/6000000: episode: 3226, duration: 23.097s, episode steps: 994, steps per second:  43, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.013370, mae: 2.162720, mean_q: 2.601956, mean_eps: 0.533079
 2335768/6000000: episode: 3227, duration: 14.419s, episode steps: 664, steps per second:  46, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.309 [0.000, 5.000],  loss: 0.011419, mae: 2.131282, mean_q: 2.565533, mean_eps: 0.532913
 2336464/6000000: episode: 3228, duration: 14.867s, episode steps: 696, steps per second:  47, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.228 [0.000, 5.000],  loss: 0.014073, mae: 2.141830, mean_q: 2.577926, mean_eps: 0.532777
 2337532/6000000: episode: 3229, duration: 23.410s, episode steps: 1068, steps per second:  46, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.138 [0.000, 5.000],  loss: 0.013638, mae: 2.148724, mean_q: 2.584306, mean_eps: 0.532601
 2338844/6000000: episode: 3230, duration: 27.913s, episode steps: 1312, steps per second:  47, episode reward: 31.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.013835, mae: 2.140909, mean_q: 2.575368, mean_eps: 0.532363
 2339642/6000000: episode: 3231, duration: 17.931s, episode steps: 798, steps per second:  45, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.011927, mae: 2.136112, mean_q: 2.571273, mean_eps: 0.532152
 2340770/6000000: episode: 3232, duration: 24.554s, episode steps: 1128, steps per second:  46, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.012742, mae: 2.135996, mean_q: 2.570975, mean_eps: 0.531959
 2341315/6000000: episode: 3233, duration: 11.716s, episode steps: 545, steps per second:  47, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.011606, mae: 2.147792, mean_q: 2.585421, mean_eps: 0.531792
 2342240/6000000: episode: 3234, duration: 20.565s, episode steps: 925, steps per second:  45, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.013934, mae: 2.129672, mean_q: 2.562824, mean_eps: 0.531645
 2342911/6000000: episode: 3235, duration: 15.227s, episode steps: 671, steps per second:  44, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.012138, mae: 2.138926, mean_q: 2.575273, mean_eps: 0.531485
 2343292/6000000: episode: 3236, duration: 8.603s, episode steps: 381, steps per second:  44, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.014288, mae: 2.138753, mean_q: 2.574409, mean_eps: 0.531380
 2343959/6000000: episode: 3237, duration: 13.895s, episode steps: 667, steps per second:  48, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.012779, mae: 2.122305, mean_q: 2.555095, mean_eps: 0.531275
 2344972/6000000: episode: 3238, duration: 20.778s, episode steps: 1013, steps per second:  49, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.028 [0.000, 5.000],  loss: 0.014209, mae: 2.142470, mean_q: 2.579354, mean_eps: 0.531107
 2345906/6000000: episode: 3239, duration: 19.438s, episode steps: 934, steps per second:  48, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.264 [0.000, 5.000],  loss: 0.012590, mae: 2.130949, mean_q: 2.565579, mean_eps: 0.530912
 2346576/6000000: episode: 3240, duration: 13.917s, episode steps: 670, steps per second:  48, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.012036, mae: 2.142059, mean_q: 2.578164, mean_eps: 0.530752
 2347485/6000000: episode: 3241, duration: 19.186s, episode steps: 909, steps per second:  47, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.227 [0.000, 5.000],  loss: 0.012767, mae: 2.138795, mean_q: 2.573877, mean_eps: 0.530594
 2348111/6000000: episode: 3242, duration: 13.275s, episode steps: 626, steps per second:  47, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.013128, mae: 2.146884, mean_q: 2.584170, mean_eps: 0.530440
 2348726/6000000: episode: 3243, duration: 13.826s, episode steps: 615, steps per second:  44, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.014555, mae: 2.136008, mean_q: 2.568842, mean_eps: 0.530316
 2349578/6000000: episode: 3244, duration: 19.450s, episode steps: 852, steps per second:  44, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.012246, mae: 2.133513, mean_q: 2.567983, mean_eps: 0.530170
 2350503/6000000: episode: 3245, duration: 23.074s, episode steps: 925, steps per second:  40, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.319 [0.000, 5.000],  loss: 0.013275, mae: 2.171264, mean_q: 2.612663, mean_eps: 0.529992
 2351317/6000000: episode: 3246, duration: 21.420s, episode steps: 814, steps per second:  38, episode reward: 27.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.012808, mae: 2.188540, mean_q: 2.636121, mean_eps: 0.529818
 2352035/6000000: episode: 3247, duration: 15.882s, episode steps: 718, steps per second:  45, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.063 [0.000, 5.000],  loss: 0.016670, mae: 2.181194, mean_q: 2.625620, mean_eps: 0.529665
 2352929/6000000: episode: 3248, duration: 20.911s, episode steps: 894, steps per second:  43, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.013698, mae: 2.171808, mean_q: 2.612565, mean_eps: 0.529504
 2353738/6000000: episode: 3249, duration: 18.557s, episode steps: 809, steps per second:  44, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.014295, mae: 2.173347, mean_q: 2.615134, mean_eps: 0.529333
 2354211/6000000: episode: 3250, duration: 10.082s, episode steps: 473, steps per second:  47, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.323 [0.000, 5.000],  loss: 0.014473, mae: 2.163042, mean_q: 2.603106, mean_eps: 0.529205
 2355317/6000000: episode: 3251, duration: 24.359s, episode steps: 1106, steps per second:  45, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 0.014252, mae: 2.166220, mean_q: 2.605938, mean_eps: 0.529047
 2356253/6000000: episode: 3252, duration: 21.909s, episode steps: 936, steps per second:  43, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.014489, mae: 2.176377, mean_q: 2.618008, mean_eps: 0.528843
 2357295/6000000: episode: 3253, duration: 22.886s, episode steps: 1042, steps per second:  46, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.013457, mae: 2.180377, mean_q: 2.623166, mean_eps: 0.528645
 2358102/6000000: episode: 3254, duration: 17.851s, episode steps: 807, steps per second:  45, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.012916, mae: 2.179196, mean_q: 2.623151, mean_eps: 0.528460
 2359085/6000000: episode: 3255, duration: 21.737s, episode steps: 983, steps per second:  45, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.923 [0.000, 5.000],  loss: 0.013052, mae: 2.182960, mean_q: 2.627689, mean_eps: 0.528281
 2359636/6000000: episode: 3256, duration: 11.969s, episode steps: 551, steps per second:  46, episode reward: 14.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.013180, mae: 2.165447, mean_q: 2.606425, mean_eps: 0.528128
 2360493/6000000: episode: 3257, duration: 17.302s, episode steps: 857, steps per second:  50, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.105 [0.000, 5.000],  loss: 0.012603, mae: 2.186490, mean_q: 2.631401, mean_eps: 0.527987
 2361511/6000000: episode: 3258, duration: 20.732s, episode steps: 1018, steps per second:  49, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.300 [0.000, 5.000],  loss: 0.012948, mae: 2.180198, mean_q: 2.623479, mean_eps: 0.527800
 2362135/6000000: episode: 3259, duration: 13.592s, episode steps: 624, steps per second:  46, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.012295, mae: 2.176231, mean_q: 2.618754, mean_eps: 0.527636
 2363057/6000000: episode: 3260, duration: 19.266s, episode steps: 922, steps per second:  48, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.013278, mae: 2.189028, mean_q: 2.636287, mean_eps: 0.527481
 2363972/6000000: episode: 3261, duration: 19.131s, episode steps: 915, steps per second:  48, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.011801, mae: 2.177375, mean_q: 2.622199, mean_eps: 0.527297
 2364373/6000000: episode: 3262, duration: 8.622s, episode steps: 401, steps per second:  47, episode reward: 10.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.843 [0.000, 5.000],  loss: 0.014162, mae: 2.159367, mean_q: 2.600116, mean_eps: 0.527166
 2365013/6000000: episode: 3263, duration: 14.214s, episode steps: 640, steps per second:  45, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.012682, mae: 2.185200, mean_q: 2.631577, mean_eps: 0.527061
 2365566/6000000: episode: 3264, duration: 12.466s, episode steps: 553, steps per second:  44, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.012350, mae: 2.213344, mean_q: 2.664874, mean_eps: 0.526942
 2366568/6000000: episode: 3265, duration: 22.975s, episode steps: 1002, steps per second:  44, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.245 [0.000, 5.000],  loss: 0.012271, mae: 2.193984, mean_q: 2.640209, mean_eps: 0.526787
 2367362/6000000: episode: 3266, duration: 18.348s, episode steps: 794, steps per second:  43, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.014548, mae: 2.203082, mean_q: 2.653236, mean_eps: 0.526607
 2367932/6000000: episode: 3267, duration: 12.803s, episode steps: 570, steps per second:  45, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.088 [0.000, 5.000],  loss: 0.014521, mae: 2.217895, mean_q: 2.671397, mean_eps: 0.526471
 2368765/6000000: episode: 3268, duration: 18.401s, episode steps: 833, steps per second:  45, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.211 [0.000, 5.000],  loss: 0.013192, mae: 2.209481, mean_q: 2.660783, mean_eps: 0.526330
 2369402/6000000: episode: 3269, duration: 14.114s, episode steps: 637, steps per second:  45, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.692 [0.000, 5.000],  loss: 0.013623, mae: 2.207810, mean_q: 2.656442, mean_eps: 0.526183
 2370391/6000000: episode: 3270, duration: 22.908s, episode steps: 989, steps per second:  43, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.013219, mae: 2.196508, mean_q: 2.643206, mean_eps: 0.526021
 2371330/6000000: episode: 3271, duration: 20.014s, episode steps: 939, steps per second:  47, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.351 [0.000, 5.000],  loss: 0.012155, mae: 2.186546, mean_q: 2.632074, mean_eps: 0.525828
 2371823/6000000: episode: 3272, duration: 10.602s, episode steps: 493, steps per second:  46, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.011585, mae: 2.187495, mean_q: 2.632472, mean_eps: 0.525685
 2372215/6000000: episode: 3273, duration: 8.884s, episode steps: 392, steps per second:  44, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.102 [0.000, 5.000],  loss: 0.014814, mae: 2.217141, mean_q: 2.667849, mean_eps: 0.525596
 2372925/6000000: episode: 3274, duration: 16.030s, episode steps: 710, steps per second:  44, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.014253, mae: 2.206691, mean_q: 2.657597, mean_eps: 0.525486
 2373420/6000000: episode: 3275, duration: 10.660s, episode steps: 495, steps per second:  46, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.764 [0.000, 5.000],  loss: 0.014856, mae: 2.198225, mean_q: 2.647434, mean_eps: 0.525366
 2374241/6000000: episode: 3276, duration: 17.783s, episode steps: 821, steps per second:  46, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.138 [0.000, 5.000],  loss: 0.012227, mae: 2.204072, mean_q: 2.653919, mean_eps: 0.525234
 2374922/6000000: episode: 3277, duration: 15.376s, episode steps: 681, steps per second:  44, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.014840, mae: 2.196138, mean_q: 2.644302, mean_eps: 0.525084
 2376008/6000000: episode: 3278, duration: 23.341s, episode steps: 1086, steps per second:  47, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.013738, mae: 2.226956, mean_q: 2.680842, mean_eps: 0.524907
 2376870/6000000: episode: 3279, duration: 17.166s, episode steps: 862, steps per second:  50, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.013968, mae: 2.210354, mean_q: 2.661736, mean_eps: 0.524712
 2377540/6000000: episode: 3280, duration: 13.857s, episode steps: 670, steps per second:  48, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.014987, mae: 2.213856, mean_q: 2.664348, mean_eps: 0.524559
 2378191/6000000: episode: 3281, duration: 13.542s, episode steps: 651, steps per second:  48, episode reward: 18.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.013617, mae: 2.192554, mean_q: 2.640363, mean_eps: 0.524427
 2378694/6000000: episode: 3282, duration: 10.816s, episode steps: 503, steps per second:  47, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.011073, mae: 2.201382, mean_q: 2.649381, mean_eps: 0.524312
 2379209/6000000: episode: 3283, duration: 10.910s, episode steps: 515, steps per second:  47, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.011206, mae: 2.222490, mean_q: 2.675922, mean_eps: 0.524210
 2380223/6000000: episode: 3284, duration: 21.519s, episode steps: 1014, steps per second:  47, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.012690, mae: 2.206430, mean_q: 2.654863, mean_eps: 0.524057
 2380865/6000000: episode: 3285, duration: 14.028s, episode steps: 642, steps per second:  46, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.014550, mae: 2.181059, mean_q: 2.624089, mean_eps: 0.523891
 2381418/6000000: episode: 3286, duration: 12.001s, episode steps: 553, steps per second:  46, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.015889, mae: 2.170759, mean_q: 2.611854, mean_eps: 0.523772
 2382252/6000000: episode: 3287, duration: 18.571s, episode steps: 834, steps per second:  45, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.012913, mae: 2.178420, mean_q: 2.622234, mean_eps: 0.523633
 2383020/6000000: episode: 3288, duration: 17.607s, episode steps: 768, steps per second:  44, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.013090, mae: 2.157853, mean_q: 2.597219, mean_eps: 0.523473
 2383901/6000000: episode: 3289, duration: 19.885s, episode steps: 881, steps per second:  44, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.098 [0.000, 5.000],  loss: 0.011923, mae: 2.170799, mean_q: 2.612757, mean_eps: 0.523308
 2384669/6000000: episode: 3290, duration: 16.926s, episode steps: 768, steps per second:  45, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.120 [0.000, 5.000],  loss: 0.013795, mae: 2.180141, mean_q: 2.624618, mean_eps: 0.523143
 2385193/6000000: episode: 3291, duration: 10.957s, episode steps: 524, steps per second:  48, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.742 [0.000, 5.000],  loss: 0.013839, mae: 2.177759, mean_q: 2.620692, mean_eps: 0.523014
 2385858/6000000: episode: 3292, duration: 14.419s, episode steps: 665, steps per second:  46, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.675 [0.000, 5.000],  loss: 0.012798, mae: 2.155696, mean_q: 2.594542, mean_eps: 0.522895
 2386505/6000000: episode: 3293, duration: 14.642s, episode steps: 647, steps per second:  44, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.189 [0.000, 5.000],  loss: 0.013351, mae: 2.183573, mean_q: 2.628249, mean_eps: 0.522764
 2387250/6000000: episode: 3294, duration: 16.702s, episode steps: 745, steps per second:  45, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.851 [0.000, 5.000],  loss: 0.013104, mae: 2.145206, mean_q: 2.581340, mean_eps: 0.522624
 2388074/6000000: episode: 3295, duration: 17.693s, episode steps: 824, steps per second:  47, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.191 [0.000, 5.000],  loss: 0.013354, mae: 2.168748, mean_q: 2.610243, mean_eps: 0.522468
 2388977/6000000: episode: 3296, duration: 21.323s, episode steps: 903, steps per second:  42, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.662 [0.000, 5.000],  loss: 0.012506, mae: 2.145230, mean_q: 2.582410, mean_eps: 0.522295
 2389672/6000000: episode: 3297, duration: 15.494s, episode steps: 695, steps per second:  45, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.268 [0.000, 5.000],  loss: 0.011568, mae: 2.165887, mean_q: 2.608187, mean_eps: 0.522135
 2390516/6000000: episode: 3298, duration: 18.075s, episode steps: 844, steps per second:  47, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.103 [0.000, 5.000],  loss: 0.012299, mae: 2.150100, mean_q: 2.589562, mean_eps: 0.521982
 2391295/6000000: episode: 3299, duration: 17.237s, episode steps: 779, steps per second:  45, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.055 [0.000, 5.000],  loss: 0.012721, mae: 2.158001, mean_q: 2.597467, mean_eps: 0.521819
 2392297/6000000: episode: 3300, duration: 22.212s, episode steps: 1002, steps per second:  45, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.013859, mae: 2.157981, mean_q: 2.597252, mean_eps: 0.521641
 2393024/6000000: episode: 3301, duration: 14.534s, episode steps: 727, steps per second:  50, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.014852, mae: 2.176806, mean_q: 2.619189, mean_eps: 0.521468
 2393524/6000000: episode: 3302, duration: 10.265s, episode steps: 500, steps per second:  49, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.012406, mae: 2.170846, mean_q: 2.611634, mean_eps: 0.521346
 2394506/6000000: episode: 3303, duration: 20.610s, episode steps: 982, steps per second:  48, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.013140, mae: 2.163876, mean_q: 2.604156, mean_eps: 0.521197
 2395497/6000000: episode: 3304, duration: 20.819s, episode steps: 991, steps per second:  48, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.201 [0.000, 5.000],  loss: 0.014417, mae: 2.155077, mean_q: 2.593320, mean_eps: 0.521000
 2396394/6000000: episode: 3305, duration: 18.464s, episode steps: 897, steps per second:  49, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.013326, mae: 2.137900, mean_q: 2.573310, mean_eps: 0.520811
 2397174/6000000: episode: 3306, duration: 16.247s, episode steps: 780, steps per second:  48, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.012798, mae: 2.151670, mean_q: 2.590561, mean_eps: 0.520643
 2397950/6000000: episode: 3307, duration: 16.173s, episode steps: 776, steps per second:  48, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.866 [0.000, 5.000],  loss: 0.015069, mae: 2.148145, mean_q: 2.585056, mean_eps: 0.520488
 2398773/6000000: episode: 3308, duration: 17.645s, episode steps: 823, steps per second:  47, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.306 [0.000, 5.000],  loss: 0.012093, mae: 2.148538, mean_q: 2.587983, mean_eps: 0.520328
 2399399/6000000: episode: 3309, duration: 13.231s, episode steps: 626, steps per second:  47, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.669 [0.000, 5.000],  loss: 0.013277, mae: 2.156632, mean_q: 2.593946, mean_eps: 0.520183
 2400287/6000000: episode: 3310, duration: 20.099s, episode steps: 888, steps per second:  44, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.150 [0.000, 5.000],  loss: 0.013392, mae: 2.164362, mean_q: 2.603661, mean_eps: 0.520032
 2400815/6000000: episode: 3311, duration: 11.822s, episode steps: 528, steps per second:  45, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.011881, mae: 2.175146, mean_q: 2.617875, mean_eps: 0.519890
 2401642/6000000: episode: 3312, duration: 17.526s, episode steps: 827, steps per second:  47, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.096 [0.000, 5.000],  loss: 0.012974, mae: 2.189137, mean_q: 2.634717, mean_eps: 0.519754
 2402268/6000000: episode: 3313, duration: 13.269s, episode steps: 626, steps per second:  47, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.204 [0.000, 5.000],  loss: 0.012792, mae: 2.187927, mean_q: 2.634091, mean_eps: 0.519609
 2403201/6000000: episode: 3314, duration: 19.766s, episode steps: 933, steps per second:  47, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.986 [0.000, 5.000],  loss: 0.012195, mae: 2.185984, mean_q: 2.633475, mean_eps: 0.519453
 2403723/6000000: episode: 3315, duration: 10.843s, episode steps: 522, steps per second:  48, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.676 [0.000, 5.000],  loss: 0.014548, mae: 2.185708, mean_q: 2.630686, mean_eps: 0.519308
 2404444/6000000: episode: 3316, duration: 14.832s, episode steps: 721, steps per second:  49, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 0.012325, mae: 2.185359, mean_q: 2.631682, mean_eps: 0.519184
 2405190/6000000: episode: 3317, duration: 15.440s, episode steps: 746, steps per second:  48, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.015359, mae: 2.190156, mean_q: 2.635311, mean_eps: 0.519037
 2405723/6000000: episode: 3318, duration: 11.740s, episode steps: 533, steps per second:  45, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.016376, mae: 2.214553, mean_q: 2.664454, mean_eps: 0.518909
 2406574/6000000: episode: 3319, duration: 18.613s, episode steps: 851, steps per second:  46, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.014119, mae: 2.216127, mean_q: 2.666872, mean_eps: 0.518770
 2407430/6000000: episode: 3320, duration: 18.584s, episode steps: 856, steps per second:  46, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.099 [0.000, 5.000],  loss: 0.012271, mae: 2.205976, mean_q: 2.654931, mean_eps: 0.518600
 2408315/6000000: episode: 3321, duration: 19.177s, episode steps: 885, steps per second:  46, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.014028, mae: 2.214879, mean_q: 2.664364, mean_eps: 0.518426
 2408741/6000000: episode: 3322, duration: 9.636s, episode steps: 426, steps per second:  44, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.317 [0.000, 5.000],  loss: 0.012408, mae: 2.206137, mean_q: 2.655544, mean_eps: 0.518294
 2409873/6000000: episode: 3323, duration: 23.852s, episode steps: 1132, steps per second:  47, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.012813, mae: 2.196561, mean_q: 2.644782, mean_eps: 0.518138
 2410536/6000000: episode: 3324, duration: 13.421s, episode steps: 663, steps per second:  49, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.113 [0.000, 5.000],  loss: 0.012555, mae: 2.223089, mean_q: 2.674806, mean_eps: 0.517959
 2411153/6000000: episode: 3325, duration: 12.731s, episode steps: 617, steps per second:  48, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.269 [0.000, 5.000],  loss: 0.015168, mae: 2.245220, mean_q: 2.702048, mean_eps: 0.517831
 2411912/6000000: episode: 3326, duration: 16.117s, episode steps: 759, steps per second:  47, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.153 [0.000, 5.000],  loss: 0.013855, mae: 2.223488, mean_q: 2.677109, mean_eps: 0.517694
 2412638/6000000: episode: 3327, duration: 15.464s, episode steps: 726, steps per second:  47, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.015551, mae: 2.230464, mean_q: 2.683954, mean_eps: 0.517545
 2413320/6000000: episode: 3328, duration: 14.665s, episode steps: 682, steps per second:  47, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.014273, mae: 2.235406, mean_q: 2.689395, mean_eps: 0.517404
 2414366/6000000: episode: 3329, duration: 23.140s, episode steps: 1046, steps per second:  45, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.991 [0.000, 5.000],  loss: 0.012806, mae: 2.224450, mean_q: 2.677983, mean_eps: 0.517232
 2415124/6000000: episode: 3330, duration: 16.412s, episode steps: 758, steps per second:  46, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.950 [0.000, 5.000],  loss: 0.013413, mae: 2.240740, mean_q: 2.701211, mean_eps: 0.517051
 2416091/6000000: episode: 3331, duration: 20.942s, episode steps: 967, steps per second:  46, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.179 [0.000, 5.000],  loss: 0.013869, mae: 2.217269, mean_q: 2.670681, mean_eps: 0.516879
 2416666/6000000: episode: 3332, duration: 13.532s, episode steps: 575, steps per second:  42, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.847 [0.000, 5.000],  loss: 0.012014, mae: 2.219604, mean_q: 2.671630, mean_eps: 0.516724
 2417338/6000000: episode: 3333, duration: 15.544s, episode steps: 672, steps per second:  43, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.900 [0.000, 5.000],  loss: 0.013562, mae: 2.219415, mean_q: 2.671785, mean_eps: 0.516600
 2418289/6000000: episode: 3334, duration: 20.510s, episode steps: 951, steps per second:  46, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.013564, mae: 2.208012, mean_q: 2.657735, mean_eps: 0.516437
 2418942/6000000: episode: 3335, duration: 14.023s, episode steps: 653, steps per second:  47, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.014327, mae: 2.221742, mean_q: 2.676068, mean_eps: 0.516277
 2419846/6000000: episode: 3336, duration: 21.755s, episode steps: 904, steps per second:  42, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.012 [0.000, 5.000],  loss: 0.012990, mae: 2.220961, mean_q: 2.675172, mean_eps: 0.516121
 2420386/6000000: episode: 3337, duration: 12.350s, episode steps: 540, steps per second:  44, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.056 [0.000, 5.000],  loss: 0.013043, mae: 2.239626, mean_q: 2.698223, mean_eps: 0.515977
 2420988/6000000: episode: 3338, duration: 13.533s, episode steps: 602, steps per second:  44, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.015462, mae: 2.213456, mean_q: 2.664315, mean_eps: 0.515863
 2421895/6000000: episode: 3339, duration: 21.211s, episode steps: 907, steps per second:  43, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.170 [0.000, 5.000],  loss: 0.015241, mae: 2.224956, mean_q: 2.678534, mean_eps: 0.515712
 2423193/6000000: episode: 3340, duration: 28.944s, episode steps: 1298, steps per second:  45, episode reward: 33.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.171 [0.000, 5.000],  loss: 0.014102, mae: 2.228684, mean_q: 2.683242, mean_eps: 0.515491
 2424523/6000000: episode: 3341, duration: 28.631s, episode steps: 1330, steps per second:  46, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.151 [0.000, 5.000],  loss: 0.013399, mae: 2.212543, mean_q: 2.663508, mean_eps: 0.515228
 2425154/6000000: episode: 3342, duration: 13.927s, episode steps: 631, steps per second:  45, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.008 [0.000, 5.000],  loss: 0.013491, mae: 2.226059, mean_q: 2.682151, mean_eps: 0.515032
 2425653/6000000: episode: 3343, duration: 10.758s, episode steps: 499, steps per second:  46, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.094 [0.000, 5.000],  loss: 0.014963, mae: 2.227302, mean_q: 2.682451, mean_eps: 0.514919
 2426302/6000000: episode: 3344, duration: 12.919s, episode steps: 649, steps per second:  50, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.013198, mae: 2.216709, mean_q: 2.668562, mean_eps: 0.514804
 2427044/6000000: episode: 3345, duration: 15.149s, episode steps: 742, steps per second:  49, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.014434, mae: 2.212952, mean_q: 2.663983, mean_eps: 0.514666
 2427664/6000000: episode: 3346, duration: 13.952s, episode steps: 620, steps per second:  44, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 0.015576, mae: 2.215566, mean_q: 2.668696, mean_eps: 0.514530
 2428360/6000000: episode: 3347, duration: 14.759s, episode steps: 696, steps per second:  47, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.013208, mae: 2.211438, mean_q: 2.663937, mean_eps: 0.514398
 2429414/6000000: episode: 3348, duration: 21.357s, episode steps: 1054, steps per second:  49, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.015112, mae: 2.225483, mean_q: 2.678826, mean_eps: 0.514223
 2430063/6000000: episode: 3349, duration: 13.619s, episode steps: 649, steps per second:  48, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.013312, mae: 2.228476, mean_q: 2.683068, mean_eps: 0.514052
 2430851/6000000: episode: 3350, duration: 16.405s, episode steps: 788, steps per second:  48, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.013059, mae: 2.223713, mean_q: 2.678154, mean_eps: 0.513909
 2431493/6000000: episode: 3351, duration: 13.872s, episode steps: 642, steps per second:  46, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.026 [0.000, 5.000],  loss: 0.013609, mae: 2.250248, mean_q: 2.710641, mean_eps: 0.513766
 2432138/6000000: episode: 3352, duration: 13.551s, episode steps: 645, steps per second:  48, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.037 [0.000, 5.000],  loss: 0.013219, mae: 2.240021, mean_q: 2.696511, mean_eps: 0.513637
 2432645/6000000: episode: 3353, duration: 10.929s, episode steps: 507, steps per second:  46, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.034 [0.000, 5.000],  loss: 0.013639, mae: 2.245929, mean_q: 2.704287, mean_eps: 0.513522
 2433532/6000000: episode: 3354, duration: 20.367s, episode steps: 887, steps per second:  44, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.015 [0.000, 5.000],  loss: 0.014382, mae: 2.255157, mean_q: 2.714238, mean_eps: 0.513382
 2434208/6000000: episode: 3355, duration: 15.908s, episode steps: 676, steps per second:  42, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.999 [0.000, 5.000],  loss: 0.016146, mae: 2.230156, mean_q: 2.686259, mean_eps: 0.513226
 2435031/6000000: episode: 3356, duration: 17.885s, episode steps: 823, steps per second:  46, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.115 [0.000, 5.000],  loss: 0.014778, mae: 2.236324, mean_q: 2.692455, mean_eps: 0.513076
 2435786/6000000: episode: 3357, duration: 16.956s, episode steps: 755, steps per second:  45, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.013420, mae: 2.234386, mean_q: 2.691383, mean_eps: 0.512918
 2436617/6000000: episode: 3358, duration: 18.862s, episode steps: 831, steps per second:  44, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.013274, mae: 2.262952, mean_q: 2.726078, mean_eps: 0.512760
 2437437/6000000: episode: 3359, duration: 17.424s, episode steps: 820, steps per second:  47, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.183 [0.000, 5.000],  loss: 0.014053, mae: 2.263661, mean_q: 2.726420, mean_eps: 0.512594
 2437921/6000000: episode: 3360, duration: 10.481s, episode steps: 484, steps per second:  46, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.013929, mae: 2.258346, mean_q: 2.720824, mean_eps: 0.512464
 2438925/6000000: episode: 3361, duration: 23.236s, episode steps: 1004, steps per second:  43, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.014169, mae: 2.261620, mean_q: 2.722579, mean_eps: 0.512315
 2439834/6000000: episode: 3362, duration: 20.481s, episode steps: 909, steps per second:  44, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 0.012500, mae: 2.254440, mean_q: 2.715276, mean_eps: 0.512124
 2440521/6000000: episode: 3363, duration: 15.030s, episode steps: 687, steps per second:  46, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.014063, mae: 2.247384, mean_q: 2.705579, mean_eps: 0.511964
 2440931/6000000: episode: 3364, duration: 9.196s, episode steps: 410, steps per second:  45, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.014202, mae: 2.256205, mean_q: 2.716755, mean_eps: 0.511855
 2441757/6000000: episode: 3365, duration: 18.132s, episode steps: 826, steps per second:  46, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.014533, mae: 2.258853, mean_q: 2.718863, mean_eps: 0.511731
 2442774/6000000: episode: 3366, duration: 20.975s, episode steps: 1017, steps per second:  48, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.159 [0.000, 5.000],  loss: 0.014736, mae: 2.273201, mean_q: 2.737086, mean_eps: 0.511547
 2443477/6000000: episode: 3367, duration: 14.170s, episode steps: 703, steps per second:  50, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.012714, mae: 2.248716, mean_q: 2.707021, mean_eps: 0.511375
 2444295/6000000: episode: 3368, duration: 16.785s, episode steps: 818, steps per second:  49, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.013854, mae: 2.247036, mean_q: 2.704327, mean_eps: 0.511223
 2445261/6000000: episode: 3369, duration: 20.700s, episode steps: 966, steps per second:  47, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.708 [0.000, 5.000],  loss: 0.014254, mae: 2.262144, mean_q: 2.723345, mean_eps: 0.511044
 2445928/6000000: episode: 3370, duration: 13.665s, episode steps: 667, steps per second:  49, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.252 [0.000, 5.000],  loss: 0.015640, mae: 2.289204, mean_q: 2.755502, mean_eps: 0.510881
 2447078/6000000: episode: 3371, duration: 24.099s, episode steps: 1150, steps per second:  48, episode reward: 31.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.152 [0.000, 5.000],  loss: 0.015255, mae: 2.263027, mean_q: 2.724627, mean_eps: 0.510700
 2447849/6000000: episode: 3372, duration: 16.330s, episode steps: 771, steps per second:  47, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.244 [0.000, 5.000],  loss: 0.014634, mae: 2.261781, mean_q: 2.722460, mean_eps: 0.510507
 2448740/6000000: episode: 3373, duration: 19.359s, episode steps: 891, steps per second:  46, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.312 [0.000, 5.000],  loss: 0.013740, mae: 2.272284, mean_q: 2.736285, mean_eps: 0.510341
 2449704/6000000: episode: 3374, duration: 22.571s, episode steps: 964, steps per second:  43, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.284 [0.000, 5.000],  loss: 0.012518, mae: 2.273340, mean_q: 2.739011, mean_eps: 0.510156
 2450163/6000000: episode: 3375, duration: 11.851s, episode steps: 459, steps per second:  39, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.967 [0.000, 5.000],  loss: 0.015102, mae: 2.279972, mean_q: 2.745857, mean_eps: 0.510014
 2450887/6000000: episode: 3376, duration: 15.572s, episode steps: 724, steps per second:  46, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.269 [0.000, 5.000],  loss: 0.015316, mae: 2.314987, mean_q: 2.791149, mean_eps: 0.509895
 2451763/6000000: episode: 3377, duration: 19.117s, episode steps: 876, steps per second:  46, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.014010, mae: 2.299596, mean_q: 2.770911, mean_eps: 0.509735
 2452495/6000000: episode: 3378, duration: 16.136s, episode steps: 732, steps per second:  45, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.014150, mae: 2.302556, mean_q: 2.771260, mean_eps: 0.509574
 2452977/6000000: episode: 3379, duration: 10.467s, episode steps: 482, steps per second:  46, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.210 [0.000, 5.000],  loss: 0.013698, mae: 2.299600, mean_q: 2.768998, mean_eps: 0.509453
 2453510/6000000: episode: 3380, duration: 11.439s, episode steps: 533, steps per second:  47, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.015236, mae: 2.309919, mean_q: 2.782597, mean_eps: 0.509351
 2454656/6000000: episode: 3381, duration: 24.919s, episode steps: 1146, steps per second:  46, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.013123, mae: 2.310441, mean_q: 2.783370, mean_eps: 0.509184
 2455384/6000000: episode: 3382, duration: 17.606s, episode steps: 728, steps per second:  41, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.179 [0.000, 5.000],  loss: 0.013836, mae: 2.289374, mean_q: 2.756265, mean_eps: 0.508996
 2456075/6000000: episode: 3383, duration: 15.239s, episode steps: 691, steps per second:  45, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.658 [0.000, 5.000],  loss: 0.013107, mae: 2.308132, mean_q: 2.779707, mean_eps: 0.508854
 2456462/6000000: episode: 3384, duration: 8.528s, episode steps: 387, steps per second:  45, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.664 [0.000, 5.000],  loss: 0.015250, mae: 2.305284, mean_q: 2.778486, mean_eps: 0.508746
 2457006/6000000: episode: 3385, duration: 11.827s, episode steps: 544, steps per second:  46, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.908 [0.000, 5.000],  loss: 0.014341, mae: 2.305668, mean_q: 2.777640, mean_eps: 0.508653
 2457986/6000000: episode: 3386, duration: 21.299s, episode steps: 980, steps per second:  46, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.014528, mae: 2.309679, mean_q: 2.779907, mean_eps: 0.508501
 2458597/6000000: episode: 3387, duration: 12.881s, episode steps: 611, steps per second:  47, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.014115, mae: 2.315982, mean_q: 2.789608, mean_eps: 0.508342
 2459275/6000000: episode: 3388, duration: 13.646s, episode steps: 678, steps per second:  50, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.919 [0.000, 5.000],  loss: 0.014346, mae: 2.299334, mean_q: 2.768773, mean_eps: 0.508213
 2459821/6000000: episode: 3389, duration: 11.026s, episode steps: 546, steps per second:  50, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.012925, mae: 2.311084, mean_q: 2.783561, mean_eps: 0.508090
 2460333/6000000: episode: 3390, duration: 10.432s, episode steps: 512, steps per second:  49, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.014354, mae: 2.320721, mean_q: 2.795493, mean_eps: 0.507984
 2460707/6000000: episode: 3391, duration: 7.710s, episode steps: 374, steps per second:  49, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.015315, mae: 2.292739, mean_q: 2.761583, mean_eps: 0.507896
 2461099/6000000: episode: 3392, duration: 8.487s, episode steps: 392, steps per second:  46, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.013135, mae: 2.305516, mean_q: 2.777402, mean_eps: 0.507820
 2462028/6000000: episode: 3393, duration: 19.925s, episode steps: 929, steps per second:  47, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.068 [0.000, 5.000],  loss: 0.014051, mae: 2.313145, mean_q: 2.787199, mean_eps: 0.507688
 2462395/6000000: episode: 3394, duration: 7.779s, episode steps: 367, steps per second:  47, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 0.014641, mae: 2.300899, mean_q: 2.769379, mean_eps: 0.507558
 2463325/6000000: episode: 3395, duration: 18.973s, episode steps: 930, steps per second:  49, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.012456, mae: 2.303011, mean_q: 2.776142, mean_eps: 0.507428
 2463690/6000000: episode: 3396, duration: 7.452s, episode steps: 365, steps per second:  49, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.140 [0.000, 5.000],  loss: 0.013438, mae: 2.297963, mean_q: 2.767097, mean_eps: 0.507298
 2464657/6000000: episode: 3397, duration: 20.122s, episode steps: 967, steps per second:  48, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.057 [0.000, 5.000],  loss: 0.013239, mae: 2.301662, mean_q: 2.770894, mean_eps: 0.507165
 2465081/6000000: episode: 3398, duration: 9.078s, episode steps: 424, steps per second:  47, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.854 [0.000, 5.000],  loss: 0.015911, mae: 2.307112, mean_q: 2.782789, mean_eps: 0.507026
 2465853/6000000: episode: 3399, duration: 16.682s, episode steps: 772, steps per second:  46, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.177 [0.000, 5.000],  loss: 0.014313, mae: 2.325723, mean_q: 2.799182, mean_eps: 0.506906
 2466562/6000000: episode: 3400, duration: 16.686s, episode steps: 709, steps per second:  42, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.323 [0.000, 5.000],  loss: 0.014020, mae: 2.301814, mean_q: 2.771297, mean_eps: 0.506758
 2467601/6000000: episode: 3401, duration: 23.940s, episode steps: 1039, steps per second:  43, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.182 [0.000, 5.000],  loss: 0.015076, mae: 2.308013, mean_q: 2.778104, mean_eps: 0.506584
 2468527/6000000: episode: 3402, duration: 19.456s, episode steps: 926, steps per second:  48, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.013208, mae: 2.303225, mean_q: 2.774578, mean_eps: 0.506387
 2469467/6000000: episode: 3403, duration: 20.389s, episode steps: 940, steps per second:  46, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.014009, mae: 2.298967, mean_q: 2.768980, mean_eps: 0.506201
 2470391/6000000: episode: 3404, duration: 19.593s, episode steps: 924, steps per second:  47, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.139 [0.000, 5.000],  loss: 0.014338, mae: 2.303286, mean_q: 2.772607, mean_eps: 0.506014
 2471086/6000000: episode: 3405, duration: 14.735s, episode steps: 695, steps per second:  47, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.273 [0.000, 5.000],  loss: 0.013430, mae: 2.308222, mean_q: 2.777928, mean_eps: 0.505852
 2471734/6000000: episode: 3406, duration: 13.579s, episode steps: 648, steps per second:  48, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.952 [0.000, 5.000],  loss: 0.014236, mae: 2.307092, mean_q: 2.779637, mean_eps: 0.505718
 2472565/6000000: episode: 3407, duration: 18.263s, episode steps: 831, steps per second:  46, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.014238, mae: 2.296352, mean_q: 2.765397, mean_eps: 0.505570
 2472961/6000000: episode: 3408, duration: 8.591s, episode steps: 396, steps per second:  46, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.684 [0.000, 5.000],  loss: 0.013688, mae: 2.279403, mean_q: 2.743981, mean_eps: 0.505447
 2473901/6000000: episode: 3409, duration: 20.201s, episode steps: 940, steps per second:  47, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.056 [0.000, 5.000],  loss: 0.013279, mae: 2.291122, mean_q: 2.758594, mean_eps: 0.505314
 2474717/6000000: episode: 3410, duration: 18.147s, episode steps: 816, steps per second:  45, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.950 [0.000, 5.000],  loss: 0.013254, mae: 2.300881, mean_q: 2.770180, mean_eps: 0.505138
 2475232/6000000: episode: 3411, duration: 11.493s, episode steps: 515, steps per second:  45, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.008 [0.000, 5.000],  loss: 0.015037, mae: 2.335183, mean_q: 2.810660, mean_eps: 0.505005
 2476122/6000000: episode: 3412, duration: 18.446s, episode steps: 890, steps per second:  48, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.014971, mae: 2.325386, mean_q: 2.800492, mean_eps: 0.504865
 2476780/6000000: episode: 3413, duration: 13.250s, episode steps: 658, steps per second:  50, episode reward: 18.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.014697, mae: 2.343477, mean_q: 2.823025, mean_eps: 0.504710
 2477315/6000000: episode: 3414, duration: 11.494s, episode steps: 535, steps per second:  47, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: 0.017407, mae: 2.327816, mean_q: 2.804196, mean_eps: 0.504591
 2477921/6000000: episode: 3415, duration: 13.348s, episode steps: 606, steps per second:  45, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.013987, mae: 2.331139, mean_q: 2.806295, mean_eps: 0.504476
 2478344/6000000: episode: 3416, duration: 9.015s, episode steps: 423, steps per second:  47, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.099 [0.000, 5.000],  loss: 0.013149, mae: 2.335471, mean_q: 2.811211, mean_eps: 0.504374
 2479036/6000000: episode: 3417, duration: 14.769s, episode steps: 692, steps per second:  47, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.013716, mae: 2.329520, mean_q: 2.803653, mean_eps: 0.504262
 2479885/6000000: episode: 3418, duration: 18.081s, episode steps: 849, steps per second:  47, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.256 [0.000, 5.000],  loss: 0.016442, mae: 2.329895, mean_q: 2.803737, mean_eps: 0.504108
 2480747/6000000: episode: 3419, duration: 18.528s, episode steps: 862, steps per second:  47, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.277 [0.000, 5.000],  loss: 0.014576, mae: 2.347183, mean_q: 2.826100, mean_eps: 0.503937
 2481499/6000000: episode: 3420, duration: 16.402s, episode steps: 752, steps per second:  46, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.097 [0.000, 5.000],  loss: 0.014206, mae: 2.371624, mean_q: 2.855091, mean_eps: 0.503776
 2482429/6000000: episode: 3421, duration: 20.091s, episode steps: 930, steps per second:  46, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.099 [0.000, 5.000],  loss: 0.013845, mae: 2.340219, mean_q: 2.818354, mean_eps: 0.503607
 2482942/6000000: episode: 3422, duration: 11.800s, episode steps: 513, steps per second:  43, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.261 [0.000, 5.000],  loss: 0.014441, mae: 2.341157, mean_q: 2.817105, mean_eps: 0.503463
 2483702/6000000: episode: 3423, duration: 18.341s, episode steps: 760, steps per second:  41, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.013882, mae: 2.347514, mean_q: 2.828464, mean_eps: 0.503336
 2484497/6000000: episode: 3424, duration: 17.140s, episode steps: 795, steps per second:  46, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.187 [0.000, 5.000],  loss: 0.015590, mae: 2.341287, mean_q: 2.818546, mean_eps: 0.503180
 2485138/6000000: episode: 3425, duration: 13.874s, episode steps: 641, steps per second:  46, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.014328, mae: 2.334303, mean_q: 2.809364, mean_eps: 0.503036
 2486623/6000000: episode: 3426, duration: 32.485s, episode steps: 1485, steps per second:  46, episode reward: 35.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.013717, mae: 2.317482, mean_q: 2.789539, mean_eps: 0.502824
 2487759/6000000: episode: 3427, duration: 23.437s, episode steps: 1136, steps per second:  48, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.013471, mae: 2.331650, mean_q: 2.807747, mean_eps: 0.502562
 2488651/6000000: episode: 3428, duration: 19.379s, episode steps: 892, steps per second:  46, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.013487, mae: 2.319080, mean_q: 2.792218, mean_eps: 0.502359
 2489334/6000000: episode: 3429, duration: 15.228s, episode steps: 683, steps per second:  45, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.204 [0.000, 5.000],  loss: 0.013558, mae: 2.336681, mean_q: 2.813414, mean_eps: 0.502202
 2490088/6000000: episode: 3430, duration: 16.584s, episode steps: 754, steps per second:  45, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.015168, mae: 2.331375, mean_q: 2.806044, mean_eps: 0.502058
 2490770/6000000: episode: 3431, duration: 14.902s, episode steps: 682, steps per second:  46, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.013542, mae: 2.323625, mean_q: 2.799427, mean_eps: 0.501914
 2491744/6000000: episode: 3432, duration: 21.253s, episode steps: 974, steps per second:  46, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: 0.014115, mae: 2.333865, mean_q: 2.812387, mean_eps: 0.501749
 2492681/6000000: episode: 3433, duration: 19.593s, episode steps: 937, steps per second:  48, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.014399, mae: 2.322502, mean_q: 2.795019, mean_eps: 0.501558
 2493713/6000000: episode: 3434, duration: 20.894s, episode steps: 1032, steps per second:  49, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.014722, mae: 2.320760, mean_q: 2.795972, mean_eps: 0.501360
 2494557/6000000: episode: 3435, duration: 18.281s, episode steps: 844, steps per second:  46, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.013941, mae: 2.331292, mean_q: 2.808965, mean_eps: 0.501173
 2495178/6000000: episode: 3436, duration: 13.019s, episode steps: 621, steps per second:  48, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.651 [0.000, 5.000],  loss: 0.014109, mae: 2.321715, mean_q: 2.796670, mean_eps: 0.501026
 2495854/6000000: episode: 3437, duration: 14.086s, episode steps: 676, steps per second:  48, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.713 [0.000, 5.000],  loss: 0.015371, mae: 2.327171, mean_q: 2.801311, mean_eps: 0.500897
 2496699/6000000: episode: 3438, duration: 16.858s, episode steps: 845, steps per second:  50, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.014391, mae: 2.347345, mean_q: 2.826314, mean_eps: 0.500745
 2497605/6000000: episode: 3439, duration: 19.418s, episode steps: 906, steps per second:  47, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.014640, mae: 2.334564, mean_q: 2.810086, mean_eps: 0.500570
 2498286/6000000: episode: 3440, duration: 14.511s, episode steps: 681, steps per second:  47, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.015673, mae: 2.329718, mean_q: 2.803666, mean_eps: 0.500411
 2498682/6000000: episode: 3441, duration: 8.920s, episode steps: 396, steps per second:  44, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.013270, mae: 2.344595, mean_q: 2.821742, mean_eps: 0.500303
 2499743/6000000: episode: 3442, duration: 23.166s, episode steps: 1061, steps per second:  46, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.244 [0.000, 5.000],  loss: 0.015590, mae: 2.332359, mean_q: 2.807305, mean_eps: 0.500158
 2500329/6000000: episode: 3443, duration: 13.381s, episode steps: 586, steps per second:  44, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.014867, mae: 2.343260, mean_q: 2.821260, mean_eps: 0.499993
 2501404/6000000: episode: 3444, duration: 23.705s, episode steps: 1075, steps per second:  45, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.015098, mae: 2.324335, mean_q: 2.800023, mean_eps: 0.499827
 2502249/6000000: episode: 3445, duration: 18.943s, episode steps: 845, steps per second:  45, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 0.014718, mae: 2.307972, mean_q: 2.780723, mean_eps: 0.499635
 2503171/6000000: episode: 3446, duration: 20.949s, episode steps: 922, steps per second:  44, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.115 [0.000, 5.000],  loss: 0.014791, mae: 2.304873, mean_q: 2.775657, mean_eps: 0.499458
 2503848/6000000: episode: 3447, duration: 15.783s, episode steps: 677, steps per second:  43, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.009 [0.000, 5.000],  loss: 0.014277, mae: 2.325432, mean_q: 2.799888, mean_eps: 0.499298
 2504670/6000000: episode: 3448, duration: 18.908s, episode steps: 822, steps per second:  43, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.012918, mae: 2.327609, mean_q: 2.803123, mean_eps: 0.499148
 2505193/6000000: episode: 3449, duration: 11.794s, episode steps: 523, steps per second:  44, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.012841, mae: 2.325629, mean_q: 2.799240, mean_eps: 0.499014
 2505621/6000000: episode: 3450, duration: 10.374s, episode steps: 428, steps per second:  41, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.017697, mae: 2.387911, mean_q: 2.875446, mean_eps: 0.498918
 2506073/6000000: episode: 3451, duration: 10.577s, episode steps: 452, steps per second:  43, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.015868, mae: 2.380426, mean_q: 2.868559, mean_eps: 0.498830
 2506618/6000000: episode: 3452, duration: 12.709s, episode steps: 545, steps per second:  43, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.018267, mae: 2.360740, mean_q: 2.839295, mean_eps: 0.498731
 2507520/6000000: episode: 3453, duration: 20.186s, episode steps: 902, steps per second:  45, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.248 [0.000, 5.000],  loss: 0.013346, mae: 2.347175, mean_q: 2.826595, mean_eps: 0.498586
 2508000/6000000: episode: 3454, duration: 11.352s, episode steps: 480, steps per second:  42, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.750 [0.000, 5.000],  loss: 0.013884, mae: 2.380070, mean_q: 2.864870, mean_eps: 0.498448
 2508932/6000000: episode: 3455, duration: 19.926s, episode steps: 932, steps per second:  47, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.244 [0.000, 5.000],  loss: 0.014350, mae: 2.359608, mean_q: 2.842091, mean_eps: 0.498307
 2509408/6000000: episode: 3456, duration: 9.914s, episode steps: 476, steps per second:  48, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.016313, mae: 2.356899, mean_q: 2.837731, mean_eps: 0.498166
 2510487/6000000: episode: 3457, duration: 22.697s, episode steps: 1079, steps per second:  48, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.043 [0.000, 5.000],  loss: 0.014191, mae: 2.357226, mean_q: 2.839170, mean_eps: 0.498011
 2511689/6000000: episode: 3458, duration: 25.356s, episode steps: 1202, steps per second:  47, episode reward: 30.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.014561, mae: 2.355786, mean_q: 2.837737, mean_eps: 0.497782
 2512215/6000000: episode: 3459, duration: 10.728s, episode steps: 526, steps per second:  49, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.016113, mae: 2.377381, mean_q: 2.863550, mean_eps: 0.497610
 2512780/6000000: episode: 3460, duration: 11.485s, episode steps: 565, steps per second:  49, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.981 [0.000, 5.000],  loss: 0.013473, mae: 2.364457, mean_q: 2.849205, mean_eps: 0.497501
 2513596/6000000: episode: 3461, duration: 17.105s, episode steps: 816, steps per second:  48, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.257 [0.000, 5.000],  loss: 0.012734, mae: 2.356019, mean_q: 2.837070, mean_eps: 0.497363
 2514561/6000000: episode: 3462, duration: 20.222s, episode steps: 965, steps per second:  48, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.008 [0.000, 5.000],  loss: 0.013341, mae: 2.366635, mean_q: 2.850338, mean_eps: 0.497184
 2515068/6000000: episode: 3463, duration: 11.265s, episode steps: 507, steps per second:  45, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.012262, mae: 2.349578, mean_q: 2.832901, mean_eps: 0.497037
 2515878/6000000: episode: 3464, duration: 18.036s, episode steps: 810, steps per second:  45, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.015201, mae: 2.407411, mean_q: 2.898819, mean_eps: 0.496906
 2516364/6000000: episode: 3465, duration: 11.080s, episode steps: 486, steps per second:  44, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.802 [0.000, 5.000],  loss: 0.014379, mae: 2.407166, mean_q: 2.898426, mean_eps: 0.496776
 2517176/6000000: episode: 3466, duration: 18.426s, episode steps: 812, steps per second:  44, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.089 [0.000, 5.000],  loss: 0.015078, mae: 2.419229, mean_q: 2.915488, mean_eps: 0.496646
 2517755/6000000: episode: 3467, duration: 12.627s, episode steps: 579, steps per second:  46, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.264 [0.000, 5.000],  loss: 0.014534, mae: 2.423120, mean_q: 2.917513, mean_eps: 0.496507
 2518670/6000000: episode: 3468, duration: 19.944s, episode steps: 915, steps per second:  46, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.015714, mae: 2.415829, mean_q: 2.908091, mean_eps: 0.496358
 2519698/6000000: episode: 3469, duration: 22.083s, episode steps: 1028, steps per second:  47, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.014389, mae: 2.416839, mean_q: 2.909360, mean_eps: 0.496163
 2520642/6000000: episode: 3470, duration: 19.437s, episode steps: 944, steps per second:  49, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.969 [0.000, 5.000],  loss: 0.014620, mae: 2.405815, mean_q: 2.895374, mean_eps: 0.495966
 2521565/6000000: episode: 3471, duration: 20.120s, episode steps: 923, steps per second:  46, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.015675, mae: 2.402146, mean_q: 2.892669, mean_eps: 0.495779
 2522437/6000000: episode: 3472, duration: 19.244s, episode steps: 872, steps per second:  45, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.204 [0.000, 5.000],  loss: 0.015704, mae: 2.431738, mean_q: 2.926003, mean_eps: 0.495600
 2523388/6000000: episode: 3473, duration: 21.065s, episode steps: 951, steps per second:  45, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.015137, mae: 2.409863, mean_q: 2.901909, mean_eps: 0.495418
 2523951/6000000: episode: 3474, duration: 12.512s, episode steps: 563, steps per second:  45, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.171 [0.000, 5.000],  loss: 0.014288, mae: 2.404204, mean_q: 2.895443, mean_eps: 0.495266
 2524782/6000000: episode: 3475, duration: 18.841s, episode steps: 831, steps per second:  44, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.016418, mae: 2.419799, mean_q: 2.913099, mean_eps: 0.495127
 2525378/6000000: episode: 3476, duration: 13.002s, episode steps: 596, steps per second:  46, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.016545, mae: 2.423949, mean_q: 2.916400, mean_eps: 0.494984
 2526365/6000000: episode: 3477, duration: 20.802s, episode steps: 987, steps per second:  47, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 0.014705, mae: 2.416240, mean_q: 2.908313, mean_eps: 0.494826
 2527071/6000000: episode: 3478, duration: 15.932s, episode steps: 706, steps per second:  44, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.848 [0.000, 5.000],  loss: 0.014195, mae: 2.401238, mean_q: 2.890990, mean_eps: 0.494656
 2527814/6000000: episode: 3479, duration: 17.525s, episode steps: 743, steps per second:  42, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.650 [0.000, 5.000],  loss: 0.014404, mae: 2.398802, mean_q: 2.887222, mean_eps: 0.494512
 2528423/6000000: episode: 3480, duration: 13.429s, episode steps: 609, steps per second:  45, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.015072, mae: 2.419538, mean_q: 2.913688, mean_eps: 0.494376
 2529363/6000000: episode: 3481, duration: 20.573s, episode steps: 940, steps per second:  46, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.014707, mae: 2.412622, mean_q: 2.904242, mean_eps: 0.494222
 2529938/6000000: episode: 3482, duration: 12.078s, episode steps: 575, steps per second:  48, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.013507, mae: 2.409710, mean_q: 2.899721, mean_eps: 0.494070
 2530882/6000000: episode: 3483, duration: 20.634s, episode steps: 944, steps per second:  46, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.997 [0.000, 5.000],  loss: 0.014357, mae: 2.426005, mean_q: 2.919376, mean_eps: 0.493918
 2531655/6000000: episode: 3484, duration: 16.830s, episode steps: 773, steps per second:  46, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.216 [0.000, 5.000],  loss: 0.015445, mae: 2.414172, mean_q: 2.906070, mean_eps: 0.493746
 2532035/6000000: episode: 3485, duration: 8.663s, episode steps: 380, steps per second:  44, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.534 [0.000, 5.000],  loss: 0.014769, mae: 2.414298, mean_q: 2.906155, mean_eps: 0.493631
 2532715/6000000: episode: 3486, duration: 14.811s, episode steps: 680, steps per second:  46, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.076 [0.000, 5.000],  loss: 0.015847, mae: 2.406479, mean_q: 2.895496, mean_eps: 0.493525
 2533470/6000000: episode: 3487, duration: 17.472s, episode steps: 755, steps per second:  43, episode reward: 23.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.013784, mae: 2.401265, mean_q: 2.892754, mean_eps: 0.493382
 2533961/6000000: episode: 3488, duration: 10.592s, episode steps: 491, steps per second:  46, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.666 [0.000, 5.000],  loss: 0.012258, mae: 2.433587, mean_q: 2.930967, mean_eps: 0.493257
 2534497/6000000: episode: 3489, duration: 11.609s, episode steps: 536, steps per second:  46, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.313 [0.000, 5.000],  loss: 0.014897, mae: 2.429497, mean_q: 2.928657, mean_eps: 0.493154
 2535469/6000000: episode: 3490, duration: 20.942s, episode steps: 972, steps per second:  46, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.164 [0.000, 5.000],  loss: 0.013774, mae: 2.419687, mean_q: 2.914737, mean_eps: 0.493003
 2536356/6000000: episode: 3491, duration: 19.833s, episode steps: 887, steps per second:  45, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.184 [0.000, 5.000],  loss: 0.013315, mae: 2.436667, mean_q: 2.933315, mean_eps: 0.492818
 2536901/6000000: episode: 3492, duration: 12.259s, episode steps: 545, steps per second:  44, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.014220, mae: 2.428030, mean_q: 2.922709, mean_eps: 0.492674
 2537781/6000000: episode: 3493, duration: 18.983s, episode steps: 880, steps per second:  46, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.016342, mae: 2.439928, mean_q: 2.935866, mean_eps: 0.492532
 2538789/6000000: episode: 3494, duration: 23.230s, episode steps: 1008, steps per second:  43, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.013242, mae: 2.436016, mean_q: 2.931448, mean_eps: 0.492343
 2539326/6000000: episode: 3495, duration: 12.157s, episode steps: 537, steps per second:  44, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.071 [0.000, 5.000],  loss: 0.015378, mae: 2.401218, mean_q: 2.890755, mean_eps: 0.492188
 2540355/6000000: episode: 3496, duration: 22.918s, episode steps: 1029, steps per second:  45, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.014720, mae: 2.429867, mean_q: 2.924249, mean_eps: 0.492032
 2541172/6000000: episode: 3497, duration: 18.522s, episode steps: 817, steps per second:  44, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.236 [0.000, 5.000],  loss: 0.014680, mae: 2.416098, mean_q: 2.907499, mean_eps: 0.491848
 2542235/6000000: episode: 3498, duration: 22.379s, episode steps: 1063, steps per second:  47, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.218 [0.000, 5.000],  loss: 0.015206, mae: 2.400696, mean_q: 2.888710, mean_eps: 0.491660
 2543068/6000000: episode: 3499, duration: 17.437s, episode steps: 833, steps per second:  48, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.789 [0.000, 5.000],  loss: 0.015740, mae: 2.424148, mean_q: 2.917253, mean_eps: 0.491470
 2544670/6000000: episode: 3500, duration: 34.697s, episode steps: 1602, steps per second:  46, episode reward: 34.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.109 [0.000, 5.000],  loss: 0.015664, mae: 2.410587, mean_q: 2.900707, mean_eps: 0.491226
 2545185/6000000: episode: 3501, duration: 10.690s, episode steps: 515, steps per second:  48, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.014638, mae: 2.421072, mean_q: 2.915342, mean_eps: 0.491014
 2545842/6000000: episode: 3502, duration: 13.781s, episode steps: 657, steps per second:  48, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.756 [0.000, 5.000],  loss: 0.016072, mae: 2.462406, mean_q: 2.963226, mean_eps: 0.490897
 2546667/6000000: episode: 3503, duration: 17.751s, episode steps: 825, steps per second:  46, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.017624, mae: 2.425379, mean_q: 2.922523, mean_eps: 0.490749
 2547489/6000000: episode: 3504, duration: 18.610s, episode steps: 822, steps per second:  44, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.013367, mae: 2.422786, mean_q: 2.916495, mean_eps: 0.490584
 2548462/6000000: episode: 3505, duration: 23.107s, episode steps: 973, steps per second:  42, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.201 [0.000, 5.000],  loss: 0.016013, mae: 2.426501, mean_q: 2.921164, mean_eps: 0.490405
 2549482/6000000: episode: 3506, duration: 23.605s, episode steps: 1020, steps per second:  43, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.291 [0.000, 5.000],  loss: 0.015165, mae: 2.438085, mean_q: 2.936100, mean_eps: 0.490206
 2549838/6000000: episode: 3507, duration: 8.263s, episode steps: 356, steps per second:  43, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.879 [0.000, 5.000],  loss: 0.017122, mae: 2.448250, mean_q: 2.944837, mean_eps: 0.490068
 2550386/6000000: episode: 3508, duration: 13.309s, episode steps: 548, steps per second:  41, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.186 [0.000, 5.000],  loss: 0.017859, mae: 2.440973, mean_q: 2.938257, mean_eps: 0.489978
 2550895/6000000: episode: 3509, duration: 11.443s, episode steps: 509, steps per second:  44, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.015206, mae: 2.422441, mean_q: 2.916770, mean_eps: 0.489872
 2551896/6000000: episode: 3510, duration: 22.567s, episode steps: 1001, steps per second:  44, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.855 [0.000, 5.000],  loss: 0.016561, mae: 2.414635, mean_q: 2.906244, mean_eps: 0.489721
 2552425/6000000: episode: 3511, duration: 11.143s, episode steps: 529, steps per second:  47, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.323 [0.000, 5.000],  loss: 0.014332, mae: 2.416766, mean_q: 2.910108, mean_eps: 0.489568
 2553967/6000000: episode: 3512, duration: 33.377s, episode steps: 1542, steps per second:  46, episode reward: 34.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.003 [0.000, 5.000],  loss: 0.015790, mae: 2.415045, mean_q: 2.907533, mean_eps: 0.489361
 2554897/6000000: episode: 3513, duration: 20.630s, episode steps: 930, steps per second:  45, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.060 [0.000, 5.000],  loss: 0.014774, mae: 2.425330, mean_q: 2.920051, mean_eps: 0.489114
 2555834/6000000: episode: 3514, duration: 19.970s, episode steps: 937, steps per second:  47, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.224 [0.000, 5.000],  loss: 0.015658, mae: 2.412261, mean_q: 2.901971, mean_eps: 0.488927
 2556524/6000000: episode: 3515, duration: 15.390s, episode steps: 690, steps per second:  45, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.216 [0.000, 5.000],  loss: 0.016137, mae: 2.405605, mean_q: 2.893784, mean_eps: 0.488764
 2557416/6000000: episode: 3516, duration: 19.453s, episode steps: 892, steps per second:  46, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.268 [0.000, 5.000],  loss: 0.016000, mae: 2.417649, mean_q: 2.910811, mean_eps: 0.488606
 2558377/6000000: episode: 3517, duration: 19.977s, episode steps: 961, steps per second:  48, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.216 [0.000, 5.000],  loss: 0.014562, mae: 2.422641, mean_q: 2.915518, mean_eps: 0.488421
 2558882/6000000: episode: 3518, duration: 10.021s, episode steps: 505, steps per second:  50, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.015456, mae: 2.418141, mean_q: 2.908837, mean_eps: 0.488274
 2559712/6000000: episode: 3519, duration: 17.048s, episode steps: 830, steps per second:  49, episode reward: 26.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.124 [0.000, 5.000],  loss: 0.015142, mae: 2.417880, mean_q: 2.909890, mean_eps: 0.488141
 2560397/6000000: episode: 3520, duration: 14.226s, episode steps: 685, steps per second:  48, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 0.014277, mae: 2.400032, mean_q: 2.888383, mean_eps: 0.487989
 2561406/6000000: episode: 3521, duration: 21.256s, episode steps: 1009, steps per second:  47, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.014319, mae: 2.402645, mean_q: 2.892659, mean_eps: 0.487820
 2562070/6000000: episode: 3522, duration: 14.291s, episode steps: 664, steps per second:  46, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.015900, mae: 2.414177, mean_q: 2.904609, mean_eps: 0.487652
 2563164/6000000: episode: 3523, duration: 23.483s, episode steps: 1094, steps per second:  47, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.015248, mae: 2.417872, mean_q: 2.909211, mean_eps: 0.487477
 2563895/6000000: episode: 3524, duration: 16.164s, episode steps: 731, steps per second:  45, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.014591, mae: 2.413535, mean_q: 2.905143, mean_eps: 0.487294
 2564865/6000000: episode: 3525, duration: 20.911s, episode steps: 970, steps per second:  46, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.015267, mae: 2.392604, mean_q: 2.879848, mean_eps: 0.487124
 2565519/6000000: episode: 3526, duration: 15.882s, episode steps: 654, steps per second:  41, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.885 [0.000, 5.000],  loss: 0.014456, mae: 2.394175, mean_q: 2.881280, mean_eps: 0.486962
 2566186/6000000: episode: 3527, duration: 15.644s, episode steps: 667, steps per second:  43, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.015119, mae: 2.405786, mean_q: 2.895857, mean_eps: 0.486830
 2567151/6000000: episode: 3528, duration: 21.278s, episode steps: 965, steps per second:  45, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.016058, mae: 2.420392, mean_q: 2.915984, mean_eps: 0.486666
 2567534/6000000: episode: 3529, duration: 8.715s, episode steps: 383, steps per second:  44, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.016127, mae: 2.382784, mean_q: 2.868713, mean_eps: 0.486532
 2568477/6000000: episode: 3530, duration: 21.294s, episode steps: 943, steps per second:  44, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.015647, mae: 2.409619, mean_q: 2.900976, mean_eps: 0.486399
 2569435/6000000: episode: 3531, duration: 21.035s, episode steps: 958, steps per second:  46, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.014487, mae: 2.416451, mean_q: 2.908001, mean_eps: 0.486209
 2570139/6000000: episode: 3532, duration: 15.283s, episode steps: 704, steps per second:  46, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.015158, mae: 2.403211, mean_q: 2.892490, mean_eps: 0.486043
 2571337/6000000: episode: 3533, duration: 28.000s, episode steps: 1198, steps per second:  43, episode reward: 30.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.119 [0.000, 5.000],  loss: 0.014690, mae: 2.397930, mean_q: 2.889045, mean_eps: 0.485852
 2571861/6000000: episode: 3534, duration: 11.645s, episode steps: 524, steps per second:  45, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.937 [0.000, 5.000],  loss: 0.015723, mae: 2.390143, mean_q: 2.877135, mean_eps: 0.485680
 2572859/6000000: episode: 3535, duration: 22.339s, episode steps: 998, steps per second:  45, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.939 [0.000, 5.000],  loss: 0.015655, mae: 2.397119, mean_q: 2.885177, mean_eps: 0.485528
 2573657/6000000: episode: 3536, duration: 18.830s, episode steps: 798, steps per second:  42, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.009 [0.000, 5.000],  loss: 0.014968, mae: 2.387714, mean_q: 2.875760, mean_eps: 0.485348
 2574599/6000000: episode: 3537, duration: 20.430s, episode steps: 942, steps per second:  46, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.061 [0.000, 5.000],  loss: 0.013332, mae: 2.375228, mean_q: 2.861658, mean_eps: 0.485174
 2575097/6000000: episode: 3538, duration: 11.425s, episode steps: 498, steps per second:  44, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.205 [0.000, 5.000],  loss: 0.015912, mae: 2.403519, mean_q: 2.894821, mean_eps: 0.485030
 2575765/6000000: episode: 3539, duration: 14.890s, episode steps: 668, steps per second:  45, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.016061, mae: 2.411779, mean_q: 2.900958, mean_eps: 0.484914
 2576986/6000000: episode: 3540, duration: 26.376s, episode steps: 1221, steps per second:  46, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.147 [0.000, 5.000],  loss: 0.015865, mae: 2.416615, mean_q: 2.909273, mean_eps: 0.484725
 2577839/6000000: episode: 3541, duration: 17.766s, episode steps: 853, steps per second:  48, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.179 [0.000, 5.000],  loss: 0.015939, mae: 2.432325, mean_q: 2.928860, mean_eps: 0.484518
 2578889/6000000: episode: 3542, duration: 21.738s, episode steps: 1050, steps per second:  48, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.016845, mae: 2.403800, mean_q: 2.892722, mean_eps: 0.484327
 2579612/6000000: episode: 3543, duration: 15.130s, episode steps: 723, steps per second:  48, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.296 [0.000, 5.000],  loss: 0.015976, mae: 2.404574, mean_q: 2.892596, mean_eps: 0.484150
 2580467/6000000: episode: 3544, duration: 17.935s, episode steps: 855, steps per second:  48, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.196 [0.000, 5.000],  loss: 0.013477, mae: 2.400144, mean_q: 2.889207, mean_eps: 0.483992
 2581106/6000000: episode: 3545, duration: 13.692s, episode steps: 639, steps per second:  47, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.014674, mae: 2.418255, mean_q: 2.911927, mean_eps: 0.483843
 2581956/6000000: episode: 3546, duration: 19.421s, episode steps: 850, steps per second:  44, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.642 [0.000, 5.000],  loss: 0.015060, mae: 2.408494, mean_q: 2.899863, mean_eps: 0.483694
 2582331/6000000: episode: 3547, duration: 9.182s, episode steps: 375, steps per second:  41, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.811 [0.000, 5.000],  loss: 0.016541, mae: 2.402462, mean_q: 2.892067, mean_eps: 0.483572
 2583177/6000000: episode: 3548, duration: 18.307s, episode steps: 846, steps per second:  46, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.186 [0.000, 5.000],  loss: 0.015311, mae: 2.400803, mean_q: 2.893569, mean_eps: 0.483449
 2584098/6000000: episode: 3549, duration: 19.662s, episode steps: 921, steps per second:  47, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.015754, mae: 2.418212, mean_q: 2.913699, mean_eps: 0.483272
 2584815/6000000: episode: 3550, duration: 16.267s, episode steps: 717, steps per second:  44, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.199 [0.000, 5.000],  loss: 0.015256, mae: 2.431166, mean_q: 2.928227, mean_eps: 0.483109
 2586092/6000000: episode: 3551, duration: 27.870s, episode steps: 1277, steps per second:  46, episode reward: 28.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.056 [0.000, 5.000],  loss: 0.015552, mae: 2.396949, mean_q: 2.889323, mean_eps: 0.482910
 2586750/6000000: episode: 3552, duration: 14.421s, episode steps: 658, steps per second:  46, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.845 [0.000, 5.000],  loss: 0.015871, mae: 2.420627, mean_q: 2.915167, mean_eps: 0.482716
 2587438/6000000: episode: 3553, duration: 15.660s, episode steps: 688, steps per second:  44, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.177 [0.000, 5.000],  loss: 0.013678, mae: 2.413504, mean_q: 2.907552, mean_eps: 0.482581
 2588051/6000000: episode: 3554, duration: 13.250s, episode steps: 613, steps per second:  46, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.013702, mae: 2.403343, mean_q: 2.895760, mean_eps: 0.482451
 2588896/6000000: episode: 3555, duration: 19.010s, episode steps: 845, steps per second:  44, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.121 [0.000, 5.000],  loss: 0.015104, mae: 2.403814, mean_q: 2.896777, mean_eps: 0.482306
 2589581/6000000: episode: 3556, duration: 15.350s, episode steps: 685, steps per second:  45, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.968 [0.000, 5.000],  loss: 0.015173, mae: 2.405404, mean_q: 2.898770, mean_eps: 0.482152
 2590247/6000000: episode: 3557, duration: 15.532s, episode steps: 666, steps per second:  43, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.013447, mae: 2.394028, mean_q: 2.886446, mean_eps: 0.482017
 2591647/6000000: episode: 3558, duration: 29.256s, episode steps: 1400, steps per second:  48, episode reward: 34.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.266 [0.000, 5.000],  loss: 0.015405, mae: 2.392156, mean_q: 2.882915, mean_eps: 0.481811
 2592418/6000000: episode: 3559, duration: 16.318s, episode steps: 771, steps per second:  47, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.014712, mae: 2.401465, mean_q: 2.892793, mean_eps: 0.481594
 2593276/6000000: episode: 3560, duration: 18.134s, episode steps: 858, steps per second:  47, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.936 [0.000, 5.000],  loss: 0.015842, mae: 2.403204, mean_q: 2.894527, mean_eps: 0.481431
 2594086/6000000: episode: 3561, duration: 17.277s, episode steps: 810, steps per second:  47, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.016948, mae: 2.412933, mean_q: 2.904939, mean_eps: 0.481264
 2594904/6000000: episode: 3562, duration: 16.999s, episode steps: 818, steps per second:  48, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.792 [0.000, 5.000],  loss: 0.016541, mae: 2.409294, mean_q: 2.901949, mean_eps: 0.481101
 2595776/6000000: episode: 3563, duration: 18.533s, episode steps: 872, steps per second:  47, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.034 [0.000, 5.000],  loss: 0.015176, mae: 2.410802, mean_q: 2.905281, mean_eps: 0.480932
 2596666/6000000: episode: 3564, duration: 20.138s, episode steps: 890, steps per second:  44, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.016440, mae: 2.435773, mean_q: 2.932229, mean_eps: 0.480756
 2597207/6000000: episode: 3565, duration: 12.160s, episode steps: 541, steps per second:  44, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.146 [0.000, 5.000],  loss: 0.016250, mae: 2.439978, mean_q: 2.937348, mean_eps: 0.480613
 2597963/6000000: episode: 3566, duration: 16.954s, episode steps: 756, steps per second:  45, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.016495, mae: 2.436909, mean_q: 2.933639, mean_eps: 0.480483
 2598862/6000000: episode: 3567, duration: 21.357s, episode steps: 899, steps per second:  42, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.016109, mae: 2.409539, mean_q: 2.901015, mean_eps: 0.480318
 2599333/6000000: episode: 3568, duration: 10.824s, episode steps: 471, steps per second:  44, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.015672, mae: 2.411207, mean_q: 2.900969, mean_eps: 0.480180
 2600309/6000000: episode: 3569, duration: 21.458s, episode steps: 976, steps per second:  45, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.222 [0.000, 5.000],  loss: 0.016231, mae: 2.414832, mean_q: 2.907652, mean_eps: 0.480036
 2601085/6000000: episode: 3570, duration: 16.876s, episode steps: 776, steps per second:  46, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.035 [0.000, 5.000],  loss: 0.014479, mae: 2.410302, mean_q: 2.901668, mean_eps: 0.479860
 2601710/6000000: episode: 3571, duration: 13.804s, episode steps: 625, steps per second:  45, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.014189, mae: 2.393251, mean_q: 2.880698, mean_eps: 0.479720
 2602701/6000000: episode: 3572, duration: 21.404s, episode steps: 991, steps per second:  46, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.013789, mae: 2.424223, mean_q: 2.919794, mean_eps: 0.479559
 2603515/6000000: episode: 3573, duration: 18.118s, episode steps: 814, steps per second:  45, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.017154, mae: 2.432052, mean_q: 2.928328, mean_eps: 0.479378
 2604285/6000000: episode: 3574, duration: 18.295s, episode steps: 770, steps per second:  42, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.334 [0.000, 5.000],  loss: 0.015395, mae: 2.427773, mean_q: 2.923911, mean_eps: 0.479220
 2605308/6000000: episode: 3575, duration: 22.809s, episode steps: 1023, steps per second:  45, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.014996, mae: 2.430719, mean_q: 2.926954, mean_eps: 0.479041
 2605913/6000000: episode: 3576, duration: 13.396s, episode steps: 605, steps per second:  45, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.258 [0.000, 5.000],  loss: 0.016159, mae: 2.455466, mean_q: 2.955738, mean_eps: 0.478878
 2606921/6000000: episode: 3577, duration: 21.653s, episode steps: 1008, steps per second:  47, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.103 [0.000, 5.000],  loss: 0.017248, mae: 2.447732, mean_q: 2.945663, mean_eps: 0.478716
 2607694/6000000: episode: 3578, duration: 15.644s, episode steps: 773, steps per second:  49, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.014504, mae: 2.456596, mean_q: 2.956609, mean_eps: 0.478538
 2608374/6000000: episode: 3579, duration: 13.903s, episode steps: 680, steps per second:  49, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.187 [0.000, 5.000],  loss: 0.014679, mae: 2.447277, mean_q: 2.945856, mean_eps: 0.478393
 2609269/6000000: episode: 3580, duration: 20.354s, episode steps: 895, steps per second:  44, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.192 [0.000, 5.000],  loss: 0.014848, mae: 2.469752, mean_q: 2.972359, mean_eps: 0.478236
 2609758/6000000: episode: 3581, duration: 11.243s, episode steps: 489, steps per second:  43, episode reward: 13.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.045 [0.000, 5.000],  loss: 0.016371, mae: 2.452381, mean_q: 2.955838, mean_eps: 0.478097
 2610864/6000000: episode: 3582, duration: 23.084s, episode steps: 1106, steps per second:  48, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.196 [0.000, 5.000],  loss: 0.016167, mae: 2.418209, mean_q: 2.911536, mean_eps: 0.477938
 2611566/6000000: episode: 3583, duration: 14.456s, episode steps: 702, steps per second:  49, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.970 [0.000, 5.000],  loss: 0.013317, mae: 2.446406, mean_q: 2.946487, mean_eps: 0.477757
 2612183/6000000: episode: 3584, duration: 13.157s, episode steps: 617, steps per second:  47, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.015802, mae: 2.428871, mean_q: 2.923253, mean_eps: 0.477625
 2613098/6000000: episode: 3585, duration: 19.643s, episode steps: 915, steps per second:  47, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.973 [0.000, 5.000],  loss: 0.015902, mae: 2.430656, mean_q: 2.925222, mean_eps: 0.477472
 2613828/6000000: episode: 3586, duration: 16.055s, episode steps: 730, steps per second:  45, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.153 [0.000, 5.000],  loss: 0.015051, mae: 2.427710, mean_q: 2.921670, mean_eps: 0.477308
 2614866/6000000: episode: 3587, duration: 25.589s, episode steps: 1038, steps per second:  41, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.262 [0.000, 5.000],  loss: 0.014681, mae: 2.431486, mean_q: 2.927090, mean_eps: 0.477131
 2615494/6000000: episode: 3588, duration: 14.668s, episode steps: 628, steps per second:  43, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.015288, mae: 2.464206, mean_q: 2.963555, mean_eps: 0.476964
 2616443/6000000: episode: 3589, duration: 20.718s, episode steps: 949, steps per second:  46, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.180 [0.000, 5.000],  loss: 0.017011, mae: 2.459136, mean_q: 2.960126, mean_eps: 0.476806
 2617073/6000000: episode: 3590, duration: 14.096s, episode steps: 630, steps per second:  45, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.044 [0.000, 5.000],  loss: 0.016446, mae: 2.466729, mean_q: 2.967701, mean_eps: 0.476648
 2617559/6000000: episode: 3591, duration: 11.089s, episode steps: 486, steps per second:  44, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.015965, mae: 2.461951, mean_q: 2.962470, mean_eps: 0.476537
 2618572/6000000: episode: 3592, duration: 21.949s, episode steps: 1013, steps per second:  46, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.015144, mae: 2.457738, mean_q: 2.957830, mean_eps: 0.476387
 2619515/6000000: episode: 3593, duration: 20.552s, episode steps: 943, steps per second:  46, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.236 [0.000, 5.000],  loss: 0.015269, mae: 2.469482, mean_q: 2.973387, mean_eps: 0.476192
 2620286/6000000: episode: 3594, duration: 17.013s, episode steps: 771, steps per second:  45, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.983 [0.000, 5.000],  loss: 0.015790, mae: 2.466793, mean_q: 2.969033, mean_eps: 0.476020
 2620953/6000000: episode: 3595, duration: 15.170s, episode steps: 667, steps per second:  44, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.016624, mae: 2.470882, mean_q: 2.976155, mean_eps: 0.475876
 2621667/6000000: episode: 3596, duration: 15.546s, episode steps: 714, steps per second:  46, episode reward: 21.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.686 [0.000, 5.000],  loss: 0.018098, mae: 2.474541, mean_q: 2.981010, mean_eps: 0.475738
 2622357/6000000: episode: 3597, duration: 15.269s, episode steps: 690, steps per second:  45, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.191 [0.000, 5.000],  loss: 0.018164, mae: 2.503098, mean_q: 3.015119, mean_eps: 0.475598
 2623284/6000000: episode: 3598, duration: 20.629s, episode steps: 927, steps per second:  45, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.179 [0.000, 5.000],  loss: 0.015310, mae: 2.473765, mean_q: 2.977825, mean_eps: 0.475436
 2623747/6000000: episode: 3599, duration: 9.562s, episode steps: 463, steps per second:  48, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.803 [0.000, 5.000],  loss: 0.013608, mae: 2.488041, mean_q: 2.996474, mean_eps: 0.475297
 2624484/6000000: episode: 3600, duration: 15.151s, episode steps: 737, steps per second:  49, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.917 [0.000, 5.000],  loss: 0.015938, mae: 2.475519, mean_q: 2.982212, mean_eps: 0.475177
 2625683/6000000: episode: 3601, duration: 26.076s, episode steps: 1199, steps per second:  46, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.063 [0.000, 5.000],  loss: 0.014247, mae: 2.489698, mean_q: 2.999267, mean_eps: 0.474984
 2626176/6000000: episode: 3602, duration: 10.476s, episode steps: 493, steps per second:  47, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.118 [0.000, 5.000],  loss: 0.019365, mae: 2.475358, mean_q: 2.979138, mean_eps: 0.474814
 2626982/6000000: episode: 3603, duration: 17.107s, episode steps: 806, steps per second:  47, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.268 [0.000, 5.000],  loss: 0.015299, mae: 2.470421, mean_q: 2.976096, mean_eps: 0.474684
 2627677/6000000: episode: 3604, duration: 15.274s, episode steps: 695, steps per second:  46, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.016555, mae: 2.472357, mean_q: 2.976448, mean_eps: 0.474534
 2628388/6000000: episode: 3605, duration: 15.641s, episode steps: 711, steps per second:  45, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.087 [0.000, 5.000],  loss: 0.016456, mae: 2.470660, mean_q: 2.976206, mean_eps: 0.474394
 2629394/6000000: episode: 3606, duration: 22.715s, episode steps: 1006, steps per second:  44, episode reward: 32.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.016117, mae: 2.474032, mean_q: 2.976814, mean_eps: 0.474222
 2630104/6000000: episode: 3607, duration: 16.839s, episode steps: 710, steps per second:  42, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.880 [0.000, 5.000],  loss: 0.015726, mae: 2.476080, mean_q: 2.981281, mean_eps: 0.474050
 2631060/6000000: episode: 3608, duration: 22.828s, episode steps: 956, steps per second:  42, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.922 [0.000, 5.000],  loss: 0.015774, mae: 2.475542, mean_q: 2.981052, mean_eps: 0.473884
 2631997/6000000: episode: 3609, duration: 21.476s, episode steps: 937, steps per second:  44, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.014688, mae: 2.462653, mean_q: 2.965356, mean_eps: 0.473694
 2632602/6000000: episode: 3610, duration: 13.473s, episode steps: 605, steps per second:  45, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.014928, mae: 2.474760, mean_q: 2.980656, mean_eps: 0.473540
 2633155/6000000: episode: 3611, duration: 12.122s, episode steps: 553, steps per second:  46, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.015033, mae: 2.482696, mean_q: 2.990026, mean_eps: 0.473424
 2634055/6000000: episode: 3612, duration: 19.625s, episode steps: 900, steps per second:  46, episode reward: 29.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: 0.014957, mae: 2.465077, mean_q: 2.967215, mean_eps: 0.473279
 2635020/6000000: episode: 3613, duration: 21.638s, episode steps: 965, steps per second:  45, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.016722, mae: 2.486899, mean_q: 2.992910, mean_eps: 0.473093
 2635771/6000000: episode: 3614, duration: 16.314s, episode steps: 751, steps per second:  46, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 0.015730, mae: 2.477329, mean_q: 2.980772, mean_eps: 0.472921
 2636615/6000000: episode: 3615, duration: 19.822s, episode steps: 844, steps per second:  43, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.014111, mae: 2.460518, mean_q: 2.962080, mean_eps: 0.472762
 2637323/6000000: episode: 3616, duration: 15.731s, episode steps: 708, steps per second:  45, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.014267, mae: 2.475597, mean_q: 2.979373, mean_eps: 0.472606
 2638368/6000000: episode: 3617, duration: 22.510s, episode steps: 1045, steps per second:  46, episode reward: 29.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.019 [0.000, 5.000],  loss: 0.015569, mae: 2.468304, mean_q: 2.972115, mean_eps: 0.472431
 2638996/6000000: episode: 3618, duration: 14.006s, episode steps: 628, steps per second:  45, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.014729, mae: 2.457047, mean_q: 2.957057, mean_eps: 0.472264
 2639365/6000000: episode: 3619, duration: 8.447s, episode steps: 369, steps per second:  44, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.981 [0.000, 5.000],  loss: 0.012546, mae: 2.457200, mean_q: 2.960146, mean_eps: 0.472164
 2640220/6000000: episode: 3620, duration: 17.926s, episode steps: 855, steps per second:  48, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.056 [0.000, 5.000],  loss: 0.015094, mae: 2.472008, mean_q: 2.976496, mean_eps: 0.472042
 2641021/6000000: episode: 3621, duration: 16.392s, episode steps: 801, steps per second:  49, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.015041, mae: 2.488861, mean_q: 2.999478, mean_eps: 0.471876
 2641459/6000000: episode: 3622, duration: 9.543s, episode steps: 438, steps per second:  46, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.015256, mae: 2.483786, mean_q: 2.992534, mean_eps: 0.471752
 2641956/6000000: episode: 3623, duration: 11.006s, episode steps: 497, steps per second:  45, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.732 [0.000, 5.000],  loss: 0.013679, mae: 2.486390, mean_q: 2.996000, mean_eps: 0.471659
 2643042/6000000: episode: 3624, duration: 22.540s, episode steps: 1086, steps per second:  48, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.800 [0.000, 5.000],  loss: 0.015329, mae: 2.486651, mean_q: 2.992755, mean_eps: 0.471500
 2643566/6000000: episode: 3625, duration: 10.891s, episode steps: 524, steps per second:  48, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.771 [0.000, 5.000],  loss: 0.015983, mae: 2.477069, mean_q: 2.981379, mean_eps: 0.471339
 2644419/6000000: episode: 3626, duration: 17.368s, episode steps: 853, steps per second:  49, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.014739, mae: 2.474332, mean_q: 2.978339, mean_eps: 0.471202
 2645588/6000000: episode: 3627, duration: 25.058s, episode steps: 1169, steps per second:  47, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.015053, mae: 2.472476, mean_q: 2.977481, mean_eps: 0.471000
 2646305/6000000: episode: 3628, duration: 15.294s, episode steps: 717, steps per second:  47, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.014964, mae: 2.457250, mean_q: 2.961143, mean_eps: 0.470811
 2646919/6000000: episode: 3629, duration: 13.372s, episode steps: 614, steps per second:  46, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.015109, mae: 2.447375, mean_q: 2.947770, mean_eps: 0.470678
 2647886/6000000: episode: 3630, duration: 22.317s, episode steps: 967, steps per second:  43, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.012729, mae: 2.458770, mean_q: 2.961445, mean_eps: 0.470520
 2648513/6000000: episode: 3631, duration: 13.964s, episode steps: 627, steps per second:  45, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.016261, mae: 2.461119, mean_q: 2.961198, mean_eps: 0.470360
 2649350/6000000: episode: 3632, duration: 18.366s, episode steps: 837, steps per second:  46, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.219 [0.000, 5.000],  loss: 0.015435, mae: 2.455077, mean_q: 2.955562, mean_eps: 0.470214
 2650147/6000000: episode: 3633, duration: 17.530s, episode steps: 797, steps per second:  45, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.014277, mae: 2.462147, mean_q: 2.966601, mean_eps: 0.470050
 2651384/6000000: episode: 3634, duration: 26.460s, episode steps: 1237, steps per second:  47, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.971 [0.000, 5.000],  loss: 0.014792, mae: 2.462286, mean_q: 2.964630, mean_eps: 0.469847
 2652395/6000000: episode: 3635, duration: 22.180s, episode steps: 1011, steps per second:  46, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.028 [0.000, 5.000],  loss: 0.015928, mae: 2.465284, mean_q: 2.969880, mean_eps: 0.469622
 2653330/6000000: episode: 3636, duration: 21.578s, episode steps: 935, steps per second:  43, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.013753, mae: 2.470185, mean_q: 2.974418, mean_eps: 0.469428
 2654086/6000000: episode: 3637, duration: 17.333s, episode steps: 756, steps per second:  44, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.014666, mae: 2.470440, mean_q: 2.974102, mean_eps: 0.469258
 2654944/6000000: episode: 3638, duration: 19.874s, episode steps: 858, steps per second:  43, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.016754, mae: 2.476865, mean_q: 2.982317, mean_eps: 0.469097
 2655885/6000000: episode: 3639, duration: 20.989s, episode steps: 941, steps per second:  45, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.015490, mae: 2.480343, mean_q: 2.986421, mean_eps: 0.468917
 2657581/6000000: episode: 3640, duration: 35.190s, episode steps: 1696, steps per second:  48, episode reward: 35.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.193 [0.000, 5.000],  loss: 0.015525, mae: 2.479091, mean_q: 2.984851, mean_eps: 0.468653
 2658538/6000000: episode: 3641, duration: 22.560s, episode steps: 957, steps per second:  42, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.319 [0.000, 5.000],  loss: 0.015629, mae: 2.477683, mean_q: 2.983049, mean_eps: 0.468388
 2659494/6000000: episode: 3642, duration: 21.008s, episode steps: 956, steps per second:  46, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.062 [0.000, 5.000],  loss: 0.014023, mae: 2.480366, mean_q: 2.988536, mean_eps: 0.468197
 2660167/6000000: episode: 3643, duration: 14.787s, episode steps: 673, steps per second:  46, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.186 [0.000, 5.000],  loss: 0.015126, mae: 2.472026, mean_q: 2.978282, mean_eps: 0.468034
 2660728/6000000: episode: 3644, duration: 12.058s, episode steps: 561, steps per second:  47, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.210 [0.000, 5.000],  loss: 0.014092, mae: 2.469831, mean_q: 2.975269, mean_eps: 0.467911
 2661254/6000000: episode: 3645, duration: 11.534s, episode steps: 526, steps per second:  46, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.015106, mae: 2.485133, mean_q: 2.989577, mean_eps: 0.467802
 2662048/6000000: episode: 3646, duration: 17.878s, episode steps: 794, steps per second:  44, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.647 [0.000, 5.000],  loss: 0.014937, mae: 2.487417, mean_q: 2.992163, mean_eps: 0.467670
 2662893/6000000: episode: 3647, duration: 19.802s, episode steps: 845, steps per second:  43, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.935 [0.000, 5.000],  loss: 0.015705, mae: 2.489324, mean_q: 2.997974, mean_eps: 0.467506
 2663754/6000000: episode: 3648, duration: 23.262s, episode steps: 861, steps per second:  37, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.014617, mae: 2.484313, mean_q: 2.992126, mean_eps: 0.467335
 2664510/6000000: episode: 3649, duration: 18.680s, episode steps: 756, steps per second:  40, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.042 [0.000, 5.000],  loss: 0.016029, mae: 2.475055, mean_q: 2.980459, mean_eps: 0.467174
 2664891/6000000: episode: 3650, duration: 8.957s, episode steps: 381, steps per second:  43, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.017033, mae: 2.502685, mean_q: 3.010307, mean_eps: 0.467060
 2665345/6000000: episode: 3651, duration: 10.402s, episode steps: 454, steps per second:  44, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.249 [0.000, 5.000],  loss: 0.013815, mae: 2.468352, mean_q: 2.970213, mean_eps: 0.466976
 2666500/6000000: episode: 3652, duration: 28.317s, episode steps: 1155, steps per second:  41, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.014843, mae: 2.470450, mean_q: 2.975227, mean_eps: 0.466816
 2667198/6000000: episode: 3653, duration: 16.388s, episode steps: 698, steps per second:  43, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.837 [0.000, 5.000],  loss: 0.014974, mae: 2.486548, mean_q: 2.993099, mean_eps: 0.466630
 2668009/6000000: episode: 3654, duration: 18.106s, episode steps: 811, steps per second:  45, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.017527, mae: 2.470228, mean_q: 2.972220, mean_eps: 0.466479
 2669089/6000000: episode: 3655, duration: 25.099s, episode steps: 1080, steps per second:  43, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.065 [0.000, 5.000],  loss: 0.017102, mae: 2.476576, mean_q: 2.979818, mean_eps: 0.466290
 2669948/6000000: episode: 3656, duration: 19.120s, episode steps: 859, steps per second:  45, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.129 [0.000, 5.000],  loss: 0.014537, mae: 2.477020, mean_q: 2.981336, mean_eps: 0.466096
 2670588/6000000: episode: 3657, duration: 14.223s, episode steps: 640, steps per second:  45, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.015201, mae: 2.507272, mean_q: 3.017105, mean_eps: 0.465947
 2671304/6000000: episode: 3658, duration: 16.523s, episode steps: 716, steps per second:  43, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.944 [0.000, 5.000],  loss: 0.013988, mae: 2.498660, mean_q: 3.008663, mean_eps: 0.465811
 2672804/6000000: episode: 3659, duration: 31.487s, episode steps: 1500, steps per second:  48, episode reward: 33.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.075 [0.000, 5.000],  loss: 0.014446, mae: 2.501536, mean_q: 3.010899, mean_eps: 0.465590
 2673542/6000000: episode: 3660, duration: 15.469s, episode steps: 738, steps per second:  48, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.163 [0.000, 5.000],  loss: 0.015213, mae: 2.485947, mean_q: 2.991858, mean_eps: 0.465366
 2673907/6000000: episode: 3661, duration: 8.088s, episode steps: 365, steps per second:  45, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.015199, mae: 2.483340, mean_q: 2.991055, mean_eps: 0.465255
 2674675/6000000: episode: 3662, duration: 16.387s, episode steps: 768, steps per second:  47, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.095 [0.000, 5.000],  loss: 0.014902, mae: 2.494066, mean_q: 3.002478, mean_eps: 0.465142
 2675392/6000000: episode: 3663, duration: 15.114s, episode steps: 717, steps per second:  47, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.148 [0.000, 5.000],  loss: 0.015693, mae: 2.503431, mean_q: 3.015063, mean_eps: 0.464994
 2675919/6000000: episode: 3664, duration: 11.652s, episode steps: 527, steps per second:  45, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.755 [0.000, 5.000],  loss: 0.017021, mae: 2.532203, mean_q: 3.047814, mean_eps: 0.464869
 2676431/6000000: episode: 3665, duration: 11.341s, episode steps: 512, steps per second:  45, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.015067, mae: 2.513267, mean_q: 3.024829, mean_eps: 0.464765
 2677146/6000000: episode: 3666, duration: 15.521s, episode steps: 715, steps per second:  46, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.015587, mae: 2.508390, mean_q: 3.018397, mean_eps: 0.464642
 2678097/6000000: episode: 3667, duration: 21.800s, episode steps: 951, steps per second:  44, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.152 [0.000, 5.000],  loss: 0.016788, mae: 2.500289, mean_q: 3.008098, mean_eps: 0.464476
 2678702/6000000: episode: 3668, duration: 13.036s, episode steps: 605, steps per second:  46, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.750 [0.000, 5.000],  loss: 0.014223, mae: 2.489708, mean_q: 2.996578, mean_eps: 0.464320
 2679798/6000000: episode: 3669, duration: 26.809s, episode steps: 1096, steps per second:  41, episode reward: 31.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.079 [0.000, 5.000],  loss: 0.015762, mae: 2.496154, mean_q: 3.002979, mean_eps: 0.464150
 2680454/6000000: episode: 3670, duration: 15.162s, episode steps: 656, steps per second:  43, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: 0.015486, mae: 2.502268, mean_q: 3.011552, mean_eps: 0.463975
 2681778/6000000: episode: 3671, duration: 28.871s, episode steps: 1324, steps per second:  46, episode reward: 31.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.202 [0.000, 5.000],  loss: 0.014279, mae: 2.502206, mean_q: 3.011093, mean_eps: 0.463777
 2683061/6000000: episode: 3672, duration: 28.968s, episode steps: 1283, steps per second:  44, episode reward: 25.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.235 [0.000, 5.000],  loss: 0.014804, mae: 2.512550, mean_q: 3.025867, mean_eps: 0.463516
 2683826/6000000: episode: 3673, duration: 16.286s, episode steps: 765, steps per second:  47, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.073 [0.000, 5.000],  loss: 0.016358, mae: 2.500042, mean_q: 3.008407, mean_eps: 0.463311
 2684669/6000000: episode: 3674, duration: 18.812s, episode steps: 843, steps per second:  45, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.015789, mae: 2.508270, mean_q: 3.022021, mean_eps: 0.463150
 2685309/6000000: episode: 3675, duration: 14.585s, episode steps: 640, steps per second:  44, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.227 [0.000, 5.000],  loss: 0.016030, mae: 2.497620, mean_q: 3.006497, mean_eps: 0.463002
 2686398/6000000: episode: 3676, duration: 23.947s, episode steps: 1089, steps per second:  45, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.098 [0.000, 5.000],  loss: 0.014105, mae: 2.488674, mean_q: 2.994317, mean_eps: 0.462829
 2687223/6000000: episode: 3677, duration: 17.729s, episode steps: 825, steps per second:  47, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.014064, mae: 2.482943, mean_q: 2.987528, mean_eps: 0.462638
 2688345/6000000: episode: 3678, duration: 24.685s, episode steps: 1122, steps per second:  45, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.014764, mae: 2.483526, mean_q: 2.988455, mean_eps: 0.462443
 2689289/6000000: episode: 3679, duration: 18.834s, episode steps: 944, steps per second:  50, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.015388, mae: 2.489816, mean_q: 2.996329, mean_eps: 0.462236
 2690471/6000000: episode: 3680, duration: 24.870s, episode steps: 1182, steps per second:  48, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.100 [0.000, 5.000],  loss: 0.016286, mae: 2.483491, mean_q: 2.989458, mean_eps: 0.462024
 2691091/6000000: episode: 3681, duration: 13.047s, episode steps: 620, steps per second:  48, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.014974, mae: 2.473859, mean_q: 2.976152, mean_eps: 0.461844
 2691612/6000000: episode: 3682, duration: 11.023s, episode steps: 521, steps per second:  47, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.655 [0.000, 5.000],  loss: 0.014559, mae: 2.482581, mean_q: 2.987095, mean_eps: 0.461730
 2692360/6000000: episode: 3683, duration: 15.806s, episode steps: 748, steps per second:  47, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.013948, mae: 2.473740, mean_q: 2.977178, mean_eps: 0.461603
 2692911/6000000: episode: 3684, duration: 11.192s, episode steps: 551, steps per second:  49, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.163 [0.000, 5.000],  loss: 0.014203, mae: 2.459494, mean_q: 2.960806, mean_eps: 0.461473
 2693312/6000000: episode: 3685, duration: 8.564s, episode steps: 401, steps per second:  47, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.017 [0.000, 5.000],  loss: 0.015859, mae: 2.478296, mean_q: 2.980663, mean_eps: 0.461378
 2694070/6000000: episode: 3686, duration: 16.247s, episode steps: 758, steps per second:  47, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.648 [0.000, 5.000],  loss: 0.015719, mae: 2.482887, mean_q: 2.987098, mean_eps: 0.461262
 2694768/6000000: episode: 3687, duration: 14.962s, episode steps: 698, steps per second:  47, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.695 [0.000, 5.000],  loss: 0.015907, mae: 2.460730, mean_q: 2.961167, mean_eps: 0.461116
 2695961/6000000: episode: 3688, duration: 28.071s, episode steps: 1193, steps per second:  43, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.014576, mae: 2.475041, mean_q: 2.977920, mean_eps: 0.460927
 2697021/6000000: episode: 3689, duration: 23.888s, episode steps: 1060, steps per second:  44, episode reward: 30.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.191 [0.000, 5.000],  loss: 0.016005, mae: 2.487099, mean_q: 2.991465, mean_eps: 0.460702
 2697804/6000000: episode: 3690, duration: 17.031s, episode steps: 783, steps per second:  46, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.014430, mae: 2.472655, mean_q: 2.975381, mean_eps: 0.460518
 2698266/6000000: episode: 3691, duration: 10.631s, episode steps: 462, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [0.000, 5.000],  loss: 0.014708, mae: 2.465263, mean_q: 2.966627, mean_eps: 0.460393
 2698758/6000000: episode: 3692, duration: 11.368s, episode steps: 492, steps per second:  43, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.013330, mae: 2.488023, mean_q: 2.992147, mean_eps: 0.460298
 2699728/6000000: episode: 3693, duration: 20.951s, episode steps: 970, steps per second:  46, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.015081, mae: 2.486388, mean_q: 2.990836, mean_eps: 0.460152
 2700092/6000000: episode: 3694, duration: 7.878s, episode steps: 364, steps per second:  46, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.228 [0.000, 5.000],  loss: 0.014780, mae: 2.484054, mean_q: 2.987496, mean_eps: 0.460018
 2700683/6000000: episode: 3695, duration: 13.083s, episode steps: 591, steps per second:  45, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.015156, mae: 2.463711, mean_q: 2.965711, mean_eps: 0.459923
 2701474/6000000: episode: 3696, duration: 17.961s, episode steps: 791, steps per second:  44, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.090 [0.000, 5.000],  loss: 0.013990, mae: 2.454253, mean_q: 2.952817, mean_eps: 0.459784
 2702289/6000000: episode: 3697, duration: 18.309s, episode steps: 815, steps per second:  45, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.226 [0.000, 5.000],  loss: 0.013169, mae: 2.460233, mean_q: 2.959672, mean_eps: 0.459624
 2703321/6000000: episode: 3698, duration: 22.758s, episode steps: 1032, steps per second:  45, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.065 [0.000, 5.000],  loss: 0.015443, mae: 2.486976, mean_q: 2.992470, mean_eps: 0.459439
 2704248/6000000: episode: 3699, duration: 21.003s, episode steps: 927, steps per second:  44, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.206 [0.000, 5.000],  loss: 0.015663, mae: 2.468396, mean_q: 2.969771, mean_eps: 0.459243
 2705174/6000000: episode: 3700, duration: 19.107s, episode steps: 926, steps per second:  48, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.102 [0.000, 5.000],  loss: 0.016470, mae: 2.467143, mean_q: 2.968800, mean_eps: 0.459058
 2705888/6000000: episode: 3701, duration: 14.478s, episode steps: 714, steps per second:  49, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.014417, mae: 2.476133, mean_q: 2.978379, mean_eps: 0.458894
 2706567/6000000: episode: 3702, duration: 14.632s, episode steps: 679, steps per second:  46, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.249 [0.000, 5.000],  loss: 0.017848, mae: 2.471949, mean_q: 2.971398, mean_eps: 0.458755
 2707131/6000000: episode: 3703, duration: 11.713s, episode steps: 564, steps per second:  48, episode reward: 17.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.085 [0.000, 5.000],  loss: 0.014363, mae: 2.479331, mean_q: 2.984875, mean_eps: 0.458630
 2707811/6000000: episode: 3704, duration: 14.255s, episode steps: 680, steps per second:  48, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.888 [0.000, 5.000],  loss: 0.015152, mae: 2.457742, mean_q: 2.958468, mean_eps: 0.458506
 2708977/6000000: episode: 3705, duration: 24.240s, episode steps: 1166, steps per second:  48, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.123 [0.000, 5.000],  loss: 0.015813, mae: 2.475086, mean_q: 2.977691, mean_eps: 0.458321
 2709928/6000000: episode: 3706, duration: 19.321s, episode steps: 951, steps per second:  49, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.145 [0.000, 5.000],  loss: 0.014116, mae: 2.473150, mean_q: 2.975131, mean_eps: 0.458110
 2710473/6000000: episode: 3707, duration: 11.497s, episode steps: 545, steps per second:  47, episode reward: 14.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.014250, mae: 2.475466, mean_q: 2.980607, mean_eps: 0.457960
 2711388/6000000: episode: 3708, duration: 19.490s, episode steps: 915, steps per second:  47, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.115 [0.000, 5.000],  loss: 0.016861, mae: 2.487119, mean_q: 2.992758, mean_eps: 0.457814
 2712306/6000000: episode: 3709, duration: 21.102s, episode steps: 918, steps per second:  44, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.135 [0.000, 5.000],  loss: 0.016076, mae: 2.483687, mean_q: 2.991499, mean_eps: 0.457631
 2713111/6000000: episode: 3710, duration: 18.644s, episode steps: 805, steps per second:  43, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.920 [0.000, 5.000],  loss: 0.015085, mae: 2.494205, mean_q: 3.002530, mean_eps: 0.457458
 2713638/6000000: episode: 3711, duration: 11.409s, episode steps: 527, steps per second:  46, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.332 [0.000, 5.000],  loss: 0.015921, mae: 2.467806, mean_q: 2.971520, mean_eps: 0.457325
 2714655/6000000: episode: 3712, duration: 21.957s, episode steps: 1017, steps per second:  46, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.176 [0.000, 5.000],  loss: 0.014073, mae: 2.482110, mean_q: 2.989328, mean_eps: 0.457171
 2715772/6000000: episode: 3713, duration: 24.583s, episode steps: 1117, steps per second:  45, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.978 [0.000, 5.000],  loss: 0.013752, mae: 2.480653, mean_q: 2.984543, mean_eps: 0.456958
 2716437/6000000: episode: 3714, duration: 14.345s, episode steps: 665, steps per second:  46, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.017246, mae: 2.484299, mean_q: 2.987052, mean_eps: 0.456779
 2717246/6000000: episode: 3715, duration: 17.284s, episode steps: 809, steps per second:  47, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.152 [0.000, 5.000],  loss: 0.014013, mae: 2.481970, mean_q: 2.985545, mean_eps: 0.456632
 2718226/6000000: episode: 3716, duration: 22.817s, episode steps: 980, steps per second:  43, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.237 [0.000, 5.000],  loss: 0.014744, mae: 2.478410, mean_q: 2.980971, mean_eps: 0.456453
 2719156/6000000: episode: 3717, duration: 20.613s, episode steps: 930, steps per second:  45, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.188 [0.000, 5.000],  loss: 0.015926, mae: 2.479603, mean_q: 2.984249, mean_eps: 0.456262
 2719948/6000000: episode: 3718, duration: 17.513s, episode steps: 792, steps per second:  45, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.014 [0.000, 5.000],  loss: 0.014909, mae: 2.488569, mean_q: 2.996590, mean_eps: 0.456090
 2720515/6000000: episode: 3719, duration: 12.158s, episode steps: 567, steps per second:  47, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.129 [0.000, 5.000],  loss: 0.012976, mae: 2.481454, mean_q: 2.989987, mean_eps: 0.455954
 2721255/6000000: episode: 3720, duration: 15.898s, episode steps: 740, steps per second:  47, episode reward: 21.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.193 [0.000, 5.000],  loss: 0.014048, mae: 2.477834, mean_q: 2.982194, mean_eps: 0.455823
 2722187/6000000: episode: 3721, duration: 19.163s, episode steps: 932, steps per second:  49, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.005 [0.000, 5.000],  loss: 0.015275, mae: 2.487158, mean_q: 2.992691, mean_eps: 0.455656
 2723150/6000000: episode: 3722, duration: 20.098s, episode steps: 963, steps per second:  48, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.957 [0.000, 5.000],  loss: 0.015147, mae: 2.490074, mean_q: 2.996064, mean_eps: 0.455466
 2724350/6000000: episode: 3723, duration: 25.643s, episode steps: 1200, steps per second:  47, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.066 [0.000, 5.000],  loss: 0.016558, mae: 2.486588, mean_q: 2.994553, mean_eps: 0.455250
 2725595/6000000: episode: 3724, duration: 25.254s, episode steps: 1245, steps per second:  49, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.916 [0.000, 5.000],  loss: 0.015657, mae: 2.488071, mean_q: 2.993494, mean_eps: 0.455006
 2726358/6000000: episode: 3725, duration: 16.151s, episode steps: 763, steps per second:  47, episode reward: 23.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.248 [0.000, 5.000],  loss: 0.015218, mae: 2.496507, mean_q: 3.002765, mean_eps: 0.454805
 2727309/6000000: episode: 3726, duration: 19.942s, episode steps: 951, steps per second:  48, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.014685, mae: 2.507236, mean_q: 3.015461, mean_eps: 0.454633
 2727898/6000000: episode: 3727, duration: 12.782s, episode steps: 589, steps per second:  46, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.669 [0.000, 5.000],  loss: 0.014479, mae: 2.487960, mean_q: 2.991529, mean_eps: 0.454479
 2728756/6000000: episode: 3728, duration: 20.280s, episode steps: 858, steps per second:  42, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.016591, mae: 2.491925, mean_q: 2.996606, mean_eps: 0.454335
 2729758/6000000: episode: 3729, duration: 23.812s, episode steps: 1002, steps per second:  42, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.158 [0.000, 5.000],  loss: 0.013935, mae: 2.509054, mean_q: 3.019946, mean_eps: 0.454149
 2730447/6000000: episode: 3730, duration: 14.963s, episode steps: 689, steps per second:  46, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.014693, mae: 2.493796, mean_q: 3.000929, mean_eps: 0.453980
 2731161/6000000: episode: 3731, duration: 16.306s, episode steps: 714, steps per second:  44, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.016174, mae: 2.497441, mean_q: 3.004378, mean_eps: 0.453839
 2732135/6000000: episode: 3732, duration: 21.952s, episode steps: 974, steps per second:  44, episode reward: 11.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.786 [0.000, 5.000],  loss: 0.015978, mae: 2.502749, mean_q: 3.010918, mean_eps: 0.453670
 2733196/6000000: episode: 3733, duration: 22.969s, episode steps: 1061, steps per second:  46, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.323 [0.000, 5.000],  loss: 0.014997, mae: 2.514462, mean_q: 3.024964, mean_eps: 0.453467
 2733792/6000000: episode: 3734, duration: 13.304s, episode steps: 596, steps per second:  45, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.013714, mae: 2.507706, mean_q: 3.017382, mean_eps: 0.453302
 2734512/6000000: episode: 3735, duration: 17.466s, episode steps: 720, steps per second:  41, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.013759, mae: 2.499157, mean_q: 3.006990, mean_eps: 0.453170
 2735033/6000000: episode: 3736, duration: 12.463s, episode steps: 521, steps per second:  42, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.016337, mae: 2.499130, mean_q: 3.006135, mean_eps: 0.453046
 2735677/6000000: episode: 3737, duration: 13.943s, episode steps: 644, steps per second:  46, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.076 [0.000, 5.000],  loss: 0.014205, mae: 2.507306, mean_q: 3.016861, mean_eps: 0.452929
 2736390/6000000: episode: 3738, duration: 15.820s, episode steps: 713, steps per second:  45, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.794 [0.000, 5.000],  loss: 0.014427, mae: 2.489180, mean_q: 2.996431, mean_eps: 0.452793
 2736960/6000000: episode: 3739, duration: 12.529s, episode steps: 570, steps per second:  45, episode reward: 14.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.165 [0.000, 5.000],  loss: 0.014412, mae: 2.512424, mean_q: 3.022926, mean_eps: 0.452665
 2737836/6000000: episode: 3740, duration: 19.079s, episode steps: 876, steps per second:  46, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.968 [0.000, 5.000],  loss: 0.015242, mae: 2.490646, mean_q: 2.998615, mean_eps: 0.452521
 2738477/6000000: episode: 3741, duration: 12.853s, episode steps: 641, steps per second:  50, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.012543, mae: 2.531851, mean_q: 3.046746, mean_eps: 0.452369
 2739232/6000000: episode: 3742, duration: 15.807s, episode steps: 755, steps per second:  48, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.911 [0.000, 5.000],  loss: 0.014992, mae: 2.519443, mean_q: 3.030251, mean_eps: 0.452229
 2740001/6000000: episode: 3743, duration: 16.417s, episode steps: 769, steps per second:  47, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.971 [0.000, 5.000],  loss: 0.016160, mae: 2.523545, mean_q: 3.036194, mean_eps: 0.452077
 2740523/6000000: episode: 3744, duration: 11.219s, episode steps: 522, steps per second:  47, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.782 [0.000, 5.000],  loss: 0.015116, mae: 2.522242, mean_q: 3.035536, mean_eps: 0.451948
 2741403/6000000: episode: 3745, duration: 19.635s, episode steps: 880, steps per second:  45, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.011 [0.000, 5.000],  loss: 0.015010, mae: 2.528071, mean_q: 3.040787, mean_eps: 0.451808
 2742481/6000000: episode: 3746, duration: 23.682s, episode steps: 1078, steps per second:  46, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.159 [0.000, 5.000],  loss: 0.015407, mae: 2.510035, mean_q: 3.019117, mean_eps: 0.451612
 2743007/6000000: episode: 3747, duration: 11.734s, episode steps: 526, steps per second:  45, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.300 [0.000, 5.000],  loss: 0.014794, mae: 2.520911, mean_q: 3.031921, mean_eps: 0.451451
 2743949/6000000: episode: 3748, duration: 21.132s, episode steps: 942, steps per second:  45, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.121 [0.000, 5.000],  loss: 0.014061, mae: 2.509437, mean_q: 3.019334, mean_eps: 0.451304
 2745035/6000000: episode: 3749, duration: 25.081s, episode steps: 1086, steps per second:  43, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.186 [0.000, 5.000],  loss: 0.014512, mae: 2.518255, mean_q: 3.029500, mean_eps: 0.451102
 2745911/6000000: episode: 3750, duration: 20.511s, episode steps: 876, steps per second:  43, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.129 [0.000, 5.000],  loss: 0.015621, mae: 2.506803, mean_q: 3.014732, mean_eps: 0.450906
 2746770/6000000: episode: 3751, duration: 18.901s, episode steps: 859, steps per second:  45, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.094 [0.000, 5.000],  loss: 0.015606, mae: 2.496294, mean_q: 3.003439, mean_eps: 0.450732
 2747516/6000000: episode: 3752, duration: 16.582s, episode steps: 746, steps per second:  45, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.013524, mae: 2.502955, mean_q: 3.011801, mean_eps: 0.450572
 2748441/6000000: episode: 3753, duration: 20.972s, episode steps: 925, steps per second:  44, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.014972, mae: 2.506995, mean_q: 3.017238, mean_eps: 0.450404
 2749144/6000000: episode: 3754, duration: 16.747s, episode steps: 703, steps per second:  42, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.166 [0.000, 5.000],  loss: 0.014945, mae: 2.512688, mean_q: 3.022543, mean_eps: 0.450242
 2749599/6000000: episode: 3755, duration: 10.585s, episode steps: 455, steps per second:  43, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 0.016327, mae: 2.493066, mean_q: 2.997539, mean_eps: 0.450126
 2750330/6000000: episode: 3756, duration: 17.558s, episode steps: 731, steps per second:  42, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.227 [0.000, 5.000],  loss: 0.014834, mae: 2.498779, mean_q: 3.006060, mean_eps: 0.450007
 2751036/6000000: episode: 3757, duration: 16.061s, episode steps: 706, steps per second:  44, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.014695, mae: 2.506491, mean_q: 3.014342, mean_eps: 0.449864
 2751968/6000000: episode: 3758, duration: 20.012s, episode steps: 932, steps per second:  47, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.211 [0.000, 5.000],  loss: 0.015908, mae: 2.524785, mean_q: 3.036473, mean_eps: 0.449700
 2752938/6000000: episode: 3759, duration: 22.138s, episode steps: 970, steps per second:  44, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.046 [0.000, 5.000],  loss: 0.013752, mae: 2.517929, mean_q: 3.028742, mean_eps: 0.449510
 2754080/6000000: episode: 3760, duration: 25.104s, episode steps: 1142, steps per second:  45, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.100 [0.000, 5.000],  loss: 0.014129, mae: 2.505713, mean_q: 3.013990, mean_eps: 0.449298
 2754746/6000000: episode: 3761, duration: 13.861s, episode steps: 666, steps per second:  48, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.013842, mae: 2.510665, mean_q: 3.019976, mean_eps: 0.449118
 2755626/6000000: episode: 3762, duration: 18.708s, episode steps: 880, steps per second:  47, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.103 [0.000, 5.000],  loss: 0.016289, mae: 2.514395, mean_q: 3.023923, mean_eps: 0.448963
 2756562/6000000: episode: 3763, duration: 19.809s, episode steps: 936, steps per second:  47, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.181 [0.000, 5.000],  loss: 0.016112, mae: 2.525009, mean_q: 3.036201, mean_eps: 0.448781
 2757648/6000000: episode: 3764, duration: 22.436s, episode steps: 1086, steps per second:  48, episode reward: 32.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.052 [0.000, 5.000],  loss: 0.015291, mae: 2.509823, mean_q: 3.018789, mean_eps: 0.448579
 2758304/6000000: episode: 3765, duration: 13.908s, episode steps: 656, steps per second:  47, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.652 [0.000, 5.000],  loss: 0.014553, mae: 2.515649, mean_q: 3.025689, mean_eps: 0.448405
 2759330/6000000: episode: 3766, duration: 21.645s, episode steps: 1026, steps per second:  47, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.105 [0.000, 5.000],  loss: 0.015579, mae: 2.510776, mean_q: 3.022170, mean_eps: 0.448237
 2759703/6000000: episode: 3767, duration: 8.400s, episode steps: 373, steps per second:  44, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.914 [0.000, 5.000],  loss: 0.015555, mae: 2.508956, mean_q: 3.017781, mean_eps: 0.448097
 2760411/6000000: episode: 3768, duration: 15.850s, episode steps: 708, steps per second:  45, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.900 [0.000, 5.000],  loss: 0.015787, mae: 2.514672, mean_q: 3.027158, mean_eps: 0.447989
 2761164/6000000: episode: 3769, duration: 18.131s, episode steps: 753, steps per second:  42, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.015532, mae: 2.529789, mean_q: 3.045373, mean_eps: 0.447843
 2762103/6000000: episode: 3770, duration: 23.498s, episode steps: 939, steps per second:  40, episode reward: 29.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.015689, mae: 2.536746, mean_q: 3.054036, mean_eps: 0.447674
 2763064/6000000: episode: 3771, duration: 21.584s, episode steps: 961, steps per second:  45, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.015088, mae: 2.541962, mean_q: 3.058425, mean_eps: 0.447484
 2763733/6000000: episode: 3772, duration: 16.981s, episode steps: 669, steps per second:  39, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.091 [0.000, 5.000],  loss: 0.014119, mae: 2.538585, mean_q: 3.053924, mean_eps: 0.447320
 2764719/6000000: episode: 3773, duration: 23.603s, episode steps: 986, steps per second:  42, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.050 [0.000, 5.000],  loss: 0.014577, mae: 2.534141, mean_q: 3.050740, mean_eps: 0.447155
 2765907/6000000: episode: 3774, duration: 25.943s, episode steps: 1188, steps per second:  46, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.143 [0.000, 5.000],  loss: 0.014982, mae: 2.528451, mean_q: 3.043081, mean_eps: 0.446938
 2766882/6000000: episode: 3775, duration: 23.871s, episode steps: 975, steps per second:  41, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.030 [0.000, 5.000],  loss: 0.014973, mae: 2.535145, mean_q: 3.051336, mean_eps: 0.446721
 2767939/6000000: episode: 3776, duration: 28.032s, episode steps: 1057, steps per second:  38, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.174 [0.000, 5.000],  loss: 0.016082, mae: 2.540995, mean_q: 3.057455, mean_eps: 0.446518
 2768604/6000000: episode: 3777, duration: 17.327s, episode steps: 665, steps per second:  38, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.286 [0.000, 5.000],  loss: 0.016016, mae: 2.532595, mean_q: 3.046548, mean_eps: 0.446346
 2769541/6000000: episode: 3778, duration: 24.237s, episode steps: 937, steps per second:  39, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.053 [0.000, 5.000],  loss: 0.013767, mae: 2.540403, mean_q: 3.057830, mean_eps: 0.446186
 2770331/6000000: episode: 3779, duration: 21.305s, episode steps: 790, steps per second:  37, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.181 [0.000, 5.000],  loss: 0.014669, mae: 2.545513, mean_q: 3.065169, mean_eps: 0.446013
 2771313/6000000: episode: 3780, duration: 22.263s, episode steps: 982, steps per second:  44, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.017200, mae: 2.560350, mean_q: 3.078755, mean_eps: 0.445836
 2771979/6000000: episode: 3781, duration: 14.409s, episode steps: 666, steps per second:  46, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.016275, mae: 2.542056, mean_q: 3.057823, mean_eps: 0.445671
 2772596/6000000: episode: 3782, duration: 13.558s, episode steps: 617, steps per second:  46, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.206 [0.000, 5.000],  loss: 0.016258, mae: 2.548868, mean_q: 3.070078, mean_eps: 0.445543
 2772996/6000000: episode: 3783, duration: 9.550s, episode steps: 400, steps per second:  42, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: 0.015861, mae: 2.553898, mean_q: 3.074300, mean_eps: 0.445441
 2773488/6000000: episode: 3784, duration: 10.926s, episode steps: 492, steps per second:  45, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.014977, mae: 2.549262, mean_q: 3.065576, mean_eps: 0.445352
 2774313/6000000: episode: 3785, duration: 17.815s, episode steps: 825, steps per second:  46, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.014552, mae: 2.556114, mean_q: 3.074840, mean_eps: 0.445220
 2774979/6000000: episode: 3786, duration: 14.886s, episode steps: 666, steps per second:  45, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.122 [0.000, 5.000],  loss: 0.016025, mae: 2.556321, mean_q: 3.078210, mean_eps: 0.445071
 2776080/6000000: episode: 3787, duration: 25.007s, episode steps: 1101, steps per second:  44, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.101 [0.000, 5.000],  loss: 0.014399, mae: 2.572296, mean_q: 3.097683, mean_eps: 0.444894
 2776991/6000000: episode: 3788, duration: 23.418s, episode steps: 911, steps per second:  39, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.013945, mae: 2.573332, mean_q: 3.098067, mean_eps: 0.444693
 2777908/6000000: episode: 3789, duration: 20.330s, episode steps: 917, steps per second:  45, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.154 [0.000, 5.000],  loss: 0.015839, mae: 2.587434, mean_q: 3.112993, mean_eps: 0.444510
 2778409/6000000: episode: 3790, duration: 10.709s, episode steps: 501, steps per second:  47, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.015044, mae: 2.585863, mean_q: 3.112374, mean_eps: 0.444368
 2779373/6000000: episode: 3791, duration: 23.367s, episode steps: 964, steps per second:  41, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.889 [0.000, 5.000],  loss: 0.015449, mae: 2.577139, mean_q: 3.102650, mean_eps: 0.444222
 2780109/6000000: episode: 3792, duration: 16.383s, episode steps: 736, steps per second:  45, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.118 [0.000, 5.000],  loss: 0.015501, mae: 2.571444, mean_q: 3.095006, mean_eps: 0.444052
 2780687/6000000: episode: 3793, duration: 12.226s, episode steps: 578, steps per second:  47, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.254 [0.000, 5.000],  loss: 0.015252, mae: 2.566121, mean_q: 3.089796, mean_eps: 0.443920
 2781603/6000000: episode: 3794, duration: 19.611s, episode steps: 916, steps per second:  47, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.090 [0.000, 5.000],  loss: 0.015260, mae: 2.543000, mean_q: 3.060514, mean_eps: 0.443771
 2782773/6000000: episode: 3795, duration: 26.180s, episode steps: 1170, steps per second:  45, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 0.015103, mae: 2.559651, mean_q: 3.079370, mean_eps: 0.443562
 2783677/6000000: episode: 3796, duration: 19.820s, episode steps: 904, steps per second:  46, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.007 [0.000, 5.000],  loss: 0.014594, mae: 2.550077, mean_q: 3.068701, mean_eps: 0.443355
 2784555/6000000: episode: 3797, duration: 19.983s, episode steps: 878, steps per second:  44, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.844 [0.000, 5.000],  loss: 0.016379, mae: 2.549350, mean_q: 3.066530, mean_eps: 0.443177
 2785427/6000000: episode: 3798, duration: 19.033s, episode steps: 872, steps per second:  46, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.017045, mae: 2.558747, mean_q: 3.076794, mean_eps: 0.443002
 2786249/6000000: episode: 3799, duration: 17.169s, episode steps: 822, steps per second:  48, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.071 [0.000, 5.000],  loss: 0.014659, mae: 2.576766, mean_q: 3.099805, mean_eps: 0.442832
 2787188/6000000: episode: 3800, duration: 19.750s, episode steps: 939, steps per second:  48, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.124 [0.000, 5.000],  loss: 0.014976, mae: 2.573621, mean_q: 3.097045, mean_eps: 0.442656
 2788061/6000000: episode: 3801, duration: 18.324s, episode steps: 873, steps per second:  48, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.257 [0.000, 5.000],  loss: 0.015959, mae: 2.567295, mean_q: 3.088922, mean_eps: 0.442475
 2788534/6000000: episode: 3802, duration: 9.830s, episode steps: 473, steps per second:  48, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.249 [0.000, 5.000],  loss: 0.015678, mae: 2.573751, mean_q: 3.095673, mean_eps: 0.442340
 2789451/6000000: episode: 3803, duration: 19.055s, episode steps: 917, steps per second:  48, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.014454, mae: 2.561437, mean_q: 3.081556, mean_eps: 0.442202
 2790684/6000000: episode: 3804, duration: 25.910s, episode steps: 1233, steps per second:  48, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 0.014794, mae: 2.558035, mean_q: 3.076885, mean_eps: 0.441987
 2791521/6000000: episode: 3805, duration: 19.188s, episode steps: 837, steps per second:  44, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.015479, mae: 2.531268, mean_q: 3.042990, mean_eps: 0.441780
 2792243/6000000: episode: 3806, duration: 16.175s, episode steps: 722, steps per second:  45, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.014702, mae: 2.539340, mean_q: 3.054285, mean_eps: 0.441624
 2793044/6000000: episode: 3807, duration: 19.677s, episode steps: 801, steps per second:  41, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.317 [0.000, 5.000],  loss: 0.014863, mae: 2.536309, mean_q: 3.049648, mean_eps: 0.441472
 2793691/6000000: episode: 3808, duration: 14.965s, episode steps: 647, steps per second:  43, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.199 [0.000, 5.000],  loss: 0.015744, mae: 2.552264, mean_q: 3.069741, mean_eps: 0.441327
 2794915/6000000: episode: 3809, duration: 27.004s, episode steps: 1224, steps per second:  45, episode reward: 32.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.035 [0.000, 5.000],  loss: 0.015982, mae: 2.544683, mean_q: 3.060859, mean_eps: 0.441140
 2795607/6000000: episode: 3810, duration: 16.189s, episode steps: 692, steps per second:  43, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.868 [0.000, 5.000],  loss: 0.014416, mae: 2.568135, mean_q: 3.089668, mean_eps: 0.440948
 2796534/6000000: episode: 3811, duration: 20.481s, episode steps: 927, steps per second:  45, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.278 [0.000, 5.000],  loss: 0.015298, mae: 2.564980, mean_q: 3.084738, mean_eps: 0.440786
 2797154/6000000: episode: 3812, duration: 13.087s, episode steps: 620, steps per second:  47, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.166 [0.000, 5.000],  loss: 0.014981, mae: 2.551725, mean_q: 3.069492, mean_eps: 0.440631
 2797912/6000000: episode: 3813, duration: 17.461s, episode steps: 758, steps per second:  43, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.016402, mae: 2.559528, mean_q: 3.077384, mean_eps: 0.440494
 2798662/6000000: episode: 3814, duration: 16.795s, episode steps: 750, steps per second:  45, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.015366, mae: 2.554081, mean_q: 3.070584, mean_eps: 0.440343
 2799312/6000000: episode: 3815, duration: 14.450s, episode steps: 650, steps per second:  45, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.015673, mae: 2.572943, mean_q: 3.094988, mean_eps: 0.440203
 2800353/6000000: episode: 3816, duration: 23.884s, episode steps: 1041, steps per second:  44, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.017014, mae: 2.542939, mean_q: 3.058295, mean_eps: 0.440034
 2801175/6000000: episode: 3817, duration: 18.629s, episode steps: 822, steps per second:  44, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.987 [0.000, 5.000],  loss: 0.014917, mae: 2.563445, mean_q: 3.083507, mean_eps: 0.439847
 2802118/6000000: episode: 3818, duration: 19.959s, episode steps: 943, steps per second:  47, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.016344, mae: 2.576849, mean_q: 3.098932, mean_eps: 0.439671
 2802609/6000000: episode: 3819, duration: 10.244s, episode steps: 491, steps per second:  48, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.149 [0.000, 5.000],  loss: 0.015375, mae: 2.562896, mean_q: 3.081991, mean_eps: 0.439527
 2803410/6000000: episode: 3820, duration: 16.766s, episode steps: 801, steps per second:  48, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.117 [0.000, 5.000],  loss: 0.015655, mae: 2.583056, mean_q: 3.107116, mean_eps: 0.439398
 2803790/6000000: episode: 3821, duration: 8.093s, episode steps: 380, steps per second:  47, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.976 [0.000, 5.000],  loss: 0.018024, mae: 2.566392, mean_q: 3.086663, mean_eps: 0.439280
 2804484/6000000: episode: 3822, duration: 14.694s, episode steps: 694, steps per second:  47, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.018100, mae: 2.564784, mean_q: 3.086228, mean_eps: 0.439173
 2805357/6000000: episode: 3823, duration: 18.295s, episode steps: 873, steps per second:  48, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.015422, mae: 2.582618, mean_q: 3.107471, mean_eps: 0.439016
 2806234/6000000: episode: 3824, duration: 18.901s, episode steps: 877, steps per second:  46, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.016789, mae: 2.588127, mean_q: 3.112264, mean_eps: 0.438841
 2806885/6000000: episode: 3825, duration: 13.804s, episode steps: 651, steps per second:  47, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.250 [0.000, 5.000],  loss: 0.014403, mae: 2.590851, mean_q: 3.116365, mean_eps: 0.438688
 2807822/6000000: episode: 3826, duration: 20.241s, episode steps: 937, steps per second:  46, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.650 [0.000, 5.000],  loss: 0.016582, mae: 2.591153, mean_q: 3.116044, mean_eps: 0.438529
 2808670/6000000: episode: 3827, duration: 18.878s, episode steps: 848, steps per second:  45, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.139 [0.000, 5.000],  loss: 0.015160, mae: 2.586212, mean_q: 3.109929, mean_eps: 0.438351
 2809395/6000000: episode: 3828, duration: 17.663s, episode steps: 725, steps per second:  41, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.210 [0.000, 5.000],  loss: 0.015937, mae: 2.589729, mean_q: 3.114575, mean_eps: 0.438194
 2810008/6000000: episode: 3829, duration: 14.317s, episode steps: 613, steps per second:  43, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.015963, mae: 2.591212, mean_q: 3.115031, mean_eps: 0.438060
 2810714/6000000: episode: 3830, duration: 16.029s, episode steps: 706, steps per second:  44, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.150 [0.000, 5.000],  loss: 0.017353, mae: 2.592260, mean_q: 3.119268, mean_eps: 0.437928
 2811528/6000000: episode: 3831, duration: 18.836s, episode steps: 814, steps per second:  43, episode reward:  7.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.022 [0.000, 5.000],  loss: 0.016509, mae: 2.594585, mean_q: 3.121870, mean_eps: 0.437776
 2812363/6000000: episode: 3832, duration: 19.131s, episode steps: 835, steps per second:  44, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.983 [0.000, 5.000],  loss: 0.017042, mae: 2.606951, mean_q: 3.135600, mean_eps: 0.437611
 2813000/6000000: episode: 3833, duration: 13.780s, episode steps: 637, steps per second:  46, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.918 [0.000, 5.000],  loss: 0.017580, mae: 2.590431, mean_q: 3.119342, mean_eps: 0.437464
 2813834/6000000: episode: 3834, duration: 18.892s, episode steps: 834, steps per second:  44, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.114 [0.000, 5.000],  loss: 0.015308, mae: 2.593707, mean_q: 3.121825, mean_eps: 0.437317
 2814871/6000000: episode: 3835, duration: 23.626s, episode steps: 1037, steps per second:  44, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.974 [0.000, 5.000],  loss: 0.015109, mae: 2.592527, mean_q: 3.119503, mean_eps: 0.437130
 2815544/6000000: episode: 3836, duration: 14.923s, episode steps: 673, steps per second:  45, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.138 [0.000, 5.000],  loss: 0.015756, mae: 2.577562, mean_q: 3.100450, mean_eps: 0.436959
 2816045/6000000: episode: 3837, duration: 10.809s, episode steps: 501, steps per second:  46, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.015425, mae: 2.557981, mean_q: 3.077347, mean_eps: 0.436841
 2816808/6000000: episode: 3838, duration: 16.609s, episode steps: 763, steps per second:  46, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.016005, mae: 2.563647, mean_q: 3.084602, mean_eps: 0.436715
 2817826/6000000: episode: 3839, duration: 22.257s, episode steps: 1018, steps per second:  46, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.960 [0.000, 5.000],  loss: 0.016128, mae: 2.565111, mean_q: 3.084357, mean_eps: 0.436537
 2818213/6000000: episode: 3840, duration: 8.134s, episode steps: 387, steps per second:  48, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.251 [0.000, 5.000],  loss: 0.014179, mae: 2.574338, mean_q: 3.096702, mean_eps: 0.436396
 2818866/6000000: episode: 3841, duration: 13.729s, episode steps: 653, steps per second:  48, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: 0.014810, mae: 2.558981, mean_q: 3.078732, mean_eps: 0.436292
 2819816/6000000: episode: 3842, duration: 20.047s, episode steps: 950, steps per second:  47, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.016185, mae: 2.568945, mean_q: 3.088971, mean_eps: 0.436132
 2820833/6000000: episode: 3843, duration: 21.398s, episode steps: 1017, steps per second:  48, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.947 [0.000, 5.000],  loss: 0.016186, mae: 2.569445, mean_q: 3.091437, mean_eps: 0.435935
 2821395/6000000: episode: 3844, duration: 12.302s, episode steps: 562, steps per second:  46, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.961 [0.000, 5.000],  loss: 0.016084, mae: 2.565521, mean_q: 3.086793, mean_eps: 0.435777
 2822063/6000000: episode: 3845, duration: 13.789s, episode steps: 668, steps per second:  48, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.003 [0.000, 5.000],  loss: 0.016174, mae: 2.548458, mean_q: 3.068580, mean_eps: 0.435654
 2822701/6000000: episode: 3846, duration: 14.042s, episode steps: 638, steps per second:  45, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.986 [0.000, 5.000],  loss: 0.014706, mae: 2.561761, mean_q: 3.082160, mean_eps: 0.435524
 2823525/6000000: episode: 3847, duration: 16.918s, episode steps: 824, steps per second:  49, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.130 [0.000, 5.000],  loss: 0.014748, mae: 2.571122, mean_q: 3.092444, mean_eps: 0.435377
 2824011/6000000: episode: 3848, duration: 10.155s, episode steps: 486, steps per second:  48, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.837 [0.000, 5.000],  loss: 0.017070, mae: 2.582887, mean_q: 3.105860, mean_eps: 0.435246
 2824822/6000000: episode: 3849, duration: 17.550s, episode steps: 811, steps per second:  46, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.002 [0.000, 5.000],  loss: 0.016877, mae: 2.569350, mean_q: 3.090291, mean_eps: 0.435117
 2825302/6000000: episode: 3850, duration: 11.469s, episode steps: 480, steps per second:  42, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.016805, mae: 2.574744, mean_q: 3.095820, mean_eps: 0.434988
 2825930/6000000: episode: 3851, duration: 15.703s, episode steps: 628, steps per second:  40, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.857 [0.000, 5.000],  loss: 0.016312, mae: 2.560843, mean_q: 3.081701, mean_eps: 0.434877
 2826743/6000000: episode: 3852, duration: 18.936s, episode steps: 813, steps per second:  43, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.042 [0.000, 5.000],  loss: 0.014724, mae: 2.556485, mean_q: 3.076818, mean_eps: 0.434733
 2827425/6000000: episode: 3853, duration: 15.536s, episode steps: 682, steps per second:  44, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.015879, mae: 2.549247, mean_q: 3.067018, mean_eps: 0.434583
 2828310/6000000: episode: 3854, duration: 20.041s, episode steps: 885, steps per second:  44, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.939 [0.000, 5.000],  loss: 0.015657, mae: 2.563990, mean_q: 3.085415, mean_eps: 0.434426
 2829058/6000000: episode: 3855, duration: 16.278s, episode steps: 748, steps per second:  46, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.015525, mae: 2.559097, mean_q: 3.077303, mean_eps: 0.434263
 2829832/6000000: episode: 3856, duration: 16.445s, episode steps: 774, steps per second:  47, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.015115, mae: 2.549390, mean_q: 3.067334, mean_eps: 0.434111
 2830483/6000000: episode: 3857, duration: 14.556s, episode steps: 651, steps per second:  45, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.015328, mae: 2.540637, mean_q: 3.056693, mean_eps: 0.433969
 2831199/6000000: episode: 3858, duration: 16.252s, episode steps: 716, steps per second:  44, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.016299, mae: 2.526139, mean_q: 3.037610, mean_eps: 0.433832
 2832000/6000000: episode: 3859, duration: 17.420s, episode steps: 801, steps per second:  46, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.247 [0.000, 5.000],  loss: 0.016125, mae: 2.541983, mean_q: 3.058298, mean_eps: 0.433680
 2832922/6000000: episode: 3860, duration: 20.297s, episode steps: 922, steps per second:  45, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.016454, mae: 2.540692, mean_q: 3.056569, mean_eps: 0.433508
 2833872/6000000: episode: 3861, duration: 21.463s, episode steps: 950, steps per second:  44, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.760 [0.000, 5.000],  loss: 0.013891, mae: 2.532403, mean_q: 3.046285, mean_eps: 0.433321
 2834479/6000000: episode: 3862, duration: 12.932s, episode steps: 607, steps per second:  47, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.255 [0.000, 5.000],  loss: 0.014679, mae: 2.520534, mean_q: 3.033841, mean_eps: 0.433165
 2835493/6000000: episode: 3863, duration: 20.491s, episode steps: 1014, steps per second:  49, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.286 [0.000, 5.000],  loss: 0.016389, mae: 2.521379, mean_q: 3.036441, mean_eps: 0.433003
 2836255/6000000: episode: 3864, duration: 16.011s, episode steps: 762, steps per second:  48, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.273 [0.000, 5.000],  loss: 0.014355, mae: 2.521560, mean_q: 3.035875, mean_eps: 0.432825
 2837114/6000000: episode: 3865, duration: 18.173s, episode steps: 859, steps per second:  47, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.012485, mae: 2.527146, mean_q: 3.040604, mean_eps: 0.432663
 2837764/6000000: episode: 3866, duration: 13.778s, episode steps: 650, steps per second:  47, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.994 [0.000, 5.000],  loss: 0.014369, mae: 2.515897, mean_q: 3.026038, mean_eps: 0.432512
 2838450/6000000: episode: 3867, duration: 14.500s, episode steps: 686, steps per second:  47, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.213 [0.000, 5.000],  loss: 0.014998, mae: 2.521062, mean_q: 3.034378, mean_eps: 0.432379
 2839409/6000000: episode: 3868, duration: 22.346s, episode steps: 959, steps per second:  43, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.114 [0.000, 5.000],  loss: 0.014054, mae: 2.530647, mean_q: 3.043631, mean_eps: 0.432214
 2839964/6000000: episode: 3869, duration: 13.314s, episode steps: 555, steps per second:  42, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.041 [0.000, 5.000],  loss: 0.014663, mae: 2.523384, mean_q: 3.033667, mean_eps: 0.432063
 2841114/6000000: episode: 3870, duration: 27.685s, episode steps: 1150, steps per second:  42, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 0.015547, mae: 2.535866, mean_q: 3.050816, mean_eps: 0.431892
 2842114/6000000: episode: 3871, duration: 24.823s, episode steps: 1000, steps per second:  40, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.312 [0.000, 5.000],  loss: 0.017624, mae: 2.539971, mean_q: 3.054542, mean_eps: 0.431677
 2842859/6000000: episode: 3872, duration: 16.354s, episode steps: 745, steps per second:  46, episode reward: 22.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.164 [0.000, 5.000],  loss: 0.014896, mae: 2.530353, mean_q: 3.043579, mean_eps: 0.431503
 2843596/6000000: episode: 3873, duration: 16.533s, episode steps: 737, steps per second:  45, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.014922, mae: 2.539600, mean_q: 3.057244, mean_eps: 0.431355
 2844374/6000000: episode: 3874, duration: 17.467s, episode steps: 778, steps per second:  45, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.014311, mae: 2.527240, mean_q: 3.040974, mean_eps: 0.431203
 2845090/6000000: episode: 3875, duration: 15.656s, episode steps: 716, steps per second:  46, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.015073, mae: 2.534087, mean_q: 3.050038, mean_eps: 0.431054
 2845982/6000000: episode: 3876, duration: 18.771s, episode steps: 892, steps per second:  48, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.014972, mae: 2.548357, mean_q: 3.066602, mean_eps: 0.430893
 2847246/6000000: episode: 3877, duration: 28.955s, episode steps: 1264, steps per second:  44, episode reward: 25.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: 0.015894, mae: 2.550256, mean_q: 3.067102, mean_eps: 0.430677
 2848012/6000000: episode: 3878, duration: 17.716s, episode steps: 766, steps per second:  43, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.014361, mae: 2.563250, mean_q: 3.083865, mean_eps: 0.430474
 2848802/6000000: episode: 3879, duration: 17.473s, episode steps: 790, steps per second:  45, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.028 [0.000, 5.000],  loss: 0.015864, mae: 2.546188, mean_q: 3.063074, mean_eps: 0.430319
 2849770/6000000: episode: 3880, duration: 21.981s, episode steps: 968, steps per second:  44, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.129 [0.000, 5.000],  loss: 0.015543, mae: 2.571546, mean_q: 3.095830, mean_eps: 0.430143
 2850403/6000000: episode: 3881, duration: 13.960s, episode steps: 633, steps per second:  45, episode reward: 19.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.262 [0.000, 5.000],  loss: 0.013717, mae: 2.526050, mean_q: 3.040152, mean_eps: 0.429983
 2851080/6000000: episode: 3882, duration: 14.666s, episode steps: 677, steps per second:  46, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.015594, mae: 2.540171, mean_q: 3.056236, mean_eps: 0.429852
 2851963/6000000: episode: 3883, duration: 19.063s, episode steps: 883, steps per second:  46, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.136 [0.000, 5.000],  loss: 0.014555, mae: 2.539385, mean_q: 3.054849, mean_eps: 0.429696
 2852943/6000000: episode: 3884, duration: 21.618s, episode steps: 980, steps per second:  45, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.015876, mae: 2.551843, mean_q: 3.069658, mean_eps: 0.429510
 2853873/6000000: episode: 3885, duration: 20.103s, episode steps: 930, steps per second:  46, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.109 [0.000, 5.000],  loss: 0.015583, mae: 2.527734, mean_q: 3.040514, mean_eps: 0.429318
 2854654/6000000: episode: 3886, duration: 15.931s, episode steps: 781, steps per second:  49, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.999 [0.000, 5.000],  loss: 0.015221, mae: 2.527811, mean_q: 3.040419, mean_eps: 0.429147
 2855527/6000000: episode: 3887, duration: 18.327s, episode steps: 873, steps per second:  48, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.891 [0.000, 5.000],  loss: 0.015078, mae: 2.515354, mean_q: 3.027304, mean_eps: 0.428982
 2856676/6000000: episode: 3888, duration: 24.387s, episode steps: 1149, steps per second:  47, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.032 [0.000, 5.000],  loss: 0.015829, mae: 2.548171, mean_q: 3.067719, mean_eps: 0.428780
 2857724/6000000: episode: 3889, duration: 24.311s, episode steps: 1048, steps per second:  43, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.014258, mae: 2.521868, mean_q: 3.034456, mean_eps: 0.428560
 2858563/6000000: episode: 3890, duration: 21.484s, episode steps: 839, steps per second:  39, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.082 [0.000, 5.000],  loss: 0.015596, mae: 2.550045, mean_q: 3.068839, mean_eps: 0.428372
 2859630/6000000: episode: 3891, duration: 25.205s, episode steps: 1067, steps per second:  42, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.013785, mae: 2.548344, mean_q: 3.065765, mean_eps: 0.428181
 2860530/6000000: episode: 3892, duration: 20.303s, episode steps: 900, steps per second:  44, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.014874, mae: 2.521478, mean_q: 3.034344, mean_eps: 0.427984
 2861675/6000000: episode: 3893, duration: 25.180s, episode steps: 1145, steps per second:  45, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.013478, mae: 2.523972, mean_q: 3.036546, mean_eps: 0.427780
 2862735/6000000: episode: 3894, duration: 24.823s, episode steps: 1060, steps per second:  43, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.242 [0.000, 5.000],  loss: 0.015756, mae: 2.531117, mean_q: 3.044773, mean_eps: 0.427559
 2863393/6000000: episode: 3895, duration: 15.522s, episode steps: 658, steps per second:  42, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.015187, mae: 2.520096, mean_q: 3.031735, mean_eps: 0.427387
 2864554/6000000: episode: 3896, duration: 25.638s, episode steps: 1161, steps per second:  45, episode reward: 31.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.117 [0.000, 5.000],  loss: 0.015876, mae: 2.536969, mean_q: 3.052600, mean_eps: 0.427205
 2865349/6000000: episode: 3897, duration: 17.757s, episode steps: 795, steps per second:  45, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.104 [0.000, 5.000],  loss: 0.016279, mae: 2.536288, mean_q: 3.053232, mean_eps: 0.427010
 2866608/6000000: episode: 3898, duration: 28.330s, episode steps: 1259, steps per second:  44, episode reward: 34.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.142 [0.000, 5.000],  loss: 0.014946, mae: 2.544334, mean_q: 3.062898, mean_eps: 0.426804
 2867236/6000000: episode: 3899, duration: 12.901s, episode steps: 628, steps per second:  49, episode reward: 18.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.017794, mae: 2.539120, mean_q: 3.058697, mean_eps: 0.426616
 2868078/6000000: episode: 3900, duration: 17.777s, episode steps: 842, steps per second:  47, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.141 [0.000, 5.000],  loss: 0.015814, mae: 2.542306, mean_q: 3.058748, mean_eps: 0.426469
 2869011/6000000: episode: 3901, duration: 20.678s, episode steps: 933, steps per second:  45, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.959 [0.000, 5.000],  loss: 0.012865, mae: 2.522665, mean_q: 3.036507, mean_eps: 0.426291
 2869427/6000000: episode: 3902, duration: 8.785s, episode steps: 416, steps per second:  47, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.016389, mae: 2.537531, mean_q: 3.052860, mean_eps: 0.426156
 2870267/6000000: episode: 3903, duration: 17.436s, episode steps: 840, steps per second:  48, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.098 [0.000, 5.000],  loss: 0.015736, mae: 2.533774, mean_q: 3.047817, mean_eps: 0.426031
 2871005/6000000: episode: 3904, duration: 16.005s, episode steps: 738, steps per second:  46, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.287 [0.000, 5.000],  loss: 0.016125, mae: 2.541647, mean_q: 3.057006, mean_eps: 0.425873
 2872069/6000000: episode: 3905, duration: 22.677s, episode steps: 1064, steps per second:  47, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.055 [0.000, 5.000],  loss: 0.014097, mae: 2.538061, mean_q: 3.054371, mean_eps: 0.425692
 2872774/6000000: episode: 3906, duration: 16.061s, episode steps: 705, steps per second:  44, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.418 [0.000, 5.000],  loss: 0.015934, mae: 2.588113, mean_q: 3.115074, mean_eps: 0.425516
 2873724/6000000: episode: 3907, duration: 22.315s, episode steps: 950, steps per second:  43, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.933 [0.000, 5.000],  loss: 0.014630, mae: 2.543787, mean_q: 3.063878, mean_eps: 0.425350
 2874369/6000000: episode: 3908, duration: 15.738s, episode steps: 645, steps per second:  41, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.015758, mae: 2.561976, mean_q: 3.080585, mean_eps: 0.425191
 2874872/6000000: episode: 3909, duration: 11.455s, episode steps: 503, steps per second:  44, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.015774, mae: 2.544843, mean_q: 3.061703, mean_eps: 0.425076
 2875403/6000000: episode: 3910, duration: 11.761s, episode steps: 531, steps per second:  45, episode reward: 14.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.147 [0.000, 5.000],  loss: 0.014857, mae: 2.565748, mean_q: 3.088948, mean_eps: 0.424973
 2876269/6000000: episode: 3911, duration: 19.622s, episode steps: 866, steps per second:  44, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.001 [0.000, 5.000],  loss: 0.017555, mae: 2.577073, mean_q: 3.102532, mean_eps: 0.424833
 2877179/6000000: episode: 3912, duration: 20.744s, episode steps: 910, steps per second:  44, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.980 [0.000, 5.000],  loss: 0.014657, mae: 2.560465, mean_q: 3.082606, mean_eps: 0.424655
 2877936/6000000: episode: 3913, duration: 16.601s, episode steps: 757, steps per second:  46, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.132 [0.000, 5.000],  loss: 0.015390, mae: 2.570281, mean_q: 3.092945, mean_eps: 0.424489
 2878569/6000000: episode: 3914, duration: 13.826s, episode steps: 633, steps per second:  46, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.896 [0.000, 5.000],  loss: 0.015643, mae: 2.567635, mean_q: 3.090642, mean_eps: 0.424350
 2879411/6000000: episode: 3915, duration: 20.027s, episode steps: 842, steps per second:  42, episode reward: 26.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.688 [0.000, 5.000],  loss: 0.013650, mae: 2.571252, mean_q: 3.095224, mean_eps: 0.424202
 2880245/6000000: episode: 3916, duration: 18.949s, episode steps: 834, steps per second:  44, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.199 [0.000, 5.000],  loss: 0.016987, mae: 2.557864, mean_q: 3.077841, mean_eps: 0.424034
 2881145/6000000: episode: 3917, duration: 20.066s, episode steps: 900, steps per second:  45, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.181 [0.000, 5.000],  loss: 0.013403, mae: 2.568129, mean_q: 3.092066, mean_eps: 0.423861
 2881530/6000000: episode: 3918, duration: 8.232s, episode steps: 385, steps per second:  47, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.694 [0.000, 5.000],  loss: 0.015944, mae: 2.569461, mean_q: 3.091592, mean_eps: 0.423732
 2882186/6000000: episode: 3919, duration: 14.177s, episode steps: 656, steps per second:  46, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.215 [0.000, 5.000],  loss: 0.015721, mae: 2.575024, mean_q: 3.097847, mean_eps: 0.423628
 2882863/6000000: episode: 3920, duration: 14.415s, episode steps: 677, steps per second:  47, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.016426, mae: 2.575469, mean_q: 3.099730, mean_eps: 0.423495
 2883901/6000000: episode: 3921, duration: 21.272s, episode steps: 1038, steps per second:  49, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.234 [0.000, 5.000],  loss: 0.014594, mae: 2.572540, mean_q: 3.096488, mean_eps: 0.423324
 2884594/6000000: episode: 3922, duration: 14.695s, episode steps: 693, steps per second:  47, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.084 [0.000, 5.000],  loss: 0.015806, mae: 2.580196, mean_q: 3.107463, mean_eps: 0.423150
 2885484/6000000: episode: 3923, duration: 20.061s, episode steps: 890, steps per second:  44, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.047 [0.000, 5.000],  loss: 0.014409, mae: 2.568296, mean_q: 3.089243, mean_eps: 0.422992
 2886002/6000000: episode: 3924, duration: 10.642s, episode steps: 518, steps per second:  49, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.149 [0.000, 5.000],  loss: 0.015176, mae: 2.588360, mean_q: 3.113500, mean_eps: 0.422852
 2886768/6000000: episode: 3925, duration: 15.977s, episode steps: 766, steps per second:  48, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.817 [0.000, 5.000],  loss: 0.015239, mae: 2.591998, mean_q: 3.119501, mean_eps: 0.422723
 2887844/6000000: episode: 3926, duration: 23.544s, episode steps: 1076, steps per second:  46, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.016370, mae: 2.573889, mean_q: 3.096812, mean_eps: 0.422539
 2888633/6000000: episode: 3927, duration: 17.081s, episode steps: 789, steps per second:  46, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.653 [0.000, 5.000],  loss: 0.014292, mae: 2.572310, mean_q: 3.094840, mean_eps: 0.422352
 2889136/6000000: episode: 3928, duration: 11.209s, episode steps: 503, steps per second:  45, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.015741, mae: 2.557655, mean_q: 3.076407, mean_eps: 0.422223
 2890050/6000000: episode: 3929, duration: 21.189s, episode steps: 914, steps per second:  43, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.015458, mae: 2.585961, mean_q: 3.111458, mean_eps: 0.422082
 2890895/6000000: episode: 3930, duration: 19.823s, episode steps: 845, steps per second:  43, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.015829, mae: 2.574792, mean_q: 3.098273, mean_eps: 0.421906
 2891879/6000000: episode: 3931, duration: 22.477s, episode steps: 984, steps per second:  44, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.323 [0.000, 5.000],  loss: 0.014939, mae: 2.574924, mean_q: 3.099961, mean_eps: 0.421723
 2892733/6000000: episode: 3932, duration: 18.583s, episode steps: 854, steps per second:  46, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.272 [0.000, 5.000],  loss: 0.013886, mae: 2.584385, mean_q: 3.110707, mean_eps: 0.421539
 2893392/6000000: episode: 3933, duration: 14.313s, episode steps: 659, steps per second:  46, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.185 [0.000, 5.000],  loss: 0.014378, mae: 2.585728, mean_q: 3.108356, mean_eps: 0.421388
 2894152/6000000: episode: 3934, duration: 16.201s, episode steps: 760, steps per second:  47, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.351 [0.000, 5.000],  loss: 0.012913, mae: 2.589515, mean_q: 3.115370, mean_eps: 0.421246
 2894944/6000000: episode: 3935, duration: 17.955s, episode steps: 792, steps per second:  44, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.795 [0.000, 5.000],  loss: 0.014801, mae: 2.594138, mean_q: 3.120336, mean_eps: 0.421091
 2895668/6000000: episode: 3936, duration: 17.088s, episode steps: 724, steps per second:  42, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.015215, mae: 2.558125, mean_q: 3.076928, mean_eps: 0.420939
 2896227/6000000: episode: 3937, duration: 13.913s, episode steps: 559, steps per second:  40, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.996 [0.000, 5.000],  loss: 0.014790, mae: 2.573346, mean_q: 3.094502, mean_eps: 0.420811
 2897045/6000000: episode: 3938, duration: 17.786s, episode steps: 818, steps per second:  46, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.133 [0.000, 5.000],  loss: 0.015573, mae: 2.581561, mean_q: 3.106793, mean_eps: 0.420673
 2898599/6000000: episode: 3939, duration: 34.704s, episode steps: 1554, steps per second:  45, episode reward: 34.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.152 [0.000, 5.000],  loss: 0.014667, mae: 2.572669, mean_q: 3.094982, mean_eps: 0.420436
 2899667/6000000: episode: 3940, duration: 21.670s, episode steps: 1068, steps per second:  49, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.818 [0.000, 5.000],  loss: 0.016188, mae: 2.585775, mean_q: 3.112170, mean_eps: 0.420174
 2900385/6000000: episode: 3941, duration: 14.729s, episode steps: 718, steps per second:  49, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.808 [0.000, 5.000],  loss: 0.015916, mae: 2.584342, mean_q: 3.111949, mean_eps: 0.419995
 2901214/6000000: episode: 3942, duration: 18.480s, episode steps: 829, steps per second:  45, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.015679, mae: 2.592277, mean_q: 3.117416, mean_eps: 0.419840
 2902028/6000000: episode: 3943, duration: 18.306s, episode steps: 814, steps per second:  44, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.904 [0.000, 5.000],  loss: 0.015198, mae: 2.611340, mean_q: 3.141585, mean_eps: 0.419676
 2902774/6000000: episode: 3944, duration: 16.247s, episode steps: 746, steps per second:  46, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.845 [0.000, 5.000],  loss: 0.014354, mae: 2.582932, mean_q: 3.106917, mean_eps: 0.419520
 2903443/6000000: episode: 3945, duration: 14.209s, episode steps: 669, steps per second:  47, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.014475, mae: 2.595195, mean_q: 3.122455, mean_eps: 0.419378
 2904366/6000000: episode: 3946, duration: 20.087s, episode steps: 923, steps per second:  46, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.713 [0.000, 5.000],  loss: 0.015894, mae: 2.590291, mean_q: 3.117439, mean_eps: 0.419219
 2905192/6000000: episode: 3947, duration: 18.492s, episode steps: 826, steps per second:  45, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.979 [0.000, 5.000],  loss: 0.015033, mae: 2.595491, mean_q: 3.122893, mean_eps: 0.419044
 2906085/6000000: episode: 3948, duration: 21.085s, episode steps: 893, steps per second:  42, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.016036, mae: 2.611670, mean_q: 3.142073, mean_eps: 0.418872
 2907015/6000000: episode: 3949, duration: 20.516s, episode steps: 930, steps per second:  45, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.018009, mae: 2.611341, mean_q: 3.139504, mean_eps: 0.418690
 2907500/6000000: episode: 3950, duration: 10.617s, episode steps: 485, steps per second:  46, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.126 [0.000, 5.000],  loss: 0.015677, mae: 2.636524, mean_q: 3.172273, mean_eps: 0.418549
 2908134/6000000: episode: 3951, duration: 14.173s, episode steps: 634, steps per second:  45, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.856 [0.000, 5.000],  loss: 0.014046, mae: 2.621538, mean_q: 3.155826, mean_eps: 0.418437
 2909012/6000000: episode: 3952, duration: 18.596s, episode steps: 878, steps per second:  47, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.833 [0.000, 5.000],  loss: 0.014988, mae: 2.613704, mean_q: 3.143989, mean_eps: 0.418286
 2909888/6000000: episode: 3953, duration: 19.424s, episode steps: 876, steps per second:  45, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.154 [0.000, 5.000],  loss: 0.013767, mae: 2.620007, mean_q: 3.152508, mean_eps: 0.418110
 2910946/6000000: episode: 3954, duration: 22.912s, episode steps: 1058, steps per second:  46, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.013729, mae: 2.608415, mean_q: 3.138506, mean_eps: 0.417917
 2911727/6000000: episode: 3955, duration: 17.340s, episode steps: 781, steps per second:  45, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.257 [0.000, 5.000],  loss: 0.015393, mae: 2.613262, mean_q: 3.143728, mean_eps: 0.417733
 2912656/6000000: episode: 3956, duration: 21.414s, episode steps: 929, steps per second:  43, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.014787, mae: 2.627804, mean_q: 3.162337, mean_eps: 0.417562
 2913592/6000000: episode: 3957, duration: 20.994s, episode steps: 936, steps per second:  45, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.075 [0.000, 5.000],  loss: 0.015045, mae: 2.612462, mean_q: 3.142908, mean_eps: 0.417376
 2914496/6000000: episode: 3958, duration: 20.087s, episode steps: 904, steps per second:  45, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.043 [0.000, 5.000],  loss: 0.015376, mae: 2.615201, mean_q: 3.147178, mean_eps: 0.417192
 2915711/6000000: episode: 3959, duration: 26.786s, episode steps: 1215, steps per second:  45, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.018 [0.000, 5.000],  loss: 0.016353, mae: 2.606146, mean_q: 3.136564, mean_eps: 0.416980
 2917346/6000000: episode: 3960, duration: 33.984s, episode steps: 1635, steps per second:  48, episode reward: 51.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.107 [0.000, 5.000],  loss: 0.014791, mae: 2.583589, mean_q: 3.109163, mean_eps: 0.416694
 2918196/6000000: episode: 3961, duration: 18.013s, episode steps: 850, steps per second:  47, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.188 [0.000, 5.000],  loss: 0.016304, mae: 2.592501, mean_q: 3.116875, mean_eps: 0.416446
 2918684/6000000: episode: 3962, duration: 10.372s, episode steps: 488, steps per second:  47, episode reward: 12.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.015474, mae: 2.601293, mean_q: 3.128511, mean_eps: 0.416312
 2919893/6000000: episode: 3963, duration: 25.559s, episode steps: 1209, steps per second:  47, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.155 [0.000, 5.000],  loss: 0.016473, mae: 2.593685, mean_q: 3.119696, mean_eps: 0.416142
 2920807/6000000: episode: 3964, duration: 20.006s, episode steps: 914, steps per second:  46, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.015785, mae: 2.598308, mean_q: 3.126386, mean_eps: 0.415930
 2921342/6000000: episode: 3965, duration: 12.426s, episode steps: 535, steps per second:  43, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.026 [0.000, 5.000],  loss: 0.015741, mae: 2.583013, mean_q: 3.110098, mean_eps: 0.415785
 2922481/6000000: episode: 3966, duration: 27.921s, episode steps: 1139, steps per second:  41, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.266 [0.000, 5.000],  loss: 0.014314, mae: 2.596606, mean_q: 3.124344, mean_eps: 0.415618
 2923213/6000000: episode: 3967, duration: 19.191s, episode steps: 732, steps per second:  38, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.015633, mae: 2.597795, mean_q: 3.127619, mean_eps: 0.415430
 2923694/6000000: episode: 3968, duration: 10.938s, episode steps: 481, steps per second:  44, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.015635, mae: 2.590187, mean_q: 3.117430, mean_eps: 0.415309
 2924588/6000000: episode: 3969, duration: 19.443s, episode steps: 894, steps per second:  46, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.008 [0.000, 5.000],  loss: 0.016569, mae: 2.602369, mean_q: 3.130079, mean_eps: 0.415172
 2925144/6000000: episode: 3970, duration: 12.438s, episode steps: 556, steps per second:  45, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.155 [0.000, 5.000],  loss: 0.015750, mae: 2.590217, mean_q: 3.117954, mean_eps: 0.415027
 2925828/6000000: episode: 3971, duration: 15.741s, episode steps: 684, steps per second:  43, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.016752, mae: 2.571703, mean_q: 3.095227, mean_eps: 0.414903
 2926745/6000000: episode: 3972, duration: 19.730s, episode steps: 917, steps per second:  46, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.057 [0.000, 5.000],  loss: 0.015668, mae: 2.587540, mean_q: 3.115332, mean_eps: 0.414743
 2927411/6000000: episode: 3973, duration: 13.492s, episode steps: 666, steps per second:  49, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.044 [0.000, 5.000],  loss: 0.014215, mae: 2.601734, mean_q: 3.132277, mean_eps: 0.414584
 2928203/6000000: episode: 3974, duration: 17.402s, episode steps: 792, steps per second:  46, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.014727, mae: 2.589909, mean_q: 3.118522, mean_eps: 0.414439
 2929315/6000000: episode: 3975, duration: 24.373s, episode steps: 1112, steps per second:  46, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.883 [0.000, 5.000],  loss: 0.014385, mae: 2.568445, mean_q: 3.092637, mean_eps: 0.414248
 2930163/6000000: episode: 3976, duration: 18.287s, episode steps: 848, steps per second:  46, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.016434, mae: 2.559264, mean_q: 3.080134, mean_eps: 0.414052
 2930717/6000000: episode: 3977, duration: 12.191s, episode steps: 554, steps per second:  45, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.015149, mae: 2.559803, mean_q: 3.080290, mean_eps: 0.413912
 2931959/6000000: episode: 3978, duration: 26.671s, episode steps: 1242, steps per second:  47, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.667 [0.000, 5.000],  loss: 0.015776, mae: 2.556536, mean_q: 3.079857, mean_eps: 0.413732
 2932786/6000000: episode: 3979, duration: 16.906s, episode steps: 827, steps per second:  49, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.086 [0.000, 5.000],  loss: 0.017418, mae: 2.557515, mean_q: 3.076619, mean_eps: 0.413526
 2933908/6000000: episode: 3980, duration: 23.927s, episode steps: 1122, steps per second:  47, episode reward: 31.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.048 [0.000, 5.000],  loss: 0.014827, mae: 2.566966, mean_q: 3.088769, mean_eps: 0.413331
 2934592/6000000: episode: 3981, duration: 15.306s, episode steps: 684, steps per second:  45, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.194 [0.000, 5.000],  loss: 0.016667, mae: 2.573484, mean_q: 3.095703, mean_eps: 0.413150
 2935367/6000000: episode: 3982, duration: 16.898s, episode steps: 775, steps per second:  46, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.883 [0.000, 5.000],  loss: 0.015114, mae: 2.565894, mean_q: 3.086762, mean_eps: 0.413004
 2936570/6000000: episode: 3983, duration: 26.172s, episode steps: 1203, steps per second:  46, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.037 [0.000, 5.000],  loss: 0.014310, mae: 2.541315, mean_q: 3.057745, mean_eps: 0.412806
 2937390/6000000: episode: 3984, duration: 18.335s, episode steps: 820, steps per second:  45, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.016249, mae: 2.548248, mean_q: 3.066129, mean_eps: 0.412604
 2938302/6000000: episode: 3985, duration: 21.619s, episode steps: 912, steps per second:  42, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.100 [0.000, 5.000],  loss: 0.014457, mae: 2.536432, mean_q: 3.052008, mean_eps: 0.412431
 2939361/6000000: episode: 3986, duration: 28.946s, episode steps: 1059, steps per second:  37, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.973 [0.000, 5.000],  loss: 0.015353, mae: 2.544497, mean_q: 3.063122, mean_eps: 0.412234
 2939888/6000000: episode: 3987, duration: 11.627s, episode steps: 527, steps per second:  45, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.016519, mae: 2.507065, mean_q: 3.015887, mean_eps: 0.412075
 2940682/6000000: episode: 3988, duration: 18.047s, episode steps: 794, steps per second:  44, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.053 [0.000, 5.000],  loss: 0.015257, mae: 2.549484, mean_q: 3.069809, mean_eps: 0.411943
 2941612/6000000: episode: 3989, duration: 20.154s, episode steps: 930, steps per second:  46, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.012 [0.000, 5.000],  loss: 0.014760, mae: 2.556870, mean_q: 3.080233, mean_eps: 0.411771
 2942508/6000000: episode: 3990, duration: 19.333s, episode steps: 896, steps per second:  46, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.878 [0.000, 5.000],  loss: 0.016456, mae: 2.571571, mean_q: 3.097337, mean_eps: 0.411588
 2943057/6000000: episode: 3991, duration: 11.909s, episode steps: 549, steps per second:  46, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.017446, mae: 2.572928, mean_q: 3.096507, mean_eps: 0.411444
 2943667/6000000: episode: 3992, duration: 12.667s, episode steps: 610, steps per second:  48, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.189 [0.000, 5.000],  loss: 0.015123, mae: 2.556790, mean_q: 3.077739, mean_eps: 0.411328
 2944470/6000000: episode: 3993, duration: 17.983s, episode steps: 803, steps per second:  45, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.189 [0.000, 5.000],  loss: 0.013506, mae: 2.563738, mean_q: 3.086508, mean_eps: 0.411186
 2945354/6000000: episode: 3994, duration: 19.962s, episode steps: 884, steps per second:  44, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.015156, mae: 2.567460, mean_q: 3.091134, mean_eps: 0.411018
 2946393/6000000: episode: 3995, duration: 22.869s, episode steps: 1039, steps per second:  45, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.236 [0.000, 5.000],  loss: 0.014487, mae: 2.551406, mean_q: 3.071793, mean_eps: 0.410825
 2947018/6000000: episode: 3996, duration: 13.420s, episode steps: 625, steps per second:  47, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.015678, mae: 2.561633, mean_q: 3.084104, mean_eps: 0.410659
 2947950/6000000: episode: 3997, duration: 19.608s, episode steps: 932, steps per second:  48, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.149 [0.000, 5.000],  loss: 0.016020, mae: 2.569330, mean_q: 3.092424, mean_eps: 0.410503
 2948651/6000000: episode: 3998, duration: 14.047s, episode steps: 701, steps per second:  50, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.981 [0.000, 5.000],  loss: 0.014796, mae: 2.561150, mean_q: 3.084077, mean_eps: 0.410340
 2949511/6000000: episode: 3999, duration: 17.855s, episode steps: 860, steps per second:  48, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.014249, mae: 2.559796, mean_q: 3.081620, mean_eps: 0.410184
 2950294/6000000: episode: 4000, duration: 17.614s, episode steps: 783, steps per second:  44, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.964 [0.000, 5.000],  loss: 0.015573, mae: 2.561615, mean_q: 3.086000, mean_eps: 0.410020
 2950965/6000000: episode: 4001, duration: 14.335s, episode steps: 671, steps per second:  47, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.902 [0.000, 5.000],  loss: 0.015077, mae: 2.531511, mean_q: 3.048266, mean_eps: 0.409874
 2952155/6000000: episode: 4002, duration: 24.689s, episode steps: 1190, steps per second:  48, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.019 [0.000, 5.000],  loss: 0.015715, mae: 2.544408, mean_q: 3.062426, mean_eps: 0.409688
 2952706/6000000: episode: 4003, duration: 11.683s, episode steps: 551, steps per second:  47, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.016920, mae: 2.555297, mean_q: 3.074505, mean_eps: 0.409514
 2953240/6000000: episode: 4004, duration: 11.470s, episode steps: 534, steps per second:  47, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.002 [0.000, 5.000],  loss: 0.015685, mae: 2.548607, mean_q: 3.069866, mean_eps: 0.409406
 2954207/6000000: episode: 4005, duration: 21.065s, episode steps: 967, steps per second:  46, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.205 [0.000, 5.000],  loss: 0.016153, mae: 2.552892, mean_q: 3.072993, mean_eps: 0.409256
 2955166/6000000: episode: 4006, duration: 21.837s, episode steps: 959, steps per second:  44, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.017292, mae: 2.565630, mean_q: 3.086289, mean_eps: 0.409063
 2956039/6000000: episode: 4007, duration: 21.385s, episode steps: 873, steps per second:  41, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.016551, mae: 2.547812, mean_q: 3.067480, mean_eps: 0.408880
 2956916/6000000: episode: 4008, duration: 19.057s, episode steps: 877, steps per second:  46, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.981 [0.000, 5.000],  loss: 0.015809, mae: 2.546054, mean_q: 3.064271, mean_eps: 0.408705
 2958049/6000000: episode: 4009, duration: 24.450s, episode steps: 1133, steps per second:  46, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.114 [0.000, 5.000],  loss: 0.015104, mae: 2.536257, mean_q: 3.051447, mean_eps: 0.408504
 2958951/6000000: episode: 4010, duration: 19.728s, episode steps: 902, steps per second:  46, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.870 [0.000, 5.000],  loss: 0.014611, mae: 2.534325, mean_q: 3.049257, mean_eps: 0.408300
 2959802/6000000: episode: 4011, duration: 17.832s, episode steps: 851, steps per second:  48, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.092 [0.000, 5.000],  loss: 0.013986, mae: 2.533317, mean_q: 3.047884, mean_eps: 0.408125
 2960701/6000000: episode: 4012, duration: 19.549s, episode steps: 899, steps per second:  46, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.909 [0.000, 5.000],  loss: 0.014950, mae: 2.507412, mean_q: 3.015973, mean_eps: 0.407950
 2961515/6000000: episode: 4013, duration: 19.425s, episode steps: 814, steps per second:  42, episode reward: 25.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.985 [0.000, 5.000],  loss: 0.015868, mae: 2.508077, mean_q: 3.019002, mean_eps: 0.407778
 2962423/6000000: episode: 4014, duration: 19.587s, episode steps: 908, steps per second:  46, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.068 [0.000, 5.000],  loss: 0.014373, mae: 2.502858, mean_q: 3.012418, mean_eps: 0.407606
 2963292/6000000: episode: 4015, duration: 18.978s, episode steps: 869, steps per second:  46, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.815 [0.000, 5.000],  loss: 0.017349, mae: 2.502149, mean_q: 3.009963, mean_eps: 0.407429
 2964775/6000000: episode: 4016, duration: 31.132s, episode steps: 1483, steps per second:  48, episode reward: 33.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.838 [0.000, 5.000],  loss: 0.015401, mae: 2.487054, mean_q: 2.994443, mean_eps: 0.407194
 2965749/6000000: episode: 4017, duration: 19.760s, episode steps: 974, steps per second:  49, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.169 [0.000, 5.000],  loss: 0.014185, mae: 2.491768, mean_q: 3.000701, mean_eps: 0.406948
 2966733/6000000: episode: 4018, duration: 22.546s, episode steps: 984, steps per second:  44, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.042 [0.000, 5.000],  loss: 0.015336, mae: 2.509756, mean_q: 3.020828, mean_eps: 0.406752
 2967227/6000000: episode: 4019, duration: 10.563s, episode steps: 494, steps per second:  47, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.830 [0.000, 5.000],  loss: 0.017888, mae: 2.513642, mean_q: 3.027511, mean_eps: 0.406604
 2968160/6000000: episode: 4020, duration: 19.631s, episode steps: 933, steps per second:  48, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.015605, mae: 2.496129, mean_q: 3.004330, mean_eps: 0.406462
 2969234/6000000: episode: 4021, duration: 21.908s, episode steps: 1074, steps per second:  49, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 0.016113, mae: 2.510199, mean_q: 3.019694, mean_eps: 0.406261
 2969901/6000000: episode: 4022, duration: 14.285s, episode steps: 667, steps per second:  47, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.115 [0.000, 5.000],  loss: 0.014741, mae: 2.503540, mean_q: 3.012012, mean_eps: 0.406086
 2970658/6000000: episode: 4023, duration: 16.983s, episode steps: 757, steps per second:  45, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.177 [0.000, 5.000],  loss: 0.012454, mae: 2.502519, mean_q: 3.013281, mean_eps: 0.405944
 2971178/6000000: episode: 4024, duration: 11.507s, episode steps: 520, steps per second:  45, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.837 [0.000, 5.000],  loss: 0.016890, mae: 2.516541, mean_q: 3.028032, mean_eps: 0.405816
 2972057/6000000: episode: 4025, duration: 22.319s, episode steps: 879, steps per second:  39, episode reward: 26.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.144 [0.000, 5.000],  loss: 0.015325, mae: 2.499684, mean_q: 3.006953, mean_eps: 0.405676
 2972951/6000000: episode: 4026, duration: 21.643s, episode steps: 894, steps per second:  41, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.917 [0.000, 5.000],  loss: 0.016272, mae: 2.499014, mean_q: 3.007488, mean_eps: 0.405499
 2973912/6000000: episode: 4027, duration: 23.046s, episode steps: 961, steps per second:  42, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.248 [0.000, 5.000],  loss: 0.016561, mae: 2.499888, mean_q: 3.007319, mean_eps: 0.405314
 2974715/6000000: episode: 4028, duration: 17.336s, episode steps: 803, steps per second:  46, episode reward: 24.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.015122, mae: 2.495556, mean_q: 3.004423, mean_eps: 0.405138
 2975648/6000000: episode: 4029, duration: 20.497s, episode steps: 933, steps per second:  46, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.070 [0.000, 5.000],  loss: 0.016064, mae: 2.537020, mean_q: 3.052664, mean_eps: 0.404964
 2976616/6000000: episode: 4030, duration: 20.304s, episode steps: 968, steps per second:  48, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.319 [0.000, 5.000],  loss: 0.015822, mae: 2.524064, mean_q: 3.037066, mean_eps: 0.404774
 2977759/6000000: episode: 4031, duration: 24.988s, episode steps: 1143, steps per second:  46, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.016723, mae: 2.542862, mean_q: 3.060645, mean_eps: 0.404563
 2978545/6000000: episode: 4032, duration: 17.478s, episode steps: 786, steps per second:  45, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.016167, mae: 2.530978, mean_q: 3.045422, mean_eps: 0.404370
 2979501/6000000: episode: 4033, duration: 20.709s, episode steps: 956, steps per second:  46, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.027 [0.000, 5.000],  loss: 0.017067, mae: 2.531771, mean_q: 3.046184, mean_eps: 0.404195
 2980350/6000000: episode: 4034, duration: 18.567s, episode steps: 849, steps per second:  46, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.014398, mae: 2.541086, mean_q: 3.058289, mean_eps: 0.404015
 2981206/6000000: episode: 4035, duration: 17.718s, episode steps: 856, steps per second:  48, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.228 [0.000, 5.000],  loss: 0.015038, mae: 2.528699, mean_q: 3.045603, mean_eps: 0.403844
 2982093/6000000: episode: 4036, duration: 18.121s, episode steps: 887, steps per second:  49, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.064 [0.000, 5.000],  loss: 0.016111, mae: 2.529725, mean_q: 3.044093, mean_eps: 0.403670
 2982573/6000000: episode: 4037, duration: 10.140s, episode steps: 480, steps per second:  47, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.017388, mae: 2.531089, mean_q: 3.045123, mean_eps: 0.403533
 2983346/6000000: episode: 4038, duration: 15.874s, episode steps: 773, steps per second:  49, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.887 [0.000, 5.000],  loss: 0.015619, mae: 2.554093, mean_q: 3.072310, mean_eps: 0.403408
 2984401/6000000: episode: 4039, duration: 21.926s, episode steps: 1055, steps per second:  48, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.015165, mae: 2.544060, mean_q: 3.061059, mean_eps: 0.403225
 2985265/6000000: episode: 4040, duration: 18.158s, episode steps: 864, steps per second:  48, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.182 [0.000, 5.000],  loss: 0.015628, mae: 2.536403, mean_q: 3.051858, mean_eps: 0.403033
 2985899/6000000: episode: 4041, duration: 13.490s, episode steps: 634, steps per second:  47, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.942 [0.000, 5.000],  loss: 0.016092, mae: 2.545783, mean_q: 3.062136, mean_eps: 0.402884
 2986400/6000000: episode: 4042, duration: 10.841s, episode steps: 501, steps per second:  46, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.012606, mae: 2.568243, mean_q: 3.091961, mean_eps: 0.402770
 2987134/6000000: episode: 4043, duration: 16.891s, episode steps: 734, steps per second:  43, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.015827, mae: 2.543513, mean_q: 3.060045, mean_eps: 0.402647
 2987923/6000000: episode: 4044, duration: 17.914s, episode steps: 789, steps per second:  44, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: 0.014215, mae: 2.549484, mean_q: 3.068910, mean_eps: 0.402494
 2988457/6000000: episode: 4045, duration: 12.647s, episode steps: 534, steps per second:  42, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.077 [0.000, 5.000],  loss: 0.016039, mae: 2.542535, mean_q: 3.061136, mean_eps: 0.402362
 2989368/6000000: episode: 4046, duration: 20.594s, episode steps: 911, steps per second:  44, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.865 [0.000, 5.000],  loss: 0.017144, mae: 2.547776, mean_q: 3.066183, mean_eps: 0.402218
 2990231/6000000: episode: 4047, duration: 19.039s, episode steps: 863, steps per second:  45, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.095 [0.000, 5.000],  loss: 0.017864, mae: 2.545333, mean_q: 3.064814, mean_eps: 0.402040
 2990866/6000000: episode: 4048, duration: 14.148s, episode steps: 635, steps per second:  45, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.950 [0.000, 5.000],  loss: 0.015080, mae: 2.582755, mean_q: 3.108238, mean_eps: 0.401890
 2991634/6000000: episode: 4049, duration: 16.856s, episode steps: 768, steps per second:  46, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.015417, mae: 2.573289, mean_q: 3.097620, mean_eps: 0.401750
 2992455/6000000: episode: 4050, duration: 17.674s, episode steps: 821, steps per second:  46, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.014853, mae: 2.574811, mean_q: 3.098295, mean_eps: 0.401591
 2993398/6000000: episode: 4051, duration: 21.053s, episode steps: 943, steps per second:  45, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.740 [0.000, 5.000],  loss: 0.014409, mae: 2.567798, mean_q: 3.092190, mean_eps: 0.401415
 2994240/6000000: episode: 4052, duration: 20.371s, episode steps: 842, steps per second:  41, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.196 [0.000, 5.000],  loss: 0.015423, mae: 2.585119, mean_q: 3.111454, mean_eps: 0.401236
 2995118/6000000: episode: 4053, duration: 19.349s, episode steps: 878, steps per second:  45, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.156 [0.000, 5.000],  loss: 0.016317, mae: 2.582658, mean_q: 3.107347, mean_eps: 0.401064
 2995858/6000000: episode: 4054, duration: 16.003s, episode steps: 740, steps per second:  46, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.014699, mae: 2.600923, mean_q: 3.131496, mean_eps: 0.400902
 2996386/6000000: episode: 4055, duration: 11.051s, episode steps: 528, steps per second:  48, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.015448, mae: 2.592115, mean_q: 3.121907, mean_eps: 0.400776
 2997358/6000000: episode: 4056, duration: 20.706s, episode steps: 972, steps per second:  47, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.017116, mae: 2.587698, mean_q: 3.112408, mean_eps: 0.400626
 2998298/6000000: episode: 4057, duration: 18.857s, episode steps: 940, steps per second:  50, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.247 [0.000, 5.000],  loss: 0.015831, mae: 2.595684, mean_q: 3.124154, mean_eps: 0.400434
 2998971/6000000: episode: 4058, duration: 14.384s, episode steps: 673, steps per second:  47, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.823 [0.000, 5.000],  loss: 0.016729, mae: 2.611626, mean_q: 3.141861, mean_eps: 0.400273
 2999831/6000000: episode: 4059, duration: 18.279s, episode steps: 860, steps per second:  47, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.151 [0.000, 5.000],  loss: 0.016134, mae: 2.606938, mean_q: 3.137930, mean_eps: 0.400120
 3000428/6000000: episode: 4060, duration: 12.619s, episode steps: 597, steps per second:  47, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.816 [0.000, 5.000],  loss: 0.015087, mae: 2.555945, mean_q: 3.073864, mean_eps: 0.399974
 3000966/6000000: episode: 4061, duration: 11.496s, episode steps: 538, steps per second:  47, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.013727, mae: 2.544157, mean_q: 3.064261, mean_eps: 0.399861
 3001961/6000000: episode: 4062, duration: 20.684s, episode steps: 995, steps per second:  48, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.016694, mae: 2.549935, mean_q: 3.067654, mean_eps: 0.399707
 3002816/6000000: episode: 4063, duration: 17.961s, episode steps: 855, steps per second:  48, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.143 [0.000, 5.000],  loss: 0.015325, mae: 2.551366, mean_q: 3.069452, mean_eps: 0.399522
 3003616/6000000: episode: 4064, duration: 17.444s, episode steps: 800, steps per second:  46, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.014257, mae: 2.560692, mean_q: 3.083468, mean_eps: 0.399357
 3004215/6000000: episode: 4065, duration: 13.031s, episode steps: 599, steps per second:  46, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.014341, mae: 2.565224, mean_q: 3.085905, mean_eps: 0.399217
 3004721/6000000: episode: 4066, duration: 12.399s, episode steps: 506, steps per second:  41, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.085 [0.000, 5.000],  loss: 0.015992, mae: 2.550038, mean_q: 3.066587, mean_eps: 0.399106
 3005543/6000000: episode: 4067, duration: 20.456s, episode steps: 822, steps per second:  40, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.014324, mae: 2.571369, mean_q: 3.094442, mean_eps: 0.398974
 3006488/6000000: episode: 4068, duration: 21.170s, episode steps: 945, steps per second:  45, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.120 [0.000, 5.000],  loss: 0.016056, mae: 2.558313, mean_q: 3.078083, mean_eps: 0.398797
 3007454/6000000: episode: 4069, duration: 21.076s, episode steps: 966, steps per second:  46, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.043 [0.000, 5.000],  loss: 0.015104, mae: 2.566442, mean_q: 3.090163, mean_eps: 0.398606
 3008329/6000000: episode: 4070, duration: 18.944s, episode steps: 875, steps per second:  46, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.351 [0.000, 5.000],  loss: 0.015883, mae: 2.564615, mean_q: 3.084772, mean_eps: 0.398422
 3008867/6000000: episode: 4071, duration: 11.852s, episode steps: 538, steps per second:  45, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.016999, mae: 2.581110, mean_q: 3.105019, mean_eps: 0.398280
 3009499/6000000: episode: 4072, duration: 13.374s, episode steps: 632, steps per second:  47, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.016886, mae: 2.559218, mean_q: 3.080348, mean_eps: 0.398164
 3010268/6000000: episode: 4073, duration: 18.717s, episode steps: 769, steps per second:  41, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.015942, mae: 2.564547, mean_q: 3.089341, mean_eps: 0.398024
 3011029/6000000: episode: 4074, duration: 17.574s, episode steps: 761, steps per second:  43, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.017251, mae: 2.579662, mean_q: 3.106733, mean_eps: 0.397870
 3011564/6000000: episode: 4075, duration: 11.636s, episode steps: 535, steps per second:  46, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.015032, mae: 2.577077, mean_q: 3.103261, mean_eps: 0.397741
 3012186/6000000: episode: 4076, duration: 13.874s, episode steps: 622, steps per second:  45, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.015610, mae: 2.553627, mean_q: 3.072653, mean_eps: 0.397625
 3013149/6000000: episode: 4077, duration: 20.944s, episode steps: 963, steps per second:  46, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.015660, mae: 2.574885, mean_q: 3.100499, mean_eps: 0.397466
 3014284/6000000: episode: 4078, duration: 23.691s, episode steps: 1135, steps per second:  48, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.140 [0.000, 5.000],  loss: 0.015127, mae: 2.580932, mean_q: 3.107767, mean_eps: 0.397257
 3015402/6000000: episode: 4079, duration: 23.178s, episode steps: 1118, steps per second:  48, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.034 [0.000, 5.000],  loss: 0.015449, mae: 2.561146, mean_q: 3.083674, mean_eps: 0.397032
 3016164/6000000: episode: 4080, duration: 16.424s, episode steps: 762, steps per second:  46, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.014937, mae: 2.556070, mean_q: 3.075202, mean_eps: 0.396844
 3017022/6000000: episode: 4081, duration: 19.253s, episode steps: 858, steps per second:  45, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.138 [0.000, 5.000],  loss: 0.016566, mae: 2.557878, mean_q: 3.076885, mean_eps: 0.396682
 3017544/6000000: episode: 4082, duration: 11.183s, episode steps: 522, steps per second:  47, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.960 [0.000, 5.000],  loss: 0.016038, mae: 2.561714, mean_q: 3.082859, mean_eps: 0.396544
 3018517/6000000: episode: 4083, duration: 20.963s, episode steps: 973, steps per second:  46, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.052 [0.000, 5.000],  loss: 0.015899, mae: 2.572602, mean_q: 3.094207, mean_eps: 0.396394
 3019384/6000000: episode: 4084, duration: 19.423s, episode steps: 867, steps per second:  45, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.879 [0.000, 5.000],  loss: 0.015187, mae: 2.554997, mean_q: 3.073885, mean_eps: 0.396210
 3020255/6000000: episode: 4085, duration: 19.915s, episode steps: 871, steps per second:  44, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.900 [0.000, 5.000],  loss: 0.014551, mae: 2.558317, mean_q: 3.079426, mean_eps: 0.396036
 3021576/6000000: episode: 4086, duration: 32.180s, episode steps: 1321, steps per second:  41, episode reward: 33.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.972 [0.000, 5.000],  loss: 0.017701, mae: 2.582272, mean_q: 3.106769, mean_eps: 0.395817
 3022119/6000000: episode: 4087, duration: 11.969s, episode steps: 543, steps per second:  45, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.013665, mae: 2.572715, mean_q: 3.097369, mean_eps: 0.395631
 3022823/6000000: episode: 4088, duration: 15.497s, episode steps: 704, steps per second:  45, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.804 [0.000, 5.000],  loss: 0.014464, mae: 2.575294, mean_q: 3.101781, mean_eps: 0.395506
 3023962/6000000: episode: 4089, duration: 25.351s, episode steps: 1139, steps per second:  45, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.278 [0.000, 5.000],  loss: 0.015196, mae: 2.586624, mean_q: 3.113244, mean_eps: 0.395322
 3024615/6000000: episode: 4090, duration: 14.366s, episode steps: 653, steps per second:  45, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.161 [0.000, 5.000],  loss: 0.017098, mae: 2.548288, mean_q: 3.066280, mean_eps: 0.395142
 3025117/6000000: episode: 4091, duration: 10.998s, episode steps: 502, steps per second:  46, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.004 [0.000, 5.000],  loss: 0.015453, mae: 2.536260, mean_q: 3.053714, mean_eps: 0.395027
 3025816/6000000: episode: 4092, duration: 14.545s, episode steps: 699, steps per second:  48, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.817 [0.000, 5.000],  loss: 0.015630, mae: 2.579944, mean_q: 3.105546, mean_eps: 0.394907
 3026761/6000000: episode: 4093, duration: 21.206s, episode steps: 945, steps per second:  45, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.235 [0.000, 5.000],  loss: 0.015744, mae: 2.571692, mean_q: 3.095434, mean_eps: 0.394742
 3027456/6000000: episode: 4094, duration: 15.430s, episode steps: 695, steps per second:  45, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.082 [0.000, 5.000],  loss: 0.015535, mae: 2.585941, mean_q: 3.112142, mean_eps: 0.394578
 3028317/6000000: episode: 4095, duration: 18.666s, episode steps: 861, steps per second:  46, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.247 [0.000, 5.000],  loss: 0.015522, mae: 2.567954, mean_q: 3.091924, mean_eps: 0.394423
 3029420/6000000: episode: 4096, duration: 24.114s, episode steps: 1103, steps per second:  46, episode reward: 30.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.188 [0.000, 5.000],  loss: 0.016823, mae: 2.576760, mean_q: 3.102265, mean_eps: 0.394226
 3029921/6000000: episode: 4097, duration: 11.214s, episode steps: 501, steps per second:  45, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.062 [0.000, 5.000],  loss: 0.016169, mae: 2.578915, mean_q: 3.102721, mean_eps: 0.394066
 3030640/6000000: episode: 4098, duration: 14.520s, episode steps: 719, steps per second:  50, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.016997, mae: 2.619550, mean_q: 3.153047, mean_eps: 0.393944
 3031303/6000000: episode: 4099, duration: 13.465s, episode steps: 663, steps per second:  49, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.011 [0.000, 5.000],  loss: 0.014923, mae: 2.618811, mean_q: 3.151611, mean_eps: 0.393806
 3032186/6000000: episode: 4100, duration: 18.531s, episode steps: 883, steps per second:  48, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.969 [0.000, 5.000],  loss: 0.016023, mae: 2.629086, mean_q: 3.164528, mean_eps: 0.393651
 3033046/6000000: episode: 4101, duration: 17.982s, episode steps: 860, steps per second:  48, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.226 [0.000, 5.000],  loss: 0.015862, mae: 2.632917, mean_q: 3.168764, mean_eps: 0.393477
 3033599/6000000: episode: 4102, duration: 11.950s, episode steps: 553, steps per second:  46, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.015783, mae: 2.622377, mean_q: 3.154838, mean_eps: 0.393336
 3034100/6000000: episode: 4103, duration: 10.696s, episode steps: 501, steps per second:  47, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.016391, mae: 2.662790, mean_q: 3.205775, mean_eps: 0.393230
 3035108/6000000: episode: 4104, duration: 21.509s, episode steps: 1008, steps per second:  47, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.272 [0.000, 5.000],  loss: 0.014442, mae: 2.607470, mean_q: 3.137934, mean_eps: 0.393080
 3036005/6000000: episode: 4105, duration: 20.756s, episode steps: 897, steps per second:  43, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.324 [0.000, 5.000],  loss: 0.015767, mae: 2.630671, mean_q: 3.166037, mean_eps: 0.392889
 3036949/6000000: episode: 4106, duration: 21.788s, episode steps: 944, steps per second:  43, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.175 [0.000, 5.000],  loss: 0.015799, mae: 2.631561, mean_q: 3.168091, mean_eps: 0.392704
 3037484/6000000: episode: 4107, duration: 13.076s, episode steps: 535, steps per second:  41, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.101 [0.000, 5.000],  loss: 0.013524, mae: 2.624121, mean_q: 3.156610, mean_eps: 0.392557
 3038554/6000000: episode: 4108, duration: 24.465s, episode steps: 1070, steps per second:  44, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.015828, mae: 2.616879, mean_q: 3.149801, mean_eps: 0.392396
 3039073/6000000: episode: 4109, duration: 11.284s, episode steps: 519, steps per second:  46, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.014280, mae: 2.635843, mean_q: 3.174134, mean_eps: 0.392237
 3039864/6000000: episode: 4110, duration: 17.317s, episode steps: 791, steps per second:  46, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.278 [0.000, 5.000],  loss: 0.014065, mae: 2.618801, mean_q: 3.153337, mean_eps: 0.392106
 3040815/6000000: episode: 4111, duration: 21.327s, episode steps: 951, steps per second:  45, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.015798, mae: 2.625555, mean_q: 3.159418, mean_eps: 0.391932
 3041778/6000000: episode: 4112, duration: 20.679s, episode steps: 963, steps per second:  47, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.904 [0.000, 5.000],  loss: 0.015726, mae: 2.651082, mean_q: 3.191269, mean_eps: 0.391741
 3042644/6000000: episode: 4113, duration: 19.568s, episode steps: 866, steps per second:  44, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.222 [0.000, 5.000],  loss: 0.017487, mae: 2.657143, mean_q: 3.196929, mean_eps: 0.391558
 3043537/6000000: episode: 4114, duration: 20.709s, episode steps: 893, steps per second:  43, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.245 [0.000, 5.000],  loss: 0.016772, mae: 2.653597, mean_q: 3.191839, mean_eps: 0.391382
 3044241/6000000: episode: 4115, duration: 15.361s, episode steps: 704, steps per second:  46, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.872 [0.000, 5.000],  loss: 0.015602, mae: 2.638070, mean_q: 3.175738, mean_eps: 0.391222
 3045188/6000000: episode: 4116, duration: 20.639s, episode steps: 947, steps per second:  46, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.163 [0.000, 5.000],  loss: 0.015314, mae: 2.634978, mean_q: 3.171405, mean_eps: 0.391057
 3046104/6000000: episode: 4117, duration: 20.200s, episode steps: 916, steps per second:  45, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.015908, mae: 2.601618, mean_q: 3.129386, mean_eps: 0.390871
 3047089/6000000: episode: 4118, duration: 20.155s, episode steps: 985, steps per second:  49, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.334 [0.000, 5.000],  loss: 0.017692, mae: 2.581435, mean_q: 3.107546, mean_eps: 0.390681
 3048096/6000000: episode: 4119, duration: 21.016s, episode steps: 1007, steps per second:  48, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.131 [0.000, 5.000],  loss: 0.015024, mae: 2.598826, mean_q: 3.129582, mean_eps: 0.390482
 3048825/6000000: episode: 4120, duration: 15.452s, episode steps: 729, steps per second:  47, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.173 [0.000, 5.000],  loss: 0.016180, mae: 2.577893, mean_q: 3.104338, mean_eps: 0.390308
 3049622/6000000: episode: 4121, duration: 16.772s, episode steps: 797, steps per second:  48, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.237 [0.000, 5.000],  loss: 0.015943, mae: 2.602684, mean_q: 3.130603, mean_eps: 0.390155
 3050672/6000000: episode: 4122, duration: 21.664s, episode steps: 1050, steps per second:  48, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.793 [0.000, 5.000],  loss: 0.016294, mae: 2.596990, mean_q: 3.124956, mean_eps: 0.389971
 3051368/6000000: episode: 4123, duration: 14.703s, episode steps: 696, steps per second:  47, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.769 [0.000, 5.000],  loss: 0.015410, mae: 2.572533, mean_q: 3.095222, mean_eps: 0.389796
 3052175/6000000: episode: 4124, duration: 17.351s, episode steps: 807, steps per second:  47, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.249 [0.000, 5.000],  loss: 0.016742, mae: 2.593874, mean_q: 3.121288, mean_eps: 0.389646
 3052957/6000000: episode: 4125, duration: 16.913s, episode steps: 782, steps per second:  46, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.016034, mae: 2.610038, mean_q: 3.141185, mean_eps: 0.389487
 3053909/6000000: episode: 4126, duration: 21.494s, episode steps: 952, steps per second:  44, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.921 [0.000, 5.000],  loss: 0.014760, mae: 2.589085, mean_q: 3.115110, mean_eps: 0.389313
 3055021/6000000: episode: 4127, duration: 27.136s, episode steps: 1112, steps per second:  41, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.165 [0.000, 5.000],  loss: 0.016063, mae: 2.592484, mean_q: 3.119565, mean_eps: 0.389107
 3056195/6000000: episode: 4128, duration: 26.314s, episode steps: 1174, steps per second:  45, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.874 [0.000, 5.000],  loss: 0.015314, mae: 2.577258, mean_q: 3.103350, mean_eps: 0.388878
 3056709/6000000: episode: 4129, duration: 11.904s, episode steps: 514, steps per second:  43, episode reward: 13.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.021 [0.000, 5.000],  loss: 0.014460, mae: 2.594614, mean_q: 3.121611, mean_eps: 0.388710
 3057867/6000000: episode: 4130, duration: 25.814s, episode steps: 1158, steps per second:  45, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.097 [0.000, 5.000],  loss: 0.015888, mae: 2.573845, mean_q: 3.097169, mean_eps: 0.388542
 3058759/6000000: episode: 4131, duration: 19.263s, episode steps: 892, steps per second:  46, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.318 [0.000, 5.000],  loss: 0.016244, mae: 2.589024, mean_q: 3.115430, mean_eps: 0.388338
 3059652/6000000: episode: 4132, duration: 20.094s, episode steps: 893, steps per second:  44, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.987 [0.000, 5.000],  loss: 0.016419, mae: 2.569404, mean_q: 3.092152, mean_eps: 0.388159
 3060499/6000000: episode: 4133, duration: 19.386s, episode steps: 847, steps per second:  44, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.195 [0.000, 5.000],  loss: 0.015653, mae: 2.569143, mean_q: 3.092579, mean_eps: 0.387985
 3061370/6000000: episode: 4134, duration: 19.546s, episode steps: 871, steps per second:  45, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.246 [0.000, 5.000],  loss: 0.015781, mae: 2.568367, mean_q: 3.089372, mean_eps: 0.387813
 3062301/6000000: episode: 4135, duration: 21.541s, episode steps: 931, steps per second:  43, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.962 [0.000, 5.000],  loss: 0.016467, mae: 2.584711, mean_q: 3.109276, mean_eps: 0.387633
 3063204/6000000: episode: 4136, duration: 19.230s, episode steps: 903, steps per second:  47, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.106 [0.000, 5.000],  loss: 0.015993, mae: 2.577998, mean_q: 3.099452, mean_eps: 0.387450
 3064446/6000000: episode: 4137, duration: 25.645s, episode steps: 1242, steps per second:  48, episode reward: 30.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.984 [0.000, 5.000],  loss: 0.013856, mae: 2.577099, mean_q: 3.101346, mean_eps: 0.387235
 3065297/6000000: episode: 4138, duration: 18.063s, episode steps: 851, steps per second:  47, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.941 [0.000, 5.000],  loss: 0.016454, mae: 2.585491, mean_q: 3.111392, mean_eps: 0.387026
 3065947/6000000: episode: 4139, duration: 13.574s, episode steps: 650, steps per second:  48, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.015232, mae: 2.589723, mean_q: 3.114290, mean_eps: 0.386876
 3066678/6000000: episode: 4140, duration: 15.261s, episode steps: 731, steps per second:  48, episode reward: 21.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.016396, mae: 2.588957, mean_q: 3.113534, mean_eps: 0.386738
 3067917/6000000: episode: 4141, duration: 26.150s, episode steps: 1239, steps per second:  47, episode reward: 34.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.698 [0.000, 5.000],  loss: 0.015711, mae: 2.590178, mean_q: 3.115715, mean_eps: 0.386540
 3068622/6000000: episode: 4142, duration: 14.707s, episode steps: 705, steps per second:  48, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.016862, mae: 2.586166, mean_q: 3.111502, mean_eps: 0.386346
 3069732/6000000: episode: 4143, duration: 23.854s, episode steps: 1110, steps per second:  47, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.250 [0.000, 5.000],  loss: 0.017086, mae: 2.579828, mean_q: 3.102488, mean_eps: 0.386165
 3070401/6000000: episode: 4144, duration: 15.067s, episode steps: 669, steps per second:  44, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.169 [0.000, 5.000],  loss: 0.015141, mae: 2.600657, mean_q: 3.129835, mean_eps: 0.385987
 3071593/6000000: episode: 4145, duration: 27.355s, episode steps: 1192, steps per second:  44, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.015539, mae: 2.635545, mean_q: 3.170779, mean_eps: 0.385800
 3072463/6000000: episode: 4146, duration: 19.285s, episode steps: 870, steps per second:  45, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.186 [0.000, 5.000],  loss: 0.016656, mae: 2.598288, mean_q: 3.123720, mean_eps: 0.385594
 3073431/6000000: episode: 4147, duration: 21.987s, episode steps: 968, steps per second:  44, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.147 [0.000, 5.000],  loss: 0.015873, mae: 2.631807, mean_q: 3.165373, mean_eps: 0.385411
 3074393/6000000: episode: 4148, duration: 20.463s, episode steps: 962, steps per second:  47, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.858 [0.000, 5.000],  loss: 0.015840, mae: 2.619303, mean_q: 3.150668, mean_eps: 0.385218
 3075489/6000000: episode: 4149, duration: 23.793s, episode steps: 1096, steps per second:  46, episode reward: 31.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.014949, mae: 2.608508, mean_q: 3.137794, mean_eps: 0.385012
 3076558/6000000: episode: 4150, duration: 23.922s, episode steps: 1069, steps per second:  45, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.014535, mae: 2.615478, mean_q: 3.146432, mean_eps: 0.384795
 3077117/6000000: episode: 4151, duration: 12.515s, episode steps: 559, steps per second:  45, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.571 [0.000, 5.000],  loss: 0.015263, mae: 2.604704, mean_q: 3.134411, mean_eps: 0.384632
 3078549/6000000: episode: 4152, duration: 31.194s, episode steps: 1432, steps per second:  46, episode reward: 33.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.017158, mae: 2.617975, mean_q: 3.149097, mean_eps: 0.384433
 3079591/6000000: episode: 4153, duration: 22.410s, episode steps: 1042, steps per second:  46, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.016012, mae: 2.600602, mean_q: 3.126870, mean_eps: 0.384186
 3080343/6000000: episode: 4154, duration: 15.229s, episode steps: 752, steps per second:  49, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.997 [0.000, 5.000],  loss: 0.015508, mae: 2.603090, mean_q: 3.131980, mean_eps: 0.384007
 3081060/6000000: episode: 4155, duration: 15.426s, episode steps: 717, steps per second:  46, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.727 [0.000, 5.000],  loss: 0.014363, mae: 2.581727, mean_q: 3.104925, mean_eps: 0.383860
 3081910/6000000: episode: 4156, duration: 18.782s, episode steps: 850, steps per second:  45, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.942 [0.000, 5.000],  loss: 0.016975, mae: 2.572843, mean_q: 3.094227, mean_eps: 0.383703
 3082698/6000000: episode: 4157, duration: 16.633s, episode steps: 788, steps per second:  47, episode reward: 24.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.085 [0.000, 5.000],  loss: 0.016841, mae: 2.582048, mean_q: 3.105407, mean_eps: 0.383539
 3083766/6000000: episode: 4158, duration: 22.716s, episode steps: 1068, steps per second:  47, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.287 [0.000, 5.000],  loss: 0.015630, mae: 2.579948, mean_q: 3.102896, mean_eps: 0.383354
 3084679/6000000: episode: 4159, duration: 19.998s, episode steps: 913, steps per second:  46, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.014970, mae: 2.575721, mean_q: 3.098087, mean_eps: 0.383156
 3085456/6000000: episode: 4160, duration: 17.218s, episode steps: 777, steps per second:  45, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.871 [0.000, 5.000],  loss: 0.015597, mae: 2.583376, mean_q: 3.107799, mean_eps: 0.382987
 3086425/6000000: episode: 4161, duration: 21.782s, episode steps: 969, steps per second:  44, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.017950, mae: 2.590322, mean_q: 3.115376, mean_eps: 0.382812
 3087600/6000000: episode: 4162, duration: 28.529s, episode steps: 1175, steps per second:  41, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.207 [0.000, 5.000],  loss: 0.016154, mae: 2.584462, mean_q: 3.108985, mean_eps: 0.382598
 3088398/6000000: episode: 4163, duration: 17.694s, episode steps: 798, steps per second:  45, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.015666, mae: 2.580891, mean_q: 3.102634, mean_eps: 0.382400
 3089444/6000000: episode: 4164, duration: 23.326s, episode steps: 1046, steps per second:  45, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.915 [0.000, 5.000],  loss: 0.016833, mae: 2.585676, mean_q: 3.109584, mean_eps: 0.382216
 3090140/6000000: episode: 4165, duration: 15.596s, episode steps: 696, steps per second:  45, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.016874, mae: 2.588636, mean_q: 3.113183, mean_eps: 0.382042
 3090671/6000000: episode: 4166, duration: 11.104s, episode steps: 531, steps per second:  48, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.017121, mae: 2.591078, mean_q: 3.118091, mean_eps: 0.381919
 3091358/6000000: episode: 4167, duration: 14.402s, episode steps: 687, steps per second:  48, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.017042, mae: 2.600126, mean_q: 3.128472, mean_eps: 0.381797
 3091867/6000000: episode: 4168, duration: 11.136s, episode steps: 509, steps per second:  46, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.017245, mae: 2.589461, mean_q: 3.113812, mean_eps: 0.381678
 3092684/6000000: episode: 4169, duration: 18.035s, episode steps: 817, steps per second:  45, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.015705, mae: 2.579805, mean_q: 3.103899, mean_eps: 0.381545
 3093706/6000000: episode: 4170, duration: 22.811s, episode steps: 1022, steps per second:  45, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.003 [0.000, 5.000],  loss: 0.016351, mae: 2.593101, mean_q: 3.120597, mean_eps: 0.381361
 3094622/6000000: episode: 4171, duration: 20.039s, episode steps: 916, steps per second:  46, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.245 [0.000, 5.000],  loss: 0.015221, mae: 2.593537, mean_q: 3.120100, mean_eps: 0.381167
 3095401/6000000: episode: 4172, duration: 17.140s, episode steps: 779, steps per second:  45, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.960 [0.000, 5.000],  loss: 0.016305, mae: 2.583452, mean_q: 3.111655, mean_eps: 0.380998
 3096436/6000000: episode: 4173, duration: 21.198s, episode steps: 1035, steps per second:  49, episode reward: 30.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.309 [0.000, 5.000],  loss: 0.014586, mae: 2.587286, mean_q: 3.114885, mean_eps: 0.380816
 3097088/6000000: episode: 4174, duration: 13.625s, episode steps: 652, steps per second:  48, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.016754, mae: 2.593998, mean_q: 3.120237, mean_eps: 0.380648
 3098036/6000000: episode: 4175, duration: 19.728s, episode steps: 948, steps per second:  48, episode reward: 28.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.801 [0.000, 5.000],  loss: 0.017093, mae: 2.591101, mean_q: 3.117147, mean_eps: 0.380488
 3099107/6000000: episode: 4176, duration: 22.942s, episode steps: 1071, steps per second:  47, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.207 [0.000, 5.000],  loss: 0.015256, mae: 2.589490, mean_q: 3.117641, mean_eps: 0.380286
 3100035/6000000: episode: 4177, duration: 19.939s, episode steps: 928, steps per second:  47, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.886 [0.000, 5.000],  loss: 0.015631, mae: 2.587525, mean_q: 3.113078, mean_eps: 0.380086
 3100986/6000000: episode: 4178, duration: 21.037s, episode steps: 951, steps per second:  45, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.143 [0.000, 5.000],  loss: 0.014413, mae: 2.604152, mean_q: 3.132671, mean_eps: 0.379898
 3101488/6000000: episode: 4179, duration: 11.173s, episode steps: 502, steps per second:  45, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.088 [0.000, 5.000],  loss: 0.016043, mae: 2.613301, mean_q: 3.143378, mean_eps: 0.379753
 3102032/6000000: episode: 4180, duration: 13.412s, episode steps: 544, steps per second:  41, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.989 [0.000, 5.000],  loss: 0.016917, mae: 2.595965, mean_q: 3.120865, mean_eps: 0.379648
 3103042/6000000: episode: 4181, duration: 24.445s, episode steps: 1010, steps per second:  41, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.135 [0.000, 5.000],  loss: 0.016105, mae: 2.606362, mean_q: 3.135881, mean_eps: 0.379493
 3103941/6000000: episode: 4182, duration: 22.022s, episode steps: 899, steps per second:  41, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.014254, mae: 2.603375, mean_q: 3.132947, mean_eps: 0.379302
 3104745/6000000: episode: 4183, duration: 17.557s, episode steps: 804, steps per second:  46, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.015613, mae: 2.613220, mean_q: 3.142973, mean_eps: 0.379131
 3105683/6000000: episode: 4184, duration: 20.408s, episode steps: 938, steps per second:  46, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.014767, mae: 2.622130, mean_q: 3.155263, mean_eps: 0.378957
 3106460/6000000: episode: 4185, duration: 16.887s, episode steps: 777, steps per second:  46, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.014817, mae: 2.629819, mean_q: 3.164335, mean_eps: 0.378786
 3107215/6000000: episode: 4186, duration: 16.379s, episode steps: 755, steps per second:  46, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.159 [0.000, 5.000],  loss: 0.015028, mae: 2.630538, mean_q: 3.163338, mean_eps: 0.378633
 3107912/6000000: episode: 4187, duration: 14.723s, episode steps: 697, steps per second:  47, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.869 [0.000, 5.000],  loss: 0.015365, mae: 2.624897, mean_q: 3.158596, mean_eps: 0.378488
 3108639/6000000: episode: 4188, duration: 16.375s, episode steps: 727, steps per second:  44, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.015419, mae: 2.610820, mean_q: 3.144644, mean_eps: 0.378345
 3109341/6000000: episode: 4189, duration: 15.932s, episode steps: 702, steps per second:  44, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.014950, mae: 2.634524, mean_q: 3.168867, mean_eps: 0.378202
 3110004/6000000: episode: 4190, duration: 14.441s, episode steps: 663, steps per second:  46, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.094 [0.000, 5.000],  loss: 0.014327, mae: 2.618362, mean_q: 3.151338, mean_eps: 0.378066
 3110567/6000000: episode: 4191, duration: 12.442s, episode steps: 563, steps per second:  45, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.194 [0.000, 5.000],  loss: 0.017676, mae: 2.653243, mean_q: 3.190834, mean_eps: 0.377943
 3111409/6000000: episode: 4192, duration: 17.655s, episode steps: 842, steps per second:  48, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.013 [0.000, 5.000],  loss: 0.016197, mae: 2.645370, mean_q: 3.182160, mean_eps: 0.377802
 3112501/6000000: episode: 4193, duration: 23.190s, episode steps: 1092, steps per second:  47, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.242 [0.000, 5.000],  loss: 0.015434, mae: 2.645787, mean_q: 3.183113, mean_eps: 0.377609
 3112974/6000000: episode: 4194, duration: 10.013s, episode steps: 473, steps per second:  47, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.089 [0.000, 5.000],  loss: 0.017633, mae: 2.670789, mean_q: 3.212483, mean_eps: 0.377452
 3113712/6000000: episode: 4195, duration: 15.223s, episode steps: 738, steps per second:  48, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.017160, mae: 2.638712, mean_q: 3.174403, mean_eps: 0.377332
 3114081/6000000: episode: 4196, duration: 7.631s, episode steps: 369, steps per second:  48, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.014220, mae: 2.627360, mean_q: 3.162221, mean_eps: 0.377221
 3115166/6000000: episode: 4197, duration: 22.637s, episode steps: 1085, steps per second:  48, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.006 [0.000, 5.000],  loss: 0.018025, mae: 2.650462, mean_q: 3.189115, mean_eps: 0.377075
 3116517/6000000: episode: 4198, duration: 28.015s, episode steps: 1351, steps per second:  48, episode reward: 33.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 0.015879, mae: 2.634750, mean_q: 3.171052, mean_eps: 0.376832
 3117305/6000000: episode: 4199, duration: 16.702s, episode steps: 788, steps per second:  47, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.015703, mae: 2.641874, mean_q: 3.177539, mean_eps: 0.376618
 3117841/6000000: episode: 4200, duration: 11.404s, episode steps: 536, steps per second:  47, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.892 [0.000, 5.000],  loss: 0.014673, mae: 2.640400, mean_q: 3.177385, mean_eps: 0.376485
 3118648/6000000: episode: 4201, duration: 18.135s, episode steps: 807, steps per second:  45, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.110 [0.000, 5.000],  loss: 0.014509, mae: 2.646235, mean_q: 3.183835, mean_eps: 0.376351
 3119522/6000000: episode: 4202, duration: 22.824s, episode steps: 874, steps per second:  38, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.014610, mae: 2.643869, mean_q: 3.181085, mean_eps: 0.376183
 3120466/6000000: episode: 4203, duration: 23.384s, episode steps: 944, steps per second:  40, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.016388, mae: 2.650407, mean_q: 3.189926, mean_eps: 0.376001
 3121444/6000000: episode: 4204, duration: 21.048s, episode steps: 978, steps per second:  46, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.070 [0.000, 5.000],  loss: 0.015388, mae: 2.664545, mean_q: 3.207017, mean_eps: 0.375809
 3122342/6000000: episode: 4205, duration: 19.229s, episode steps: 898, steps per second:  47, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.036 [0.000, 5.000],  loss: 0.015978, mae: 2.667470, mean_q: 3.211004, mean_eps: 0.375622
 3123029/6000000: episode: 4206, duration: 14.985s, episode steps: 687, steps per second:  46, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.007 [0.000, 5.000],  loss: 0.016661, mae: 2.643075, mean_q: 3.182633, mean_eps: 0.375463
 3124091/6000000: episode: 4207, duration: 22.643s, episode steps: 1062, steps per second:  47, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.094 [0.000, 5.000],  loss: 0.016782, mae: 2.647082, mean_q: 3.183911, mean_eps: 0.375288
 3124630/6000000: episode: 4208, duration: 11.654s, episode steps: 539, steps per second:  46, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.688 [0.000, 5.000],  loss: 0.014725, mae: 2.666897, mean_q: 3.210019, mean_eps: 0.375128
 3125422/6000000: episode: 4209, duration: 18.464s, episode steps: 792, steps per second:  43, episode reward: 24.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.016176, mae: 2.649255, mean_q: 3.187083, mean_eps: 0.374995
 3126786/6000000: episode: 4210, duration: 30.513s, episode steps: 1364, steps per second:  45, episode reward: 33.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.071 [0.000, 5.000],  loss: 0.015932, mae: 2.677409, mean_q: 3.219896, mean_eps: 0.374779
 3127496/6000000: episode: 4211, duration: 16.331s, episode steps: 710, steps per second:  43, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.900 [0.000, 5.000],  loss: 0.015216, mae: 2.683817, mean_q: 3.230919, mean_eps: 0.374572
 3128139/6000000: episode: 4212, duration: 14.013s, episode steps: 643, steps per second:  46, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.933 [0.000, 5.000],  loss: 0.014271, mae: 2.663520, mean_q: 3.206825, mean_eps: 0.374437
 3128796/6000000: episode: 4213, duration: 13.806s, episode steps: 657, steps per second:  48, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.910 [0.000, 5.000],  loss: 0.014830, mae: 2.677912, mean_q: 3.223530, mean_eps: 0.374307
 3129437/6000000: episode: 4214, duration: 12.940s, episode steps: 641, steps per second:  50, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.014033, mae: 2.682064, mean_q: 3.228641, mean_eps: 0.374177
 3130429/6000000: episode: 4215, duration: 20.878s, episode steps: 992, steps per second:  48, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.973 [0.000, 5.000],  loss: 0.018005, mae: 2.686966, mean_q: 3.233994, mean_eps: 0.374013
 3131171/6000000: episode: 4216, duration: 15.701s, episode steps: 742, steps per second:  47, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.863 [0.000, 5.000],  loss: 0.016435, mae: 2.700692, mean_q: 3.248262, mean_eps: 0.373840
 3131667/6000000: episode: 4217, duration: 10.251s, episode steps: 496, steps per second:  48, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.183 [0.000, 5.000],  loss: 0.014338, mae: 2.709893, mean_q: 3.263034, mean_eps: 0.373716
 3133138/6000000: episode: 4218, duration: 30.734s, episode steps: 1471, steps per second:  48, episode reward: 29.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.016574, mae: 2.684638, mean_q: 3.230816, mean_eps: 0.373520
 3134024/6000000: episode: 4219, duration: 18.233s, episode steps: 886, steps per second:  49, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.269 [0.000, 5.000],  loss: 0.015087, mae: 2.689233, mean_q: 3.235886, mean_eps: 0.373284
 3134438/6000000: episode: 4220, duration: 9.123s, episode steps: 414, steps per second:  45, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.014881, mae: 2.707760, mean_q: 3.257518, mean_eps: 0.373154
 3135380/6000000: episode: 4221, duration: 20.563s, episode steps: 942, steps per second:  46, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.027 [0.000, 5.000],  loss: 0.017124, mae: 2.664129, mean_q: 3.205581, mean_eps: 0.373018
 3135933/6000000: episode: 4222, duration: 12.752s, episode steps: 553, steps per second:  43, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.183 [0.000, 5.000],  loss: 0.014361, mae: 2.702432, mean_q: 3.253582, mean_eps: 0.372869
 3136896/6000000: episode: 4223, duration: 22.534s, episode steps: 963, steps per second:  43, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.688 [0.000, 5.000],  loss: 0.015272, mae: 2.694476, mean_q: 3.241998, mean_eps: 0.372717
 3137743/6000000: episode: 4224, duration: 19.536s, episode steps: 847, steps per second:  43, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.950 [0.000, 5.000],  loss: 0.016735, mae: 2.691934, mean_q: 3.239617, mean_eps: 0.372536
 3138370/6000000: episode: 4225, duration: 14.792s, episode steps: 627, steps per second:  42, episode reward: 18.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.247 [0.000, 5.000],  loss: 0.015951, mae: 2.687176, mean_q: 3.232405, mean_eps: 0.372389
 3139287/6000000: episode: 4226, duration: 21.724s, episode steps: 917, steps per second:  42, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.193 [0.000, 5.000],  loss: 0.017610, mae: 2.690343, mean_q: 3.235397, mean_eps: 0.372234
 3140274/6000000: episode: 4227, duration: 21.447s, episode steps: 987, steps per second:  46, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.942 [0.000, 5.000],  loss: 0.014950, mae: 2.688813, mean_q: 3.235020, mean_eps: 0.372044
 3141107/6000000: episode: 4228, duration: 18.278s, episode steps: 833, steps per second:  46, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.016202, mae: 2.689746, mean_q: 3.235206, mean_eps: 0.371862
 3142047/6000000: episode: 4229, duration: 22.127s, episode steps: 940, steps per second:  42, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.323 [0.000, 5.000],  loss: 0.016439, mae: 2.691422, mean_q: 3.237699, mean_eps: 0.371685
 3142763/6000000: episode: 4230, duration: 16.291s, episode steps: 716, steps per second:  44, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.187 [0.000, 5.000],  loss: 0.015178, mae: 2.676774, mean_q: 3.222146, mean_eps: 0.371519
 3143701/6000000: episode: 4231, duration: 21.115s, episode steps: 938, steps per second:  44, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.159 [0.000, 5.000],  loss: 0.016679, mae: 2.696839, mean_q: 3.243660, mean_eps: 0.371354
 3144228/6000000: episode: 4232, duration: 12.440s, episode steps: 527, steps per second:  42, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.199 [0.000, 5.000],  loss: 0.013892, mae: 2.691511, mean_q: 3.240609, mean_eps: 0.371207
 3144711/6000000: episode: 4233, duration: 10.510s, episode steps: 483, steps per second:  46, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.886 [0.000, 5.000],  loss: 0.015838, mae: 2.700420, mean_q: 3.247974, mean_eps: 0.371106
 3145567/6000000: episode: 4234, duration: 17.438s, episode steps: 856, steps per second:  49, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.255 [0.000, 5.000],  loss: 0.017564, mae: 2.695274, mean_q: 3.242210, mean_eps: 0.370972
 3146214/6000000: episode: 4235, duration: 14.019s, episode steps: 647, steps per second:  46, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.015842, mae: 2.697900, mean_q: 3.246718, mean_eps: 0.370822
 3147393/6000000: episode: 4236, duration: 26.124s, episode steps: 1179, steps per second:  45, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.070 [0.000, 5.000],  loss: 0.017799, mae: 2.679204, mean_q: 3.225806, mean_eps: 0.370639
 3148095/6000000: episode: 4237, duration: 15.185s, episode steps: 702, steps per second:  46, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.876 [0.000, 5.000],  loss: 0.016910, mae: 2.698065, mean_q: 3.248264, mean_eps: 0.370451
 3149009/6000000: episode: 4238, duration: 18.826s, episode steps: 914, steps per second:  49, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.017014, mae: 2.688418, mean_q: 3.234950, mean_eps: 0.370290
 3149662/6000000: episode: 4239, duration: 13.307s, episode steps: 653, steps per second:  49, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.217 [0.000, 5.000],  loss: 0.015672, mae: 2.704815, mean_q: 3.254830, mean_eps: 0.370133
 3150685/6000000: episode: 4240, duration: 22.847s, episode steps: 1023, steps per second:  45, episode reward: 31.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.952 [0.000, 5.000],  loss: 0.015444, mae: 2.692285, mean_q: 3.239137, mean_eps: 0.369965
 3151541/6000000: episode: 4241, duration: 18.946s, episode steps: 856, steps per second:  45, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.062 [0.000, 5.000],  loss: 0.017113, mae: 2.699256, mean_q: 3.249253, mean_eps: 0.369777
 3152522/6000000: episode: 4242, duration: 24.464s, episode steps: 981, steps per second:  40, episode reward: 30.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.324 [0.000, 5.000],  loss: 0.017450, mae: 2.703459, mean_q: 3.250957, mean_eps: 0.369594
 3153120/6000000: episode: 4243, duration: 14.269s, episode steps: 598, steps per second:  42, episode reward: 17.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.724 [0.000, 5.000],  loss: 0.016615, mae: 2.701042, mean_q: 3.251024, mean_eps: 0.369436
 3154032/6000000: episode: 4244, duration: 20.541s, episode steps: 912, steps per second:  44, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.901 [0.000, 5.000],  loss: 0.016353, mae: 2.683876, mean_q: 3.231323, mean_eps: 0.369285
 3154936/6000000: episode: 4245, duration: 19.534s, episode steps: 904, steps per second:  46, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.016547, mae: 2.711801, mean_q: 3.262953, mean_eps: 0.369104
 3155870/6000000: episode: 4246, duration: 21.202s, episode steps: 934, steps per second:  44, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.013782, mae: 2.703342, mean_q: 3.253604, mean_eps: 0.368920
 3156992/6000000: episode: 4247, duration: 25.956s, episode steps: 1122, steps per second:  43, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.209 [0.000, 5.000],  loss: 0.014449, mae: 2.708887, mean_q: 3.259245, mean_eps: 0.368714
 3158015/6000000: episode: 4248, duration: 25.973s, episode steps: 1023, steps per second:  39, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.688 [0.000, 5.000],  loss: 0.016308, mae: 2.713787, mean_q: 3.264703, mean_eps: 0.368500
 3159157/6000000: episode: 4249, duration: 25.753s, episode steps: 1142, steps per second:  44, episode reward: 32.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.852 [0.000, 5.000],  loss: 0.015644, mae: 2.710145, mean_q: 3.260990, mean_eps: 0.368283
 3159966/6000000: episode: 4250, duration: 18.458s, episode steps: 809, steps per second:  44, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.004 [0.000, 5.000],  loss: 0.016573, mae: 2.711719, mean_q: 3.263870, mean_eps: 0.368088
 3160905/6000000: episode: 4251, duration: 21.476s, episode steps: 939, steps per second:  44, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.092 [0.000, 5.000],  loss: 0.015658, mae: 2.718689, mean_q: 3.270070, mean_eps: 0.367913
 3161771/6000000: episode: 4252, duration: 17.650s, episode steps: 866, steps per second:  49, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.796 [0.000, 5.000],  loss: 0.014231, mae: 2.720502, mean_q: 3.271725, mean_eps: 0.367732
 3163018/6000000: episode: 4253, duration: 26.680s, episode steps: 1247, steps per second:  47, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.018 [0.000, 5.000],  loss: 0.015402, mae: 2.712156, mean_q: 3.264185, mean_eps: 0.367521
 3163890/6000000: episode: 4254, duration: 19.151s, episode steps: 872, steps per second:  46, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.287 [0.000, 5.000],  loss: 0.015235, mae: 2.717707, mean_q: 3.270656, mean_eps: 0.367309
 3164769/6000000: episode: 4255, duration: 18.362s, episode steps: 879, steps per second:  48, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.228 [0.000, 5.000],  loss: 0.016063, mae: 2.690514, mean_q: 3.236097, mean_eps: 0.367134
 3166004/6000000: episode: 4256, duration: 25.671s, episode steps: 1235, steps per second:  48, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.015304, mae: 2.708226, mean_q: 3.260699, mean_eps: 0.366923
 3167112/6000000: episode: 4257, duration: 23.416s, episode steps: 1108, steps per second:  47, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.235 [0.000, 5.000],  loss: 0.016376, mae: 2.696844, mean_q: 3.245462, mean_eps: 0.366689
 3168349/6000000: episode: 4258, duration: 28.768s, episode steps: 1237, steps per second:  43, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.810 [0.000, 5.000],  loss: 0.016499, mae: 2.703732, mean_q: 3.253731, mean_eps: 0.366454
 3169329/6000000: episode: 4259, duration: 22.385s, episode steps: 980, steps per second:  44, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.199 [0.000, 5.000],  loss: 0.015327, mae: 2.729844, mean_q: 3.283459, mean_eps: 0.366232
 3170191/6000000: episode: 4260, duration: 18.690s, episode steps: 862, steps per second:  46, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.136 [0.000, 5.000],  loss: 0.015831, mae: 2.712959, mean_q: 3.265734, mean_eps: 0.366048
 3171816/6000000: episode: 4261, duration: 34.625s, episode steps: 1625, steps per second:  47, episode reward: 28.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.038 [0.000, 5.000],  loss: 0.016832, mae: 2.706695, mean_q: 3.257432, mean_eps: 0.365800
 3172522/6000000: episode: 4262, duration: 14.588s, episode steps: 706, steps per second:  48, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.174 [0.000, 5.000],  loss: 0.017004, mae: 2.718805, mean_q: 3.270572, mean_eps: 0.365566
 3173326/6000000: episode: 4263, duration: 17.431s, episode steps: 804, steps per second:  46, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.121 [0.000, 5.000],  loss: 0.015436, mae: 2.729903, mean_q: 3.284313, mean_eps: 0.365415
 3174213/6000000: episode: 4264, duration: 21.076s, episode steps: 887, steps per second:  42, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.067 [0.000, 5.000],  loss: 0.014652, mae: 2.706220, mean_q: 3.256547, mean_eps: 0.365246
 3175088/6000000: episode: 4265, duration: 19.450s, episode steps: 875, steps per second:  45, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.779 [0.000, 5.000],  loss: 0.014949, mae: 2.724705, mean_q: 3.279858, mean_eps: 0.365070
 3176323/6000000: episode: 4266, duration: 27.515s, episode steps: 1235, steps per second:  45, episode reward: 33.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.145 [0.000, 5.000],  loss: 0.016847, mae: 2.718438, mean_q: 3.270608, mean_eps: 0.364859
 3177523/6000000: episode: 4267, duration: 26.800s, episode steps: 1200, steps per second:  45, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.073 [0.000, 5.000],  loss: 0.015689, mae: 2.708538, mean_q: 3.258545, mean_eps: 0.364616
 3178306/6000000: episode: 4268, duration: 18.538s, episode steps: 783, steps per second:  42, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.294 [0.000, 5.000],  loss: 0.016179, mae: 2.726909, mean_q: 3.279086, mean_eps: 0.364417
 3178928/6000000: episode: 4269, duration: 14.169s, episode steps: 622, steps per second:  44, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.989 [0.000, 5.000],  loss: 0.015959, mae: 2.727654, mean_q: 3.283453, mean_eps: 0.364277
 3179794/6000000: episode: 4270, duration: 18.425s, episode steps: 866, steps per second:  47, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.003 [0.000, 5.000],  loss: 0.015367, mae: 2.730941, mean_q: 3.287180, mean_eps: 0.364128
 3180646/6000000: episode: 4271, duration: 17.442s, episode steps: 852, steps per second:  49, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.137 [0.000, 5.000],  loss: 0.018576, mae: 2.737919, mean_q: 3.292819, mean_eps: 0.363956
 3181023/6000000: episode: 4272, duration: 7.651s, episode steps: 377, steps per second:  49, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.759 [0.000, 5.000],  loss: 0.017146, mae: 2.727829, mean_q: 3.284197, mean_eps: 0.363833
 3181962/6000000: episode: 4273, duration: 19.071s, episode steps: 939, steps per second:  49, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.133 [0.000, 5.000],  loss: 0.015133, mae: 2.714987, mean_q: 3.266551, mean_eps: 0.363702
 3182764/6000000: episode: 4274, duration: 16.718s, episode steps: 802, steps per second:  48, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.796 [0.000, 5.000],  loss: 0.013362, mae: 2.713785, mean_q: 3.264950, mean_eps: 0.363528
 3183660/6000000: episode: 4275, duration: 19.506s, episode steps: 896, steps per second:  46, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.272 [0.000, 5.000],  loss: 0.016385, mae: 2.726197, mean_q: 3.280153, mean_eps: 0.363358
 3184561/6000000: episode: 4276, duration: 20.837s, episode steps: 901, steps per second:  43, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.015092, mae: 2.724883, mean_q: 3.276493, mean_eps: 0.363178
 3184938/6000000: episode: 4277, duration: 9.061s, episode steps: 377, steps per second:  42, episode reward:  9.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.324 [0.000, 5.000],  loss: 0.014404, mae: 2.714815, mean_q: 3.265270, mean_eps: 0.363050
 3185855/6000000: episode: 4278, duration: 20.848s, episode steps: 917, steps per second:  44, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.015192, mae: 2.737061, mean_q: 3.292427, mean_eps: 0.362921
 3187157/6000000: episode: 4279, duration: 31.000s, episode steps: 1302, steps per second:  42, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.134 [0.000, 5.000],  loss: 0.015048, mae: 2.730328, mean_q: 3.286549, mean_eps: 0.362699
 3187916/6000000: episode: 4280, duration: 16.974s, episode steps: 759, steps per second:  45, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.256 [0.000, 5.000],  loss: 0.016498, mae: 2.727840, mean_q: 3.281794, mean_eps: 0.362493
 3188953/6000000: episode: 4281, duration: 21.989s, episode steps: 1037, steps per second:  47, episode reward: 30.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.948 [0.000, 5.000],  loss: 0.017256, mae: 2.734376, mean_q: 3.290814, mean_eps: 0.362313
 3189721/6000000: episode: 4282, duration: 16.645s, episode steps: 768, steps per second:  46, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.772 [0.000, 5.000],  loss: 0.015718, mae: 2.733813, mean_q: 3.289716, mean_eps: 0.362132
 3190653/6000000: episode: 4283, duration: 20.352s, episode steps: 932, steps per second:  46, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.134 [0.000, 5.000],  loss: 0.016088, mae: 2.734818, mean_q: 3.289200, mean_eps: 0.361962
 3191273/6000000: episode: 4284, duration: 13.438s, episode steps: 620, steps per second:  46, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.124 [0.000, 5.000],  loss: 0.016363, mae: 2.744855, mean_q: 3.301162, mean_eps: 0.361807
 3191920/6000000: episode: 4285, duration: 14.250s, episode steps: 647, steps per second:  45, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.124 [0.000, 5.000],  loss: 0.015656, mae: 2.739757, mean_q: 3.296256, mean_eps: 0.361681
 3192645/6000000: episode: 4286, duration: 15.335s, episode steps: 725, steps per second:  47, episode reward: 23.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.016786, mae: 2.750449, mean_q: 3.308143, mean_eps: 0.361544
 3193315/6000000: episode: 4287, duration: 14.904s, episode steps: 670, steps per second:  45, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.246 [0.000, 5.000],  loss: 0.016675, mae: 2.732227, mean_q: 3.290131, mean_eps: 0.361404
 3194318/6000000: episode: 4288, duration: 20.394s, episode steps: 1003, steps per second:  49, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.015014, mae: 2.741083, mean_q: 3.298174, mean_eps: 0.361237
 3195173/6000000: episode: 4289, duration: 17.729s, episode steps: 855, steps per second:  48, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.299 [0.000, 5.000],  loss: 0.015108, mae: 2.738256, mean_q: 3.295161, mean_eps: 0.361051
 3195991/6000000: episode: 4290, duration: 17.358s, episode steps: 818, steps per second:  47, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.317 [0.000, 5.000],  loss: 0.015706, mae: 2.745317, mean_q: 3.303169, mean_eps: 0.360884
 3196873/6000000: episode: 4291, duration: 18.951s, episode steps: 882, steps per second:  47, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.027 [0.000, 5.000],  loss: 0.016236, mae: 2.743508, mean_q: 3.300338, mean_eps: 0.360714
 3197407/6000000: episode: 4292, duration: 10.568s, episode steps: 534, steps per second:  51, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.000 [0.000, 5.000],  loss: 0.017460, mae: 2.738242, mean_q: 3.296112, mean_eps: 0.360572
 3198548/6000000: episode: 4293, duration: 23.167s, episode steps: 1141, steps per second:  49, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.015955, mae: 2.750895, mean_q: 3.309809, mean_eps: 0.360405
 3199347/6000000: episode: 4294, duration: 16.437s, episode steps: 799, steps per second:  49, episode reward: 26.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.016451, mae: 2.732196, mean_q: 3.287151, mean_eps: 0.360211
 3200419/6000000: episode: 4295, duration: 21.930s, episode steps: 1072, steps per second:  49, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.822 [0.000, 5.000],  loss: 0.017565, mae: 2.741021, mean_q: 3.299257, mean_eps: 0.360024
 3201070/6000000: episode: 4296, duration: 14.780s, episode steps: 651, steps per second:  44, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.664 [0.000, 5.000],  loss: 0.015959, mae: 2.730099, mean_q: 3.291292, mean_eps: 0.359851
 3201838/6000000: episode: 4297, duration: 18.266s, episode steps: 768, steps per second:  42, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.151 [0.000, 5.000],  loss: 0.016622, mae: 2.736658, mean_q: 3.292612, mean_eps: 0.359709
 3202852/6000000: episode: 4298, duration: 22.785s, episode steps: 1014, steps per second:  45, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.159 [0.000, 5.000],  loss: 0.016153, mae: 2.752856, mean_q: 3.312675, mean_eps: 0.359531
 3203521/6000000: episode: 4299, duration: 14.527s, episode steps: 669, steps per second:  46, episode reward: 20.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.015315, mae: 2.739719, mean_q: 3.296446, mean_eps: 0.359363
 3204562/6000000: episode: 4300, duration: 22.925s, episode steps: 1041, steps per second:  45, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.953 [0.000, 5.000],  loss: 0.016855, mae: 2.758409, mean_q: 3.318309, mean_eps: 0.359192
 3205393/6000000: episode: 4301, duration: 18.345s, episode steps: 831, steps per second:  45, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.203 [0.000, 5.000],  loss: 0.016014, mae: 2.727583, mean_q: 3.281423, mean_eps: 0.359004
 3206898/6000000: episode: 4302, duration: 32.622s, episode steps: 1505, steps per second:  46, episode reward: 27.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.256 [0.000, 5.000],  loss: 0.017612, mae: 2.723854, mean_q: 3.277093, mean_eps: 0.358771
 3207838/6000000: episode: 4303, duration: 21.269s, episode steps: 940, steps per second:  44, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.760 [0.000, 5.000],  loss: 0.016569, mae: 2.702836, mean_q: 3.252596, mean_eps: 0.358526
 3208772/6000000: episode: 4304, duration: 20.431s, episode steps: 934, steps per second:  46, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.005 [0.000, 5.000],  loss: 0.016964, mae: 2.725746, mean_q: 3.278465, mean_eps: 0.358339
 3209573/6000000: episode: 4305, duration: 17.397s, episode steps: 801, steps per second:  46, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 0.019506, mae: 2.736530, mean_q: 3.293893, mean_eps: 0.358166
 3210497/6000000: episode: 4306, duration: 19.678s, episode steps: 924, steps per second:  47, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.188 [0.000, 5.000],  loss: 0.016749, mae: 2.713884, mean_q: 3.265375, mean_eps: 0.357993
 3211128/6000000: episode: 4307, duration: 12.982s, episode steps: 631, steps per second:  49, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.044 [0.000, 5.000],  loss: 0.018578, mae: 2.670845, mean_q: 3.214393, mean_eps: 0.357838
 3211763/6000000: episode: 4308, duration: 12.759s, episode steps: 635, steps per second:  50, episode reward: 17.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.044 [0.000, 5.000],  loss: 0.014462, mae: 2.685740, mean_q: 3.233955, mean_eps: 0.357711
 3212377/6000000: episode: 4309, duration: 13.828s, episode steps: 614, steps per second:  44, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.070 [0.000, 5.000],  loss: 0.016065, mae: 2.700780, mean_q: 3.249584, mean_eps: 0.357586
 3213846/6000000: episode: 4310, duration: 31.341s, episode steps: 1469, steps per second:  47, episode reward: 36.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.946 [0.000, 5.000],  loss: 0.015335, mae: 2.687337, mean_q: 3.234299, mean_eps: 0.357378
 3215095/6000000: episode: 4311, duration: 25.627s, episode steps: 1249, steps per second:  49, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.106 [0.000, 5.000],  loss: 0.016689, mae: 2.688922, mean_q: 3.237738, mean_eps: 0.357106
 3215909/6000000: episode: 4312, duration: 16.896s, episode steps: 814, steps per second:  48, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.209 [0.000, 5.000],  loss: 0.014438, mae: 2.669945, mean_q: 3.213975, mean_eps: 0.356900
 3216939/6000000: episode: 4313, duration: 22.602s, episode steps: 1030, steps per second:  46, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.936 [0.000, 5.000],  loss: 0.015369, mae: 2.692646, mean_q: 3.240458, mean_eps: 0.356715
 3217539/6000000: episode: 4314, duration: 14.433s, episode steps: 600, steps per second:  42, episode reward: 15.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.017239, mae: 2.668962, mean_q: 3.209758, mean_eps: 0.356552
 3218767/6000000: episode: 4315, duration: 30.953s, episode steps: 1228, steps per second:  40, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.015066, mae: 2.688905, mean_q: 3.235302, mean_eps: 0.356370
 3219458/6000000: episode: 4316, duration: 17.442s, episode steps: 691, steps per second:  40, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.999 [0.000, 5.000],  loss: 0.014706, mae: 2.695064, mean_q: 3.243136, mean_eps: 0.356178
 3220197/6000000: episode: 4317, duration: 17.696s, episode steps: 739, steps per second:  42, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.016519, mae: 2.690170, mean_q: 3.237306, mean_eps: 0.356034
 3221327/6000000: episode: 4318, duration: 26.676s, episode steps: 1130, steps per second:  42, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.926 [0.000, 5.000],  loss: 0.015600, mae: 2.699275, mean_q: 3.248297, mean_eps: 0.355848
 3222096/6000000: episode: 4319, duration: 18.291s, episode steps: 769, steps per second:  42, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.016412, mae: 2.707870, mean_q: 3.256297, mean_eps: 0.355658
 3223029/6000000: episode: 4320, duration: 22.759s, episode steps: 933, steps per second:  41, episode reward: 29.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.205 [0.000, 5.000],  loss: 0.016333, mae: 2.691469, mean_q: 3.241186, mean_eps: 0.355488
 3223788/6000000: episode: 4321, duration: 18.472s, episode steps: 759, steps per second:  41, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.669 [0.000, 5.000],  loss: 0.016629, mae: 2.690366, mean_q: 3.239886, mean_eps: 0.355318
 3224570/6000000: episode: 4322, duration: 19.278s, episode steps: 782, steps per second:  41, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.015397, mae: 2.680087, mean_q: 3.225994, mean_eps: 0.355164
 3225220/6000000: episode: 4323, duration: 15.943s, episode steps: 650, steps per second:  41, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.013887, mae: 2.691104, mean_q: 3.241250, mean_eps: 0.355021
 3226159/6000000: episode: 4324, duration: 23.301s, episode steps: 939, steps per second:  40, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.015379, mae: 2.694009, mean_q: 3.241436, mean_eps: 0.354862
 3227164/6000000: episode: 4325, duration: 24.066s, episode steps: 1005, steps per second:  42, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.001 [0.000, 5.000],  loss: 0.017091, mae: 2.700703, mean_q: 3.248910, mean_eps: 0.354668
 3228276/6000000: episode: 4326, duration: 26.494s, episode steps: 1112, steps per second:  42, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.119 [0.000, 5.000],  loss: 0.014832, mae: 2.698325, mean_q: 3.247029, mean_eps: 0.354456
 3229025/6000000: episode: 4327, duration: 18.045s, episode steps: 749, steps per second:  42, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.009 [0.000, 5.000],  loss: 0.016605, mae: 2.681791, mean_q: 3.227149, mean_eps: 0.354270
 3229810/6000000: episode: 4328, duration: 19.525s, episode steps: 785, steps per second:  40, episode reward: 25.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.038 [0.000, 5.000],  loss: 0.016969, mae: 2.719421, mean_q: 3.272721, mean_eps: 0.354116
 3230404/6000000: episode: 4329, duration: 15.331s, episode steps: 594, steps per second:  39, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.266 [0.000, 5.000],  loss: 0.016313, mae: 2.699720, mean_q: 3.249724, mean_eps: 0.353979
 3231649/6000000: episode: 4330, duration: 31.194s, episode steps: 1245, steps per second:  40, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.124 [0.000, 5.000],  loss: 0.015063, mae: 2.704902, mean_q: 3.257100, mean_eps: 0.353795
 3232512/6000000: episode: 4331, duration: 21.579s, episode steps: 863, steps per second:  40, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.769 [0.000, 5.000],  loss: 0.015547, mae: 2.702150, mean_q: 3.254330, mean_eps: 0.353584
 3233261/6000000: episode: 4332, duration: 18.409s, episode steps: 749, steps per second:  41, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.099 [0.000, 5.000],  loss: 0.015562, mae: 2.693877, mean_q: 3.244101, mean_eps: 0.353423
 3234150/6000000: episode: 4333, duration: 21.642s, episode steps: 889, steps per second:  41, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.016152, mae: 2.708449, mean_q: 3.260018, mean_eps: 0.353259
 3235030/6000000: episode: 4334, duration: 21.725s, episode steps: 880, steps per second:  41, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.272 [0.000, 5.000],  loss: 0.015674, mae: 2.695958, mean_q: 3.243815, mean_eps: 0.353082
 3236112/6000000: episode: 4335, duration: 26.001s, episode steps: 1082, steps per second:  42, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.202 [0.000, 5.000],  loss: 0.015469, mae: 2.710368, mean_q: 3.262039, mean_eps: 0.352886
 3236634/6000000: episode: 4336, duration: 11.539s, episode steps: 522, steps per second:  45, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.659 [0.000, 5.000],  loss: 0.015369, mae: 2.688888, mean_q: 3.235459, mean_eps: 0.352726
 3237479/6000000: episode: 4337, duration: 19.077s, episode steps: 845, steps per second:  44, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.015904, mae: 2.716168, mean_q: 3.267728, mean_eps: 0.352589
 3238032/6000000: episode: 4338, duration: 12.968s, episode steps: 553, steps per second:  43, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.752 [0.000, 5.000],  loss: 0.015639, mae: 2.715872, mean_q: 3.268467, mean_eps: 0.352449
 3238558/6000000: episode: 4339, duration: 12.105s, episode steps: 526, steps per second:  43, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.787 [0.000, 5.000],  loss: 0.015732, mae: 2.707330, mean_q: 3.258915, mean_eps: 0.352341
 3238900/6000000: episode: 4340, duration: 7.470s, episode steps: 342, steps per second:  46, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.883 [0.000, 5.000],  loss: 0.016270, mae: 2.694841, mean_q: 3.244679, mean_eps: 0.352254
 3239705/6000000: episode: 4341, duration: 19.291s, episode steps: 805, steps per second:  42, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.730 [0.000, 5.000],  loss: 0.016891, mae: 2.726388, mean_q: 3.280743, mean_eps: 0.352140
 3240198/6000000: episode: 4342, duration: 12.018s, episode steps: 493, steps per second:  41, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.154 [0.000, 5.000],  loss: 0.014959, mae: 2.727167, mean_q: 3.280091, mean_eps: 0.352010
 3241238/6000000: episode: 4343, duration: 25.270s, episode steps: 1040, steps per second:  41, episode reward: 31.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.178 [0.000, 5.000],  loss: 0.017496, mae: 2.735348, mean_q: 3.289423, mean_eps: 0.351856
 3242168/6000000: episode: 4344, duration: 22.465s, episode steps: 930, steps per second:  41, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.129 [0.000, 5.000],  loss: 0.015004, mae: 2.722135, mean_q: 3.274939, mean_eps: 0.351660
 3242985/6000000: episode: 4345, duration: 17.237s, episode steps: 817, steps per second:  47, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.015679, mae: 2.708197, mean_q: 3.258762, mean_eps: 0.351485
 3243931/6000000: episode: 4346, duration: 19.783s, episode steps: 946, steps per second:  48, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.015811, mae: 2.727398, mean_q: 3.281326, mean_eps: 0.351308
 3244967/6000000: episode: 4347, duration: 21.472s, episode steps: 1036, steps per second:  48, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.183 [0.000, 5.000],  loss: 0.015392, mae: 2.726523, mean_q: 3.280958, mean_eps: 0.351110
 3245614/6000000: episode: 4348, duration: 13.321s, episode steps: 647, steps per second:  49, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.713 [0.000, 5.000],  loss: 0.014610, mae: 2.726342, mean_q: 3.283445, mean_eps: 0.350942
 3246600/6000000: episode: 4349, duration: 20.813s, episode steps: 986, steps per second:  47, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.016187, mae: 2.745600, mean_q: 3.305527, mean_eps: 0.350779
 3247322/6000000: episode: 4350, duration: 16.072s, episode steps: 722, steps per second:  45, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.967 [0.000, 5.000],  loss: 0.015708, mae: 2.754572, mean_q: 3.315934, mean_eps: 0.350608
 3247683/6000000: episode: 4351, duration: 7.941s, episode steps: 361, steps per second:  45, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.945 [0.000, 5.000],  loss: 0.014312, mae: 2.757663, mean_q: 3.319751, mean_eps: 0.350500
 3248218/6000000: episode: 4352, duration: 12.870s, episode steps: 535, steps per second:  42, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.016811, mae: 2.731177, mean_q: 3.285565, mean_eps: 0.350410
 3249002/6000000: episode: 4353, duration: 17.869s, episode steps: 784, steps per second:  44, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 0.016569, mae: 2.738731, mean_q: 3.295333, mean_eps: 0.350278
 3250236/6000000: episode: 4354, duration: 27.235s, episode steps: 1234, steps per second:  45, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 0.015987, mae: 2.728285, mean_q: 3.285399, mean_eps: 0.350076
 3251392/6000000: episode: 4355, duration: 25.611s, episode steps: 1156, steps per second:  45, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.015972, mae: 2.750538, mean_q: 3.310470, mean_eps: 0.349838
 3252306/6000000: episode: 4356, duration: 19.579s, episode steps: 914, steps per second:  47, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.017549, mae: 2.734150, mean_q: 3.290143, mean_eps: 0.349630
 3253004/6000000: episode: 4357, duration: 14.567s, episode steps: 698, steps per second:  48, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.015587, mae: 2.753856, mean_q: 3.315511, mean_eps: 0.349469
 3253966/6000000: episode: 4358, duration: 22.144s, episode steps: 962, steps per second:  43, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.048 [0.000, 5.000],  loss: 0.016241, mae: 2.729716, mean_q: 3.284398, mean_eps: 0.349303
 3254820/6000000: episode: 4359, duration: 18.854s, episode steps: 854, steps per second:  45, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.129 [0.000, 5.000],  loss: 0.016881, mae: 2.757735, mean_q: 3.317899, mean_eps: 0.349122
 3255503/6000000: episode: 4360, duration: 14.804s, episode steps: 683, steps per second:  46, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.015814, mae: 2.763806, mean_q: 3.325503, mean_eps: 0.348968
 3256329/6000000: episode: 4361, duration: 18.184s, episode steps: 826, steps per second:  45, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.019 [0.000, 5.000],  loss: 0.015147, mae: 2.746978, mean_q: 3.305766, mean_eps: 0.348817
 3257115/6000000: episode: 4362, duration: 17.423s, episode steps: 786, steps per second:  45, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.016952, mae: 2.753447, mean_q: 3.312005, mean_eps: 0.348656
 3257826/6000000: episode: 4363, duration: 14.734s, episode steps: 711, steps per second:  48, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.016411, mae: 2.754899, mean_q: 3.316942, mean_eps: 0.348506
 3258478/6000000: episode: 4364, duration: 13.064s, episode steps: 652, steps per second:  50, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.104 [0.000, 5.000],  loss: 0.015048, mae: 2.733826, mean_q: 3.293011, mean_eps: 0.348370
 3259385/6000000: episode: 4365, duration: 19.110s, episode steps: 907, steps per second:  47, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.077 [0.000, 5.000],  loss: 0.015161, mae: 2.758993, mean_q: 3.319933, mean_eps: 0.348214
 3260293/6000000: episode: 4366, duration: 19.038s, episode steps: 908, steps per second:  48, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.014326, mae: 2.748576, mean_q: 3.308312, mean_eps: 0.348032
 3261112/6000000: episode: 4367, duration: 16.953s, episode steps: 819, steps per second:  48, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.332 [0.000, 5.000],  loss: 0.016660, mae: 2.760964, mean_q: 3.320911, mean_eps: 0.347860
 3261886/6000000: episode: 4368, duration: 15.854s, episode steps: 774, steps per second:  49, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.924 [0.000, 5.000],  loss: 0.017456, mae: 2.738845, mean_q: 3.294525, mean_eps: 0.347700
 3262652/6000000: episode: 4369, duration: 15.959s, episode steps: 766, steps per second:  48, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.070 [0.000, 5.000],  loss: 0.016843, mae: 2.735524, mean_q: 3.292066, mean_eps: 0.347546
 3263812/6000000: episode: 4370, duration: 24.696s, episode steps: 1160, steps per second:  47, episode reward: 33.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.014813, mae: 2.759006, mean_q: 3.319887, mean_eps: 0.347354
 3264590/6000000: episode: 4371, duration: 16.998s, episode steps: 778, steps per second:  46, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.713 [0.000, 5.000],  loss: 0.016122, mae: 2.740122, mean_q: 3.297179, mean_eps: 0.347160
 3265852/6000000: episode: 4372, duration: 29.242s, episode steps: 1262, steps per second:  43, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.015969, mae: 2.763328, mean_q: 3.327000, mean_eps: 0.346956
 3267148/6000000: episode: 4373, duration: 28.176s, episode steps: 1296, steps per second:  46, episode reward: 29.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.014980, mae: 2.767254, mean_q: 3.331473, mean_eps: 0.346700
 3267810/6000000: episode: 4374, duration: 14.760s, episode steps: 662, steps per second:  45, episode reward: 18.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.015503, mae: 2.747465, mean_q: 3.306911, mean_eps: 0.346504
 3268619/6000000: episode: 4375, duration: 17.743s, episode steps: 809, steps per second:  46, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.957 [0.000, 5.000],  loss: 0.016064, mae: 2.753858, mean_q: 3.312762, mean_eps: 0.346357
 3270697/6000000: episode: 4376, duration: 46.987s, episode steps: 2078, steps per second:  44, episode reward: 57.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.264 [0.000, 5.000],  loss: 0.016389, mae: 2.756578, mean_q: 3.315998, mean_eps: 0.346068
 3271774/6000000: episode: 4377, duration: 23.940s, episode steps: 1077, steps per second:  45, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.015182, mae: 2.744431, mean_q: 3.302060, mean_eps: 0.345753
 3272802/6000000: episode: 4378, duration: 23.772s, episode steps: 1028, steps per second:  43, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.015 [0.000, 5.000],  loss: 0.016378, mae: 2.734671, mean_q: 3.291638, mean_eps: 0.345542
 3274197/6000000: episode: 4379, duration: 30.852s, episode steps: 1395, steps per second:  45, episode reward: 31.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.125 [0.000, 5.000],  loss: 0.016625, mae: 2.742686, mean_q: 3.299789, mean_eps: 0.345300
 3275206/6000000: episode: 4380, duration: 20.533s, episode steps: 1009, steps per second:  49, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.093 [0.000, 5.000],  loss: 0.016098, mae: 2.750084, mean_q: 3.308050, mean_eps: 0.345060
 3275915/6000000: episode: 4381, duration: 15.728s, episode steps: 709, steps per second:  45, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.210 [0.000, 5.000],  loss: 0.017167, mae: 2.756922, mean_q: 3.318172, mean_eps: 0.344888
 3276590/6000000: episode: 4382, duration: 14.535s, episode steps: 675, steps per second:  46, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.016772, mae: 2.762164, mean_q: 3.322109, mean_eps: 0.344750
 3277312/6000000: episode: 4383, duration: 14.651s, episode steps: 722, steps per second:  49, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.015469, mae: 2.749212, mean_q: 3.308079, mean_eps: 0.344610
 3278494/6000000: episode: 4384, duration: 24.567s, episode steps: 1182, steps per second:  48, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.206 [0.000, 5.000],  loss: 0.016642, mae: 2.751732, mean_q: 3.309609, mean_eps: 0.344420
 3278948/6000000: episode: 4385, duration: 9.578s, episode steps: 454, steps per second:  47, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.753 [0.000, 5.000],  loss: 0.017338, mae: 2.713660, mean_q: 3.267443, mean_eps: 0.344256
 3279835/6000000: episode: 4386, duration: 18.550s, episode steps: 887, steps per second:  48, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.885 [0.000, 5.000],  loss: 0.015925, mae: 2.750699, mean_q: 3.310661, mean_eps: 0.344122
 3280771/6000000: episode: 4387, duration: 20.042s, episode steps: 936, steps per second:  47, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.956 [0.000, 5.000],  loss: 0.015818, mae: 2.782359, mean_q: 3.348897, mean_eps: 0.343940
 3281631/6000000: episode: 4388, duration: 20.302s, episode steps: 860, steps per second:  42, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.002 [0.000, 5.000],  loss: 0.016914, mae: 2.772286, mean_q: 3.333469, mean_eps: 0.343760
 3282026/6000000: episode: 4389, duration: 9.206s, episode steps: 395, steps per second:  43, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.015 [0.000, 5.000],  loss: 0.015163, mae: 2.773524, mean_q: 3.336071, mean_eps: 0.343634
 3282957/6000000: episode: 4390, duration: 20.384s, episode steps: 931, steps per second:  46, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.015938, mae: 2.772631, mean_q: 3.336243, mean_eps: 0.343502
 3283779/6000000: episode: 4391, duration: 18.010s, episode steps: 822, steps per second:  46, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.287 [0.000, 5.000],  loss: 0.014236, mae: 2.777154, mean_q: 3.341905, mean_eps: 0.343326
 3284680/6000000: episode: 4392, duration: 20.451s, episode steps: 901, steps per second:  44, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.097 [0.000, 5.000],  loss: 0.016838, mae: 2.771564, mean_q: 3.334940, mean_eps: 0.343154
 3285303/6000000: episode: 4393, duration: 13.335s, episode steps: 623, steps per second:  47, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.165 [0.000, 5.000],  loss: 0.015813, mae: 2.777738, mean_q: 3.343499, mean_eps: 0.343002
 3286179/6000000: episode: 4394, duration: 18.348s, episode steps: 876, steps per second:  48, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.650 [0.000, 5.000],  loss: 0.015398, mae: 2.754424, mean_q: 3.316026, mean_eps: 0.342852
 3286937/6000000: episode: 4395, duration: 18.010s, episode steps: 758, steps per second:  42, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.044 [0.000, 5.000],  loss: 0.016646, mae: 2.779898, mean_q: 3.344921, mean_eps: 0.342688
 3287925/6000000: episode: 4396, duration: 22.247s, episode steps: 988, steps per second:  44, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.874 [0.000, 5.000],  loss: 0.016294, mae: 2.759641, mean_q: 3.322431, mean_eps: 0.342514
 3288633/6000000: episode: 4397, duration: 15.567s, episode steps: 708, steps per second:  45, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.174 [0.000, 5.000],  loss: 0.015727, mae: 2.750536, mean_q: 3.308360, mean_eps: 0.342344
 3289324/6000000: episode: 4398, duration: 15.139s, episode steps: 691, steps per second:  46, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.014683, mae: 2.770388, mean_q: 3.333721, mean_eps: 0.342204
 3290023/6000000: episode: 4399, duration: 15.224s, episode steps: 699, steps per second:  46, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.014991, mae: 2.764353, mean_q: 3.325352, mean_eps: 0.342066
 3290740/6000000: episode: 4400, duration: 15.209s, episode steps: 717, steps per second:  47, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.014245, mae: 2.789743, mean_q: 3.356902, mean_eps: 0.341924
 3291609/6000000: episode: 4401, duration: 17.746s, episode steps: 869, steps per second:  49, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.016204, mae: 2.776779, mean_q: 3.341766, mean_eps: 0.341765
 3292443/6000000: episode: 4402, duration: 17.574s, episode steps: 834, steps per second:  47, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.266 [0.000, 5.000],  loss: 0.014063, mae: 2.783195, mean_q: 3.348275, mean_eps: 0.341595
 3293371/6000000: episode: 4403, duration: 19.497s, episode steps: 928, steps per second:  48, episode reward: 28.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.879 [0.000, 5.000],  loss: 0.014497, mae: 2.787265, mean_q: 3.355175, mean_eps: 0.341419
 3293771/6000000: episode: 4404, duration: 8.246s, episode steps: 400, steps per second:  49, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.012319, mae: 2.761604, mean_q: 3.325480, mean_eps: 0.341286
 3294417/6000000: episode: 4405, duration: 13.362s, episode steps: 646, steps per second:  48, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.017142, mae: 2.763054, mean_q: 3.324519, mean_eps: 0.341181
 3295251/6000000: episode: 4406, duration: 17.267s, episode steps: 834, steps per second:  48, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.236 [0.000, 5.000],  loss: 0.016897, mae: 2.764610, mean_q: 3.325638, mean_eps: 0.341033
 3296169/6000000: episode: 4407, duration: 19.061s, episode steps: 918, steps per second:  48, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.016433, mae: 2.769397, mean_q: 3.332207, mean_eps: 0.340858
 3297039/6000000: episode: 4408, duration: 19.071s, episode steps: 870, steps per second:  46, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.218 [0.000, 5.000],  loss: 0.015097, mae: 2.782671, mean_q: 3.348963, mean_eps: 0.340679
 3297840/6000000: episode: 4409, duration: 18.333s, episode steps: 801, steps per second:  44, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.747 [0.000, 5.000],  loss: 0.013947, mae: 2.765541, mean_q: 3.330596, mean_eps: 0.340512
 3298867/6000000: episode: 4410, duration: 23.443s, episode steps: 1027, steps per second:  44, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.015 [0.000, 5.000],  loss: 0.016863, mae: 2.757361, mean_q: 3.320359, mean_eps: 0.340330
 3299953/6000000: episode: 4411, duration: 23.856s, episode steps: 1086, steps per second:  46, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.024 [0.000, 5.000],  loss: 0.017144, mae: 2.789538, mean_q: 3.357419, mean_eps: 0.340118
 3301028/6000000: episode: 4412, duration: 23.403s, episode steps: 1075, steps per second:  46, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.016815, mae: 2.774965, mean_q: 3.340032, mean_eps: 0.339902
 3301804/6000000: episode: 4413, duration: 17.089s, episode steps: 776, steps per second:  45, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.026 [0.000, 5.000],  loss: 0.014737, mae: 2.779191, mean_q: 3.347255, mean_eps: 0.339717
 3302513/6000000: episode: 4414, duration: 15.037s, episode steps: 709, steps per second:  47, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.996 [0.000, 5.000],  loss: 0.015206, mae: 2.792987, mean_q: 3.360864, mean_eps: 0.339568
 3303445/6000000: episode: 4415, duration: 19.885s, episode steps: 932, steps per second:  47, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.781 [0.000, 5.000],  loss: 0.015360, mae: 2.781962, mean_q: 3.348281, mean_eps: 0.339404
 3304411/6000000: episode: 4416, duration: 21.310s, episode steps: 966, steps per second:  45, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.014296, mae: 2.790273, mean_q: 3.359072, mean_eps: 0.339214
 3305452/6000000: episode: 4417, duration: 22.941s, episode steps: 1041, steps per second:  45, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.014932, mae: 2.777915, mean_q: 3.343177, mean_eps: 0.339014
 3306297/6000000: episode: 4418, duration: 18.384s, episode steps: 845, steps per second:  46, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.156 [0.000, 5.000],  loss: 0.014938, mae: 2.808225, mean_q: 3.378656, mean_eps: 0.338825
 3307329/6000000: episode: 4419, duration: 21.708s, episode steps: 1032, steps per second:  48, episode reward: 29.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.139 [0.000, 5.000],  loss: 0.016461, mae: 2.805202, mean_q: 3.376598, mean_eps: 0.338637
 3307828/6000000: episode: 4420, duration: 10.120s, episode steps: 499, steps per second:  49, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.016255, mae: 2.814410, mean_q: 3.387455, mean_eps: 0.338484
 3308546/6000000: episode: 4421, duration: 14.636s, episode steps: 718, steps per second:  49, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.016446, mae: 2.796076, mean_q: 3.364850, mean_eps: 0.338363
 3309207/6000000: episode: 4422, duration: 13.771s, episode steps: 661, steps per second:  48, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.017199, mae: 2.787752, mean_q: 3.354261, mean_eps: 0.338225
 3309883/6000000: episode: 4423, duration: 14.268s, episode steps: 676, steps per second:  47, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.250 [0.000, 5.000],  loss: 0.015804, mae: 2.808582, mean_q: 3.377952, mean_eps: 0.338091
 3310716/6000000: episode: 4424, duration: 18.402s, episode steps: 833, steps per second:  45, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.239 [0.000, 5.000],  loss: 0.015171, mae: 2.796744, mean_q: 3.367757, mean_eps: 0.337940
 3311768/6000000: episode: 4425, duration: 21.776s, episode steps: 1052, steps per second:  48, episode reward: 29.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.016273, mae: 2.774683, mean_q: 3.341434, mean_eps: 0.337752
 3312611/6000000: episode: 4426, duration: 18.173s, episode steps: 843, steps per second:  46, episode reward: 26.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.929 [0.000, 5.000],  loss: 0.016379, mae: 2.780901, mean_q: 3.347688, mean_eps: 0.337562
 3313464/6000000: episode: 4427, duration: 18.862s, episode steps: 853, steps per second:  45, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.128 [0.000, 5.000],  loss: 0.014508, mae: 2.786693, mean_q: 3.353660, mean_eps: 0.337393
 3314431/6000000: episode: 4428, duration: 21.441s, episode steps: 967, steps per second:  45, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.009 [0.000, 5.000],  loss: 0.016404, mae: 2.790157, mean_q: 3.356520, mean_eps: 0.337211
 3315343/6000000: episode: 4429, duration: 21.056s, episode steps: 912, steps per second:  43, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.016554, mae: 2.792745, mean_q: 3.360864, mean_eps: 0.337023
 3316170/6000000: episode: 4430, duration: 18.018s, episode steps: 827, steps per second:  46, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.023 [0.000, 5.000],  loss: 0.016198, mae: 2.811444, mean_q: 3.383176, mean_eps: 0.336849
 3316891/6000000: episode: 4431, duration: 15.847s, episode steps: 721, steps per second:  45, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.951 [0.000, 5.000],  loss: 0.016697, mae: 2.803392, mean_q: 3.372574, mean_eps: 0.336694
 3317546/6000000: episode: 4432, duration: 14.650s, episode steps: 655, steps per second:  45, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.750 [0.000, 5.000],  loss: 0.016723, mae: 2.782774, mean_q: 3.349550, mean_eps: 0.336556
 3318341/6000000: episode: 4433, duration: 17.683s, episode steps: 795, steps per second:  45, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.966 [0.000, 5.000],  loss: 0.014835, mae: 2.804696, mean_q: 3.375051, mean_eps: 0.336411
 3319176/6000000: episode: 4434, duration: 17.554s, episode steps: 835, steps per second:  48, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.099 [0.000, 5.000],  loss: 0.016315, mae: 2.793980, mean_q: 3.361411, mean_eps: 0.336248
 3320091/6000000: episode: 4435, duration: 22.266s, episode steps: 915, steps per second:  41, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.016938, mae: 2.777886, mean_q: 3.342783, mean_eps: 0.336074
 3320766/6000000: episode: 4436, duration: 16.804s, episode steps: 675, steps per second:  40, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.773 [0.000, 5.000],  loss: 0.015921, mae: 2.791828, mean_q: 3.358477, mean_eps: 0.335914
 3321418/6000000: episode: 4437, duration: 14.307s, episode steps: 652, steps per second:  46, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.923 [0.000, 5.000],  loss: 0.015678, mae: 2.805111, mean_q: 3.376010, mean_eps: 0.335782
 3322105/6000000: episode: 4438, duration: 15.162s, episode steps: 687, steps per second:  45, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.013880, mae: 2.791568, mean_q: 3.358313, mean_eps: 0.335648
 3322951/6000000: episode: 4439, duration: 18.602s, episode steps: 846, steps per second:  45, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.015862, mae: 2.788066, mean_q: 3.354585, mean_eps: 0.335494
 3323781/6000000: episode: 4440, duration: 17.360s, episode steps: 830, steps per second:  48, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.271 [0.000, 5.000],  loss: 0.015336, mae: 2.775971, mean_q: 3.340086, mean_eps: 0.335327
 3324465/6000000: episode: 4441, duration: 13.610s, episode steps: 684, steps per second:  50, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.017209, mae: 2.775853, mean_q: 3.340489, mean_eps: 0.335175
 3325367/6000000: episode: 4442, duration: 19.026s, episode steps: 902, steps per second:  47, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.979 [0.000, 5.000],  loss: 0.016028, mae: 2.786055, mean_q: 3.352326, mean_eps: 0.335017
 3325870/6000000: episode: 4443, duration: 10.827s, episode steps: 503, steps per second:  46, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.944 [0.000, 5.000],  loss: 0.014709, mae: 2.804439, mean_q: 3.374243, mean_eps: 0.334876
 3327089/6000000: episode: 4444, duration: 26.686s, episode steps: 1219, steps per second:  46, episode reward: 32.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.220 [0.000, 5.000],  loss: 0.016963, mae: 2.805343, mean_q: 3.376000, mean_eps: 0.334704
 3328069/6000000: episode: 4445, duration: 21.238s, episode steps: 980, steps per second:  46, episode reward: 31.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 1.967 [0.000, 5.000],  loss: 0.016023, mae: 2.810987, mean_q: 3.382402, mean_eps: 0.334484
 3328942/6000000: episode: 4446, duration: 18.999s, episode steps: 873, steps per second:  46, episode reward: 29.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 1.772 [0.000, 5.000],  loss: 0.015595, mae: 2.805788, mean_q: 3.376972, mean_eps: 0.334299
 3329583/6000000: episode: 4447, duration: 14.800s, episode steps: 641, steps per second:  43, episode reward: 19.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.958 [0.000, 5.000],  loss: 0.018052, mae: 2.803476, mean_q: 3.374225, mean_eps: 0.334148
 3330263/6000000: episode: 4448, duration: 14.384s, episode steps: 680, steps per second:  47, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.091 [0.000, 5.000],  loss: 0.017437, mae: 2.793972, mean_q: 3.363580, mean_eps: 0.334016
 3331124/6000000: episode: 4449, duration: 21.080s, episode steps: 861, steps per second:  41, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.016684, mae: 2.799060, mean_q: 3.368363, mean_eps: 0.333862
 3331861/6000000: episode: 4450, duration: 17.296s, episode steps: 737, steps per second:  43, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.017782, mae: 2.823217, mean_q: 3.396760, mean_eps: 0.333702
 3332772/6000000: episode: 4451, duration: 19.341s, episode steps: 911, steps per second:  47, episode reward: 28.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.183 [0.000, 5.000],  loss: 0.016517, mae: 2.785353, mean_q: 3.350413, mean_eps: 0.333537
 3333743/6000000: episode: 4452, duration: 21.119s, episode steps: 971, steps per second:  46, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.768 [0.000, 5.000],  loss: 0.017467, mae: 2.791781, mean_q: 3.359563, mean_eps: 0.333349
 3334397/6000000: episode: 4453, duration: 13.818s, episode steps: 654, steps per second:  47, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.468 [0.000, 5.000],  loss: 0.017065, mae: 2.796649, mean_q: 3.365423, mean_eps: 0.333186
 3335311/6000000: episode: 4454, duration: 19.361s, episode steps: 914, steps per second:  47, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.187 [0.000, 5.000],  loss: 0.015727, mae: 2.805848, mean_q: 3.376783, mean_eps: 0.333029
 3335999/6000000: episode: 4455, duration: 14.581s, episode steps: 688, steps per second:  47, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.749 [0.000, 5.000],  loss: 0.017174, mae: 2.806010, mean_q: 3.375331, mean_eps: 0.332869
 3336805/6000000: episode: 4456, duration: 17.982s, episode steps: 806, steps per second:  45, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.015738, mae: 2.803123, mean_q: 3.371934, mean_eps: 0.332720
 3337802/6000000: episode: 4457, duration: 22.136s, episode steps: 997, steps per second:  45, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.016219, mae: 2.821541, mean_q: 3.395570, mean_eps: 0.332539
 3338701/6000000: episode: 4458, duration: 20.076s, episode steps: 899, steps per second:  45, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.017184, mae: 2.796917, mean_q: 3.364528, mean_eps: 0.332350
 3339563/6000000: episode: 4459, duration: 18.510s, episode steps: 862, steps per second:  47, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.014670, mae: 2.810934, mean_q: 3.382381, mean_eps: 0.332174
 3340448/6000000: episode: 4460, duration: 18.419s, episode steps: 885, steps per second:  48, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 0.015333, mae: 2.791558, mean_q: 3.360178, mean_eps: 0.331999
 3341382/6000000: episode: 4461, duration: 18.985s, episode steps: 934, steps per second:  49, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.017445, mae: 2.797412, mean_q: 3.366020, mean_eps: 0.331817
 3342019/6000000: episode: 4462, duration: 13.863s, episode steps: 637, steps per second:  46, episode reward: 18.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.014955, mae: 2.810812, mean_q: 3.381453, mean_eps: 0.331660
 3342547/6000000: episode: 4463, duration: 11.411s, episode steps: 528, steps per second:  46, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.208 [0.000, 5.000],  loss: 0.019312, mae: 2.788732, mean_q: 3.353486, mean_eps: 0.331544
 3343207/6000000: episode: 4464, duration: 13.834s, episode steps: 660, steps per second:  48, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.005 [0.000, 5.000],  loss: 0.015396, mae: 2.780146, mean_q: 3.345406, mean_eps: 0.331425
 3344296/6000000: episode: 4465, duration: 22.331s, episode steps: 1089, steps per second:  49, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.025 [0.000, 5.000],  loss: 0.016967, mae: 2.782483, mean_q: 3.349194, mean_eps: 0.331250
 3345185/6000000: episode: 4466, duration: 18.385s, episode steps: 889, steps per second:  48, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.017713, mae: 2.786022, mean_q: 3.351867, mean_eps: 0.331052
 3345814/6000000: episode: 4467, duration: 12.868s, episode steps: 629, steps per second:  49, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.016294, mae: 2.797685, mean_q: 3.368130, mean_eps: 0.330900
 3346584/6000000: episode: 4468, duration: 16.737s, episode steps: 770, steps per second:  46, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.019746, mae: 2.785557, mean_q: 3.350018, mean_eps: 0.330760
 3347531/6000000: episode: 4469, duration: 21.864s, episode steps: 947, steps per second:  43, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.045 [0.000, 5.000],  loss: 0.017569, mae: 2.803005, mean_q: 3.372630, mean_eps: 0.330589
 3348365/6000000: episode: 4470, duration: 19.974s, episode steps: 834, steps per second:  42, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.249 [0.000, 5.000],  loss: 0.017634, mae: 2.786735, mean_q: 3.353617, mean_eps: 0.330410
 3349249/6000000: episode: 4471, duration: 19.482s, episode steps: 884, steps per second:  45, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.016460, mae: 2.769215, mean_q: 3.333656, mean_eps: 0.330238
 3350295/6000000: episode: 4472, duration: 22.975s, episode steps: 1046, steps per second:  46, episode reward: 31.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.016831, mae: 2.789434, mean_q: 3.354611, mean_eps: 0.330046
 3351012/6000000: episode: 4473, duration: 16.510s, episode steps: 717, steps per second:  43, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.070 [0.000, 5.000],  loss: 0.015862, mae: 2.784497, mean_q: 3.349766, mean_eps: 0.329870
 3351808/6000000: episode: 4474, duration: 17.355s, episode steps: 796, steps per second:  46, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.917 [0.000, 5.000],  loss: 0.015788, mae: 2.804384, mean_q: 3.373167, mean_eps: 0.329718
 3352674/6000000: episode: 4475, duration: 20.360s, episode steps: 866, steps per second:  43, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.794 [0.000, 5.000],  loss: 0.017211, mae: 2.780374, mean_q: 3.344473, mean_eps: 0.329552
 3353465/6000000: episode: 4476, duration: 19.284s, episode steps: 791, steps per second:  41, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.732 [0.000, 5.000],  loss: 0.016738, mae: 2.775309, mean_q: 3.340967, mean_eps: 0.329386
 3354453/6000000: episode: 4477, duration: 21.168s, episode steps: 988, steps per second:  47, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.015506, mae: 2.799693, mean_q: 3.367538, mean_eps: 0.329208
 3355391/6000000: episode: 4478, duration: 20.534s, episode steps: 938, steps per second:  46, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.016897, mae: 2.813514, mean_q: 3.383826, mean_eps: 0.329016
 3356239/6000000: episode: 4479, duration: 18.503s, episode steps: 848, steps per second:  46, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.016187, mae: 2.810426, mean_q: 3.380150, mean_eps: 0.328837
 3357637/6000000: episode: 4480, duration: 28.341s, episode steps: 1398, steps per second:  49, episode reward: 33.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.017641, mae: 2.806764, mean_q: 3.375168, mean_eps: 0.328612
 3357975/6000000: episode: 4481, duration: 7.016s, episode steps: 338, steps per second:  48, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.030 [0.000, 5.000],  loss: 0.016389, mae: 2.785801, mean_q: 3.353382, mean_eps: 0.328439
 3358342/6000000: episode: 4482, duration: 7.702s, episode steps: 367, steps per second:  48, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.013805, mae: 2.798700, mean_q: 3.368424, mean_eps: 0.328368
 3359114/6000000: episode: 4483, duration: 16.425s, episode steps: 772, steps per second:  47, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.016642, mae: 2.789995, mean_q: 3.357770, mean_eps: 0.328254
 3360032/6000000: episode: 4484, duration: 19.297s, episode steps: 918, steps per second:  48, episode reward: 28.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.016828, mae: 2.795318, mean_q: 3.364290, mean_eps: 0.328086
 3361123/6000000: episode: 4485, duration: 22.896s, episode steps: 1091, steps per second:  48, episode reward: 31.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.246 [0.000, 5.000],  loss: 0.016961, mae: 2.776723, mean_q: 3.340246, mean_eps: 0.327885
 3361793/6000000: episode: 4486, duration: 14.291s, episode steps: 670, steps per second:  47, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.046 [0.000, 5.000],  loss: 0.014788, mae: 2.777119, mean_q: 3.341814, mean_eps: 0.327708
 3362774/6000000: episode: 4487, duration: 21.020s, episode steps: 981, steps per second:  47, episode reward: 28.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.201 [0.000, 5.000],  loss: 0.016614, mae: 2.773811, mean_q: 3.337219, mean_eps: 0.327543
 3364139/6000000: episode: 4488, duration: 30.020s, episode steps: 1365, steps per second:  45, episode reward: 28.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.147 [0.000, 5.000],  loss: 0.018440, mae: 2.771286, mean_q: 3.332334, mean_eps: 0.327309
 3364825/6000000: episode: 4489, duration: 17.040s, episode steps: 686, steps per second:  40, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.921 [0.000, 5.000],  loss: 0.017419, mae: 2.750412, mean_q: 3.310806, mean_eps: 0.327104
 3365806/6000000: episode: 4490, duration: 21.375s, episode steps: 981, steps per second:  46, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.986 [0.000, 5.000],  loss: 0.017433, mae: 2.799197, mean_q: 3.367912, mean_eps: 0.326937
 3366904/6000000: episode: 4491, duration: 25.301s, episode steps: 1098, steps per second:  43, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.201 [0.000, 5.000],  loss: 0.016522, mae: 2.790631, mean_q: 3.356607, mean_eps: 0.326729
 3367867/6000000: episode: 4492, duration: 23.287s, episode steps: 963, steps per second:  41, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.016636, mae: 2.779065, mean_q: 3.344421, mean_eps: 0.326523
 3369227/6000000: episode: 4493, duration: 30.138s, episode steps: 1360, steps per second:  45, episode reward: 35.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.955 [0.000, 5.000],  loss: 0.017308, mae: 2.786111, mean_q: 3.353070, mean_eps: 0.326291
 3369865/6000000: episode: 4494, duration: 14.469s, episode steps: 638, steps per second:  44, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.806 [0.000, 5.000],  loss: 0.015079, mae: 2.786900, mean_q: 3.355205, mean_eps: 0.326091
 3370645/6000000: episode: 4495, duration: 17.542s, episode steps: 780, steps per second:  44, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.854 [0.000, 5.000],  loss: 0.015036, mae: 2.809053, mean_q: 3.380623, mean_eps: 0.325949
 3371570/6000000: episode: 4496, duration: 19.974s, episode steps: 925, steps per second:  46, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.968 [0.000, 5.000],  loss: 0.016817, mae: 2.811250, mean_q: 3.381695, mean_eps: 0.325778
 3372273/6000000: episode: 4497, duration: 15.710s, episode steps: 703, steps per second:  45, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.020 [0.000, 5.000],  loss: 0.015258, mae: 2.781199, mean_q: 3.348667, mean_eps: 0.325616
 3373440/6000000: episode: 4498, duration: 23.943s, episode steps: 1167, steps per second:  49, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.087 [0.000, 5.000],  loss: 0.016860, mae: 2.821105, mean_q: 3.393764, mean_eps: 0.325429
 3374049/6000000: episode: 4499, duration: 12.645s, episode steps: 609, steps per second:  48, episode reward: 17.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.744 [0.000, 5.000],  loss: 0.016267, mae: 2.797880, mean_q: 3.366280, mean_eps: 0.325251
 3375145/6000000: episode: 4500, duration: 23.505s, episode steps: 1096, steps per second:  47, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.262 [0.000, 5.000],  loss: 0.017286, mae: 2.808513, mean_q: 3.379687, mean_eps: 0.325080
 3375848/6000000: episode: 4501, duration: 15.210s, episode steps: 703, steps per second:  46, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.013467, mae: 2.803460, mean_q: 3.373336, mean_eps: 0.324901
 3376383/6000000: episode: 4502, duration: 11.362s, episode steps: 535, steps per second:  47, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.135 [0.000, 5.000],  loss: 0.016779, mae: 2.808324, mean_q: 3.377382, mean_eps: 0.324777
 3377468/6000000: episode: 4503, duration: 23.187s, episode steps: 1085, steps per second:  47, episode reward: 30.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.049 [0.000, 5.000],  loss: 0.015990, mae: 2.824392, mean_q: 3.399338, mean_eps: 0.324615
 3378075/6000000: episode: 4504, duration: 13.322s, episode steps: 607, steps per second:  46, episode reward: 17.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.018020, mae: 2.829889, mean_q: 3.403495, mean_eps: 0.324446
 3378554/6000000: episode: 4505, duration: 10.604s, episode steps: 479, steps per second:  45, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.635 [0.000, 5.000],  loss: 0.017024, mae: 2.817001, mean_q: 3.389419, mean_eps: 0.324337
 3379756/6000000: episode: 4506, duration: 27.699s, episode steps: 1202, steps per second:  43, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.265 [0.000, 5.000],  loss: 0.016373, mae: 2.821980, mean_q: 3.396947, mean_eps: 0.324169
 3380559/6000000: episode: 4507, duration: 22.005s, episode steps: 803, steps per second:  36, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.015831, mae: 2.830203, mean_q: 3.404335, mean_eps: 0.323969
 3381494/6000000: episode: 4508, duration: 21.178s, episode steps: 935, steps per second:  44, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.016754, mae: 2.826845, mean_q: 3.400451, mean_eps: 0.323795
 3382499/6000000: episode: 4509, duration: 21.935s, episode steps: 1005, steps per second:  46, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.016641, mae: 2.831346, mean_q: 3.405777, mean_eps: 0.323601
 3383297/6000000: episode: 4510, duration: 18.045s, episode steps: 798, steps per second:  44, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.187 [0.000, 5.000],  loss: 0.017316, mae: 2.841049, mean_q: 3.416153, mean_eps: 0.323420
 3383847/6000000: episode: 4511, duration: 12.315s, episode steps: 550, steps per second:  45, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.133 [0.000, 5.000],  loss: 0.017852, mae: 2.843314, mean_q: 3.418804, mean_eps: 0.323286
 3384784/6000000: episode: 4512, duration: 20.497s, episode steps: 937, steps per second:  46, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.623 [0.000, 5.000],  loss: 0.016617, mae: 2.833204, mean_q: 3.409957, mean_eps: 0.323137
 3386012/6000000: episode: 4513, duration: 28.353s, episode steps: 1228, steps per second:  43, episode reward: 32.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.016 [0.000, 5.000],  loss: 0.018165, mae: 2.814646, mean_q: 3.385786, mean_eps: 0.322921
 3386914/6000000: episode: 4514, duration: 19.767s, episode steps: 902, steps per second:  46, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.993 [0.000, 5.000],  loss: 0.014976, mae: 2.808622, mean_q: 3.379484, mean_eps: 0.322708
 3387592/6000000: episode: 4515, duration: 14.601s, episode steps: 678, steps per second:  46, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.833 [0.000, 5.000],  loss: 0.014861, mae: 2.817524, mean_q: 3.390826, mean_eps: 0.322550
 3388662/6000000: episode: 4516, duration: 23.592s, episode steps: 1070, steps per second:  45, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.698 [0.000, 5.000],  loss: 0.016334, mae: 2.807131, mean_q: 3.379227, mean_eps: 0.322375
 3389568/6000000: episode: 4517, duration: 18.799s, episode steps: 906, steps per second:  48, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.092 [0.000, 5.000],  loss: 0.014812, mae: 2.808015, mean_q: 3.379154, mean_eps: 0.322177
 3390187/6000000: episode: 4518, duration: 12.821s, episode steps: 619, steps per second:  48, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.016017, mae: 2.823049, mean_q: 3.396291, mean_eps: 0.322025
 3391118/6000000: episode: 4519, duration: 19.756s, episode steps: 931, steps per second:  47, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.130 [0.000, 5.000],  loss: 0.014955, mae: 2.860042, mean_q: 3.439904, mean_eps: 0.321870
 3392002/6000000: episode: 4520, duration: 18.689s, episode steps: 884, steps per second:  47, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.014787, mae: 2.855070, mean_q: 3.436923, mean_eps: 0.321688
 3393362/6000000: episode: 4521, duration: 28.781s, episode steps: 1360, steps per second:  47, episode reward: 28.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.046 [0.000, 5.000],  loss: 0.016196, mae: 2.848123, mean_q: 3.426380, mean_eps: 0.321464
 3394323/6000000: episode: 4522, duration: 20.193s, episode steps: 961, steps per second:  48, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.065 [0.000, 5.000],  loss: 0.016660, mae: 2.840169, mean_q: 3.418247, mean_eps: 0.321232
 3394911/6000000: episode: 4523, duration: 12.370s, episode steps: 588, steps per second:  48, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.966 [0.000, 5.000],  loss: 0.014380, mae: 2.837251, mean_q: 3.414464, mean_eps: 0.321077
 3395656/6000000: episode: 4524, duration: 15.720s, episode steps: 745, steps per second:  47, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.883 [0.000, 5.000],  loss: 0.015447, mae: 2.858343, mean_q: 3.438315, mean_eps: 0.320944
 3396610/6000000: episode: 4525, duration: 21.835s, episode steps: 954, steps per second:  44, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.096 [0.000, 5.000],  loss: 0.015776, mae: 2.867518, mean_q: 3.448615, mean_eps: 0.320774
 3397758/6000000: episode: 4526, duration: 26.424s, episode steps: 1148, steps per second:  43, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.122 [0.000, 5.000],  loss: 0.016420, mae: 2.840166, mean_q: 3.416092, mean_eps: 0.320563
 3398451/6000000: episode: 4527, duration: 15.234s, episode steps: 693, steps per second:  45, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.947 [0.000, 5.000],  loss: 0.016399, mae: 2.835527, mean_q: 3.411110, mean_eps: 0.320379
 3399153/6000000: episode: 4528, duration: 15.486s, episode steps: 702, steps per second:  45, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.912 [0.000, 5.000],  loss: 0.016851, mae: 2.855285, mean_q: 3.435468, mean_eps: 0.320240
 3400106/6000000: episode: 4529, duration: 20.896s, episode steps: 953, steps per second:  46, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.078 [0.000, 5.000],  loss: 0.018004, mae: 2.864210, mean_q: 3.446165, mean_eps: 0.320074
 3401247/6000000: episode: 4530, duration: 24.762s, episode steps: 1141, steps per second:  46, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.017722, mae: 2.894644, mean_q: 3.483292, mean_eps: 0.319865
 3402171/6000000: episode: 4531, duration: 20.956s, episode steps: 924, steps per second:  44, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.016809, mae: 2.894453, mean_q: 3.483542, mean_eps: 0.319658
 3402937/6000000: episode: 4532, duration: 18.024s, episode steps: 766, steps per second:  42, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.016379, mae: 2.896982, mean_q: 3.484438, mean_eps: 0.319489
 3403767/6000000: episode: 4533, duration: 18.679s, episode steps: 830, steps per second:  44, episode reward: 27.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.106 [0.000, 5.000],  loss: 0.015960, mae: 2.890741, mean_q: 3.477822, mean_eps: 0.319330
 3404865/6000000: episode: 4534, duration: 24.121s, episode steps: 1098, steps per second:  46, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.204 [0.000, 5.000],  loss: 0.016715, mae: 2.903145, mean_q: 3.493241, mean_eps: 0.319137
 3406029/6000000: episode: 4535, duration: 25.536s, episode steps: 1164, steps per second:  46, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.017426, mae: 2.923406, mean_q: 3.516256, mean_eps: 0.318910
 3406867/6000000: episode: 4536, duration: 16.989s, episode steps: 838, steps per second:  49, episode reward: 24.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.266 [0.000, 5.000],  loss: 0.016325, mae: 2.909637, mean_q: 3.499612, mean_eps: 0.318710
 3407936/6000000: episode: 4537, duration: 24.043s, episode steps: 1069, steps per second:  44, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.901 [0.000, 5.000],  loss: 0.016870, mae: 2.906600, mean_q: 3.496921, mean_eps: 0.318520
 3408820/6000000: episode: 4538, duration: 19.090s, episode steps: 884, steps per second:  46, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 0.017238, mae: 2.910878, mean_q: 3.501275, mean_eps: 0.318325
 3409914/6000000: episode: 4539, duration: 22.641s, episode steps: 1094, steps per second:  48, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.024 [0.000, 5.000],  loss: 0.016936, mae: 2.916697, mean_q: 3.509694, mean_eps: 0.318127
 3410609/6000000: episode: 4540, duration: 14.809s, episode steps: 695, steps per second:  47, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.096 [0.000, 5.000],  loss: 0.015607, mae: 2.947984, mean_q: 3.546768, mean_eps: 0.317948
 3411568/6000000: episode: 4541, duration: 21.562s, episode steps: 959, steps per second:  44, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.893 [0.000, 5.000],  loss: 0.018786, mae: 2.944979, mean_q: 3.543472, mean_eps: 0.317782
 3412688/6000000: episode: 4542, duration: 25.348s, episode steps: 1120, steps per second:  44, episode reward: 33.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.035 [0.000, 5.000],  loss: 0.015579, mae: 2.947427, mean_q: 3.548396, mean_eps: 0.317575
 3413430/6000000: episode: 4543, duration: 17.298s, episode steps: 742, steps per second:  43, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.016722, mae: 2.934448, mean_q: 3.530465, mean_eps: 0.317388
 3414449/6000000: episode: 4544, duration: 22.997s, episode steps: 1019, steps per second:  44, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.016483, mae: 2.938306, mean_q: 3.536015, mean_eps: 0.317212
 3414985/6000000: episode: 4545, duration: 11.991s, episode steps: 536, steps per second:  45, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.069 [0.000, 5.000],  loss: 0.018896, mae: 2.937530, mean_q: 3.535407, mean_eps: 0.317056
 3415590/6000000: episode: 4546, duration: 12.889s, episode steps: 605, steps per second:  47, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.921 [0.000, 5.000],  loss: 0.015375, mae: 2.973258, mean_q: 3.578572, mean_eps: 0.316942
 3416403/6000000: episode: 4547, duration: 17.265s, episode steps: 813, steps per second:  47, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.046 [0.000, 5.000],  loss: 0.017906, mae: 2.962327, mean_q: 3.565672, mean_eps: 0.316801
 3417768/6000000: episode: 4548, duration: 29.556s, episode steps: 1365, steps per second:  46, episode reward: 35.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.081 [0.000, 5.000],  loss: 0.016950, mae: 2.963280, mean_q: 3.567152, mean_eps: 0.316583
 3418488/6000000: episode: 4549, duration: 17.234s, episode steps: 720, steps per second:  42, episode reward: 21.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.332 [0.000, 5.000],  loss: 0.015353, mae: 2.961976, mean_q: 3.564726, mean_eps: 0.316375
 3419094/6000000: episode: 4550, duration: 14.536s, episode steps: 606, steps per second:  42, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.177 [0.000, 5.000],  loss: 0.017867, mae: 2.947555, mean_q: 3.548700, mean_eps: 0.316242
 3419825/6000000: episode: 4551, duration: 16.240s, episode steps: 731, steps per second:  45, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.016849, mae: 2.962815, mean_q: 3.567839, mean_eps: 0.316108
 3420750/6000000: episode: 4552, duration: 20.990s, episode steps: 925, steps per second:  44, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.089 [0.000, 5.000],  loss: 0.017561, mae: 2.950246, mean_q: 3.549656, mean_eps: 0.315942
 3421714/6000000: episode: 4553, duration: 21.771s, episode steps: 964, steps per second:  44, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.016932, mae: 2.947207, mean_q: 3.547371, mean_eps: 0.315754
 3422044/6000000: episode: 4554, duration: 7.032s, episode steps: 330, steps per second:  47, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.294 [0.000, 5.000],  loss: 0.016559, mae: 2.925189, mean_q: 3.518958, mean_eps: 0.315624
 3422713/6000000: episode: 4555, duration: 13.756s, episode steps: 669, steps per second:  49, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.017185, mae: 2.933580, mean_q: 3.529383, mean_eps: 0.315524
 3423292/6000000: episode: 4556, duration: 12.750s, episode steps: 579, steps per second:  45, episode reward: 17.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.190 [0.000, 5.000],  loss: 0.016060, mae: 2.962188, mean_q: 3.564632, mean_eps: 0.315400
 3424041/6000000: episode: 4557, duration: 16.541s, episode steps: 749, steps per second:  45, episode reward: 22.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.163 [0.000, 5.000],  loss: 0.017495, mae: 2.943064, mean_q: 3.540815, mean_eps: 0.315267
 3424609/6000000: episode: 4558, duration: 12.017s, episode steps: 568, steps per second:  47, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.889 [0.000, 5.000],  loss: 0.016959, mae: 2.957314, mean_q: 3.559461, mean_eps: 0.315135
 3425269/6000000: episode: 4559, duration: 13.569s, episode steps: 660, steps per second:  49, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.603 [0.000, 5.000],  loss: 0.016219, mae: 2.927869, mean_q: 3.525730, mean_eps: 0.315012
 3426342/6000000: episode: 4560, duration: 22.479s, episode steps: 1073, steps per second:  48, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.016796, mae: 2.919650, mean_q: 3.512744, mean_eps: 0.314839
 3427107/6000000: episode: 4561, duration: 15.777s, episode steps: 765, steps per second:  48, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.966 [0.000, 5.000],  loss: 0.017502, mae: 2.912347, mean_q: 3.502428, mean_eps: 0.314655
 3427631/6000000: episode: 4562, duration: 11.575s, episode steps: 524, steps per second:  45, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.985 [0.000, 5.000],  loss: 0.015360, mae: 2.900071, mean_q: 3.489316, mean_eps: 0.314526
 3428583/6000000: episode: 4563, duration: 20.767s, episode steps: 952, steps per second:  46, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.491 [0.000, 5.000],  loss: 0.015786, mae: 2.918886, mean_q: 3.515330, mean_eps: 0.314379
 3429404/6000000: episode: 4564, duration: 19.389s, episode steps: 821, steps per second:  42, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.250 [0.000, 5.000],  loss: 0.016552, mae: 2.941095, mean_q: 3.537124, mean_eps: 0.314202
 3430311/6000000: episode: 4565, duration: 20.810s, episode steps: 907, steps per second:  44, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.923 [0.000, 5.000],  loss: 0.017561, mae: 2.918907, mean_q: 3.512515, mean_eps: 0.314029
 3430988/6000000: episode: 4566, duration: 15.304s, episode steps: 677, steps per second:  44, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.548 [0.000, 5.000],  loss: 0.016433, mae: 2.926536, mean_q: 3.521872, mean_eps: 0.313870
 3431633/6000000: episode: 4567, duration: 14.792s, episode steps: 645, steps per second:  44, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.017806, mae: 2.915321, mean_q: 3.506290, mean_eps: 0.313738
 3432795/6000000: episode: 4568, duration: 26.122s, episode steps: 1162, steps per second:  44, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.775 [0.000, 5.000],  loss: 0.016607, mae: 2.913120, mean_q: 3.505246, mean_eps: 0.313557
 3433469/6000000: episode: 4569, duration: 14.691s, episode steps: 674, steps per second:  46, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.188 [0.000, 5.000],  loss: 0.017695, mae: 2.926922, mean_q: 3.521532, mean_eps: 0.313374
 3434398/6000000: episode: 4570, duration: 21.117s, episode steps: 929, steps per second:  44, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.207 [0.000, 5.000],  loss: 0.016228, mae: 2.888960, mean_q: 3.478037, mean_eps: 0.313213
 3435750/6000000: episode: 4571, duration: 31.521s, episode steps: 1352, steps per second:  43, episode reward: 31.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: 0.016860, mae: 2.896474, mean_q: 3.485139, mean_eps: 0.312985
 3436780/6000000: episode: 4572, duration: 23.227s, episode steps: 1030, steps per second:  44, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.627 [0.000, 5.000],  loss: 0.016400, mae: 2.911061, mean_q: 3.503370, mean_eps: 0.312747
 3437560/6000000: episode: 4573, duration: 17.785s, episode steps: 780, steps per second:  44, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.016655, mae: 2.907524, mean_q: 3.496215, mean_eps: 0.312566
 3438063/6000000: episode: 4574, duration: 10.818s, episode steps: 503, steps per second:  46, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.015042, mae: 2.917096, mean_q: 3.509273, mean_eps: 0.312438
 3439590/6000000: episode: 4575, duration: 31.059s, episode steps: 1527, steps per second:  49, episode reward: 33.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.986 [0.000, 5.000],  loss: 0.017381, mae: 2.909952, mean_q: 3.500955, mean_eps: 0.312235
 3440400/6000000: episode: 4576, duration: 17.264s, episode steps: 810, steps per second:  47, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.013927, mae: 2.894350, mean_q: 3.483519, mean_eps: 0.312001
 3441290/6000000: episode: 4577, duration: 19.678s, episode steps: 890, steps per second:  45, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.673 [0.000, 5.000],  loss: 0.016423, mae: 2.890153, mean_q: 3.475993, mean_eps: 0.311831
 3442911/6000000: episode: 4578, duration: 35.022s, episode steps: 1621, steps per second:  46, episode reward: 34.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.958 [0.000, 5.000],  loss: 0.016679, mae: 2.886147, mean_q: 3.471931, mean_eps: 0.311580
 3443706/6000000: episode: 4579, duration: 17.712s, episode steps: 795, steps per second:  45, episode reward: 26.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.142 [0.000, 5.000],  loss: 0.014702, mae: 2.891884, mean_q: 3.480906, mean_eps: 0.311338
 3444986/6000000: episode: 4580, duration: 29.154s, episode steps: 1280, steps per second:  44, episode reward: 33.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.015494, mae: 2.908044, mean_q: 3.500616, mean_eps: 0.311131
 3446108/6000000: episode: 4581, duration: 27.389s, episode steps: 1122, steps per second:  41, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.171 [0.000, 5.000],  loss: 0.016371, mae: 2.885199, mean_q: 3.471013, mean_eps: 0.310891
 3447015/6000000: episode: 4582, duration: 21.728s, episode steps: 907, steps per second:  42, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.002 [0.000, 5.000],  loss: 0.017945, mae: 2.888340, mean_q: 3.474565, mean_eps: 0.310688
 3447840/6000000: episode: 4583, duration: 19.189s, episode steps: 825, steps per second:  43, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.015676, mae: 2.897079, mean_q: 3.485793, mean_eps: 0.310515
 3449019/6000000: episode: 4584, duration: 26.542s, episode steps: 1179, steps per second:  44, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.075 [0.000, 5.000],  loss: 0.015913, mae: 2.905471, mean_q: 3.495000, mean_eps: 0.310314
 3449564/6000000: episode: 4585, duration: 12.138s, episode steps: 545, steps per second:  45, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.248 [0.000, 5.000],  loss: 0.017900, mae: 2.909775, mean_q: 3.500542, mean_eps: 0.310142
 3450596/6000000: episode: 4586, duration: 23.194s, episode steps: 1032, steps per second:  44, episode reward: 29.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.015995, mae: 2.882085, mean_q: 3.468530, mean_eps: 0.309984
 3451624/6000000: episode: 4587, duration: 22.892s, episode steps: 1028, steps per second:  45, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.017374, mae: 2.888424, mean_q: 3.475513, mean_eps: 0.309778
 3452358/6000000: episode: 4588, duration: 16.048s, episode steps: 734, steps per second:  46, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.184 [0.000, 5.000],  loss: 0.015931, mae: 2.848907, mean_q: 3.426372, mean_eps: 0.309602
 3453147/6000000: episode: 4589, duration: 16.792s, episode steps: 789, steps per second:  47, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.626 [0.000, 5.000],  loss: 0.015772, mae: 2.867390, mean_q: 3.450133, mean_eps: 0.309450
 3453993/6000000: episode: 4590, duration: 18.529s, episode steps: 846, steps per second:  46, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.084 [0.000, 5.000],  loss: 0.013727, mae: 2.864656, mean_q: 3.447188, mean_eps: 0.309286
 3455136/6000000: episode: 4591, duration: 23.520s, episode steps: 1143, steps per second:  49, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.135 [0.000, 5.000],  loss: 0.016941, mae: 2.878045, mean_q: 3.461712, mean_eps: 0.309087
 3456171/6000000: episode: 4592, duration: 22.160s, episode steps: 1035, steps per second:  47, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.135 [0.000, 5.000],  loss: 0.016380, mae: 2.854238, mean_q: 3.433203, mean_eps: 0.308870
 3456898/6000000: episode: 4593, duration: 15.583s, episode steps: 727, steps per second:  47, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.015411, mae: 2.853791, mean_q: 3.434627, mean_eps: 0.308693
 3458112/6000000: episode: 4594, duration: 25.057s, episode steps: 1214, steps per second:  48, episode reward: 35.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.258 [0.000, 5.000],  loss: 0.016876, mae: 2.873037, mean_q: 3.456307, mean_eps: 0.308499
 3459025/6000000: episode: 4595, duration: 19.300s, episode steps: 913, steps per second:  47, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.015722, mae: 2.865883, mean_q: 3.446929, mean_eps: 0.308286
 3459971/6000000: episode: 4596, duration: 19.760s, episode steps: 946, steps per second:  48, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.015524, mae: 2.859546, mean_q: 3.438221, mean_eps: 0.308100
 3460473/6000000: episode: 4597, duration: 11.211s, episode steps: 502, steps per second:  45, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.157 [0.000, 5.000],  loss: 0.014592, mae: 2.852950, mean_q: 3.432598, mean_eps: 0.307956
 3461678/6000000: episode: 4598, duration: 28.135s, episode steps: 1205, steps per second:  43, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.108 [0.000, 5.000],  loss: 0.016752, mae: 2.856983, mean_q: 3.438212, mean_eps: 0.307785
 3462345/6000000: episode: 4599, duration: 17.788s, episode steps: 667, steps per second:  37, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.162 [0.000, 5.000],  loss: 0.014801, mae: 2.861369, mean_q: 3.447290, mean_eps: 0.307598
 3463404/6000000: episode: 4600, duration: 23.966s, episode steps: 1059, steps per second:  44, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.015526, mae: 2.856459, mean_q: 3.438564, mean_eps: 0.307425
 3464354/6000000: episode: 4601, duration: 21.238s, episode steps: 950, steps per second:  45, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.033 [0.000, 5.000],  loss: 0.016358, mae: 2.848409, mean_q: 3.427088, mean_eps: 0.307224
 3465527/6000000: episode: 4602, duration: 25.393s, episode steps: 1173, steps per second:  46, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.691 [0.000, 5.000],  loss: 0.014959, mae: 2.848413, mean_q: 3.427778, mean_eps: 0.307012
 3466309/6000000: episode: 4603, duration: 16.759s, episode steps: 782, steps per second:  47, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.077 [0.000, 5.000],  loss: 0.016030, mae: 2.855135, mean_q: 3.432820, mean_eps: 0.306816
 3466840/6000000: episode: 4604, duration: 11.922s, episode steps: 531, steps per second:  45, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.996 [0.000, 5.000],  loss: 0.018234, mae: 2.830511, mean_q: 3.405145, mean_eps: 0.306685
 3467555/6000000: episode: 4605, duration: 16.785s, episode steps: 715, steps per second:  43, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.947 [0.000, 5.000],  loss: 0.016147, mae: 2.826799, mean_q: 3.403933, mean_eps: 0.306561
 3468253/6000000: episode: 4606, duration: 16.117s, episode steps: 698, steps per second:  43, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.015949, mae: 2.825012, mean_q: 3.398178, mean_eps: 0.306419
 3469476/6000000: episode: 4607, duration: 26.798s, episode steps: 1223, steps per second:  46, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.944 [0.000, 5.000],  loss: 0.016503, mae: 2.837944, mean_q: 3.414169, mean_eps: 0.306227
 3470307/6000000: episode: 4608, duration: 18.940s, episode steps: 831, steps per second:  44, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.178 [0.000, 5.000],  loss: 0.017993, mae: 2.834534, mean_q: 3.409773, mean_eps: 0.306022
 3470817/6000000: episode: 4609, duration: 10.761s, episode steps: 510, steps per second:  47, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.288 [0.000, 5.000],  loss: 0.015908, mae: 2.850383, mean_q: 3.428999, mean_eps: 0.305888
 3471891/6000000: episode: 4610, duration: 21.512s, episode steps: 1074, steps per second:  50, episode reward: 30.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.306 [0.000, 5.000],  loss: 0.018201, mae: 2.866317, mean_q: 3.448366, mean_eps: 0.305729
 3472793/6000000: episode: 4611, duration: 19.535s, episode steps: 902, steps per second:  46, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.210 [0.000, 5.000],  loss: 0.016457, mae: 2.842895, mean_q: 3.421386, mean_eps: 0.305532
 3473601/6000000: episode: 4612, duration: 17.600s, episode steps: 808, steps per second:  46, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.861 [0.000, 5.000],  loss: 0.015837, mae: 2.832575, mean_q: 3.408576, mean_eps: 0.305360
 3474365/6000000: episode: 4613, duration: 15.914s, episode steps: 764, steps per second:  48, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 0.017525, mae: 2.848377, mean_q: 3.429990, mean_eps: 0.305203
 3475099/6000000: episode: 4614, duration: 15.607s, episode steps: 734, steps per second:  47, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.016202, mae: 2.857722, mean_q: 3.440493, mean_eps: 0.305054
 3475895/6000000: episode: 4615, duration: 16.901s, episode steps: 796, steps per second:  47, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.148 [0.000, 5.000],  loss: 0.016059, mae: 2.843535, mean_q: 3.423675, mean_eps: 0.304901
 3476801/6000000: episode: 4616, duration: 19.862s, episode steps: 906, steps per second:  46, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.989 [0.000, 5.000],  loss: 0.015459, mae: 2.847405, mean_q: 3.427096, mean_eps: 0.304730
 3477778/6000000: episode: 4617, duration: 24.357s, episode steps: 977, steps per second:  40, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.950 [0.000, 5.000],  loss: 0.016817, mae: 2.840120, mean_q: 3.416862, mean_eps: 0.304542
 3478707/6000000: episode: 4618, duration: 25.261s, episode steps: 929, steps per second:  37, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.015860, mae: 2.833210, mean_q: 3.407815, mean_eps: 0.304352
 3479581/6000000: episode: 4619, duration: 19.350s, episode steps: 874, steps per second:  45, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.263 [0.000, 5.000],  loss: 0.014968, mae: 2.843785, mean_q: 3.420407, mean_eps: 0.304171
 3480492/6000000: episode: 4620, duration: 20.363s, episode steps: 911, steps per second:  45, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.017618, mae: 2.852728, mean_q: 3.432556, mean_eps: 0.303993
 3481824/6000000: episode: 4621, duration: 29.472s, episode steps: 1332, steps per second:  45, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.257 [0.000, 5.000],  loss: 0.014916, mae: 2.852154, mean_q: 3.430645, mean_eps: 0.303769
 3483145/6000000: episode: 4622, duration: 30.115s, episode steps: 1321, steps per second:  44, episode reward: 32.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.917 [0.000, 5.000],  loss: 0.015369, mae: 2.830688, mean_q: 3.406346, mean_eps: 0.303503
 3483781/6000000: episode: 4623, duration: 14.567s, episode steps: 636, steps per second:  44, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.017072, mae: 2.863378, mean_q: 3.443894, mean_eps: 0.303307
 3484678/6000000: episode: 4624, duration: 19.863s, episode steps: 897, steps per second:  45, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.194 [0.000, 5.000],  loss: 0.014334, mae: 2.851357, mean_q: 3.431468, mean_eps: 0.303154
 3485542/6000000: episode: 4625, duration: 19.466s, episode steps: 864, steps per second:  44, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.015541, mae: 2.845459, mean_q: 3.422335, mean_eps: 0.302978
 3486093/6000000: episode: 4626, duration: 12.240s, episode steps: 551, steps per second:  45, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.031 [0.000, 5.000],  loss: 0.017910, mae: 2.805825, mean_q: 3.375229, mean_eps: 0.302836
 3486816/6000000: episode: 4627, duration: 15.850s, episode steps: 723, steps per second:  46, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.095 [0.000, 5.000],  loss: 0.016130, mae: 2.838791, mean_q: 3.416515, mean_eps: 0.302709
 3487878/6000000: episode: 4628, duration: 21.908s, episode steps: 1062, steps per second:  48, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.202 [0.000, 5.000],  loss: 0.015628, mae: 2.828413, mean_q: 3.404067, mean_eps: 0.302531
 3488674/6000000: episode: 4629, duration: 17.770s, episode steps: 796, steps per second:  45, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.976 [0.000, 5.000],  loss: 0.016170, mae: 2.824854, mean_q: 3.398724, mean_eps: 0.302345
 3489507/6000000: episode: 4630, duration: 18.365s, episode steps: 833, steps per second:  45, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.016097, mae: 2.821076, mean_q: 3.394135, mean_eps: 0.302182
 3490437/6000000: episode: 4631, duration: 18.686s, episode steps: 930, steps per second:  50, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.201 [0.000, 5.000],  loss: 0.014865, mae: 2.810277, mean_q: 3.383604, mean_eps: 0.302006
 3491223/6000000: episode: 4632, duration: 16.114s, episode steps: 786, steps per second:  49, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.901 [0.000, 5.000],  loss: 0.018110, mae: 2.852962, mean_q: 3.432215, mean_eps: 0.301834
 3492556/6000000: episode: 4633, duration: 27.627s, episode steps: 1333, steps per second:  48, episode reward: 27.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.020 [0.000, 5.000],  loss: 0.015299, mae: 2.849788, mean_q: 3.428999, mean_eps: 0.301622
 3493522/6000000: episode: 4634, duration: 20.313s, episode steps: 966, steps per second:  48, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 0.017414, mae: 2.855132, mean_q: 3.434234, mean_eps: 0.301392
 3494226/6000000: episode: 4635, duration: 16.528s, episode steps: 704, steps per second:  43, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.287 [0.000, 5.000],  loss: 0.015913, mae: 2.830904, mean_q: 3.406910, mean_eps: 0.301225
 3495164/6000000: episode: 4636, duration: 22.324s, episode steps: 938, steps per second:  42, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.309 [0.000, 5.000],  loss: 0.014208, mae: 2.831642, mean_q: 3.408672, mean_eps: 0.301061
 3496446/6000000: episode: 4637, duration: 28.294s, episode steps: 1282, steps per second:  45, episode reward: 31.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.125 [0.000, 5.000],  loss: 0.015674, mae: 2.864201, mean_q: 3.444545, mean_eps: 0.300839
 3497362/6000000: episode: 4638, duration: 20.736s, episode steps: 916, steps per second:  44, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.015360, mae: 2.865122, mean_q: 3.447243, mean_eps: 0.300619
 3498383/6000000: episode: 4639, duration: 22.827s, episode steps: 1021, steps per second:  45, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.113 [0.000, 5.000],  loss: 0.015973, mae: 2.857949, mean_q: 3.438451, mean_eps: 0.300426
 3499244/6000000: episode: 4640, duration: 18.943s, episode steps: 861, steps per second:  45, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.094 [0.000, 5.000],  loss: 0.016903, mae: 2.842900, mean_q: 3.420424, mean_eps: 0.300238
 3500289/6000000: episode: 4641, duration: 23.414s, episode steps: 1045, steps per second:  45, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.233 [0.000, 5.000],  loss: 0.017801, mae: 2.842218, mean_q: 3.418655, mean_eps: 0.300047
 3501564/6000000: episode: 4642, duration: 28.185s, episode steps: 1275, steps per second:  45, episode reward: 31.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.181 [0.000, 5.000],  loss: 0.016355, mae: 2.852356, mean_q: 3.430704, mean_eps: 0.299815
 3502293/6000000: episode: 4643, duration: 16.524s, episode steps: 729, steps per second:  44, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.015450, mae: 2.846340, mean_q: 3.424109, mean_eps: 0.299614
 3503263/6000000: episode: 4644, duration: 21.744s, episode steps: 970, steps per second:  45, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.954 [0.000, 5.000],  loss: 0.015455, mae: 2.858620, mean_q: 3.438305, mean_eps: 0.299444
 3503979/6000000: episode: 4645, duration: 14.870s, episode steps: 716, steps per second:  48, episode reward: 21.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.916 [0.000, 5.000],  loss: 0.016098, mae: 2.868206, mean_q: 3.449765, mean_eps: 0.299276
 3504734/6000000: episode: 4646, duration: 16.386s, episode steps: 755, steps per second:  46, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.151 [0.000, 5.000],  loss: 0.016979, mae: 2.843868, mean_q: 3.422472, mean_eps: 0.299129
 3506187/6000000: episode: 4647, duration: 32.431s, episode steps: 1453, steps per second:  45, episode reward: 31.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.171 [0.000, 5.000],  loss: 0.016072, mae: 2.868385, mean_q: 3.449590, mean_eps: 0.298908
 3507279/6000000: episode: 4648, duration: 23.223s, episode steps: 1092, steps per second:  47, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.236 [0.000, 5.000],  loss: 0.016418, mae: 2.866964, mean_q: 3.450678, mean_eps: 0.298654
 3508034/6000000: episode: 4649, duration: 16.343s, episode steps: 755, steps per second:  46, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.828 [0.000, 5.000],  loss: 0.017238, mae: 2.886400, mean_q: 3.472549, mean_eps: 0.298469
 3509039/6000000: episode: 4650, duration: 22.244s, episode steps: 1005, steps per second:  45, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.294 [0.000, 5.000],  loss: 0.016407, mae: 2.881279, mean_q: 3.465474, mean_eps: 0.298293
 3509848/6000000: episode: 4651, duration: 18.523s, episode steps: 809, steps per second:  44, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.016569, mae: 2.892850, mean_q: 3.480191, mean_eps: 0.298112
 3510355/6000000: episode: 4652, duration: 11.991s, episode steps: 507, steps per second:  42, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.892 [0.000, 5.000],  loss: 0.017554, mae: 2.872335, mean_q: 3.455203, mean_eps: 0.297980
 3511493/6000000: episode: 4653, duration: 26.412s, episode steps: 1138, steps per second:  43, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.991 [0.000, 5.000],  loss: 0.016944, mae: 2.871543, mean_q: 3.454874, mean_eps: 0.297815
 3512347/6000000: episode: 4654, duration: 19.072s, episode steps: 854, steps per second:  45, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.741 [0.000, 5.000],  loss: 0.015389, mae: 2.872883, mean_q: 3.456320, mean_eps: 0.297616
 3513318/6000000: episode: 4655, duration: 22.946s, episode steps: 971, steps per second:  42, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.334 [0.000, 5.000],  loss: 0.017393, mae: 2.884041, mean_q: 3.467677, mean_eps: 0.297434
 3514222/6000000: episode: 4656, duration: 20.503s, episode steps: 904, steps per second:  44, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.656 [0.000, 5.000],  loss: 0.015747, mae: 2.883014, mean_q: 3.467759, mean_eps: 0.297246
 3515477/6000000: episode: 4657, duration: 28.347s, episode steps: 1255, steps per second:  44, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.175 [0.000, 5.000],  loss: 0.016815, mae: 2.895752, mean_q: 3.484009, mean_eps: 0.297030
 3516411/6000000: episode: 4658, duration: 21.432s, episode steps: 934, steps per second:  44, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.832 [0.000, 5.000],  loss: 0.016708, mae: 2.862401, mean_q: 3.442401, mean_eps: 0.296811
 3517076/6000000: episode: 4659, duration: 14.575s, episode steps: 665, steps per second:  46, episode reward: 19.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.719 [0.000, 5.000],  loss: 0.016325, mae: 2.882077, mean_q: 3.468020, mean_eps: 0.296652
 3517872/6000000: episode: 4660, duration: 17.748s, episode steps: 796, steps per second:  45, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.065 [0.000, 5.000],  loss: 0.016909, mae: 2.870382, mean_q: 3.452880, mean_eps: 0.296506
 3518500/6000000: episode: 4661, duration: 14.161s, episode steps: 628, steps per second:  44, episode reward: 17.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.014660, mae: 2.896218, mean_q: 3.486071, mean_eps: 0.296363
 3519525/6000000: episode: 4662, duration: 22.672s, episode steps: 1025, steps per second:  45, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.023 [0.000, 5.000],  loss: 0.016771, mae: 2.872097, mean_q: 3.454612, mean_eps: 0.296198
 3520557/6000000: episode: 4663, duration: 21.170s, episode steps: 1032, steps per second:  49, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.194 [0.000, 5.000],  loss: 0.017518, mae: 2.882815, mean_q: 3.469772, mean_eps: 0.295992
 3521329/6000000: episode: 4664, duration: 16.210s, episode steps: 772, steps per second:  48, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.780 [0.000, 5.000],  loss: 0.018016, mae: 2.882591, mean_q: 3.469151, mean_eps: 0.295811
 3522239/6000000: episode: 4665, duration: 19.415s, episode steps: 910, steps per second:  47, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.966 [0.000, 5.000],  loss: 0.015284, mae: 2.883666, mean_q: 3.469216, mean_eps: 0.295643
 3522803/6000000: episode: 4666, duration: 11.457s, episode steps: 564, steps per second:  49, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.227 [0.000, 5.000],  loss: 0.013425, mae: 2.878103, mean_q: 3.461651, mean_eps: 0.295496
 3523667/6000000: episode: 4667, duration: 17.868s, episode steps: 864, steps per second:  48, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.554 [0.000, 5.000],  loss: 0.017224, mae: 2.889134, mean_q: 3.476522, mean_eps: 0.295353
 3524862/6000000: episode: 4668, duration: 25.855s, episode steps: 1195, steps per second:  46, episode reward: 32.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.015676, mae: 2.879656, mean_q: 3.462829, mean_eps: 0.295147
 3525795/6000000: episode: 4669, duration: 20.483s, episode steps: 933, steps per second:  46, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.189 [0.000, 5.000],  loss: 0.017835, mae: 2.897374, mean_q: 3.484471, mean_eps: 0.294934
 3526769/6000000: episode: 4670, duration: 22.010s, episode steps: 974, steps per second:  44, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.016508, mae: 2.899539, mean_q: 3.487956, mean_eps: 0.294744
 3527493/6000000: episode: 4671, duration: 16.319s, episode steps: 724, steps per second:  44, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.685 [0.000, 5.000],  loss: 0.017563, mae: 2.886262, mean_q: 3.471750, mean_eps: 0.294574
 3529119/6000000: episode: 4672, duration: 35.529s, episode steps: 1626, steps per second:  46, episode reward: 35.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.016833, mae: 2.887930, mean_q: 3.474077, mean_eps: 0.294339
 3530185/6000000: episode: 4673, duration: 25.140s, episode steps: 1066, steps per second:  42, episode reward: 30.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.016649, mae: 2.897479, mean_q: 3.485244, mean_eps: 0.294070
 3530765/6000000: episode: 4674, duration: 12.492s, episode steps: 580, steps per second:  46, episode reward: 16.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.015209, mae: 2.905844, mean_q: 3.494225, mean_eps: 0.293905
 3531570/6000000: episode: 4675, duration: 17.452s, episode steps: 805, steps per second:  46, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.014620, mae: 2.895836, mean_q: 3.485104, mean_eps: 0.293766
 3532083/6000000: episode: 4676, duration: 11.787s, episode steps: 513, steps per second:  44, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.957 [0.000, 5.000],  loss: 0.014381, mae: 2.897864, mean_q: 3.488045, mean_eps: 0.293635
 3532958/6000000: episode: 4677, duration: 19.388s, episode steps: 875, steps per second:  45, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.016332, mae: 2.917946, mean_q: 3.509788, mean_eps: 0.293496
 3533999/6000000: episode: 4678, duration: 22.585s, episode steps: 1041, steps per second:  46, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.018133, mae: 2.914759, mean_q: 3.507439, mean_eps: 0.293304
 3535019/6000000: episode: 4679, duration: 22.408s, episode steps: 1020, steps per second:  46, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.983 [0.000, 5.000],  loss: 0.016491, mae: 2.903117, mean_q: 3.491939, mean_eps: 0.293098
 3536154/6000000: episode: 4680, duration: 23.887s, episode steps: 1135, steps per second:  48, episode reward: 32.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.112 [0.000, 5.000],  loss: 0.015264, mae: 2.911103, mean_q: 3.502929, mean_eps: 0.292883
 3537589/6000000: episode: 4681, duration: 30.670s, episode steps: 1435, steps per second:  47, episode reward: 32.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.257 [0.000, 5.000],  loss: 0.017006, mae: 2.936145, mean_q: 3.531081, mean_eps: 0.292626
 3538560/6000000: episode: 4682, duration: 20.411s, episode steps: 971, steps per second:  48, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.028 [0.000, 5.000],  loss: 0.017442, mae: 2.902331, mean_q: 3.492252, mean_eps: 0.292385
 3539337/6000000: episode: 4683, duration: 16.579s, episode steps: 777, steps per second:  47, episode reward: 23.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.275 [0.000, 5.000],  loss: 0.016287, mae: 2.902165, mean_q: 3.492318, mean_eps: 0.292210
 3540110/6000000: episode: 4684, duration: 16.091s, episode steps: 773, steps per second:  48, episode reward: 24.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.732 [0.000, 5.000],  loss: 0.018492, mae: 2.928079, mean_q: 3.523059, mean_eps: 0.292055
 3541177/6000000: episode: 4685, duration: 22.819s, episode steps: 1067, steps per second:  47, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.997 [0.000, 5.000],  loss: 0.017418, mae: 2.919106, mean_q: 3.512510, mean_eps: 0.291871
 3542205/6000000: episode: 4686, duration: 22.454s, episode steps: 1028, steps per second:  46, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.017990, mae: 2.931068, mean_q: 3.525710, mean_eps: 0.291662
 3542578/6000000: episode: 4687, duration: 8.034s, episode steps: 373, steps per second:  46, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.893 [0.000, 5.000],  loss: 0.014818, mae: 2.914758, mean_q: 3.508095, mean_eps: 0.291522
 3543277/6000000: episode: 4688, duration: 16.137s, episode steps: 699, steps per second:  43, episode reward: 23.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.013561, mae: 2.907251, mean_q: 3.497197, mean_eps: 0.291414
 3544628/6000000: episode: 4689, duration: 32.131s, episode steps: 1351, steps per second:  42, episode reward: 31.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.258 [0.000, 5.000],  loss: 0.016428, mae: 2.910146, mean_q: 3.500421, mean_eps: 0.291210
 3545147/6000000: episode: 4690, duration: 11.613s, episode steps: 519, steps per second:  45, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.938 [0.000, 5.000],  loss: 0.016360, mae: 2.892969, mean_q: 3.481147, mean_eps: 0.291023
 3546230/6000000: episode: 4691, duration: 23.112s, episode steps: 1083, steps per second:  47, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.016728, mae: 2.930390, mean_q: 3.524639, mean_eps: 0.290862
 3547152/6000000: episode: 4692, duration: 19.621s, episode steps: 922, steps per second:  47, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.015065, mae: 2.924741, mean_q: 3.518460, mean_eps: 0.290662
 3548036/6000000: episode: 4693, duration: 18.800s, episode steps: 884, steps per second:  47, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.016239, mae: 2.923362, mean_q: 3.515140, mean_eps: 0.290482
 3548900/6000000: episode: 4694, duration: 19.401s, episode steps: 864, steps per second:  45, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.950 [0.000, 5.000],  loss: 0.017308, mae: 2.915840, mean_q: 3.506654, mean_eps: 0.290307
 3549804/6000000: episode: 4695, duration: 19.824s, episode steps: 904, steps per second:  46, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.105 [0.000, 5.000],  loss: 0.015285, mae: 2.919186, mean_q: 3.511167, mean_eps: 0.290130
 3550699/6000000: episode: 4696, duration: 19.479s, episode steps: 895, steps per second:  46, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.079 [0.000, 5.000],  loss: 0.016581, mae: 2.921673, mean_q: 3.512671, mean_eps: 0.289950
 3551280/6000000: episode: 4697, duration: 12.944s, episode steps: 581, steps per second:  45, episode reward: 17.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.176 [0.000, 5.000],  loss: 0.015052, mae: 2.923298, mean_q: 3.515379, mean_eps: 0.289802
 3552044/6000000: episode: 4698, duration: 16.731s, episode steps: 764, steps per second:  46, episode reward: 22.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.160 [0.000, 5.000],  loss: 0.016091, mae: 2.936321, mean_q: 3.531912, mean_eps: 0.289668
 3552677/6000000: episode: 4699, duration: 12.947s, episode steps: 633, steps per second:  49, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.629 [0.000, 5.000],  loss: 0.014663, mae: 2.940435, mean_q: 3.539183, mean_eps: 0.289528
 3553286/6000000: episode: 4700, duration: 12.349s, episode steps: 609, steps per second:  49, episode reward: 15.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.014564, mae: 2.925012, mean_q: 3.517971, mean_eps: 0.289404
 3554745/6000000: episode: 4701, duration: 30.393s, episode steps: 1459, steps per second:  48, episode reward: 29.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.102 [0.000, 5.000],  loss: 0.016438, mae: 2.930852, mean_q: 3.526007, mean_eps: 0.289197
 3555451/6000000: episode: 4702, duration: 15.029s, episode steps: 706, steps per second:  47, episode reward: 20.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.664 [0.000, 5.000],  loss: 0.016096, mae: 2.903429, mean_q: 3.495279, mean_eps: 0.288980
 3556116/6000000: episode: 4703, duration: 14.162s, episode steps: 665, steps per second:  47, episode reward: 18.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.095 [0.000, 5.000],  loss: 0.015494, mae: 2.888691, mean_q: 3.475156, mean_eps: 0.288844
 3557207/6000000: episode: 4704, duration: 22.814s, episode steps: 1091, steps per second:  48, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.999 [0.000, 5.000],  loss: 0.016880, mae: 2.900313, mean_q: 3.487916, mean_eps: 0.288668
 3558180/6000000: episode: 4705, duration: 20.666s, episode steps: 973, steps per second:  47, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.212 [0.000, 5.000],  loss: 0.014999, mae: 2.902597, mean_q: 3.492110, mean_eps: 0.288462
 3559085/6000000: episode: 4706, duration: 19.123s, episode steps: 905, steps per second:  47, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.263 [0.000, 5.000],  loss: 0.016799, mae: 2.891815, mean_q: 3.477983, mean_eps: 0.288274
 3559895/6000000: episode: 4707, duration: 18.348s, episode steps: 810, steps per second:  44, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.048 [0.000, 5.000],  loss: 0.015160, mae: 2.890913, mean_q: 3.477429, mean_eps: 0.288102
 3560672/6000000: episode: 4708, duration: 17.525s, episode steps: 777, steps per second:  44, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.049 [0.000, 5.000],  loss: 0.017717, mae: 2.888326, mean_q: 3.474183, mean_eps: 0.287944
 3561342/6000000: episode: 4709, duration: 14.168s, episode steps: 670, steps per second:  47, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.842 [0.000, 5.000],  loss: 0.016193, mae: 2.875876, mean_q: 3.458980, mean_eps: 0.287799
 3562150/6000000: episode: 4710, duration: 17.534s, episode steps: 808, steps per second:  46, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.119 [0.000, 5.000],  loss: 0.013969, mae: 2.890138, mean_q: 3.476618, mean_eps: 0.287651
 3563220/6000000: episode: 4711, duration: 23.570s, episode steps: 1070, steps per second:  45, episode reward: 34.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 1.822 [0.000, 5.000],  loss: 0.015209, mae: 2.869654, mean_q: 3.452844, mean_eps: 0.287463
 3563971/6000000: episode: 4712, duration: 15.684s, episode steps: 751, steps per second:  48, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.017475, mae: 2.875438, mean_q: 3.459369, mean_eps: 0.287281
 3564614/6000000: episode: 4713, duration: 13.663s, episode steps: 643, steps per second:  47, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.613 [0.000, 5.000],  loss: 0.014862, mae: 2.892327, mean_q: 3.480791, mean_eps: 0.287142
 3565455/6000000: episode: 4714, duration: 18.750s, episode steps: 841, steps per second:  45, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.771 [0.000, 5.000],  loss: 0.015314, mae: 2.881777, mean_q: 3.466608, mean_eps: 0.286993
 3567007/6000000: episode: 4715, duration: 34.405s, episode steps: 1552, steps per second:  45, episode reward: 36.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.745 [0.000, 5.000],  loss: 0.014670, mae: 2.857599, mean_q: 3.439163, mean_eps: 0.286754
 3567944/6000000: episode: 4716, duration: 21.104s, episode steps: 937, steps per second:  44, episode reward: 28.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.962 [0.000, 5.000],  loss: 0.017181, mae: 2.865153, mean_q: 3.445921, mean_eps: 0.286505
 3568767/6000000: episode: 4717, duration: 18.558s, episode steps: 823, steps per second:  44, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.014452, mae: 2.856680, mean_q: 3.435930, mean_eps: 0.286329
 3569407/6000000: episode: 4718, duration: 13.159s, episode steps: 640, steps per second:  49, episode reward: 18.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.030 [0.000, 5.000],  loss: 0.014502, mae: 2.856432, mean_q: 3.435424, mean_eps: 0.286183
 3570203/6000000: episode: 4719, duration: 16.491s, episode steps: 796, steps per second:  48, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.016099, mae: 2.861714, mean_q: 3.441556, mean_eps: 0.286039
 3571375/6000000: episode: 4720, duration: 24.648s, episode steps: 1172, steps per second:  48, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.016886, mae: 2.874301, mean_q: 3.455571, mean_eps: 0.285842
 3571946/6000000: episode: 4721, duration: 11.880s, episode steps: 571, steps per second:  48, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.015569, mae: 2.864168, mean_q: 3.443998, mean_eps: 0.285668
 3572622/6000000: episode: 4722, duration: 14.064s, episode steps: 676, steps per second:  48, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.015975, mae: 2.839260, mean_q: 3.414544, mean_eps: 0.285543
 3573355/6000000: episode: 4723, duration: 15.049s, episode steps: 733, steps per second:  49, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.319 [0.000, 5.000],  loss: 0.014637, mae: 2.867054, mean_q: 3.449382, mean_eps: 0.285402
 3574817/6000000: episode: 4724, duration: 31.300s, episode steps: 1462, steps per second:  47, episode reward: 31.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.015102, mae: 2.840198, mean_q: 3.417480, mean_eps: 0.285183
 3575734/6000000: episode: 4725, duration: 19.627s, episode steps: 917, steps per second:  47, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.192 [0.000, 5.000],  loss: 0.014618, mae: 2.864213, mean_q: 3.445663, mean_eps: 0.284945
 3576700/6000000: episode: 4726, duration: 22.990s, episode steps: 966, steps per second:  42, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.121 [0.000, 5.000],  loss: 0.017076, mae: 2.863074, mean_q: 3.444133, mean_eps: 0.284757
 3577879/6000000: episode: 4727, duration: 26.246s, episode steps: 1179, steps per second:  45, episode reward: 30.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.785 [0.000, 5.000],  loss: 0.015316, mae: 2.858448, mean_q: 3.438090, mean_eps: 0.284542
 3579065/6000000: episode: 4728, duration: 25.950s, episode steps: 1186, steps per second:  46, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.161 [0.000, 5.000],  loss: 0.014571, mae: 2.847804, mean_q: 3.428783, mean_eps: 0.284306
 3580014/6000000: episode: 4729, duration: 20.494s, episode steps: 949, steps per second:  46, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.014621, mae: 2.844870, mean_q: 3.421478, mean_eps: 0.284092
 3580955/6000000: episode: 4730, duration: 20.029s, episode steps: 941, steps per second:  47, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.063 [0.000, 5.000],  loss: 0.016447, mae: 2.898959, mean_q: 3.489208, mean_eps: 0.283903
 3582674/6000000: episode: 4731, duration: 39.085s, episode steps: 1719, steps per second:  44, episode reward: 49.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.014118, mae: 2.924818, mean_q: 3.519198, mean_eps: 0.283637
 3583399/6000000: episode: 4732, duration: 15.737s, episode steps: 725, steps per second:  46, episode reward: 20.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.015793, mae: 2.905385, mean_q: 3.494634, mean_eps: 0.283393
 3584709/6000000: episode: 4733, duration: 29.034s, episode steps: 1310, steps per second:  45, episode reward: 24.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.058 [0.000, 5.000],  loss: 0.015449, mae: 2.906846, mean_q: 3.496753, mean_eps: 0.283189
 3585629/6000000: episode: 4734, duration: 20.283s, episode steps: 920, steps per second:  45, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.098 [0.000, 5.000],  loss: 0.016390, mae: 2.926308, mean_q: 3.521198, mean_eps: 0.282966
 3586374/6000000: episode: 4735, duration: 15.168s, episode steps: 745, steps per second:  49, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.762 [0.000, 5.000],  loss: 0.014069, mae: 2.916186, mean_q: 3.510308, mean_eps: 0.282800
 3587622/6000000: episode: 4736, duration: 26.709s, episode steps: 1248, steps per second:  47, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.995 [0.000, 5.000],  loss: 0.016308, mae: 2.927253, mean_q: 3.521640, mean_eps: 0.282600
 3588290/6000000: episode: 4737, duration: 14.725s, episode steps: 668, steps per second:  45, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.111 [0.000, 5.000],  loss: 0.017402, mae: 2.917635, mean_q: 3.508615, mean_eps: 0.282409
 3589835/6000000: episode: 4738, duration: 32.249s, episode steps: 1545, steps per second:  48, episode reward: 34.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.152 [0.000, 5.000],  loss: 0.016383, mae: 2.911941, mean_q: 3.503490, mean_eps: 0.282188
 3590732/6000000: episode: 4739, duration: 20.260s, episode steps: 897, steps per second:  44, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.246 [0.000, 5.000],  loss: 0.014612, mae: 2.897116, mean_q: 3.485422, mean_eps: 0.281944
 3591268/6000000: episode: 4740, duration: 11.791s, episode steps: 536, steps per second:  45, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.015747, mae: 2.881412, mean_q: 3.465768, mean_eps: 0.281800
 3592085/6000000: episode: 4741, duration: 18.657s, episode steps: 817, steps per second:  44, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.767 [0.000, 5.000],  loss: 0.016987, mae: 2.888682, mean_q: 3.474972, mean_eps: 0.281665
 3593488/6000000: episode: 4742, duration: 32.775s, episode steps: 1403, steps per second:  43, episode reward: 32.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.215 [0.000, 5.000],  loss: 0.016480, mae: 2.881489, mean_q: 3.466682, mean_eps: 0.281443
 3594457/6000000: episode: 4743, duration: 22.798s, episode steps: 969, steps per second:  43, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.317 [0.000, 5.000],  loss: 0.014499, mae: 2.900757, mean_q: 3.490348, mean_eps: 0.281206
 3595854/6000000: episode: 4744, duration: 31.319s, episode steps: 1397, steps per second:  45, episode reward: 28.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.902 [0.000, 5.000],  loss: 0.017216, mae: 2.877195, mean_q: 3.460727, mean_eps: 0.280969
 3596773/6000000: episode: 4745, duration: 19.982s, episode steps: 919, steps per second:  46, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.017304, mae: 2.882132, mean_q: 3.466357, mean_eps: 0.280737
 3597501/6000000: episode: 4746, duration: 15.493s, episode steps: 728, steps per second:  47, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.728 [0.000, 5.000],  loss: 0.015269, mae: 2.883416, mean_q: 3.471057, mean_eps: 0.280572
 3598292/6000000: episode: 4747, duration: 18.244s, episode steps: 791, steps per second:  43, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.116 [0.000, 5.000],  loss: 0.016261, mae: 2.860539, mean_q: 3.441999, mean_eps: 0.280421
 3599119/6000000: episode: 4748, duration: 18.437s, episode steps: 827, steps per second:  45, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.050 [0.000, 5.000],  loss: 0.014983, mae: 2.861952, mean_q: 3.444879, mean_eps: 0.280259
 3599707/6000000: episode: 4749, duration: 12.721s, episode steps: 588, steps per second:  46, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.099 [0.000, 5.000],  loss: 0.018147, mae: 2.850745, mean_q: 3.428963, mean_eps: 0.280118
 3600288/6000000: episode: 4750, duration: 13.128s, episode steps: 581, steps per second:  44, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.065 [0.000, 5.000],  loss: 0.016953, mae: 2.866080, mean_q: 3.448313, mean_eps: 0.280001
 3601350/6000000: episode: 4751, duration: 24.479s, episode steps: 1062, steps per second:  43, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.176 [0.000, 5.000],  loss: 0.016403, mae: 2.847448, mean_q: 3.425514, mean_eps: 0.279836
 3602005/6000000: episode: 4752, duration: 14.092s, episode steps: 655, steps per second:  46, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.818 [0.000, 5.000],  loss: 0.014550, mae: 2.847183, mean_q: 3.427718, mean_eps: 0.279664
 3602757/6000000: episode: 4753, duration: 15.138s, episode steps: 752, steps per second:  50, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.015549, mae: 2.859299, mean_q: 3.440138, mean_eps: 0.279524
 3603436/6000000: episode: 4754, duration: 15.059s, episode steps: 679, steps per second:  45, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.119 [0.000, 5.000],  loss: 0.017560, mae: 2.850477, mean_q: 3.427692, mean_eps: 0.279381
 3604377/6000000: episode: 4755, duration: 20.435s, episode steps: 941, steps per second:  46, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.205 [0.000, 5.000],  loss: 0.015409, mae: 2.844829, mean_q: 3.422352, mean_eps: 0.279219
 3604876/6000000: episode: 4756, duration: 10.483s, episode steps: 499, steps per second:  48, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.747 [0.000, 5.000],  loss: 0.015823, mae: 2.842724, mean_q: 3.421530, mean_eps: 0.279075
 3606099/6000000: episode: 4757, duration: 25.792s, episode steps: 1223, steps per second:  47, episode reward: 30.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.071 [0.000, 5.000],  loss: 0.016527, mae: 2.843838, mean_q: 3.422621, mean_eps: 0.278903
 3607077/6000000: episode: 4758, duration: 20.846s, episode steps: 978, steps per second:  47, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.226 [0.000, 5.000],  loss: 0.016124, mae: 2.853224, mean_q: 3.432242, mean_eps: 0.278682
 3608002/6000000: episode: 4759, duration: 20.569s, episode steps: 925, steps per second:  45, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.016267, mae: 2.850260, mean_q: 3.429931, mean_eps: 0.278492
 3608564/6000000: episode: 4760, duration: 12.327s, episode steps: 562, steps per second:  46, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.014734, mae: 2.859375, mean_q: 3.438962, mean_eps: 0.278344
 3609829/6000000: episode: 4761, duration: 30.158s, episode steps: 1265, steps per second:  42, episode reward: 28.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.120 [0.000, 5.000],  loss: 0.015065, mae: 2.857136, mean_q: 3.436165, mean_eps: 0.278161
 3610696/6000000: episode: 4762, duration: 18.964s, episode steps: 867, steps per second:  46, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.016736, mae: 2.875592, mean_q: 3.458958, mean_eps: 0.277948
 3611572/6000000: episode: 4763, duration: 19.007s, episode steps: 876, steps per second:  46, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.693 [0.000, 5.000],  loss: 0.016123, mae: 2.839527, mean_q: 3.417202, mean_eps: 0.277774
 3612179/6000000: episode: 4764, duration: 13.086s, episode steps: 607, steps per second:  46, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.017794, mae: 2.834996, mean_q: 3.410512, mean_eps: 0.277625
 3613352/6000000: episode: 4765, duration: 25.357s, episode steps: 1173, steps per second:  46, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 0.015825, mae: 2.864099, mean_q: 3.444141, mean_eps: 0.277447
 3614068/6000000: episode: 4766, duration: 15.174s, episode steps: 716, steps per second:  47, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.637 [0.000, 5.000],  loss: 0.014168, mae: 2.871057, mean_q: 3.455249, mean_eps: 0.277258
 3614842/6000000: episode: 4767, duration: 17.360s, episode steps: 774, steps per second:  45, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.143 [0.000, 5.000],  loss: 0.017627, mae: 2.855245, mean_q: 3.433792, mean_eps: 0.277109
 3616114/6000000: episode: 4768, duration: 28.094s, episode steps: 1272, steps per second:  45, episode reward: 28.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.153 [0.000, 5.000],  loss: 0.016094, mae: 2.863654, mean_q: 3.445418, mean_eps: 0.276904
 3617078/6000000: episode: 4769, duration: 21.155s, episode steps: 964, steps per second:  46, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.932 [0.000, 5.000],  loss: 0.014940, mae: 2.849057, mean_q: 3.425799, mean_eps: 0.276681
 3618073/6000000: episode: 4770, duration: 21.655s, episode steps: 995, steps per second:  46, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.098 [0.000, 5.000],  loss: 0.016019, mae: 2.864714, mean_q: 3.445126, mean_eps: 0.276485
 3619152/6000000: episode: 4771, duration: 21.747s, episode steps: 1079, steps per second:  50, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.988 [0.000, 5.000],  loss: 0.016374, mae: 2.856615, mean_q: 3.433601, mean_eps: 0.276278
 3620186/6000000: episode: 4772, duration: 21.989s, episode steps: 1034, steps per second:  47, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.015999, mae: 2.853554, mean_q: 3.430502, mean_eps: 0.276066
 3620736/6000000: episode: 4773, duration: 12.277s, episode steps: 550, steps per second:  45, episode reward: 15.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.016 [0.000, 5.000],  loss: 0.014805, mae: 2.826792, mean_q: 3.398810, mean_eps: 0.275908
 3621256/6000000: episode: 4774, duration: 11.418s, episode steps: 520, steps per second:  46, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.015999, mae: 2.847820, mean_q: 3.425137, mean_eps: 0.275801
 3622256/6000000: episode: 4775, duration: 21.318s, episode steps: 1000, steps per second:  47, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.808 [0.000, 5.000],  loss: 0.015625, mae: 2.837146, mean_q: 3.412538, mean_eps: 0.275649
 3623218/6000000: episode: 4776, duration: 20.723s, episode steps: 962, steps per second:  46, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.544 [0.000, 5.000],  loss: 0.016893, mae: 2.839016, mean_q: 3.415374, mean_eps: 0.275453
 3624339/6000000: episode: 4777, duration: 24.923s, episode steps: 1121, steps per second:  45, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.067 [0.000, 5.000],  loss: 0.016207, mae: 2.820619, mean_q: 3.392749, mean_eps: 0.275244
 3625061/6000000: episode: 4778, duration: 16.270s, episode steps: 722, steps per second:  44, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.440 [0.000, 5.000],  loss: 0.014612, mae: 2.827686, mean_q: 3.401634, mean_eps: 0.275060
 3625993/6000000: episode: 4779, duration: 22.621s, episode steps: 932, steps per second:  41, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.264 [0.000, 5.000],  loss: 0.014996, mae: 2.830719, mean_q: 3.406110, mean_eps: 0.274894
 3626863/6000000: episode: 4780, duration: 20.075s, episode steps: 870, steps per second:  43, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.898 [0.000, 5.000],  loss: 0.015210, mae: 2.845687, mean_q: 3.426712, mean_eps: 0.274714
 3627990/6000000: episode: 4781, duration: 25.288s, episode steps: 1127, steps per second:  45, episode reward: 30.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.975 [0.000, 5.000],  loss: 0.015740, mae: 2.844207, mean_q: 3.421495, mean_eps: 0.274515
 3629056/6000000: episode: 4782, duration: 24.692s, episode steps: 1066, steps per second:  43, episode reward: 30.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.965 [0.000, 5.000],  loss: 0.016454, mae: 2.843520, mean_q: 3.418765, mean_eps: 0.274296
 3629732/6000000: episode: 4783, duration: 14.738s, episode steps: 676, steps per second:  46, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.017015, mae: 2.857631, mean_q: 3.437425, mean_eps: 0.274122
 3630677/6000000: episode: 4784, duration: 21.223s, episode steps: 945, steps per second:  45, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.954 [0.000, 5.000],  loss: 0.016164, mae: 2.865524, mean_q: 3.445022, mean_eps: 0.273959
 3631555/6000000: episode: 4785, duration: 20.768s, episode steps: 878, steps per second:  42, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.140 [0.000, 5.000],  loss: 0.016293, mae: 2.857562, mean_q: 3.436786, mean_eps: 0.273777
 3632481/6000000: episode: 4786, duration: 20.010s, episode steps: 926, steps per second:  46, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 0.015778, mae: 2.871890, mean_q: 3.454111, mean_eps: 0.273596
 3633373/6000000: episode: 4787, duration: 19.843s, episode steps: 892, steps per second:  45, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.837 [0.000, 5.000],  loss: 0.015344, mae: 2.852039, mean_q: 3.431550, mean_eps: 0.273414
 3634481/6000000: episode: 4788, duration: 24.317s, episode steps: 1108, steps per second:  46, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.165 [0.000, 5.000],  loss: 0.015223, mae: 2.853633, mean_q: 3.431885, mean_eps: 0.273214
 3635076/6000000: episode: 4789, duration: 12.127s, episode steps: 595, steps per second:  49, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.319 [0.000, 5.000],  loss: 0.015213, mae: 2.877571, mean_q: 3.460006, mean_eps: 0.273044
 3636283/6000000: episode: 4790, duration: 24.934s, episode steps: 1207, steps per second:  48, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.018025, mae: 2.849060, mean_q: 3.426844, mean_eps: 0.272864
 3637002/6000000: episode: 4791, duration: 15.430s, episode steps: 719, steps per second:  47, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.013998, mae: 2.846131, mean_q: 3.425564, mean_eps: 0.272672
 3637941/6000000: episode: 4792, duration: 19.328s, episode steps: 939, steps per second:  49, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.015896, mae: 2.876407, mean_q: 3.460401, mean_eps: 0.272506
 3639028/6000000: episode: 4793, duration: 23.149s, episode steps: 1087, steps per second:  47, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.962 [0.000, 5.000],  loss: 0.016600, mae: 2.853231, mean_q: 3.434594, mean_eps: 0.272303
 3639863/6000000: episode: 4794, duration: 18.615s, episode steps: 835, steps per second:  45, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.016462, mae: 2.852389, mean_q: 3.430306, mean_eps: 0.272111
 3640844/6000000: episode: 4795, duration: 22.137s, episode steps: 981, steps per second:  44, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.733 [0.000, 5.000],  loss: 0.014605, mae: 2.848073, mean_q: 3.426693, mean_eps: 0.271930
 3641480/6000000: episode: 4796, duration: 14.521s, episode steps: 636, steps per second:  44, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.014068, mae: 2.870585, mean_q: 3.453565, mean_eps: 0.271768
 3642110/6000000: episode: 4797, duration: 14.935s, episode steps: 630, steps per second:  42, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.092 [0.000, 5.000],  loss: 0.013771, mae: 2.888162, mean_q: 3.475022, mean_eps: 0.271641
 3643037/6000000: episode: 4798, duration: 21.385s, episode steps: 927, steps per second:  43, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.015637, mae: 2.860247, mean_q: 3.439776, mean_eps: 0.271485
 3644154/6000000: episode: 4799, duration: 24.342s, episode steps: 1117, steps per second:  46, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.914 [0.000, 5.000],  loss: 0.015366, mae: 2.873525, mean_q: 3.456442, mean_eps: 0.271281
 3645024/6000000: episode: 4800, duration: 19.017s, episode steps: 870, steps per second:  46, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.139 [0.000, 5.000],  loss: 0.016351, mae: 2.860910, mean_q: 3.440515, mean_eps: 0.271082
 3646211/6000000: episode: 4801, duration: 25.742s, episode steps: 1187, steps per second:  46, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.120 [0.000, 5.000],  loss: 0.016769, mae: 2.905350, mean_q: 3.493647, mean_eps: 0.270877
 3647616/6000000: episode: 4802, duration: 31.612s, episode steps: 1405, steps per second:  44, episode reward: 30.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 0.016102, mae: 2.896943, mean_q: 3.484115, mean_eps: 0.270618
 3648526/6000000: episode: 4803, duration: 20.475s, episode steps: 910, steps per second:  44, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.017028, mae: 2.899957, mean_q: 3.486745, mean_eps: 0.270386
 3649575/6000000: episode: 4804, duration: 23.364s, episode steps: 1049, steps per second:  45, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.042 [0.000, 5.000],  loss: 0.016073, mae: 2.901225, mean_q: 3.488035, mean_eps: 0.270190
 3650665/6000000: episode: 4805, duration: 24.850s, episode steps: 1090, steps per second:  44, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.033 [0.000, 5.000],  loss: 0.016223, mae: 2.889939, mean_q: 3.476373, mean_eps: 0.269976
 3651953/6000000: episode: 4806, duration: 26.391s, episode steps: 1288, steps per second:  49, episode reward: 31.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.152 [0.000, 5.000],  loss: 0.017431, mae: 2.895140, mean_q: 3.480516, mean_eps: 0.269738
 3652973/6000000: episode: 4807, duration: 22.030s, episode steps: 1020, steps per second:  46, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.201 [0.000, 5.000],  loss: 0.016102, mae: 2.894010, mean_q: 3.481649, mean_eps: 0.269507
 3653813/6000000: episode: 4808, duration: 17.862s, episode steps: 840, steps per second:  47, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.014952, mae: 2.892805, mean_q: 3.479667, mean_eps: 0.269321
 3654922/6000000: episode: 4809, duration: 23.531s, episode steps: 1109, steps per second:  47, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.918 [0.000, 5.000],  loss: 0.017102, mae: 2.894204, mean_q: 3.480484, mean_eps: 0.269126
 3656494/6000000: episode: 4810, duration: 34.975s, episode steps: 1572, steps per second:  45, episode reward: 33.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.968 [0.000, 5.000],  loss: 0.016601, mae: 2.872479, mean_q: 3.455683, mean_eps: 0.268858
 3657511/6000000: episode: 4811, duration: 22.752s, episode steps: 1017, steps per second:  45, episode reward: 29.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.015968, mae: 2.875100, mean_q: 3.458039, mean_eps: 0.268600
 3658597/6000000: episode: 4812, duration: 25.381s, episode steps: 1086, steps per second:  43, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.016289, mae: 2.871033, mean_q: 3.452432, mean_eps: 0.268389
 3659331/6000000: episode: 4813, duration: 17.446s, episode steps: 734, steps per second:  42, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.017632, mae: 2.882554, mean_q: 3.466026, mean_eps: 0.268207
 3660271/6000000: episode: 4814, duration: 21.121s, episode steps: 940, steps per second:  45, episode reward: 28.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.284 [0.000, 5.000],  loss: 0.015229, mae: 2.875499, mean_q: 3.456955, mean_eps: 0.268040
 3660957/6000000: episode: 4815, duration: 14.986s, episode steps: 686, steps per second:  46, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.014508, mae: 2.862405, mean_q: 3.444270, mean_eps: 0.267877
 3661758/6000000: episode: 4816, duration: 17.971s, episode steps: 801, steps per second:  45, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.016523, mae: 2.867097, mean_q: 3.449076, mean_eps: 0.267728
 3662305/6000000: episode: 4817, duration: 11.523s, episode steps: 547, steps per second:  47, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.519 [0.000, 5.000],  loss: 0.016302, mae: 2.877092, mean_q: 3.458693, mean_eps: 0.267594
 3663181/6000000: episode: 4818, duration: 18.964s, episode steps: 876, steps per second:  46, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.016554, mae: 2.855784, mean_q: 3.433275, mean_eps: 0.267451
 3663919/6000000: episode: 4819, duration: 17.182s, episode steps: 738, steps per second:  43, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.772 [0.000, 5.000],  loss: 0.016063, mae: 2.863708, mean_q: 3.446055, mean_eps: 0.267290
 3665114/6000000: episode: 4820, duration: 26.499s, episode steps: 1195, steps per second:  45, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.222 [0.000, 5.000],  loss: 0.014642, mae: 2.876153, mean_q: 3.459663, mean_eps: 0.267097
 3665757/6000000: episode: 4821, duration: 14.906s, episode steps: 643, steps per second:  43, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.014383, mae: 2.843033, mean_q: 3.419063, mean_eps: 0.266913
 3666505/6000000: episode: 4822, duration: 16.433s, episode steps: 748, steps per second:  46, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.124 [0.000, 5.000],  loss: 0.015290, mae: 2.865770, mean_q: 3.447171, mean_eps: 0.266774
 3667352/6000000: episode: 4823, duration: 17.699s, episode steps: 847, steps per second:  48, episode reward: 25.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.974 [0.000, 5.000],  loss: 0.016180, mae: 2.847714, mean_q: 3.425314, mean_eps: 0.266614
 3668359/6000000: episode: 4824, duration: 20.219s, episode steps: 1007, steps per second:  50, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.939 [0.000, 5.000],  loss: 0.016311, mae: 2.846164, mean_q: 3.421706, mean_eps: 0.266429
 3669306/6000000: episode: 4825, duration: 20.202s, episode steps: 947, steps per second:  47, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.117 [0.000, 5.000],  loss: 0.014525, mae: 2.843853, mean_q: 3.421320, mean_eps: 0.266234
 3670185/6000000: episode: 4826, duration: 18.690s, episode steps: 879, steps per second:  47, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.281 [0.000, 5.000],  loss: 0.013715, mae: 2.856034, mean_q: 3.439255, mean_eps: 0.266051
 3671105/6000000: episode: 4827, duration: 19.328s, episode steps: 920, steps per second:  48, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.300 [0.000, 5.000],  loss: 0.017908, mae: 2.861054, mean_q: 3.440023, mean_eps: 0.265871
 3671627/6000000: episode: 4828, duration: 11.381s, episode steps: 522, steps per second:  46, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.255 [0.000, 5.000],  loss: 0.014516, mae: 2.850421, mean_q: 3.430070, mean_eps: 0.265727
 3672474/6000000: episode: 4829, duration: 19.107s, episode steps: 847, steps per second:  44, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.309 [0.000, 5.000],  loss: 0.016412, mae: 2.840226, mean_q: 3.414028, mean_eps: 0.265590
 3673888/6000000: episode: 4830, duration: 33.445s, episode steps: 1414, steps per second:  42, episode reward: 33.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.326 [0.000, 5.000],  loss: 0.016468, mae: 2.844848, mean_q: 3.419718, mean_eps: 0.265364
 3675035/6000000: episode: 4831, duration: 28.672s, episode steps: 1147, steps per second:  40, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.023 [0.000, 5.000],  loss: 0.015299, mae: 2.844342, mean_q: 3.420365, mean_eps: 0.265108
 3675542/6000000: episode: 4832, duration: 11.357s, episode steps: 507, steps per second:  45, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.712 [0.000, 5.000],  loss: 0.016434, mae: 2.841425, mean_q: 3.415216, mean_eps: 0.264942
 3676396/6000000: episode: 4833, duration: 18.415s, episode steps: 854, steps per second:  46, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.861 [0.000, 5.000],  loss: 0.013611, mae: 2.803863, mean_q: 3.371017, mean_eps: 0.264806
 3677374/6000000: episode: 4834, duration: 22.191s, episode steps: 978, steps per second:  44, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.790 [0.000, 5.000],  loss: 0.015208, mae: 2.861586, mean_q: 3.441190, mean_eps: 0.264623
 3677908/6000000: episode: 4835, duration: 11.853s, episode steps: 534, steps per second:  45, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.011 [0.000, 5.000],  loss: 0.015050, mae: 2.862629, mean_q: 3.446216, mean_eps: 0.264472
 3679090/6000000: episode: 4836, duration: 24.691s, episode steps: 1182, steps per second:  48, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 0.016404, mae: 2.847430, mean_q: 3.426160, mean_eps: 0.264300
 3680256/6000000: episode: 4837, duration: 26.839s, episode steps: 1166, steps per second:  43, episode reward: 31.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.041 [0.000, 5.000],  loss: 0.014697, mae: 2.843875, mean_q: 3.421424, mean_eps: 0.264066
 3680995/6000000: episode: 4838, duration: 16.434s, episode steps: 739, steps per second:  45, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.116 [0.000, 5.000],  loss: 0.016426, mae: 2.836459, mean_q: 3.412625, mean_eps: 0.263875
 3681973/6000000: episode: 4839, duration: 21.490s, episode steps: 978, steps per second:  46, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.153 [0.000, 5.000],  loss: 0.014624, mae: 2.844591, mean_q: 3.423832, mean_eps: 0.263703
 3683044/6000000: episode: 4840, duration: 23.758s, episode steps: 1071, steps per second:  45, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.190 [0.000, 5.000],  loss: 0.015684, mae: 2.826109, mean_q: 3.398961, mean_eps: 0.263498
 3684196/6000000: episode: 4841, duration: 24.353s, episode steps: 1152, steps per second:  47, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.141 [0.000, 5.000],  loss: 0.015585, mae: 2.827008, mean_q: 3.401705, mean_eps: 0.263276
 3684985/6000000: episode: 4842, duration: 17.026s, episode steps: 789, steps per second:  46, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.127 [0.000, 5.000],  loss: 0.014962, mae: 2.835877, mean_q: 3.410657, mean_eps: 0.263082
 3685951/6000000: episode: 4843, duration: 21.254s, episode steps: 966, steps per second:  45, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.947 [0.000, 5.000],  loss: 0.015771, mae: 2.801372, mean_q: 3.368244, mean_eps: 0.262906
 3686819/6000000: episode: 4844, duration: 18.029s, episode steps: 868, steps per second:  48, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.787 [0.000, 5.000],  loss: 0.014919, mae: 2.810878, mean_q: 3.381827, mean_eps: 0.262723
 3687696/6000000: episode: 4845, duration: 18.465s, episode steps: 877, steps per second:  47, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.878 [0.000, 5.000],  loss: 0.014883, mae: 2.808332, mean_q: 3.376693, mean_eps: 0.262549
 3688209/6000000: episode: 4846, duration: 10.880s, episode steps: 513, steps per second:  47, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: 0.016702, mae: 2.806458, mean_q: 3.374542, mean_eps: 0.262410
 3688701/6000000: episode: 4847, duration: 10.664s, episode steps: 492, steps per second:  46, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.866 [0.000, 5.000],  loss: 0.016938, mae: 2.812682, mean_q: 3.382447, mean_eps: 0.262309
 3689637/6000000: episode: 4848, duration: 21.269s, episode steps: 936, steps per second:  44, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.016361, mae: 2.805608, mean_q: 3.373645, mean_eps: 0.262166
 3690598/6000000: episode: 4849, duration: 22.565s, episode steps: 961, steps per second:  43, episode reward: 29.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.124 [0.000, 5.000],  loss: 0.014817, mae: 2.814527, mean_q: 3.385013, mean_eps: 0.261976
 3691463/6000000: episode: 4850, duration: 22.184s, episode steps: 865, steps per second:  39, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.091 [0.000, 5.000],  loss: 0.013600, mae: 2.808963, mean_q: 3.378122, mean_eps: 0.261794
 3692381/6000000: episode: 4851, duration: 20.356s, episode steps: 918, steps per second:  45, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.015062, mae: 2.831299, mean_q: 3.403485, mean_eps: 0.261616
 3693265/6000000: episode: 4852, duration: 20.216s, episode steps: 884, steps per second:  44, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.852 [0.000, 5.000],  loss: 0.015365, mae: 2.816908, mean_q: 3.388426, mean_eps: 0.261435
 3694274/6000000: episode: 4853, duration: 22.350s, episode steps: 1009, steps per second:  45, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.267 [0.000, 5.000],  loss: 0.016605, mae: 2.825904, mean_q: 3.400595, mean_eps: 0.261246
 3694825/6000000: episode: 4854, duration: 11.609s, episode steps: 551, steps per second:  47, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.014566, mae: 2.794785, mean_q: 3.364556, mean_eps: 0.261090
 3696038/6000000: episode: 4855, duration: 26.031s, episode steps: 1213, steps per second:  47, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.015118, mae: 2.806564, mean_q: 3.375841, mean_eps: 0.260914
 3696967/6000000: episode: 4856, duration: 21.141s, episode steps: 929, steps per second:  44, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.312 [0.000, 5.000],  loss: 0.014768, mae: 2.807785, mean_q: 3.376961, mean_eps: 0.260700
 3697758/6000000: episode: 4857, duration: 17.434s, episode steps: 791, steps per second:  45, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.105 [0.000, 5.000],  loss: 0.014994, mae: 2.784427, mean_q: 3.351615, mean_eps: 0.260528
 3698600/6000000: episode: 4858, duration: 18.295s, episode steps: 842, steps per second:  46, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.095 [0.000, 5.000],  loss: 0.014924, mae: 2.818495, mean_q: 3.389531, mean_eps: 0.260364
 3699763/6000000: episode: 4859, duration: 25.248s, episode steps: 1163, steps per second:  46, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.194 [0.000, 5.000],  loss: 0.016218, mae: 2.823620, mean_q: 3.396462, mean_eps: 0.260164
 3700662/6000000: episode: 4860, duration: 18.476s, episode steps: 899, steps per second:  49, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.013953, mae: 2.799021, mean_q: 3.367328, mean_eps: 0.259958
 3701763/6000000: episode: 4861, duration: 23.109s, episode steps: 1101, steps per second:  48, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.086 [0.000, 5.000],  loss: 0.015269, mae: 2.811318, mean_q: 3.380919, mean_eps: 0.259758
 3702753/6000000: episode: 4862, duration: 20.897s, episode steps: 990, steps per second:  47, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.236 [0.000, 5.000],  loss: 0.016708, mae: 2.823797, mean_q: 3.393892, mean_eps: 0.259548
 3703323/6000000: episode: 4863, duration: 12.092s, episode steps: 570, steps per second:  47, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.154 [0.000, 5.000],  loss: 0.014420, mae: 2.814869, mean_q: 3.385104, mean_eps: 0.259392
 3704237/6000000: episode: 4864, duration: 20.344s, episode steps: 914, steps per second:  45, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.015614, mae: 2.811575, mean_q: 3.381834, mean_eps: 0.259244
 3704983/6000000: episode: 4865, duration: 17.020s, episode steps: 746, steps per second:  44, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.015853, mae: 2.804433, mean_q: 3.372084, mean_eps: 0.259078
 3706055/6000000: episode: 4866, duration: 23.768s, episode steps: 1072, steps per second:  45, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.706 [0.000, 5.000],  loss: 0.013533, mae: 2.784739, mean_q: 3.350484, mean_eps: 0.258896
 3707168/6000000: episode: 4867, duration: 26.381s, episode steps: 1113, steps per second:  42, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.132 [0.000, 5.000],  loss: 0.014069, mae: 2.800977, mean_q: 3.369276, mean_eps: 0.258678
 3707908/6000000: episode: 4868, duration: 17.448s, episode steps: 740, steps per second:  42, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.045 [0.000, 5.000],  loss: 0.015992, mae: 2.794511, mean_q: 3.360687, mean_eps: 0.258493
 3709196/6000000: episode: 4869, duration: 28.033s, episode steps: 1288, steps per second:  46, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.081 [0.000, 5.000],  loss: 0.015980, mae: 2.783074, mean_q: 3.347963, mean_eps: 0.258290
 3710030/6000000: episode: 4870, duration: 18.841s, episode steps: 834, steps per second:  44, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.813 [0.000, 5.000],  loss: 0.015117, mae: 2.781845, mean_q: 3.347430, mean_eps: 0.258078
 3710970/6000000: episode: 4871, duration: 20.518s, episode steps: 940, steps per second:  46, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.739 [0.000, 5.000],  loss: 0.013754, mae: 2.768701, mean_q: 3.329849, mean_eps: 0.257900
 3711812/6000000: episode: 4872, duration: 17.562s, episode steps: 842, steps per second:  48, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.996 [0.000, 5.000],  loss: 0.014907, mae: 2.761557, mean_q: 3.321236, mean_eps: 0.257722
 3712490/6000000: episode: 4873, duration: 15.181s, episode steps: 678, steps per second:  45, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.022 [0.000, 5.000],  loss: 0.014986, mae: 2.795615, mean_q: 3.362385, mean_eps: 0.257570
 3713239/6000000: episode: 4874, duration: 16.617s, episode steps: 749, steps per second:  45, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.168 [0.000, 5.000],  loss: 0.014138, mae: 2.750904, mean_q: 3.308397, mean_eps: 0.257427
 3714191/6000000: episode: 4875, duration: 20.706s, episode steps: 952, steps per second:  46, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.064 [0.000, 5.000],  loss: 0.014844, mae: 2.775153, mean_q: 3.336925, mean_eps: 0.257257
 3714913/6000000: episode: 4876, duration: 15.835s, episode steps: 722, steps per second:  46, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.749 [0.000, 5.000],  loss: 0.014976, mae: 2.787664, mean_q: 3.350550, mean_eps: 0.257090
 3715700/6000000: episode: 4877, duration: 16.763s, episode steps: 787, steps per second:  47, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.013957, mae: 2.784237, mean_q: 3.347872, mean_eps: 0.256939
 3716491/6000000: episode: 4878, duration: 16.100s, episode steps: 791, steps per second:  49, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.015894, mae: 2.769235, mean_q: 3.330759, mean_eps: 0.256781
 3717665/6000000: episode: 4879, duration: 24.175s, episode steps: 1174, steps per second:  49, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.106 [0.000, 5.000],  loss: 0.014006, mae: 2.757639, mean_q: 3.317363, mean_eps: 0.256584
 3718905/6000000: episode: 4880, duration: 26.468s, episode steps: 1240, steps per second:  47, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.016590, mae: 2.768900, mean_q: 3.330617, mean_eps: 0.256343
 3719926/6000000: episode: 4881, duration: 21.319s, episode steps: 1021, steps per second:  48, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.202 [0.000, 5.000],  loss: 0.015462, mae: 2.772994, mean_q: 3.336380, mean_eps: 0.256117
 3720847/6000000: episode: 4882, duration: 19.603s, episode steps: 921, steps per second:  47, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.228 [0.000, 5.000],  loss: 0.015343, mae: 2.768921, mean_q: 3.328763, mean_eps: 0.255923
 3722118/6000000: episode: 4883, duration: 27.912s, episode steps: 1271, steps per second:  46, episode reward: 30.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.014367, mae: 2.759929, mean_q: 3.318421, mean_eps: 0.255704
 3722641/6000000: episode: 4884, duration: 12.541s, episode steps: 523, steps per second:  42, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.015270, mae: 2.768695, mean_q: 3.331712, mean_eps: 0.255524
 3723793/6000000: episode: 4885, duration: 26.509s, episode steps: 1152, steps per second:  43, episode reward: 32.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.111 [0.000, 5.000],  loss: 0.015071, mae: 2.769337, mean_q: 3.329265, mean_eps: 0.255356
 3724350/6000000: episode: 4886, duration: 13.132s, episode steps: 557, steps per second:  42, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.754 [0.000, 5.000],  loss: 0.015891, mae: 2.744552, mean_q: 3.301684, mean_eps: 0.255186
 3725190/6000000: episode: 4887, duration: 18.747s, episode steps: 840, steps per second:  45, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.245 [0.000, 5.000],  loss: 0.014698, mae: 2.763326, mean_q: 3.323240, mean_eps: 0.255046
 3726684/6000000: episode: 4888, duration: 32.589s, episode steps: 1494, steps per second:  46, episode reward: 30.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.015434, mae: 2.765292, mean_q: 3.324835, mean_eps: 0.254813
 3727446/6000000: episode: 4889, duration: 17.223s, episode steps: 762, steps per second:  44, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.139 [0.000, 5.000],  loss: 0.014974, mae: 2.769823, mean_q: 3.329239, mean_eps: 0.254587
 3729127/6000000: episode: 4890, duration: 36.402s, episode steps: 1681, steps per second:  46, episode reward: 34.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.640 [0.000, 5.000],  loss: 0.014790, mae: 2.754972, mean_q: 3.314229, mean_eps: 0.254343
 3730030/6000000: episode: 4891, duration: 19.913s, episode steps: 903, steps per second:  45, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.013918, mae: 2.759845, mean_q: 3.321065, mean_eps: 0.254084
 3731518/6000000: episode: 4892, duration: 32.302s, episode steps: 1488, steps per second:  46, episode reward: 30.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.806 [0.000, 5.000],  loss: 0.014217, mae: 2.753743, mean_q: 3.313380, mean_eps: 0.253845
 3732587/6000000: episode: 4893, duration: 23.479s, episode steps: 1069, steps per second:  46, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.912 [0.000, 5.000],  loss: 0.013908, mae: 2.749069, mean_q: 3.306761, mean_eps: 0.253590
 3733548/6000000: episode: 4894, duration: 19.443s, episode steps: 961, steps per second:  49, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.013872, mae: 2.750641, mean_q: 3.308603, mean_eps: 0.253387
 3734452/6000000: episode: 4895, duration: 18.897s, episode steps: 904, steps per second:  48, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.048 [0.000, 5.000],  loss: 0.013313, mae: 2.748585, mean_q: 3.306264, mean_eps: 0.253200
 3735406/6000000: episode: 4896, duration: 20.296s, episode steps: 954, steps per second:  47, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.014591, mae: 2.747559, mean_q: 3.303581, mean_eps: 0.253014
 3736060/6000000: episode: 4897, duration: 13.595s, episode steps: 654, steps per second:  48, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.142 [0.000, 5.000],  loss: 0.014209, mae: 2.748419, mean_q: 3.304644, mean_eps: 0.252854
 3737288/6000000: episode: 4898, duration: 25.942s, episode steps: 1228, steps per second:  47, episode reward: 25.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.595 [0.000, 5.000],  loss: 0.013917, mae: 2.736470, mean_q: 3.291447, mean_eps: 0.252666
 3738198/6000000: episode: 4899, duration: 19.951s, episode steps: 910, steps per second:  46, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.984 [0.000, 5.000],  loss: 0.016928, mae: 2.751186, mean_q: 3.309288, mean_eps: 0.252452
 3738713/6000000: episode: 4900, duration: 11.358s, episode steps: 515, steps per second:  45, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.017017, mae: 2.719030, mean_q: 3.271599, mean_eps: 0.252309
 3739750/6000000: episode: 4901, duration: 22.899s, episode steps: 1037, steps per second:  45, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.041 [0.000, 5.000],  loss: 0.014055, mae: 2.739747, mean_q: 3.296478, mean_eps: 0.252154
 3740122/6000000: episode: 4902, duration: 8.416s, episode steps: 372, steps per second:  44, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.707 [0.000, 5.000],  loss: 0.013968, mae: 2.754238, mean_q: 3.313896, mean_eps: 0.252013
 3741210/6000000: episode: 4903, duration: 24.822s, episode steps: 1088, steps per second:  44, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.015252, mae: 2.731042, mean_q: 3.287536, mean_eps: 0.251867
 3742177/6000000: episode: 4904, duration: 21.196s, episode steps: 967, steps per second:  46, episode reward: 28.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.715 [0.000, 5.000],  loss: 0.016913, mae: 2.750059, mean_q: 3.308372, mean_eps: 0.251661
 3743076/6000000: episode: 4905, duration: 20.658s, episode steps: 899, steps per second:  44, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.024 [0.000, 5.000],  loss: 0.014758, mae: 2.746076, mean_q: 3.304267, mean_eps: 0.251475
 3743444/6000000: episode: 4906, duration: 8.419s, episode steps: 368, steps per second:  44, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.016545, mae: 2.760693, mean_q: 3.317809, mean_eps: 0.251348
 3744159/6000000: episode: 4907, duration: 15.569s, episode steps: 715, steps per second:  46, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.015746, mae: 2.741172, mean_q: 3.296231, mean_eps: 0.251240
 3744801/6000000: episode: 4908, duration: 13.455s, episode steps: 642, steps per second:  48, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.014008, mae: 2.736666, mean_q: 3.295600, mean_eps: 0.251104
 3745835/6000000: episode: 4909, duration: 24.030s, episode steps: 1034, steps per second:  43, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.070 [0.000, 5.000],  loss: 0.015050, mae: 2.744856, mean_q: 3.300488, mean_eps: 0.250936
 3746744/6000000: episode: 4910, duration: 20.170s, episode steps: 909, steps per second:  45, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.072 [0.000, 5.000],  loss: 0.014846, mae: 2.746246, mean_q: 3.301439, mean_eps: 0.250742
 3747764/6000000: episode: 4911, duration: 22.582s, episode steps: 1020, steps per second:  45, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.236 [0.000, 5.000],  loss: 0.014461, mae: 2.753330, mean_q: 3.312102, mean_eps: 0.250550
 3748738/6000000: episode: 4912, duration: 21.091s, episode steps: 974, steps per second:  46, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.098 [0.000, 5.000],  loss: 0.016589, mae: 2.737543, mean_q: 3.291974, mean_eps: 0.250350
 3750121/6000000: episode: 4913, duration: 28.016s, episode steps: 1383, steps per second:  49, episode reward: 34.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.015477, mae: 2.762485, mean_q: 3.322347, mean_eps: 0.250114
 3750667/6000000: episode: 4914, duration: 11.111s, episode steps: 546, steps per second:  49, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.266 [0.000, 5.000],  loss: 0.015712, mae: 2.776071, mean_q: 3.339325, mean_eps: 0.249921
 3751550/6000000: episode: 4915, duration: 19.489s, episode steps: 883, steps per second:  45, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.142 [0.000, 5.000],  loss: 0.014995, mae: 2.765878, mean_q: 3.325486, mean_eps: 0.249778
 3752456/6000000: episode: 4916, duration: 19.088s, episode steps: 906, steps per second:  47, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.688 [0.000, 5.000],  loss: 0.015372, mae: 2.774042, mean_q: 3.338012, mean_eps: 0.249600
 3752947/6000000: episode: 4917, duration: 10.349s, episode steps: 491, steps per second:  47, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.015465, mae: 2.770384, mean_q: 3.334256, mean_eps: 0.249460
 3753948/6000000: episode: 4918, duration: 21.701s, episode steps: 1001, steps per second:  46, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.979 [0.000, 5.000],  loss: 0.014575, mae: 2.771222, mean_q: 3.333745, mean_eps: 0.249311
 3754830/6000000: episode: 4919, duration: 19.374s, episode steps: 882, steps per second:  46, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.177 [0.000, 5.000],  loss: 0.014241, mae: 2.776793, mean_q: 3.340341, mean_eps: 0.249122
 3755961/6000000: episode: 4920, duration: 24.810s, episode steps: 1131, steps per second:  46, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.149 [0.000, 5.000],  loss: 0.015940, mae: 2.779260, mean_q: 3.343392, mean_eps: 0.248921
 3756814/6000000: episode: 4921, duration: 21.117s, episode steps: 853, steps per second:  40, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.246 [0.000, 5.000],  loss: 0.017403, mae: 2.759703, mean_q: 3.317692, mean_eps: 0.248722
 3757882/6000000: episode: 4922, duration: 25.388s, episode steps: 1068, steps per second:  42, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.015059, mae: 2.775878, mean_q: 3.337693, mean_eps: 0.248530
 3758435/6000000: episode: 4923, duration: 13.529s, episode steps: 553, steps per second:  41, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.015736, mae: 2.765595, mean_q: 3.325769, mean_eps: 0.248368
 3759697/6000000: episode: 4924, duration: 28.558s, episode steps: 1262, steps per second:  44, episode reward: 34.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.947 [0.000, 5.000],  loss: 0.014085, mae: 2.771131, mean_q: 3.334551, mean_eps: 0.248187
 3760587/6000000: episode: 4925, duration: 19.423s, episode steps: 890, steps per second:  46, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.954 [0.000, 5.000],  loss: 0.016470, mae: 2.775663, mean_q: 3.339460, mean_eps: 0.247972
 3761181/6000000: episode: 4926, duration: 12.794s, episode steps: 594, steps per second:  46, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.015240, mae: 2.780323, mean_q: 3.344159, mean_eps: 0.247823
 3761728/6000000: episode: 4927, duration: 12.342s, episode steps: 547, steps per second:  44, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.015043, mae: 2.755740, mean_q: 3.312589, mean_eps: 0.247709
 3762501/6000000: episode: 4928, duration: 18.116s, episode steps: 773, steps per second:  43, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.169 [0.000, 5.000],  loss: 0.014729, mae: 2.761293, mean_q: 3.320653, mean_eps: 0.247577
 3763259/6000000: episode: 4929, duration: 16.500s, episode steps: 758, steps per second:  46, episode reward: 23.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 0.015434, mae: 2.757848, mean_q: 3.317881, mean_eps: 0.247424
 3763983/6000000: episode: 4930, duration: 16.121s, episode steps: 724, steps per second:  45, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.985 [0.000, 5.000],  loss: 0.015320, mae: 2.767400, mean_q: 3.328225, mean_eps: 0.247276
 3765130/6000000: episode: 4931, duration: 25.712s, episode steps: 1147, steps per second:  45, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.894 [0.000, 5.000],  loss: 0.015020, mae: 2.754769, mean_q: 3.314528, mean_eps: 0.247089
 3766022/6000000: episode: 4932, duration: 18.781s, episode steps: 892, steps per second:  47, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.941 [0.000, 5.000],  loss: 0.015094, mae: 2.762042, mean_q: 3.324751, mean_eps: 0.246885
 3767093/6000000: episode: 4933, duration: 23.179s, episode steps: 1071, steps per second:  46, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.017644, mae: 2.764838, mean_q: 3.324535, mean_eps: 0.246688
 3768044/6000000: episode: 4934, duration: 21.090s, episode steps: 951, steps per second:  45, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.014546, mae: 2.742999, mean_q: 3.299274, mean_eps: 0.246486
 3769298/6000000: episode: 4935, duration: 26.766s, episode steps: 1254, steps per second:  47, episode reward: 34.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.152 [0.000, 5.000],  loss: 0.016360, mae: 2.735638, mean_q: 3.289794, mean_eps: 0.246266
 3769794/6000000: episode: 4936, duration: 10.481s, episode steps: 496, steps per second:  47, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.002 [0.000, 5.000],  loss: 0.015530, mae: 2.767530, mean_q: 3.329926, mean_eps: 0.246091
 3770741/6000000: episode: 4937, duration: 20.007s, episode steps: 947, steps per second:  47, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.074 [0.000, 5.000],  loss: 0.014583, mae: 2.730968, mean_q: 3.284989, mean_eps: 0.245946
 3771677/6000000: episode: 4938, duration: 20.990s, episode steps: 936, steps per second:  45, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.838 [0.000, 5.000],  loss: 0.016286, mae: 2.759208, mean_q: 3.320124, mean_eps: 0.245758
 3772616/6000000: episode: 4939, duration: 22.285s, episode steps: 939, steps per second:  42, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.131 [0.000, 5.000],  loss: 0.015943, mae: 2.733399, mean_q: 3.290796, mean_eps: 0.245571
 3773423/6000000: episode: 4940, duration: 19.015s, episode steps: 807, steps per second:  42, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.879 [0.000, 5.000],  loss: 0.016452, mae: 2.727085, mean_q: 3.281254, mean_eps: 0.245396
 3774823/6000000: episode: 4941, duration: 30.553s, episode steps: 1400, steps per second:  46, episode reward: 35.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.944 [0.000, 5.000],  loss: 0.016024, mae: 2.728263, mean_q: 3.283964, mean_eps: 0.245176
 3775746/6000000: episode: 4942, duration: 19.976s, episode steps: 923, steps per second:  46, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.016347, mae: 2.742202, mean_q: 3.298172, mean_eps: 0.244943
 3776567/6000000: episode: 4943, duration: 17.817s, episode steps: 821, steps per second:  46, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.812 [0.000, 5.000],  loss: 0.015848, mae: 2.734204, mean_q: 3.288190, mean_eps: 0.244769
 3777787/6000000: episode: 4944, duration: 26.019s, episode steps: 1220, steps per second:  47, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.898 [0.000, 5.000],  loss: 0.014969, mae: 2.713981, mean_q: 3.264368, mean_eps: 0.244565
 3778665/6000000: episode: 4945, duration: 21.025s, episode steps: 878, steps per second:  42, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.206 [0.000, 5.000],  loss: 0.013792, mae: 2.715750, mean_q: 3.267523, mean_eps: 0.244355
 3779905/6000000: episode: 4946, duration: 28.066s, episode steps: 1240, steps per second:  44, episode reward: 20.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.513 [0.000, 5.000],  loss: 0.014981, mae: 2.724901, mean_q: 3.279148, mean_eps: 0.244143
 3780633/6000000: episode: 4947, duration: 15.701s, episode steps: 728, steps per second:  46, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.136 [0.000, 5.000],  loss: 0.016537, mae: 2.731735, mean_q: 3.284581, mean_eps: 0.243946
 3781448/6000000: episode: 4948, duration: 18.124s, episode steps: 815, steps per second:  45, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.605 [0.000, 5.000],  loss: 0.015280, mae: 2.727203, mean_q: 3.279668, mean_eps: 0.243792
 3782522/6000000: episode: 4949, duration: 22.100s, episode steps: 1074, steps per second:  49, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.602 [0.000, 5.000],  loss: 0.014129, mae: 2.731825, mean_q: 3.287869, mean_eps: 0.243603
 3783458/6000000: episode: 4950, duration: 19.675s, episode steps: 936, steps per second:  48, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.017320, mae: 2.748460, mean_q: 3.302450, mean_eps: 0.243402
 3784425/6000000: episode: 4951, duration: 21.453s, episode steps: 967, steps per second:  45, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.015144, mae: 2.724026, mean_q: 3.275666, mean_eps: 0.243212
 3785052/6000000: episode: 4952, duration: 13.074s, episode steps: 627, steps per second:  48, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.015346, mae: 2.726033, mean_q: 3.277923, mean_eps: 0.243052
 3785908/6000000: episode: 4953, duration: 17.568s, episode steps: 856, steps per second:  49, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.044 [0.000, 5.000],  loss: 0.016442, mae: 2.740953, mean_q: 3.295666, mean_eps: 0.242904
 3786831/6000000: episode: 4954, duration: 19.205s, episode steps: 923, steps per second:  48, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.095 [0.000, 5.000],  loss: 0.014044, mae: 2.717250, mean_q: 3.268703, mean_eps: 0.242726
 3787972/6000000: episode: 4955, duration: 25.105s, episode steps: 1141, steps per second:  45, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.015504, mae: 2.741527, mean_q: 3.296281, mean_eps: 0.242520
 3788786/6000000: episode: 4956, duration: 18.531s, episode steps: 814, steps per second:  44, episode reward: 24.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.920 [0.000, 5.000],  loss: 0.015182, mae: 2.741560, mean_q: 3.297913, mean_eps: 0.242324
 3789449/6000000: episode: 4957, duration: 16.554s, episode steps: 663, steps per second:  40, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.237 [0.000, 5.000],  loss: 0.014828, mae: 2.737525, mean_q: 3.292471, mean_eps: 0.242176
 3790226/6000000: episode: 4958, duration: 17.947s, episode steps: 777, steps per second:  43, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.015932, mae: 2.728198, mean_q: 3.279953, mean_eps: 0.242032
 3791077/6000000: episode: 4959, duration: 19.223s, episode steps: 851, steps per second:  44, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.902 [0.000, 5.000],  loss: 0.016320, mae: 2.725277, mean_q: 3.279843, mean_eps: 0.241870
 3792390/6000000: episode: 4960, duration: 28.370s, episode steps: 1313, steps per second:  46, episode reward: 33.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.047 [0.000, 5.000],  loss: 0.015214, mae: 2.718429, mean_q: 3.269863, mean_eps: 0.241653
 3793062/6000000: episode: 4961, duration: 13.908s, episode steps: 672, steps per second:  48, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.990 [0.000, 5.000],  loss: 0.013769, mae: 2.742406, mean_q: 3.298750, mean_eps: 0.241455
 3793713/6000000: episode: 4962, duration: 13.542s, episode steps: 651, steps per second:  48, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.278 [0.000, 5.000],  loss: 0.015620, mae: 2.723312, mean_q: 3.277658, mean_eps: 0.241322
 3794389/6000000: episode: 4963, duration: 15.215s, episode steps: 676, steps per second:  44, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.720 [0.000, 5.000],  loss: 0.014257, mae: 2.730985, mean_q: 3.286699, mean_eps: 0.241190
 3795027/6000000: episode: 4964, duration: 14.503s, episode steps: 638, steps per second:  44, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.082 [0.000, 5.000],  loss: 0.015554, mae: 2.733650, mean_q: 3.288875, mean_eps: 0.241058
 3795991/6000000: episode: 4965, duration: 21.465s, episode steps: 964, steps per second:  45, episode reward: 31.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.170 [0.000, 5.000],  loss: 0.014921, mae: 2.704773, mean_q: 3.252742, mean_eps: 0.240898
 3797031/6000000: episode: 4966, duration: 23.527s, episode steps: 1040, steps per second:  44, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.176 [0.000, 5.000],  loss: 0.014063, mae: 2.711333, mean_q: 3.263181, mean_eps: 0.240698
 3797754/6000000: episode: 4967, duration: 16.386s, episode steps: 723, steps per second:  44, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.911 [0.000, 5.000],  loss: 0.015143, mae: 2.714891, mean_q: 3.265774, mean_eps: 0.240522
 3798947/6000000: episode: 4968, duration: 25.091s, episode steps: 1193, steps per second:  48, episode reward: 32.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.141 [0.000, 5.000],  loss: 0.014456, mae: 2.712217, mean_q: 3.262767, mean_eps: 0.240330
 3799639/6000000: episode: 4969, duration: 14.568s, episode steps: 692, steps per second:  48, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.647 [0.000, 5.000],  loss: 0.014673, mae: 2.694261, mean_q: 3.239919, mean_eps: 0.240142
 3800592/6000000: episode: 4970, duration: 20.204s, episode steps: 953, steps per second:  47, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.183 [0.000, 5.000],  loss: 0.016395, mae: 2.722164, mean_q: 3.273423, mean_eps: 0.239977
 3801524/6000000: episode: 4971, duration: 19.681s, episode steps: 932, steps per second:  47, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.015150, mae: 2.739887, mean_q: 3.294423, mean_eps: 0.239789
 3802022/6000000: episode: 4972, duration: 10.705s, episode steps: 498, steps per second:  47, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.036 [0.000, 5.000],  loss: 0.014013, mae: 2.738003, mean_q: 3.292643, mean_eps: 0.239646
 3802908/6000000: episode: 4973, duration: 19.089s, episode steps: 886, steps per second:  46, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.015627, mae: 2.737990, mean_q: 3.294072, mean_eps: 0.239507
 3804012/6000000: episode: 4974, duration: 25.480s, episode steps: 1104, steps per second:  43, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.043 [0.000, 5.000],  loss: 0.015390, mae: 2.743834, mean_q: 3.300462, mean_eps: 0.239308
 3804667/6000000: episode: 4975, duration: 16.163s, episode steps: 655, steps per second:  41, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.995 [0.000, 5.000],  loss: 0.014110, mae: 2.719216, mean_q: 3.271293, mean_eps: 0.239132
 3805736/6000000: episode: 4976, duration: 25.702s, episode steps: 1069, steps per second:  42, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.090 [0.000, 5.000],  loss: 0.013995, mae: 2.742741, mean_q: 3.298586, mean_eps: 0.238960
 3806731/6000000: episode: 4977, duration: 22.247s, episode steps: 995, steps per second:  45, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.015405, mae: 2.720351, mean_q: 3.272816, mean_eps: 0.238754
 3807988/6000000: episode: 4978, duration: 27.936s, episode steps: 1257, steps per second:  45, episode reward: 32.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.015538, mae: 2.709106, mean_q: 3.258414, mean_eps: 0.238528
 3808707/6000000: episode: 4979, duration: 16.565s, episode steps: 719, steps per second:  43, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.115 [0.000, 5.000],  loss: 0.013763, mae: 2.711228, mean_q: 3.262794, mean_eps: 0.238331
 3809464/6000000: episode: 4980, duration: 16.246s, episode steps: 757, steps per second:  47, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.719 [0.000, 5.000],  loss: 0.014485, mae: 2.723776, mean_q: 3.276623, mean_eps: 0.238183
 3810471/6000000: episode: 4981, duration: 21.932s, episode steps: 1007, steps per second:  46, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.810 [0.000, 5.000],  loss: 0.017096, mae: 2.719175, mean_q: 3.270432, mean_eps: 0.238007
 3811281/6000000: episode: 4982, duration: 18.528s, episode steps: 810, steps per second:  44, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.751 [0.000, 5.000],  loss: 0.014276, mae: 2.699558, mean_q: 3.247683, mean_eps: 0.237825
 3811765/6000000: episode: 4983, duration: 11.099s, episode steps: 484, steps per second:  44, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.095 [0.000, 5.000],  loss: 0.014042, mae: 2.701708, mean_q: 3.250969, mean_eps: 0.237695
 3812996/6000000: episode: 4984, duration: 27.443s, episode steps: 1231, steps per second:  45, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.068 [0.000, 5.000],  loss: 0.013490, mae: 2.708733, mean_q: 3.258713, mean_eps: 0.237524
 3813657/6000000: episode: 4985, duration: 14.960s, episode steps: 661, steps per second:  44, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.017 [0.000, 5.000],  loss: 0.014141, mae: 2.710496, mean_q: 3.260546, mean_eps: 0.237335
 3815077/6000000: episode: 4986, duration: 29.891s, episode steps: 1420, steps per second:  48, episode reward: 33.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.014138, mae: 2.726287, mean_q: 3.279677, mean_eps: 0.237126
 3815583/6000000: episode: 4987, duration: 10.543s, episode steps: 506, steps per second:  48, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.528 [0.000, 5.000],  loss: 0.015722, mae: 2.695701, mean_q: 3.243777, mean_eps: 0.236934
 3816489/6000000: episode: 4988, duration: 19.413s, episode steps: 906, steps per second:  47, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.734 [0.000, 5.000],  loss: 0.016783, mae: 2.713959, mean_q: 3.267091, mean_eps: 0.236793
 3817563/6000000: episode: 4989, duration: 23.155s, episode steps: 1074, steps per second:  46, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.102 [0.000, 5.000],  loss: 0.013955, mae: 2.677429, mean_q: 3.221571, mean_eps: 0.236595
 3818718/6000000: episode: 4990, duration: 24.310s, episode steps: 1155, steps per second:  48, episode reward: 32.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.286 [0.000, 5.000],  loss: 0.016054, mae: 2.706425, mean_q: 3.255653, mean_eps: 0.236372
 3819817/6000000: episode: 4991, duration: 23.623s, episode steps: 1099, steps per second:  47, episode reward: 30.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.062 [0.000, 5.000],  loss: 0.015162, mae: 2.698468, mean_q: 3.245362, mean_eps: 0.236146
 3821273/6000000: episode: 4992, duration: 32.543s, episode steps: 1456, steps per second:  45, episode reward: 32.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.141 [0.000, 5.000],  loss: 0.015260, mae: 2.707404, mean_q: 3.256185, mean_eps: 0.235891
 3822032/6000000: episode: 4993, duration: 19.610s, episode steps: 759, steps per second:  39, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.752 [0.000, 5.000],  loss: 0.014077, mae: 2.692310, mean_q: 3.237414, mean_eps: 0.235670
 3822430/6000000: episode: 4994, duration: 9.949s, episode steps: 398, steps per second:  40, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.050 [0.000, 5.000],  loss: 0.014125, mae: 2.708189, mean_q: 3.259324, mean_eps: 0.235554
 3823229/6000000: episode: 4995, duration: 17.171s, episode steps: 799, steps per second:  47, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.478 [0.000, 5.000],  loss: 0.014839, mae: 2.691883, mean_q: 3.238567, mean_eps: 0.235434
 3824001/6000000: episode: 4996, duration: 17.154s, episode steps: 772, steps per second:  45, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.015008, mae: 2.690105, mean_q: 3.236843, mean_eps: 0.235277
 3824942/6000000: episode: 4997, duration: 20.557s, episode steps: 941, steps per second:  46, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.990 [0.000, 5.000],  loss: 0.013506, mae: 2.687629, mean_q: 3.233389, mean_eps: 0.235106
 3825602/6000000: episode: 4998, duration: 14.119s, episode steps: 660, steps per second:  47, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.005 [0.000, 5.000],  loss: 0.015471, mae: 2.745709, mean_q: 3.302548, mean_eps: 0.234946
 3826575/6000000: episode: 4999, duration: 20.568s, episode steps: 973, steps per second:  47, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.969 [0.000, 5.000],  loss: 0.014637, mae: 2.747323, mean_q: 3.304688, mean_eps: 0.234782
 3827557/6000000: episode: 5000, duration: 22.087s, episode steps: 982, steps per second:  44, episode reward: 31.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 1.973 [0.000, 5.000],  loss: 0.014811, mae: 2.723635, mean_q: 3.275893, mean_eps: 0.234587
 3828538/6000000: episode: 5001, duration: 21.760s, episode steps: 981, steps per second:  45, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.969 [0.000, 5.000],  loss: 0.014301, mae: 2.741501, mean_q: 3.297713, mean_eps: 0.234390
 3829543/6000000: episode: 5002, duration: 21.965s, episode steps: 1005, steps per second:  46, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.015570, mae: 2.734854, mean_q: 3.288571, mean_eps: 0.234192
 3830648/6000000: episode: 5003, duration: 24.739s, episode steps: 1105, steps per second:  45, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.685 [0.000, 5.000],  loss: 0.015315, mae: 2.736804, mean_q: 3.292057, mean_eps: 0.233981
 3831386/6000000: episode: 5004, duration: 15.200s, episode steps: 738, steps per second:  49, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.096 [0.000, 5.000],  loss: 0.014465, mae: 2.733594, mean_q: 3.287179, mean_eps: 0.233797
 3832183/6000000: episode: 5005, duration: 16.344s, episode steps: 797, steps per second:  49, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.016816, mae: 2.734762, mean_q: 3.288504, mean_eps: 0.233643
 3833005/6000000: episode: 5006, duration: 17.677s, episode steps: 822, steps per second:  46, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.730 [0.000, 5.000],  loss: 0.014707, mae: 2.726839, mean_q: 3.279875, mean_eps: 0.233481
 3833739/6000000: episode: 5007, duration: 15.714s, episode steps: 734, steps per second:  47, episode reward: 22.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.883 [0.000, 5.000],  loss: 0.015870, mae: 2.728451, mean_q: 3.282095, mean_eps: 0.233326
 3834785/6000000: episode: 5008, duration: 22.196s, episode steps: 1046, steps per second:  47, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.015075, mae: 2.715209, mean_q: 3.265823, mean_eps: 0.233148
 3835435/6000000: episode: 5009, duration: 14.346s, episode steps: 650, steps per second:  45, episode reward: 18.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.655 [0.000, 5.000],  loss: 0.014400, mae: 2.723054, mean_q: 3.273585, mean_eps: 0.232978
 3836200/6000000: episode: 5010, duration: 17.799s, episode steps: 765, steps per second:  43, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.885 [0.000, 5.000],  loss: 0.016619, mae: 2.732563, mean_q: 3.285499, mean_eps: 0.232837
 3837077/6000000: episode: 5011, duration: 22.276s, episode steps: 877, steps per second:  39, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.897 [0.000, 5.000],  loss: 0.015549, mae: 2.707798, mean_q: 3.256163, mean_eps: 0.232672
 3837869/6000000: episode: 5012, duration: 19.141s, episode steps: 792, steps per second:  41, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.014005, mae: 2.707319, mean_q: 3.255078, mean_eps: 0.232505
 3839015/6000000: episode: 5013, duration: 26.512s, episode steps: 1146, steps per second:  43, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.015171, mae: 2.716033, mean_q: 3.265956, mean_eps: 0.232312
 3840273/6000000: episode: 5014, duration: 28.283s, episode steps: 1258, steps per second:  44, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.153 [0.000, 5.000],  loss: 0.014344, mae: 2.728516, mean_q: 3.282004, mean_eps: 0.232071
 3841193/6000000: episode: 5015, duration: 21.298s, episode steps: 920, steps per second:  43, episode reward: 28.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.014203, mae: 2.710070, mean_q: 3.260307, mean_eps: 0.231853
 3841846/6000000: episode: 5016, duration: 14.288s, episode steps: 653, steps per second:  46, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.884 [0.000, 5.000],  loss: 0.015394, mae: 2.709078, mean_q: 3.256208, mean_eps: 0.231696
 3842783/6000000: episode: 5017, duration: 20.013s, episode steps: 937, steps per second:  47, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.015526, mae: 2.712338, mean_q: 3.261476, mean_eps: 0.231537
 3843716/6000000: episode: 5018, duration: 23.015s, episode steps: 933, steps per second:  41, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.015912, mae: 2.731582, mean_q: 3.284398, mean_eps: 0.231350
 3844377/6000000: episode: 5019, duration: 15.089s, episode steps: 661, steps per second:  44, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.224 [0.000, 5.000],  loss: 0.014326, mae: 2.722205, mean_q: 3.273197, mean_eps: 0.231191
 3845485/6000000: episode: 5020, duration: 25.026s, episode steps: 1108, steps per second:  44, episode reward: 31.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.963 [0.000, 5.000],  loss: 0.015884, mae: 2.719889, mean_q: 3.269005, mean_eps: 0.231014
 3846809/6000000: episode: 5021, duration: 28.516s, episode steps: 1324, steps per second:  46, episode reward: 32.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.179 [0.000, 5.000],  loss: 0.015266, mae: 2.707869, mean_q: 3.255421, mean_eps: 0.230770
 3847527/6000000: episode: 5022, duration: 14.593s, episode steps: 718, steps per second:  49, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.015287, mae: 2.704692, mean_q: 3.252584, mean_eps: 0.230566
 3848153/6000000: episode: 5023, duration: 13.294s, episode steps: 626, steps per second:  47, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.014930, mae: 2.704243, mean_q: 3.252733, mean_eps: 0.230432
 3849427/6000000: episode: 5024, duration: 28.209s, episode steps: 1274, steps per second:  45, episode reward: 27.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.976 [0.000, 5.000],  loss: 0.015346, mae: 2.693662, mean_q: 3.239937, mean_eps: 0.230242
 3850267/6000000: episode: 5025, duration: 17.616s, episode steps: 840, steps per second:  48, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.244 [0.000, 5.000],  loss: 0.014149, mae: 2.679631, mean_q: 3.222360, mean_eps: 0.230031
 3850817/6000000: episode: 5026, duration: 11.608s, episode steps: 550, steps per second:  47, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.014846, mae: 2.699694, mean_q: 3.247846, mean_eps: 0.229892
 3851469/6000000: episode: 5027, duration: 14.603s, episode steps: 652, steps per second:  45, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.771 [0.000, 5.000],  loss: 0.014206, mae: 2.688580, mean_q: 3.234251, mean_eps: 0.229771
 3852098/6000000: episode: 5028, duration: 13.694s, episode steps: 629, steps per second:  46, episode reward: 17.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.924 [0.000, 5.000],  loss: 0.013100, mae: 2.718031, mean_q: 3.267826, mean_eps: 0.229643
 3852963/6000000: episode: 5029, duration: 20.046s, episode steps: 865, steps per second:  43, episode reward: 27.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.016616, mae: 2.698344, mean_q: 3.243112, mean_eps: 0.229494
 3854069/6000000: episode: 5030, duration: 26.151s, episode steps: 1106, steps per second:  42, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.014420, mae: 2.700200, mean_q: 3.247565, mean_eps: 0.229297
 3854861/6000000: episode: 5031, duration: 18.863s, episode steps: 792, steps per second:  42, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.047 [0.000, 5.000],  loss: 0.014419, mae: 2.700412, mean_q: 3.248435, mean_eps: 0.229107
 3855311/6000000: episode: 5032, duration: 9.926s, episode steps: 450, steps per second:  45, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.014779, mae: 2.705774, mean_q: 3.254076, mean_eps: 0.228983
 3855940/6000000: episode: 5033, duration: 13.994s, episode steps: 629, steps per second:  45, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.013060, mae: 2.684078, mean_q: 3.227681, mean_eps: 0.228875
 3856720/6000000: episode: 5034, duration: 17.437s, episode steps: 780, steps per second:  45, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.088 [0.000, 5.000],  loss: 0.014362, mae: 2.678675, mean_q: 3.221329, mean_eps: 0.228734
 3858111/6000000: episode: 5035, duration: 30.515s, episode steps: 1391, steps per second:  46, episode reward: 32.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.669 [0.000, 5.000],  loss: 0.015628, mae: 2.701142, mean_q: 3.249257, mean_eps: 0.228517
 3858742/6000000: episode: 5036, duration: 13.818s, episode steps: 631, steps per second:  46, episode reward: 17.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.990 [0.000, 5.000],  loss: 0.013230, mae: 2.663873, mean_q: 3.205232, mean_eps: 0.228315
 3859408/6000000: episode: 5037, duration: 14.751s, episode steps: 666, steps per second:  45, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.066 [0.000, 5.000],  loss: 0.014328, mae: 2.683830, mean_q: 3.229868, mean_eps: 0.228185
 3860155/6000000: episode: 5038, duration: 17.265s, episode steps: 747, steps per second:  43, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.731 [0.000, 5.000],  loss: 0.013375, mae: 2.674103, mean_q: 3.217443, mean_eps: 0.228044
 3861518/6000000: episode: 5039, duration: 29.775s, episode steps: 1363, steps per second:  46, episode reward: 32.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.073 [0.000, 5.000],  loss: 0.014485, mae: 2.677103, mean_q: 3.220058, mean_eps: 0.227833
 3863046/6000000: episode: 5040, duration: 33.421s, episode steps: 1528, steps per second:  46, episode reward: 28.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.014921, mae: 2.677362, mean_q: 3.218736, mean_eps: 0.227544
 3864215/6000000: episode: 5041, duration: 24.046s, episode steps: 1169, steps per second:  49, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.012964, mae: 2.672072, mean_q: 3.214063, mean_eps: 0.227274
 3865048/6000000: episode: 5042, duration: 17.965s, episode steps: 833, steps per second:  46, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.146 [0.000, 5.000],  loss: 0.014415, mae: 2.679661, mean_q: 3.221570, mean_eps: 0.227074
 3865907/6000000: episode: 5043, duration: 18.346s, episode steps: 859, steps per second:  47, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.182 [0.000, 5.000],  loss: 0.015283, mae: 2.657299, mean_q: 3.195757, mean_eps: 0.226905
 3866851/6000000: episode: 5044, duration: 19.840s, episode steps: 944, steps per second:  48, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.008 [0.000, 5.000],  loss: 0.015544, mae: 2.637152, mean_q: 3.171064, mean_eps: 0.226724
 3867740/6000000: episode: 5045, duration: 19.275s, episode steps: 889, steps per second:  46, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.013200, mae: 2.659107, mean_q: 3.197733, mean_eps: 0.226541
 3869122/6000000: episode: 5046, duration: 30.844s, episode steps: 1382, steps per second:  45, episode reward: 31.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.323 [0.000, 5.000],  loss: 0.015770, mae: 2.663831, mean_q: 3.203437, mean_eps: 0.226314
 3870408/6000000: episode: 5047, duration: 29.507s, episode steps: 1286, steps per second:  44, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.014950, mae: 2.654335, mean_q: 3.191623, mean_eps: 0.226047
 3871655/6000000: episode: 5048, duration: 28.255s, episode steps: 1247, steps per second:  44, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.868 [0.000, 5.000],  loss: 0.014773, mae: 2.656936, mean_q: 3.195502, mean_eps: 0.225794
 3873358/6000000: episode: 5049, duration: 41.160s, episode steps: 1703, steps per second:  41, episode reward: 33.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.967 [0.000, 5.000],  loss: 0.013907, mae: 2.674111, mean_q: 3.216636, mean_eps: 0.225499
 3874016/6000000: episode: 5050, duration: 15.504s, episode steps: 658, steps per second:  42, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.935 [0.000, 5.000],  loss: 0.016941, mae: 2.647691, mean_q: 3.185086, mean_eps: 0.225263
 3874779/6000000: episode: 5051, duration: 17.072s, episode steps: 763, steps per second:  45, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.014852, mae: 2.667791, mean_q: 3.209544, mean_eps: 0.225121
 3875448/6000000: episode: 5052, duration: 15.909s, episode steps: 669, steps per second:  42, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.014393, mae: 2.622023, mean_q: 3.153144, mean_eps: 0.224978
 3876389/6000000: episode: 5053, duration: 23.088s, episode steps: 941, steps per second:  41, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.015425, mae: 2.624309, mean_q: 3.154988, mean_eps: 0.224816
 3877055/6000000: episode: 5054, duration: 14.740s, episode steps: 666, steps per second:  45, episode reward: 20.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.910 [0.000, 5.000],  loss: 0.016483, mae: 2.629220, mean_q: 3.161493, mean_eps: 0.224656
 3877457/6000000: episode: 5055, duration: 9.027s, episode steps: 402, steps per second:  45, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.353 [0.000, 5.000],  loss: 0.015787, mae: 2.625919, mean_q: 3.156599, mean_eps: 0.224549
 3878359/6000000: episode: 5056, duration: 19.608s, episode steps: 902, steps per second:  46, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.013561, mae: 2.627628, mean_q: 3.160302, mean_eps: 0.224418
 3879359/6000000: episode: 5057, duration: 21.309s, episode steps: 1000, steps per second:  47, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.979 [0.000, 5.000],  loss: 0.014176, mae: 2.619455, mean_q: 3.149671, mean_eps: 0.224228
 3879853/6000000: episode: 5058, duration: 10.163s, episode steps: 494, steps per second:  49, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.281 [0.000, 5.000],  loss: 0.013532, mae: 2.625399, mean_q: 3.158976, mean_eps: 0.224079
 3880740/6000000: episode: 5059, duration: 18.190s, episode steps: 887, steps per second:  49, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.570 [0.000, 5.000],  loss: 0.015004, mae: 2.590383, mean_q: 3.117683, mean_eps: 0.223941
 3881518/6000000: episode: 5060, duration: 16.452s, episode steps: 778, steps per second:  47, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.883 [0.000, 5.000],  loss: 0.016305, mae: 2.615778, mean_q: 3.144485, mean_eps: 0.223774
 3882401/6000000: episode: 5061, duration: 18.394s, episode steps: 883, steps per second:  48, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.112 [0.000, 5.000],  loss: 0.014145, mae: 2.611798, mean_q: 3.142672, mean_eps: 0.223608
 3883384/6000000: episode: 5062, duration: 20.257s, episode steps: 983, steps per second:  49, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.535 [0.000, 5.000],  loss: 0.015434, mae: 2.600820, mean_q: 3.127487, mean_eps: 0.223422
 3884132/6000000: episode: 5063, duration: 15.748s, episode steps: 748, steps per second:  47, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.607 [0.000, 5.000],  loss: 0.013893, mae: 2.607243, mean_q: 3.138450, mean_eps: 0.223249
 3885036/6000000: episode: 5064, duration: 19.656s, episode steps: 904, steps per second:  46, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 0.015935, mae: 2.614516, mean_q: 3.143704, mean_eps: 0.223084
 3885776/6000000: episode: 5065, duration: 16.246s, episode steps: 740, steps per second:  46, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.050 [0.000, 5.000],  loss: 0.012455, mae: 2.600490, mean_q: 3.128432, mean_eps: 0.222919
 3887011/6000000: episode: 5066, duration: 28.559s, episode steps: 1235, steps per second:  43, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.140 [0.000, 5.000],  loss: 0.015713, mae: 2.604543, mean_q: 3.131495, mean_eps: 0.222722
 3888045/6000000: episode: 5067, duration: 24.027s, episode steps: 1034, steps per second:  43, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.013685, mae: 2.600119, mean_q: 3.126713, mean_eps: 0.222494
 3889198/6000000: episode: 5068, duration: 25.435s, episode steps: 1153, steps per second:  45, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.815 [0.000, 5.000],  loss: 0.014734, mae: 2.596638, mean_q: 3.122873, mean_eps: 0.222276
 3890562/6000000: episode: 5069, duration: 30.104s, episode steps: 1364, steps per second:  45, episode reward: 32.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.713 [0.000, 5.000],  loss: 0.014140, mae: 2.617967, mean_q: 3.148857, mean_eps: 0.222024
 3891350/6000000: episode: 5070, duration: 16.666s, episode steps: 788, steps per second:  47, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.119 [0.000, 5.000],  loss: 0.016111, mae: 2.608415, mean_q: 3.135724, mean_eps: 0.221809
 3892147/6000000: episode: 5071, duration: 17.726s, episode steps: 797, steps per second:  45, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.908 [0.000, 5.000],  loss: 0.014216, mae: 2.617687, mean_q: 3.147856, mean_eps: 0.221650
 3893092/6000000: episode: 5072, duration: 20.550s, episode steps: 945, steps per second:  46, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.914 [0.000, 5.000],  loss: 0.015775, mae: 2.645461, mean_q: 3.182776, mean_eps: 0.221476
 3894018/6000000: episode: 5073, duration: 20.671s, episode steps: 926, steps per second:  45, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.018 [0.000, 5.000],  loss: 0.013964, mae: 2.616023, mean_q: 3.146021, mean_eps: 0.221289
 3894757/6000000: episode: 5074, duration: 16.217s, episode steps: 739, steps per second:  46, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.862 [0.000, 5.000],  loss: 0.014398, mae: 2.623309, mean_q: 3.155206, mean_eps: 0.221122
 3895648/6000000: episode: 5075, duration: 18.723s, episode steps: 891, steps per second:  48, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.547 [0.000, 5.000],  loss: 0.015050, mae: 2.617565, mean_q: 3.147731, mean_eps: 0.220960
 3896653/6000000: episode: 5076, duration: 21.197s, episode steps: 1005, steps per second:  47, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.787 [0.000, 5.000],  loss: 0.015555, mae: 2.609240, mean_q: 3.138936, mean_eps: 0.220770
 3897515/6000000: episode: 5077, duration: 17.830s, episode steps: 862, steps per second:  48, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.821 [0.000, 5.000],  loss: 0.016014, mae: 2.612876, mean_q: 3.142636, mean_eps: 0.220583
 3898714/6000000: episode: 5078, duration: 25.160s, episode steps: 1199, steps per second:  48, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.698 [0.000, 5.000],  loss: 0.015663, mae: 2.613818, mean_q: 3.143683, mean_eps: 0.220377
 3899566/6000000: episode: 5079, duration: 17.610s, episode steps: 852, steps per second:  48, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.095 [0.000, 5.000],  loss: 0.016110, mae: 2.589829, mean_q: 3.116032, mean_eps: 0.220172
 3900561/6000000: episode: 5080, duration: 21.678s, episode steps: 995, steps per second:  46, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.014965, mae: 2.616224, mean_q: 3.146854, mean_eps: 0.219987
 3901798/6000000: episode: 5081, duration: 27.463s, episode steps: 1237, steps per second:  45, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.897 [0.000, 5.000],  loss: 0.014325, mae: 2.619887, mean_q: 3.150869, mean_eps: 0.219764
 3902878/6000000: episode: 5082, duration: 26.092s, episode steps: 1080, steps per second:  41, episode reward: 31.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.013836, mae: 2.604509, mean_q: 3.132568, mean_eps: 0.219532
 3904020/6000000: episode: 5083, duration: 26.317s, episode steps: 1142, steps per second:  43, episode reward: 38.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.140 [0.000, 5.000],  loss: 0.014055, mae: 2.606583, mean_q: 3.134229, mean_eps: 0.219310
 3905015/6000000: episode: 5084, duration: 22.571s, episode steps: 995, steps per second:  44, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.065 [0.000, 5.000],  loss: 0.015474, mae: 2.625839, mean_q: 3.157495, mean_eps: 0.219097
 3906078/6000000: episode: 5085, duration: 23.406s, episode steps: 1063, steps per second:  45, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.261 [0.000, 5.000],  loss: 0.014551, mae: 2.609499, mean_q: 3.138214, mean_eps: 0.218891
 3906823/6000000: episode: 5086, duration: 15.808s, episode steps: 745, steps per second:  47, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.014181, mae: 2.614428, mean_q: 3.144154, mean_eps: 0.218710
 3907343/6000000: episode: 5087, duration: 11.298s, episode steps: 520, steps per second:  46, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.169 [0.000, 5.000],  loss: 0.014822, mae: 2.604944, mean_q: 3.132518, mean_eps: 0.218584
 3907957/6000000: episode: 5088, duration: 13.028s, episode steps: 614, steps per second:  47, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.643 [0.000, 5.000],  loss: 0.015871, mae: 2.609907, mean_q: 3.138643, mean_eps: 0.218470
 3908901/6000000: episode: 5089, duration: 21.532s, episode steps: 944, steps per second:  44, episode reward: 30.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.015288, mae: 2.621895, mean_q: 3.152644, mean_eps: 0.218314
 3910181/6000000: episode: 5090, duration: 27.999s, episode steps: 1280, steps per second:  46, episode reward: 32.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.062 [0.000, 5.000],  loss: 0.015228, mae: 2.597504, mean_q: 3.124270, mean_eps: 0.218092
 3911059/6000000: episode: 5091, duration: 18.983s, episode steps: 878, steps per second:  46, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.892 [0.000, 5.000],  loss: 0.014767, mae: 2.622848, mean_q: 3.154910, mean_eps: 0.217876
 3911923/6000000: episode: 5092, duration: 18.338s, episode steps: 864, steps per second:  47, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.014631, mae: 2.602602, mean_q: 3.130170, mean_eps: 0.217702
 3912321/6000000: episode: 5093, duration: 8.547s, episode steps: 398, steps per second:  47, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.940 [0.000, 5.000],  loss: 0.017684, mae: 2.640995, mean_q: 3.176962, mean_eps: 0.217576
 3913244/6000000: episode: 5094, duration: 19.115s, episode steps: 923, steps per second:  48, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.135 [0.000, 5.000],  loss: 0.013937, mae: 2.620431, mean_q: 3.152790, mean_eps: 0.217444
 3914038/6000000: episode: 5095, duration: 17.007s, episode steps: 794, steps per second:  47, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.219 [0.000, 5.000],  loss: 0.016033, mae: 2.634060, mean_q: 3.166131, mean_eps: 0.217272
 3915227/6000000: episode: 5096, duration: 24.768s, episode steps: 1189, steps per second:  48, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.080 [0.000, 5.000],  loss: 0.013290, mae: 2.594974, mean_q: 3.123676, mean_eps: 0.217074
 3916086/6000000: episode: 5097, duration: 17.893s, episode steps: 859, steps per second:  48, episode reward: 27.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.756 [0.000, 5.000],  loss: 0.014582, mae: 2.629420, mean_q: 3.162214, mean_eps: 0.216869
 3917011/6000000: episode: 5098, duration: 19.735s, episode steps: 925, steps per second:  47, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.640 [0.000, 5.000],  loss: 0.013990, mae: 2.612714, mean_q: 3.143964, mean_eps: 0.216690
 3917719/6000000: episode: 5099, duration: 15.279s, episode steps: 708, steps per second:  46, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.419 [0.000, 5.000],  loss: 0.014461, mae: 2.613658, mean_q: 3.147603, mean_eps: 0.216527
 3918382/6000000: episode: 5100, duration: 14.500s, episode steps: 663, steps per second:  46, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.934 [0.000, 5.000],  loss: 0.013729, mae: 2.607616, mean_q: 3.135927, mean_eps: 0.216390
 3919373/6000000: episode: 5101, duration: 22.728s, episode steps: 991, steps per second:  44, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.742 [0.000, 5.000],  loss: 0.015280, mae: 2.614409, mean_q: 3.145353, mean_eps: 0.216224
 3920219/6000000: episode: 5102, duration: 20.250s, episode steps: 846, steps per second:  42, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.615 [0.000, 5.000],  loss: 0.014338, mae: 2.620299, mean_q: 3.154914, mean_eps: 0.216041
 3920989/6000000: episode: 5103, duration: 16.824s, episode steps: 770, steps per second:  46, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.573 [0.000, 5.000],  loss: 0.014035, mae: 2.637361, mean_q: 3.173229, mean_eps: 0.215879
 3921665/6000000: episode: 5104, duration: 14.636s, episode steps: 676, steps per second:  46, episode reward: 20.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.558 [0.000, 5.000],  loss: 0.013992, mae: 2.609837, mean_q: 3.138447, mean_eps: 0.215734
 3922662/6000000: episode: 5105, duration: 22.656s, episode steps: 997, steps per second:  44, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.066 [0.000, 5.000],  loss: 0.014657, mae: 2.617421, mean_q: 3.147634, mean_eps: 0.215567
 3923561/6000000: episode: 5106, duration: 19.398s, episode steps: 899, steps per second:  46, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.781 [0.000, 5.000],  loss: 0.014747, mae: 2.620405, mean_q: 3.153306, mean_eps: 0.215378
 3924431/6000000: episode: 5107, duration: 18.859s, episode steps: 870, steps per second:  46, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.779 [0.000, 5.000],  loss: 0.015262, mae: 2.615335, mean_q: 3.145528, mean_eps: 0.215201
 3925070/6000000: episode: 5108, duration: 15.532s, episode steps: 639, steps per second:  41, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 0.014587, mae: 2.631635, mean_q: 3.165487, mean_eps: 0.215050
 3925571/6000000: episode: 5109, duration: 12.211s, episode steps: 501, steps per second:  41, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.182 [0.000, 5.000],  loss: 0.016159, mae: 2.617605, mean_q: 3.149373, mean_eps: 0.214936
 3926660/6000000: episode: 5110, duration: 23.966s, episode steps: 1089, steps per second:  45, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.040 [0.000, 5.000],  loss: 0.014400, mae: 2.615631, mean_q: 3.147846, mean_eps: 0.214777
 3927456/6000000: episode: 5111, duration: 17.528s, episode steps: 796, steps per second:  45, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.299 [0.000, 5.000],  loss: 0.016621, mae: 2.626469, mean_q: 3.158001, mean_eps: 0.214589
 3928386/6000000: episode: 5112, duration: 19.943s, episode steps: 930, steps per second:  47, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.161 [0.000, 5.000],  loss: 0.013193, mae: 2.613559, mean_q: 3.144468, mean_eps: 0.214416
 3929199/6000000: episode: 5113, duration: 16.793s, episode steps: 813, steps per second:  48, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.279 [0.000, 5.000],  loss: 0.013792, mae: 2.614851, mean_q: 3.147626, mean_eps: 0.214242
 3930094/6000000: episode: 5114, duration: 18.812s, episode steps: 895, steps per second:  48, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.013571, mae: 2.612654, mean_q: 3.142592, mean_eps: 0.214071
 3930888/6000000: episode: 5115, duration: 16.928s, episode steps: 794, steps per second:  47, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.015226, mae: 2.631422, mean_q: 3.166312, mean_eps: 0.213902
 3931925/6000000: episode: 5116, duration: 21.769s, episode steps: 1037, steps per second:  48, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.015715, mae: 2.654982, mean_q: 3.195881, mean_eps: 0.213719
 3932906/6000000: episode: 5117, duration: 20.925s, episode steps: 981, steps per second:  47, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.140 [0.000, 5.000],  loss: 0.015035, mae: 2.634443, mean_q: 3.168291, mean_eps: 0.213517
 3933490/6000000: episode: 5118, duration: 12.651s, episode steps: 584, steps per second:  46, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.815 [0.000, 5.000],  loss: 0.014018, mae: 2.645151, mean_q: 3.180828, mean_eps: 0.213360
 3934436/6000000: episode: 5119, duration: 21.196s, episode steps: 946, steps per second:  45, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.690 [0.000, 5.000],  loss: 0.015889, mae: 2.645726, mean_q: 3.183671, mean_eps: 0.213208
 3935550/6000000: episode: 5120, duration: 26.398s, episode steps: 1114, steps per second:  42, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.944 [0.000, 5.000],  loss: 0.014856, mae: 2.627985, mean_q: 3.161687, mean_eps: 0.213002
 3936492/6000000: episode: 5121, duration: 23.114s, episode steps: 942, steps per second:  41, episode reward: 29.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.273 [0.000, 5.000],  loss: 0.014228, mae: 2.617261, mean_q: 3.149549, mean_eps: 0.212796
 3937365/6000000: episode: 5122, duration: 19.785s, episode steps: 873, steps per second:  44, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.953 [0.000, 5.000],  loss: 0.014416, mae: 2.622420, mean_q: 3.155490, mean_eps: 0.212614
 3938247/6000000: episode: 5123, duration: 19.491s, episode steps: 882, steps per second:  45, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.662 [0.000, 5.000],  loss: 0.013794, mae: 2.639419, mean_q: 3.178105, mean_eps: 0.212439
 3939116/6000000: episode: 5124, duration: 19.098s, episode steps: 869, steps per second:  46, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.582 [0.000, 5.000],  loss: 0.014751, mae: 2.618378, mean_q: 3.154620, mean_eps: 0.212264
 3939735/6000000: episode: 5125, duration: 13.065s, episode steps: 619, steps per second:  47, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.654 [0.000, 5.000],  loss: 0.014671, mae: 2.614527, mean_q: 3.147475, mean_eps: 0.212115
 3940742/6000000: episode: 5126, duration: 22.070s, episode steps: 1007, steps per second:  46, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.168 [0.000, 5.000],  loss: 0.014888, mae: 2.623426, mean_q: 3.157006, mean_eps: 0.211952
 3941782/6000000: episode: 5127, duration: 24.339s, episode steps: 1040, steps per second:  43, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.015578, mae: 2.628405, mean_q: 3.163519, mean_eps: 0.211748
 3942549/6000000: episode: 5128, duration: 17.039s, episode steps: 767, steps per second:  45, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.011913, mae: 2.639929, mean_q: 3.178002, mean_eps: 0.211567
 3943416/6000000: episode: 5129, duration: 19.335s, episode steps: 867, steps per second:  45, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.029 [0.000, 5.000],  loss: 0.014776, mae: 2.618049, mean_q: 3.148676, mean_eps: 0.211404
 3944338/6000000: episode: 5130, duration: 20.955s, episode steps: 922, steps per second:  44, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.072 [0.000, 5.000],  loss: 0.013973, mae: 2.637262, mean_q: 3.173699, mean_eps: 0.211225
 3945058/6000000: episode: 5131, duration: 15.162s, episode steps: 720, steps per second:  47, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.812 [0.000, 5.000],  loss: 0.015270, mae: 2.645758, mean_q: 3.183333, mean_eps: 0.211060
 3946075/6000000: episode: 5132, duration: 21.734s, episode steps: 1017, steps per second:  47, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.018 [0.000, 5.000],  loss: 0.014527, mae: 2.644509, mean_q: 3.182296, mean_eps: 0.210887
 3947110/6000000: episode: 5133, duration: 22.650s, episode steps: 1035, steps per second:  46, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.778 [0.000, 5.000],  loss: 0.014366, mae: 2.651586, mean_q: 3.192356, mean_eps: 0.210682
 3948359/6000000: episode: 5134, duration: 26.627s, episode steps: 1249, steps per second:  47, episode reward: 30.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.054 [0.000, 5.000],  loss: 0.013909, mae: 2.657454, mean_q: 3.197902, mean_eps: 0.210453
 3949092/6000000: episode: 5135, duration: 15.427s, episode steps: 733, steps per second:  48, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.992 [0.000, 5.000],  loss: 0.015163, mae: 2.664660, mean_q: 3.205983, mean_eps: 0.210255
 3949975/6000000: episode: 5136, duration: 19.142s, episode steps: 883, steps per second:  46, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.716 [0.000, 5.000],  loss: 0.016341, mae: 2.648257, mean_q: 3.184678, mean_eps: 0.210094
 3950790/6000000: episode: 5137, duration: 18.135s, episode steps: 815, steps per second:  45, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.842 [0.000, 5.000],  loss: 0.014027, mae: 2.631334, mean_q: 3.166286, mean_eps: 0.209924
 3951529/6000000: episode: 5138, duration: 16.270s, episode steps: 739, steps per second:  45, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.087 [0.000, 5.000],  loss: 0.013678, mae: 2.616674, mean_q: 3.148124, mean_eps: 0.209768
 3952255/6000000: episode: 5139, duration: 17.930s, episode steps: 726, steps per second:  40, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.950 [0.000, 5.000],  loss: 0.013938, mae: 2.614724, mean_q: 3.145340, mean_eps: 0.209622
 3952874/6000000: episode: 5140, duration: 15.306s, episode steps: 619, steps per second:  40, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.943 [0.000, 5.000],  loss: 0.014617, mae: 2.633354, mean_q: 3.167375, mean_eps: 0.209487
 3953948/6000000: episode: 5141, duration: 23.525s, episode steps: 1074, steps per second:  46, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.014122, mae: 2.618169, mean_q: 3.149934, mean_eps: 0.209318
 3954715/6000000: episode: 5142, duration: 16.715s, episode steps: 767, steps per second:  46, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.808 [0.000, 5.000],  loss: 0.015764, mae: 2.644905, mean_q: 3.179891, mean_eps: 0.209134
 3955777/6000000: episode: 5143, duration: 23.298s, episode steps: 1062, steps per second:  46, episode reward: 30.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.224 [0.000, 5.000],  loss: 0.014612, mae: 2.634071, mean_q: 3.169217, mean_eps: 0.208951
 3956385/6000000: episode: 5144, duration: 13.351s, episode steps: 608, steps per second:  46, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.015298, mae: 2.628878, mean_q: 3.164016, mean_eps: 0.208784
 3957363/6000000: episode: 5145, duration: 21.596s, episode steps: 978, steps per second:  45, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.698 [0.000, 5.000],  loss: 0.015691, mae: 2.647739, mean_q: 3.184224, mean_eps: 0.208625
 3958276/6000000: episode: 5146, duration: 21.007s, episode steps: 913, steps per second:  43, episode reward: 28.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.815 [0.000, 5.000],  loss: 0.014471, mae: 2.659431, mean_q: 3.198923, mean_eps: 0.208436
 3958668/6000000: episode: 5147, duration: 8.848s, episode steps: 392, steps per second:  44, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.949 [0.000, 5.000],  loss: 0.013076, mae: 2.649201, mean_q: 3.187337, mean_eps: 0.208306
 3959584/6000000: episode: 5148, duration: 20.205s, episode steps: 916, steps per second:  45, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.013354, mae: 2.634073, mean_q: 3.168801, mean_eps: 0.208175
 3960697/6000000: episode: 5149, duration: 24.263s, episode steps: 1113, steps per second:  46, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.085 [0.000, 5.000],  loss: 0.013169, mae: 2.650419, mean_q: 3.189536, mean_eps: 0.207972
 3961718/6000000: episode: 5150, duration: 21.290s, episode steps: 1021, steps per second:  48, episode reward: 29.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.013703, mae: 2.619224, mean_q: 3.150977, mean_eps: 0.207758
 3962494/6000000: episode: 5151, duration: 16.499s, episode steps: 776, steps per second:  47, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.043 [0.000, 5.000],  loss: 0.014711, mae: 2.626281, mean_q: 3.159533, mean_eps: 0.207579
 3963666/6000000: episode: 5152, duration: 26.076s, episode steps: 1172, steps per second:  45, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.930 [0.000, 5.000],  loss: 0.014185, mae: 2.631036, mean_q: 3.167948, mean_eps: 0.207384
 3964405/6000000: episode: 5153, duration: 15.465s, episode steps: 739, steps per second:  48, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.288 [0.000, 5.000],  loss: 0.015097, mae: 2.627954, mean_q: 3.161606, mean_eps: 0.207193
 3965047/6000000: episode: 5154, duration: 14.218s, episode steps: 642, steps per second:  45, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: 0.012872, mae: 2.626792, mean_q: 3.159272, mean_eps: 0.207055
 3966083/6000000: episode: 5155, duration: 23.023s, episode steps: 1036, steps per second:  45, episode reward: 29.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.015129, mae: 2.648337, mean_q: 3.185096, mean_eps: 0.206887
 3967227/6000000: episode: 5156, duration: 25.266s, episode steps: 1144, steps per second:  45, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.096 [0.000, 5.000],  loss: 0.014259, mae: 2.663158, mean_q: 3.202662, mean_eps: 0.206669
 3968238/6000000: episode: 5157, duration: 23.556s, episode steps: 1011, steps per second:  43, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.873 [0.000, 5.000],  loss: 0.015283, mae: 2.653735, mean_q: 3.190788, mean_eps: 0.206454
 3969111/6000000: episode: 5158, duration: 21.246s, episode steps: 873, steps per second:  41, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.884 [0.000, 5.000],  loss: 0.014725, mae: 2.626614, mean_q: 3.158780, mean_eps: 0.206265
 3969670/6000000: episode: 5159, duration: 12.448s, episode steps: 559, steps per second:  45, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.016409, mae: 2.627199, mean_q: 3.159723, mean_eps: 0.206122
 3970472/6000000: episode: 5160, duration: 17.878s, episode steps: 802, steps per second:  45, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.706 [0.000, 5.000],  loss: 0.016341, mae: 2.646929, mean_q: 3.183700, mean_eps: 0.205986
 3971221/6000000: episode: 5161, duration: 16.519s, episode steps: 749, steps per second:  45, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.754 [0.000, 5.000],  loss: 0.014598, mae: 2.626901, mean_q: 3.159577, mean_eps: 0.205831
 3971816/6000000: episode: 5162, duration: 13.236s, episode steps: 595, steps per second:  45, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.014058, mae: 2.657493, mean_q: 3.197763, mean_eps: 0.205696
 3972692/6000000: episode: 5163, duration: 20.783s, episode steps: 876, steps per second:  42, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.839 [0.000, 5.000],  loss: 0.014620, mae: 2.626518, mean_q: 3.162322, mean_eps: 0.205550
 3973531/6000000: episode: 5164, duration: 19.300s, episode steps: 839, steps per second:  43, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.323 [0.000, 5.000],  loss: 0.015883, mae: 2.631780, mean_q: 3.166177, mean_eps: 0.205378
 3974250/6000000: episode: 5165, duration: 16.119s, episode steps: 719, steps per second:  45, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.217 [0.000, 5.000],  loss: 0.014578, mae: 2.641773, mean_q: 3.179214, mean_eps: 0.205222
 3974877/6000000: episode: 5166, duration: 14.323s, episode steps: 627, steps per second:  44, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.936 [0.000, 5.000],  loss: 0.012927, mae: 2.643289, mean_q: 3.182232, mean_eps: 0.205087
 3975917/6000000: episode: 5167, duration: 22.901s, episode steps: 1040, steps per second:  45, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.704 [0.000, 5.000],  loss: 0.013823, mae: 2.625356, mean_q: 3.158690, mean_eps: 0.204920
 3976711/6000000: episode: 5168, duration: 17.748s, episode steps: 794, steps per second:  45, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.137 [0.000, 5.000],  loss: 0.013666, mae: 2.624981, mean_q: 3.159024, mean_eps: 0.204737
 3977412/6000000: episode: 5169, duration: 15.394s, episode steps: 701, steps per second:  46, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.014719, mae: 2.641313, mean_q: 3.177708, mean_eps: 0.204588
 3978266/6000000: episode: 5170, duration: 17.582s, episode steps: 854, steps per second:  49, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.012772, mae: 2.631229, mean_q: 3.166808, mean_eps: 0.204432
 3979034/6000000: episode: 5171, duration: 17.349s, episode steps: 768, steps per second:  44, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.841 [0.000, 5.000],  loss: 0.016289, mae: 2.619956, mean_q: 3.153034, mean_eps: 0.204270
 3979733/6000000: episode: 5172, duration: 15.797s, episode steps: 699, steps per second:  44, episode reward: 21.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.014445, mae: 2.617901, mean_q: 3.149359, mean_eps: 0.204123
 3980818/6000000: episode: 5173, duration: 23.649s, episode steps: 1085, steps per second:  46, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.799 [0.000, 5.000],  loss: 0.013319, mae: 2.639535, mean_q: 3.178232, mean_eps: 0.203945
 3981760/6000000: episode: 5174, duration: 21.124s, episode steps: 942, steps per second:  45, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.014096, mae: 2.669063, mean_q: 3.211719, mean_eps: 0.203742
 3982256/6000000: episode: 5175, duration: 10.936s, episode steps: 496, steps per second:  45, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.825 [0.000, 5.000],  loss: 0.014061, mae: 2.668318, mean_q: 3.211818, mean_eps: 0.203599
 3983323/6000000: episode: 5176, duration: 23.276s, episode steps: 1067, steps per second:  46, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.015527, mae: 2.647933, mean_q: 3.185306, mean_eps: 0.203442
 3984415/6000000: episode: 5177, duration: 26.113s, episode steps: 1092, steps per second:  42, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.568 [0.000, 5.000],  loss: 0.013666, mae: 2.659761, mean_q: 3.200642, mean_eps: 0.203226
 3985387/6000000: episode: 5178, duration: 23.384s, episode steps: 972, steps per second:  42, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.984 [0.000, 5.000],  loss: 0.015538, mae: 2.640686, mean_q: 3.177736, mean_eps: 0.203020
 3986250/6000000: episode: 5179, duration: 19.405s, episode steps: 863, steps per second:  44, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.955 [0.000, 5.000],  loss: 0.014458, mae: 2.625855, mean_q: 3.158393, mean_eps: 0.202836
 3987178/6000000: episode: 5180, duration: 20.901s, episode steps: 928, steps per second:  44, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.555 [0.000, 5.000],  loss: 0.015842, mae: 2.645589, mean_q: 3.183538, mean_eps: 0.202657
 3988480/6000000: episode: 5181, duration: 29.743s, episode steps: 1302, steps per second:  44, episode reward: 33.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.676 [0.000, 5.000],  loss: 0.013821, mae: 2.630025, mean_q: 3.163283, mean_eps: 0.202434
 3989390/6000000: episode: 5182, duration: 21.007s, episode steps: 910, steps per second:  43, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.836 [0.000, 5.000],  loss: 0.015359, mae: 2.637211, mean_q: 3.171783, mean_eps: 0.202213
 3990000/6000000: episode: 5183, duration: 14.539s, episode steps: 610, steps per second:  42, episode reward: 15.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.998 [0.000, 5.000],  loss: 0.013449, mae: 2.649081, mean_q: 3.188109, mean_eps: 0.202061
 3991127/6000000: episode: 5184, duration: 25.268s, episode steps: 1127, steps per second:  45, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.014494, mae: 2.647774, mean_q: 3.186162, mean_eps: 0.201888
 3991682/6000000: episode: 5185, duration: 12.170s, episode steps: 555, steps per second:  46, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.267 [0.000, 5.000],  loss: 0.014380, mae: 2.651629, mean_q: 3.191318, mean_eps: 0.201719
 3993499/6000000: episode: 5186, duration: 39.480s, episode steps: 1817, steps per second:  46, episode reward: 35.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.824 [0.000, 5.000],  loss: 0.014817, mae: 2.638413, mean_q: 3.174736, mean_eps: 0.201482
 3994462/6000000: episode: 5187, duration: 19.628s, episode steps: 963, steps per second:  49, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.109 [0.000, 5.000],  loss: 0.013133, mae: 2.639925, mean_q: 3.175634, mean_eps: 0.201204
 3995608/6000000: episode: 5188, duration: 25.413s, episode steps: 1146, steps per second:  45, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.093 [0.000, 5.000],  loss: 0.015729, mae: 2.670132, mean_q: 3.211621, mean_eps: 0.200993
 3996275/6000000: episode: 5189, duration: 14.600s, episode steps: 667, steps per second:  46, episode reward: 19.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.694 [0.000, 5.000],  loss: 0.013226, mae: 2.657311, mean_q: 3.198623, mean_eps: 0.200812
 3997040/6000000: episode: 5190, duration: 16.940s, episode steps: 765, steps per second:  45, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.014563, mae: 2.659704, mean_q: 3.201274, mean_eps: 0.200669
 3998272/6000000: episode: 5191, duration: 26.914s, episode steps: 1232, steps per second:  46, episode reward: 32.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.869 [0.000, 5.000],  loss: 0.015028, mae: 2.676141, mean_q: 3.221076, mean_eps: 0.200469
 3998908/6000000: episode: 5192, duration: 13.596s, episode steps: 636, steps per second:  47, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.014550, mae: 2.659590, mean_q: 3.201250, mean_eps: 0.200282
 4000296/6000000: episode: 5193, duration: 30.986s, episode steps: 1388, steps per second:  45, episode reward: 30.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.143 [0.000, 5.000],  loss: 0.014727, mae: 2.660664, mean_q: 3.201198, mean_eps: 0.200080
 4001223/6000000: episode: 5194, duration: 22.719s, episode steps: 927, steps per second:  41, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.964 [0.000, 5.000],  loss: 0.014836, mae: 2.685937, mean_q: 3.233365, mean_eps: 0.199848
 4002116/6000000: episode: 5195, duration: 20.032s, episode steps: 893, steps per second:  45, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.931 [0.000, 5.000],  loss: 0.016865, mae: 2.680470, mean_q: 3.223315, mean_eps: 0.199666
 4003178/6000000: episode: 5196, duration: 24.670s, episode steps: 1062, steps per second:  43, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.798 [0.000, 5.000],  loss: 0.016027, mae: 2.684010, mean_q: 3.229670, mean_eps: 0.199471
 4003774/6000000: episode: 5197, duration: 13.942s, episode steps: 596, steps per second:  43, episode reward: 16.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.015471, mae: 2.683592, mean_q: 3.228784, mean_eps: 0.199305
 4005094/6000000: episode: 5198, duration: 30.283s, episode steps: 1320, steps per second:  44, episode reward: 34.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.211 [0.000, 5.000],  loss: 0.014501, mae: 2.682831, mean_q: 3.228560, mean_eps: 0.199113
 4005939/6000000: episode: 5199, duration: 19.702s, episode steps: 845, steps per second:  43, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.191 [0.000, 5.000],  loss: 0.016645, mae: 2.696809, mean_q: 3.244721, mean_eps: 0.198897
 4006776/6000000: episode: 5200, duration: 18.657s, episode steps: 837, steps per second:  45, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.014754, mae: 2.669526, mean_q: 3.212565, mean_eps: 0.198729
 4007444/6000000: episode: 5201, duration: 15.326s, episode steps: 668, steps per second:  44, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.013666, mae: 2.670485, mean_q: 3.214779, mean_eps: 0.198578
 4008384/6000000: episode: 5202, duration: 20.845s, episode steps: 940, steps per second:  45, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.013508, mae: 2.658378, mean_q: 3.199390, mean_eps: 0.198418
 4009120/6000000: episode: 5203, duration: 16.492s, episode steps: 736, steps per second:  45, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.014005, mae: 2.697451, mean_q: 3.245089, mean_eps: 0.198250
 4010012/6000000: episode: 5204, duration: 18.258s, episode steps: 892, steps per second:  49, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.015007, mae: 2.693797, mean_q: 3.240267, mean_eps: 0.198087
 4010757/6000000: episode: 5205, duration: 15.874s, episode steps: 745, steps per second:  47, episode reward: 21.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.306 [0.000, 5.000],  loss: 0.014238, mae: 2.668902, mean_q: 3.211864, mean_eps: 0.197923
 4011583/6000000: episode: 5206, duration: 18.571s, episode steps: 826, steps per second:  44, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.946 [0.000, 5.000],  loss: 0.013890, mae: 2.656088, mean_q: 3.195582, mean_eps: 0.197766
 4012160/6000000: episode: 5207, duration: 12.608s, episode steps: 577, steps per second:  46, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.768 [0.000, 5.000],  loss: 0.014698, mae: 2.674306, mean_q: 3.218508, mean_eps: 0.197626
 4013308/6000000: episode: 5208, duration: 24.316s, episode steps: 1148, steps per second:  47, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.014571, mae: 2.680189, mean_q: 3.224883, mean_eps: 0.197454
 4014246/6000000: episode: 5209, duration: 20.867s, episode steps: 938, steps per second:  45, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.632 [0.000, 5.000],  loss: 0.015105, mae: 2.680177, mean_q: 3.226767, mean_eps: 0.197245
 4015148/6000000: episode: 5210, duration: 21.246s, episode steps: 902, steps per second:  42, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.779 [0.000, 5.000],  loss: 0.014585, mae: 2.670617, mean_q: 3.214640, mean_eps: 0.197061
 4015806/6000000: episode: 5211, duration: 15.922s, episode steps: 658, steps per second:  41, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.015655, mae: 2.693679, mean_q: 3.240376, mean_eps: 0.196905
 4016801/6000000: episode: 5212, duration: 25.177s, episode steps: 995, steps per second:  40, episode reward: 30.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.870 [0.000, 5.000],  loss: 0.015063, mae: 2.673149, mean_q: 3.216048, mean_eps: 0.196739
 4017693/6000000: episode: 5213, duration: 20.153s, episode steps: 892, steps per second:  44, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.657 [0.000, 5.000],  loss: 0.015419, mae: 2.681780, mean_q: 3.225797, mean_eps: 0.196550
 4018568/6000000: episode: 5214, duration: 19.519s, episode steps: 875, steps per second:  45, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.817 [0.000, 5.000],  loss: 0.015113, mae: 2.677797, mean_q: 3.220199, mean_eps: 0.196374
 4019762/6000000: episode: 5215, duration: 29.700s, episode steps: 1194, steps per second:  40, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.566 [0.000, 5.000],  loss: 0.014912, mae: 2.667194, mean_q: 3.209960, mean_eps: 0.196167
 4020969/6000000: episode: 5216, duration: 28.123s, episode steps: 1207, steps per second:  43, episode reward: 30.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.912 [0.000, 5.000],  loss: 0.015295, mae: 2.674397, mean_q: 3.217874, mean_eps: 0.195927
 4021985/6000000: episode: 5217, duration: 23.745s, episode steps: 1016, steps per second:  43, episode reward: 31.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.199 [0.000, 5.000],  loss: 0.014562, mae: 2.686793, mean_q: 3.232092, mean_eps: 0.195704
 4022748/6000000: episode: 5218, duration: 16.797s, episode steps: 763, steps per second:  45, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.153 [0.000, 5.000],  loss: 0.015037, mae: 2.691163, mean_q: 3.237665, mean_eps: 0.195527
 4023883/6000000: episode: 5219, duration: 25.598s, episode steps: 1135, steps per second:  44, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.014389, mae: 2.672137, mean_q: 3.215130, mean_eps: 0.195337
 4024733/6000000: episode: 5220, duration: 18.784s, episode steps: 850, steps per second:  45, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.858 [0.000, 5.000],  loss: 0.015164, mae: 2.679838, mean_q: 3.223422, mean_eps: 0.195138
 4025614/6000000: episode: 5221, duration: 18.122s, episode steps: 881, steps per second:  49, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.015643, mae: 2.674068, mean_q: 3.219112, mean_eps: 0.194965
 4026499/6000000: episode: 5222, duration: 18.675s, episode steps: 885, steps per second:  47, episode reward: 27.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.668 [0.000, 5.000],  loss: 0.016230, mae: 2.664316, mean_q: 3.205292, mean_eps: 0.194789
 4027206/6000000: episode: 5223, duration: 14.972s, episode steps: 707, steps per second:  47, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.931 [0.000, 5.000],  loss: 0.014657, mae: 2.664245, mean_q: 3.207960, mean_eps: 0.194630
 4027629/6000000: episode: 5224, duration: 8.941s, episode steps: 423, steps per second:  47, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 3.128 [0.000, 5.000],  loss: 0.013255, mae: 2.659516, mean_q: 3.201300, mean_eps: 0.194516
 4028890/6000000: episode: 5225, duration: 26.845s, episode steps: 1261, steps per second:  47, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.175 [0.000, 5.000],  loss: 0.015316, mae: 2.670226, mean_q: 3.212745, mean_eps: 0.194348
 4029814/6000000: episode: 5226, duration: 20.229s, episode steps: 924, steps per second:  46, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.159 [0.000, 5.000],  loss: 0.014305, mae: 2.673492, mean_q: 3.217187, mean_eps: 0.194130
 4030839/6000000: episode: 5227, duration: 23.625s, episode steps: 1025, steps per second:  43, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.009 [0.000, 5.000],  loss: 0.013961, mae: 2.677895, mean_q: 3.223261, mean_eps: 0.193935
 4031596/6000000: episode: 5228, duration: 18.993s, episode steps: 757, steps per second:  40, episode reward: 21.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.054 [0.000, 5.000],  loss: 0.014560, mae: 2.654818, mean_q: 3.194522, mean_eps: 0.193757
 4032468/6000000: episode: 5229, duration: 21.245s, episode steps: 872, steps per second:  41, episode reward: 27.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.781 [0.000, 5.000],  loss: 0.015200, mae: 2.690105, mean_q: 3.236567, mean_eps: 0.193594
 4033712/6000000: episode: 5230, duration: 28.065s, episode steps: 1244, steps per second:  44, episode reward: 41.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 1.703 [0.000, 5.000],  loss: 0.014033, mae: 2.679971, mean_q: 3.224910, mean_eps: 0.193382
 4034578/6000000: episode: 5231, duration: 19.593s, episode steps: 866, steps per second:  44, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.719 [0.000, 5.000],  loss: 0.014869, mae: 2.669290, mean_q: 3.211499, mean_eps: 0.193171
 4035631/6000000: episode: 5232, duration: 23.390s, episode steps: 1053, steps per second:  45, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.012 [0.000, 5.000],  loss: 0.015762, mae: 2.675988, mean_q: 3.221551, mean_eps: 0.192979
 4036548/6000000: episode: 5233, duration: 21.631s, episode steps: 917, steps per second:  42, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.916 [0.000, 5.000],  loss: 0.015694, mae: 2.669199, mean_q: 3.212886, mean_eps: 0.192782
 4037177/6000000: episode: 5234, duration: 15.386s, episode steps: 629, steps per second:  41, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.140 [0.000, 5.000],  loss: 0.014971, mae: 2.690245, mean_q: 3.236922, mean_eps: 0.192628
 4038038/6000000: episode: 5235, duration: 20.501s, episode steps: 861, steps per second:  42, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.161 [0.000, 5.000],  loss: 0.014029, mae: 2.664748, mean_q: 3.207227, mean_eps: 0.192478
 4039084/6000000: episode: 5236, duration: 23.408s, episode steps: 1046, steps per second:  45, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.828 [0.000, 5.000],  loss: 0.015465, mae: 2.676158, mean_q: 3.220791, mean_eps: 0.192288
 4040018/6000000: episode: 5237, duration: 20.514s, episode steps: 934, steps per second:  46, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.832 [0.000, 5.000],  loss: 0.014699, mae: 2.672745, mean_q: 3.218152, mean_eps: 0.192090
 4040870/6000000: episode: 5238, duration: 18.646s, episode steps: 852, steps per second:  46, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.123 [0.000, 5.000],  loss: 0.016677, mae: 2.668458, mean_q: 3.210551, mean_eps: 0.191911
 4041716/6000000: episode: 5239, duration: 17.573s, episode steps: 846, steps per second:  48, episode reward: 25.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.236 [0.000, 5.000],  loss: 0.015254, mae: 2.685234, mean_q: 3.231303, mean_eps: 0.191742
 4042336/6000000: episode: 5240, duration: 12.760s, episode steps: 620, steps per second:  49, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.016226, mae: 2.705807, mean_q: 3.254748, mean_eps: 0.191595
 4043475/6000000: episode: 5241, duration: 25.608s, episode steps: 1139, steps per second:  44, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.016680, mae: 2.673259, mean_q: 3.214997, mean_eps: 0.191419
 4044308/6000000: episode: 5242, duration: 18.399s, episode steps: 833, steps per second:  45, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.329 [0.000, 5.000],  loss: 0.015986, mae: 2.674963, mean_q: 3.219412, mean_eps: 0.191222
 4045430/6000000: episode: 5243, duration: 24.895s, episode steps: 1122, steps per second:  45, episode reward: 32.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.832 [0.000, 5.000],  loss: 0.014471, mae: 2.683187, mean_q: 3.227948, mean_eps: 0.191026
 4046123/6000000: episode: 5244, duration: 15.569s, episode steps: 693, steps per second:  45, episode reward: 20.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.720 [0.000, 5.000],  loss: 0.013109, mae: 2.686663, mean_q: 3.233401, mean_eps: 0.190845
 4047087/6000000: episode: 5245, duration: 22.311s, episode steps: 964, steps per second:  43, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.017 [0.000, 5.000],  loss: 0.014968, mae: 2.691632, mean_q: 3.239379, mean_eps: 0.190679
 4047902/6000000: episode: 5246, duration: 18.703s, episode steps: 815, steps per second:  44, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.209 [0.000, 5.000],  loss: 0.015779, mae: 2.679607, mean_q: 3.223437, mean_eps: 0.190501
 4048789/6000000: episode: 5247, duration: 20.475s, episode steps: 887, steps per second:  43, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.324 [0.000, 5.000],  loss: 0.013974, mae: 2.675624, mean_q: 3.217966, mean_eps: 0.190331
 4049841/6000000: episode: 5248, duration: 23.722s, episode steps: 1052, steps per second:  44, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.244 [0.000, 5.000],  loss: 0.014309, mae: 2.689512, mean_q: 3.234433, mean_eps: 0.190137
 4050702/6000000: episode: 5249, duration: 19.159s, episode steps: 861, steps per second:  45, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.226 [0.000, 5.000],  loss: 0.015266, mae: 2.689993, mean_q: 3.236532, mean_eps: 0.189946
 4051752/6000000: episode: 5250, duration: 24.295s, episode steps: 1050, steps per second:  43, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.968 [0.000, 5.000],  loss: 0.014922, mae: 2.704519, mean_q: 3.253744, mean_eps: 0.189755
 4052414/6000000: episode: 5251, duration: 14.503s, episode steps: 662, steps per second:  46, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.076 [0.000, 5.000],  loss: 0.014584, mae: 2.679091, mean_q: 3.223501, mean_eps: 0.189584
 4053461/6000000: episode: 5252, duration: 24.210s, episode steps: 1047, steps per second:  43, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.262 [0.000, 5.000],  loss: 0.015911, mae: 2.694303, mean_q: 3.241358, mean_eps: 0.189412
 4054233/6000000: episode: 5253, duration: 17.533s, episode steps: 772, steps per second:  44, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.880 [0.000, 5.000],  loss: 0.013586, mae: 2.681709, mean_q: 3.225771, mean_eps: 0.189230
 4055093/6000000: episode: 5254, duration: 18.804s, episode steps: 860, steps per second:  46, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.014904, mae: 2.689554, mean_q: 3.234610, mean_eps: 0.189067
 4055947/6000000: episode: 5255, duration: 18.829s, episode steps: 854, steps per second:  45, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.125 [0.000, 5.000],  loss: 0.013899, mae: 2.688914, mean_q: 3.234797, mean_eps: 0.188896
 4056954/6000000: episode: 5256, duration: 21.658s, episode steps: 1007, steps per second:  46, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.006 [0.000, 5.000],  loss: 0.013819, mae: 2.703290, mean_q: 3.253337, mean_eps: 0.188710
 4058426/6000000: episode: 5257, duration: 30.529s, episode steps: 1472, steps per second:  48, episode reward: 33.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.243 [0.000, 5.000],  loss: 0.015459, mae: 2.697882, mean_q: 3.245519, mean_eps: 0.188462
 4059499/6000000: episode: 5258, duration: 23.996s, episode steps: 1073, steps per second:  45, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.766 [0.000, 5.000],  loss: 0.013585, mae: 2.697207, mean_q: 3.245840, mean_eps: 0.188208
 4060400/6000000: episode: 5259, duration: 20.886s, episode steps: 901, steps per second:  43, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.618 [0.000, 5.000],  loss: 0.014385, mae: 2.670845, mean_q: 3.212833, mean_eps: 0.188010
 4061148/6000000: episode: 5260, duration: 18.378s, episode steps: 748, steps per second:  41, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.021 [0.000, 5.000],  loss: 0.014557, mae: 2.665459, mean_q: 3.205952, mean_eps: 0.187846
 4061942/6000000: episode: 5261, duration: 21.184s, episode steps: 794, steps per second:  37, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.102 [0.000, 5.000],  loss: 0.016126, mae: 2.678216, mean_q: 3.221073, mean_eps: 0.187691
 4062714/6000000: episode: 5262, duration: 20.133s, episode steps: 772, steps per second:  38, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.135 [0.000, 5.000],  loss: 0.015090, mae: 2.669901, mean_q: 3.211742, mean_eps: 0.187534
 4063897/6000000: episode: 5263, duration: 32.833s, episode steps: 1183, steps per second:  36, episode reward: 32.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.014595, mae: 2.680163, mean_q: 3.223964, mean_eps: 0.187339
 4064684/6000000: episode: 5264, duration: 19.506s, episode steps: 787, steps per second:  40, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.014318, mae: 2.681492, mean_q: 3.225491, mean_eps: 0.187142
 4065168/6000000: episode: 5265, duration: 11.263s, episode steps: 484, steps per second:  43, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.167 [0.000, 5.000],  loss: 0.016246, mae: 2.653548, mean_q: 3.192165, mean_eps: 0.187015
 4066128/6000000: episode: 5266, duration: 20.984s, episode steps: 960, steps per second:  46, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.759 [0.000, 5.000],  loss: 0.013269, mae: 2.648022, mean_q: 3.186517, mean_eps: 0.186871
 4066739/6000000: episode: 5267, duration: 13.875s, episode steps: 611, steps per second:  44, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.098 [0.000, 5.000],  loss: 0.013215, mae: 2.642959, mean_q: 3.180748, mean_eps: 0.186714
 4068070/6000000: episode: 5268, duration: 28.304s, episode steps: 1331, steps per second:  47, episode reward: 33.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.569 [0.000, 5.000],  loss: 0.014460, mae: 2.664237, mean_q: 3.204902, mean_eps: 0.186519
 4068814/6000000: episode: 5269, duration: 15.832s, episode steps: 744, steps per second:  47, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.015376, mae: 2.657270, mean_q: 3.198731, mean_eps: 0.186312
 4069664/6000000: episode: 5270, duration: 19.378s, episode steps: 850, steps per second:  44, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.104 [0.000, 5.000],  loss: 0.016308, mae: 2.635063, mean_q: 3.168986, mean_eps: 0.186152
 4070802/6000000: episode: 5271, duration: 25.294s, episode steps: 1138, steps per second:  45, episode reward: 33.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.975 [0.000, 5.000],  loss: 0.014185, mae: 2.667466, mean_q: 3.208289, mean_eps: 0.185954
 4071561/6000000: episode: 5272, duration: 17.076s, episode steps: 759, steps per second:  44, episode reward: 23.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.014096, mae: 2.669107, mean_q: 3.210855, mean_eps: 0.185764
 4072760/6000000: episode: 5273, duration: 26.060s, episode steps: 1199, steps per second:  46, episode reward: 32.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.149 [0.000, 5.000],  loss: 0.015918, mae: 2.678328, mean_q: 3.219649, mean_eps: 0.185568
 4073974/6000000: episode: 5274, duration: 24.668s, episode steps: 1214, steps per second:  49, episode reward: 33.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.658 [0.000, 5.000],  loss: 0.014398, mae: 2.667572, mean_q: 3.210832, mean_eps: 0.185327
 4074836/6000000: episode: 5275, duration: 19.440s, episode steps: 862, steps per second:  44, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.990 [0.000, 5.000],  loss: 0.016273, mae: 2.653509, mean_q: 3.191442, mean_eps: 0.185119
 4075778/6000000: episode: 5276, duration: 20.162s, episode steps: 942, steps per second:  47, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.845 [0.000, 5.000],  loss: 0.015712, mae: 2.673235, mean_q: 3.215507, mean_eps: 0.184939
 4076865/6000000: episode: 5277, duration: 22.937s, episode steps: 1087, steps per second:  47, episode reward: 31.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.119 [0.000, 5.000],  loss: 0.016163, mae: 2.669260, mean_q: 3.210447, mean_eps: 0.184736
 4078118/6000000: episode: 5278, duration: 26.569s, episode steps: 1253, steps per second:  47, episode reward: 33.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.245 [0.000, 5.000],  loss: 0.015899, mae: 2.681983, mean_q: 3.226836, mean_eps: 0.184502
 4079323/6000000: episode: 5279, duration: 26.674s, episode steps: 1205, steps per second:  45, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.633 [0.000, 5.000],  loss: 0.014254, mae: 2.654021, mean_q: 3.194203, mean_eps: 0.184256
 4080044/6000000: episode: 5280, duration: 17.752s, episode steps: 721, steps per second:  41, episode reward: 20.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.767 [0.000, 5.000],  loss: 0.012981, mae: 2.666242, mean_q: 3.207823, mean_eps: 0.184064
 4080797/6000000: episode: 5281, duration: 18.124s, episode steps: 753, steps per second:  42, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.009 [0.000, 5.000],  loss: 0.014861, mae: 2.683295, mean_q: 3.228864, mean_eps: 0.183916
 4081750/6000000: episode: 5282, duration: 20.813s, episode steps: 953, steps per second:  46, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.153 [0.000, 5.000],  loss: 0.015791, mae: 2.685942, mean_q: 3.229609, mean_eps: 0.183745
 4082605/6000000: episode: 5283, duration: 18.641s, episode steps: 855, steps per second:  46, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.074 [0.000, 5.000],  loss: 0.014867, mae: 2.676137, mean_q: 3.220252, mean_eps: 0.183564
 4083540/6000000: episode: 5284, duration: 20.795s, episode steps: 935, steps per second:  45, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.057 [0.000, 5.000],  loss: 0.014531, mae: 2.682255, mean_q: 3.227108, mean_eps: 0.183386
 4084344/6000000: episode: 5285, duration: 17.674s, episode steps: 804, steps per second:  45, episode reward: 26.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.172 [0.000, 5.000],  loss: 0.014248, mae: 2.679122, mean_q: 3.223551, mean_eps: 0.183212
 4085295/6000000: episode: 5286, duration: 21.596s, episode steps: 951, steps per second:  44, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.202 [0.000, 5.000],  loss: 0.017310, mae: 2.702740, mean_q: 3.248903, mean_eps: 0.183036
 4086318/6000000: episode: 5287, duration: 22.271s, episode steps: 1023, steps per second:  46, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.177 [0.000, 5.000],  loss: 0.015470, mae: 2.696917, mean_q: 3.243153, mean_eps: 0.182839
 4087172/6000000: episode: 5288, duration: 18.191s, episode steps: 854, steps per second:  47, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.187 [0.000, 5.000],  loss: 0.014445, mae: 2.706070, mean_q: 3.256255, mean_eps: 0.182651
 4087914/6000000: episode: 5289, duration: 16.396s, episode steps: 742, steps per second:  45, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.522 [0.000, 5.000],  loss: 0.016301, mae: 2.713627, mean_q: 3.263936, mean_eps: 0.182492
 4088617/6000000: episode: 5290, duration: 15.070s, episode steps: 703, steps per second:  47, episode reward: 20.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.762 [0.000, 5.000],  loss: 0.015343, mae: 2.717491, mean_q: 3.268414, mean_eps: 0.182347
 4089674/6000000: episode: 5291, duration: 22.125s, episode steps: 1057, steps per second:  48, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.839 [0.000, 5.000],  loss: 0.015662, mae: 2.714530, mean_q: 3.265889, mean_eps: 0.182171
 4090611/6000000: episode: 5292, duration: 19.879s, episode steps: 937, steps per second:  47, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.853 [0.000, 5.000],  loss: 0.015607, mae: 2.728411, mean_q: 3.283131, mean_eps: 0.181972
 4091659/6000000: episode: 5293, duration: 23.791s, episode steps: 1048, steps per second:  44, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.775 [0.000, 5.000],  loss: 0.015396, mae: 2.722136, mean_q: 3.275793, mean_eps: 0.181773
 4092481/6000000: episode: 5294, duration: 17.277s, episode steps: 822, steps per second:  48, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.043 [0.000, 5.000],  loss: 0.015587, mae: 2.728197, mean_q: 3.283474, mean_eps: 0.181586
 4093002/6000000: episode: 5295, duration: 11.077s, episode steps: 521, steps per second:  47, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.326 [0.000, 5.000],  loss: 0.013265, mae: 2.730462, mean_q: 3.284607, mean_eps: 0.181452
 4093868/6000000: episode: 5296, duration: 18.469s, episode steps: 866, steps per second:  47, episode reward: 26.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.014050, mae: 2.733798, mean_q: 3.289439, mean_eps: 0.181313
 4095181/6000000: episode: 5297, duration: 28.507s, episode steps: 1313, steps per second:  46, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.103 [0.000, 5.000],  loss: 0.014985, mae: 2.727863, mean_q: 3.281773, mean_eps: 0.181095
 4095547/6000000: episode: 5298, duration: 7.837s, episode steps: 366, steps per second:  47, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.167 [0.000, 5.000],  loss: 0.014406, mae: 2.756508, mean_q: 3.316106, mean_eps: 0.180927
 4096435/6000000: episode: 5299, duration: 21.062s, episode steps: 888, steps per second:  42, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.014219, mae: 2.749852, mean_q: 3.307435, mean_eps: 0.180802
 4097076/6000000: episode: 5300, duration: 15.972s, episode steps: 641, steps per second:  40, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.015856, mae: 2.738429, mean_q: 3.293800, mean_eps: 0.180649
 4098118/6000000: episode: 5301, duration: 23.630s, episode steps: 1042, steps per second:  44, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.318 [0.000, 5.000],  loss: 0.014089, mae: 2.735004, mean_q: 3.291825, mean_eps: 0.180481
 4099792/6000000: episode: 5302, duration: 37.228s, episode steps: 1674, steps per second:  45, episode reward: 40.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.015232, mae: 2.728748, mean_q: 3.283379, mean_eps: 0.180209
 4100431/6000000: episode: 5303, duration: 14.102s, episode steps: 639, steps per second:  45, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.171 [0.000, 5.000],  loss: 0.015851, mae: 2.719355, mean_q: 3.272531, mean_eps: 0.179978
 4101266/6000000: episode: 5304, duration: 18.190s, episode steps: 835, steps per second:  46, episode reward: 25.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.546 [0.000, 5.000],  loss: 0.015352, mae: 2.721084, mean_q: 3.273317, mean_eps: 0.179830
 4102483/6000000: episode: 5305, duration: 28.282s, episode steps: 1217, steps per second:  43, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.273 [0.000, 5.000],  loss: 0.014676, mae: 2.712838, mean_q: 3.264855, mean_eps: 0.179625
 4103134/6000000: episode: 5306, duration: 14.157s, episode steps: 651, steps per second:  46, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.011 [0.000, 5.000],  loss: 0.016255, mae: 2.728929, mean_q: 3.282816, mean_eps: 0.179438
 4104022/6000000: episode: 5307, duration: 19.922s, episode steps: 888, steps per second:  45, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.923 [0.000, 5.000],  loss: 0.014576, mae: 2.689955, mean_q: 3.236427, mean_eps: 0.179284
 4104737/6000000: episode: 5308, duration: 15.891s, episode steps: 715, steps per second:  45, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.266 [0.000, 5.000],  loss: 0.014772, mae: 2.708588, mean_q: 3.257793, mean_eps: 0.179124
 4105981/6000000: episode: 5309, duration: 25.980s, episode steps: 1244, steps per second:  48, episode reward: 33.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.014 [0.000, 5.000],  loss: 0.014864, mae: 2.693252, mean_q: 3.239747, mean_eps: 0.178928
 4106876/6000000: episode: 5310, duration: 18.646s, episode steps: 895, steps per second:  48, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.185 [0.000, 5.000],  loss: 0.014634, mae: 2.702251, mean_q: 3.251162, mean_eps: 0.178714
 4107632/6000000: episode: 5311, duration: 16.467s, episode steps: 756, steps per second:  46, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.874 [0.000, 5.000],  loss: 0.015483, mae: 2.688902, mean_q: 3.234417, mean_eps: 0.178550
 4108970/6000000: episode: 5312, duration: 27.770s, episode steps: 1338, steps per second:  48, episode reward: 30.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.277 [0.000, 5.000],  loss: 0.015587, mae: 2.698201, mean_q: 3.245248, mean_eps: 0.178340
 4110079/6000000: episode: 5313, duration: 23.600s, episode steps: 1109, steps per second:  47, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.867 [0.000, 5.000],  loss: 0.015206, mae: 2.693250, mean_q: 3.242393, mean_eps: 0.178095
 4110810/6000000: episode: 5314, duration: 15.838s, episode steps: 731, steps per second:  46, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.108 [0.000, 5.000],  loss: 0.014721, mae: 2.734453, mean_q: 3.290436, mean_eps: 0.177911
 4111692/6000000: episode: 5315, duration: 20.489s, episode steps: 882, steps per second:  43, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.786 [0.000, 5.000],  loss: 0.014123, mae: 2.744464, mean_q: 3.302456, mean_eps: 0.177750
 4112342/6000000: episode: 5316, duration: 15.488s, episode steps: 650, steps per second:  42, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.897 [0.000, 5.000],  loss: 0.015359, mae: 2.755757, mean_q: 3.314373, mean_eps: 0.177597
 4112976/6000000: episode: 5317, duration: 16.690s, episode steps: 634, steps per second:  38, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.472 [0.000, 5.000],  loss: 0.015178, mae: 2.731079, mean_q: 3.287054, mean_eps: 0.177468
 4113750/6000000: episode: 5318, duration: 18.726s, episode steps: 774, steps per second:  41, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.891 [0.000, 5.000],  loss: 0.014988, mae: 2.736632, mean_q: 3.292783, mean_eps: 0.177328
 4114853/6000000: episode: 5319, duration: 24.665s, episode steps: 1103, steps per second:  45, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.899 [0.000, 5.000],  loss: 0.015160, mae: 2.733567, mean_q: 3.289027, mean_eps: 0.177140
 4115714/6000000: episode: 5320, duration: 18.934s, episode steps: 861, steps per second:  45, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.016852, mae: 2.722655, mean_q: 3.276700, mean_eps: 0.176943
 4116653/6000000: episode: 5321, duration: 19.830s, episode steps: 939, steps per second:  47, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.118 [0.000, 5.000],  loss: 0.016136, mae: 2.735313, mean_q: 3.289940, mean_eps: 0.176763
 4117307/6000000: episode: 5322, duration: 14.115s, episode steps: 654, steps per second:  46, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.804 [0.000, 5.000],  loss: 0.013933, mae: 2.731614, mean_q: 3.290090, mean_eps: 0.176604
 4118185/6000000: episode: 5323, duration: 20.377s, episode steps: 878, steps per second:  43, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.014274, mae: 2.703035, mean_q: 3.252581, mean_eps: 0.176451
 4119132/6000000: episode: 5324, duration: 21.139s, episode steps: 947, steps per second:  45, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.108 [0.000, 5.000],  loss: 0.014584, mae: 2.738167, mean_q: 3.294446, mean_eps: 0.176268
 4119734/6000000: episode: 5325, duration: 13.430s, episode steps: 602, steps per second:  45, episode reward: 17.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.055 [0.000, 5.000],  loss: 0.012410, mae: 2.704571, mean_q: 3.254347, mean_eps: 0.176114
 4120380/6000000: episode: 5326, duration: 14.918s, episode steps: 646, steps per second:  43, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.797 [0.000, 5.000],  loss: 0.015048, mae: 2.696719, mean_q: 3.243254, mean_eps: 0.175989
 4121577/6000000: episode: 5327, duration: 27.062s, episode steps: 1197, steps per second:  44, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.014471, mae: 2.700888, mean_q: 3.250705, mean_eps: 0.175804
 4122653/6000000: episode: 5328, duration: 22.035s, episode steps: 1076, steps per second:  49, episode reward: 30.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.118 [0.000, 5.000],  loss: 0.016267, mae: 2.695155, mean_q: 3.242450, mean_eps: 0.175577
 4123400/6000000: episode: 5329, duration: 16.064s, episode steps: 747, steps per second:  47, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.797 [0.000, 5.000],  loss: 0.014473, mae: 2.677314, mean_q: 3.222660, mean_eps: 0.175395
 4124070/6000000: episode: 5330, duration: 14.632s, episode steps: 670, steps per second:  46, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.017407, mae: 2.703203, mean_q: 3.253493, mean_eps: 0.175253
 4125095/6000000: episode: 5331, duration: 22.076s, episode steps: 1025, steps per second:  46, episode reward: 29.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.837 [0.000, 5.000],  loss: 0.014297, mae: 2.682872, mean_q: 3.227536, mean_eps: 0.175084
 4125728/6000000: episode: 5332, duration: 13.833s, episode steps: 633, steps per second:  46, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.585 [0.000, 5.000],  loss: 0.015423, mae: 2.686664, mean_q: 3.230839, mean_eps: 0.174918
 4126452/6000000: episode: 5333, duration: 16.227s, episode steps: 724, steps per second:  45, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.146 [0.000, 5.000],  loss: 0.015271, mae: 2.689667, mean_q: 3.235345, mean_eps: 0.174782
 4127199/6000000: episode: 5334, duration: 16.749s, episode steps: 747, steps per second:  45, episode reward: 24.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 1.989 [0.000, 5.000],  loss: 0.014904, mae: 2.688270, mean_q: 3.235901, mean_eps: 0.174635
 4128075/6000000: episode: 5335, duration: 19.864s, episode steps: 876, steps per second:  44, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.928 [0.000, 5.000],  loss: 0.014099, mae: 2.686857, mean_q: 3.233476, mean_eps: 0.174473
 4128817/6000000: episode: 5336, duration: 18.992s, episode steps: 742, steps per second:  39, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.937 [0.000, 5.000],  loss: 0.014101, mae: 2.689070, mean_q: 3.235244, mean_eps: 0.174311
 4129922/6000000: episode: 5337, duration: 26.432s, episode steps: 1105, steps per second:  42, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.258 [0.000, 5.000],  loss: 0.015325, mae: 2.676774, mean_q: 3.221767, mean_eps: 0.174126
 4130842/6000000: episode: 5338, duration: 20.210s, episode steps: 920, steps per second:  46, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.747 [0.000, 5.000],  loss: 0.013862, mae: 2.675957, mean_q: 3.221361, mean_eps: 0.173924
 4131438/6000000: episode: 5339, duration: 14.981s, episode steps: 596, steps per second:  40, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.015849, mae: 2.672189, mean_q: 3.216061, mean_eps: 0.173772
 4131883/6000000: episode: 5340, duration: 10.322s, episode steps: 445, steps per second:  43, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.476 [0.000, 5.000],  loss: 0.016222, mae: 2.681831, mean_q: 3.229159, mean_eps: 0.173668
 4132709/6000000: episode: 5341, duration: 17.676s, episode steps: 826, steps per second:  47, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.994 [0.000, 5.000],  loss: 0.015083, mae: 2.682711, mean_q: 3.227245, mean_eps: 0.173541
 4133221/6000000: episode: 5342, duration: 11.075s, episode steps: 512, steps per second:  46, episode reward: 13.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.771 [0.000, 5.000],  loss: 0.015726, mae: 2.699048, mean_q: 3.246384, mean_eps: 0.173407
 4133990/6000000: episode: 5343, duration: 17.551s, episode steps: 769, steps per second:  44, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.014633, mae: 2.687501, mean_q: 3.233165, mean_eps: 0.173279
 4134838/6000000: episode: 5344, duration: 19.701s, episode steps: 848, steps per second:  43, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.012 [0.000, 5.000],  loss: 0.014965, mae: 2.704043, mean_q: 3.253380, mean_eps: 0.173117
 4135622/6000000: episode: 5345, duration: 17.687s, episode steps: 784, steps per second:  44, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.966 [0.000, 5.000],  loss: 0.016587, mae: 2.678524, mean_q: 3.221592, mean_eps: 0.172954
 4136563/6000000: episode: 5346, duration: 21.438s, episode steps: 941, steps per second:  44, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.918 [0.000, 5.000],  loss: 0.014687, mae: 2.658670, mean_q: 3.199283, mean_eps: 0.172782
 4137554/6000000: episode: 5347, duration: 22.237s, episode steps: 991, steps per second:  45, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.144 [0.000, 5.000],  loss: 0.015329, mae: 2.677132, mean_q: 3.220869, mean_eps: 0.172588
 4138389/6000000: episode: 5348, duration: 17.788s, episode steps: 835, steps per second:  47, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.825 [0.000, 5.000],  loss: 0.015157, mae: 2.667733, mean_q: 3.209415, mean_eps: 0.172406
 4139435/6000000: episode: 5349, duration: 22.450s, episode steps: 1046, steps per second:  47, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.235 [0.000, 5.000],  loss: 0.015053, mae: 2.663524, mean_q: 3.203859, mean_eps: 0.172218
 4139986/6000000: episode: 5350, duration: 11.986s, episode steps: 551, steps per second:  46, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.626 [0.000, 5.000],  loss: 0.015129, mae: 2.639561, mean_q: 3.175809, mean_eps: 0.172058
 4140965/6000000: episode: 5351, duration: 20.901s, episode steps: 979, steps per second:  47, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: 0.015421, mae: 2.668295, mean_q: 3.209067, mean_eps: 0.171905
 4141888/6000000: episode: 5352, duration: 20.113s, episode steps: 923, steps per second:  46, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.997 [0.000, 5.000],  loss: 0.015581, mae: 2.672592, mean_q: 3.213592, mean_eps: 0.171715
 4142571/6000000: episode: 5353, duration: 14.984s, episode steps: 683, steps per second:  46, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.142 [0.000, 5.000],  loss: 0.015830, mae: 2.655603, mean_q: 3.193264, mean_eps: 0.171554
 4143751/6000000: episode: 5354, duration: 26.855s, episode steps: 1180, steps per second:  44, episode reward: 30.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.226 [0.000, 5.000],  loss: 0.016784, mae: 2.678089, mean_q: 3.221286, mean_eps: 0.171368
 4144707/6000000: episode: 5355, duration: 23.035s, episode steps: 956, steps per second:  42, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.112 [0.000, 5.000],  loss: 0.014288, mae: 2.670821, mean_q: 3.213487, mean_eps: 0.171154
 4145950/6000000: episode: 5356, duration: 29.661s, episode steps: 1243, steps per second:  42, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.693 [0.000, 5.000],  loss: 0.015464, mae: 2.666804, mean_q: 3.207932, mean_eps: 0.170934
 4146637/6000000: episode: 5357, duration: 15.317s, episode steps: 687, steps per second:  45, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.266 [0.000, 5.000],  loss: 0.015968, mae: 2.674204, mean_q: 3.216044, mean_eps: 0.170741
 4147359/6000000: episode: 5358, duration: 15.992s, episode steps: 722, steps per second:  45, episode reward: 20.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.181 [0.000, 5.000],  loss: 0.012897, mae: 2.655122, mean_q: 3.193751, mean_eps: 0.170600
 4148553/6000000: episode: 5359, duration: 26.763s, episode steps: 1194, steps per second:  45, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.915 [0.000, 5.000],  loss: 0.015474, mae: 2.650427, mean_q: 3.188824, mean_eps: 0.170409
 4149349/6000000: episode: 5360, duration: 17.393s, episode steps: 796, steps per second:  46, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.117 [0.000, 5.000],  loss: 0.014177, mae: 2.649265, mean_q: 3.189165, mean_eps: 0.170210
 4150214/6000000: episode: 5361, duration: 20.648s, episode steps: 865, steps per second:  42, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.113 [0.000, 5.000],  loss: 0.014699, mae: 2.661238, mean_q: 3.202263, mean_eps: 0.170044
 4150895/6000000: episode: 5362, duration: 15.645s, episode steps: 681, steps per second:  44, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.884 [0.000, 5.000],  loss: 0.014940, mae: 2.636213, mean_q: 3.170710, mean_eps: 0.169889
 4151879/6000000: episode: 5363, duration: 21.911s, episode steps: 984, steps per second:  45, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.159 [0.000, 5.000],  loss: 0.015324, mae: 2.642197, mean_q: 3.179488, mean_eps: 0.169723
 4153031/6000000: episode: 5364, duration: 25.943s, episode steps: 1152, steps per second:  44, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.677 [0.000, 5.000],  loss: 0.016101, mae: 2.647046, mean_q: 3.183415, mean_eps: 0.169509
 4153871/6000000: episode: 5365, duration: 18.585s, episode steps: 840, steps per second:  45, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.015902, mae: 2.636407, mean_q: 3.170395, mean_eps: 0.169310
 4154502/6000000: episode: 5366, duration: 13.165s, episode steps: 631, steps per second:  48, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.553 [0.000, 5.000],  loss: 0.015492, mae: 2.636394, mean_q: 3.171650, mean_eps: 0.169163
 4155121/6000000: episode: 5367, duration: 14.141s, episode steps: 619, steps per second:  44, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.872 [0.000, 5.000],  loss: 0.014787, mae: 2.632287, mean_q: 3.165756, mean_eps: 0.169038
 4155861/6000000: episode: 5368, duration: 17.513s, episode steps: 740, steps per second:  42, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.015492, mae: 2.632726, mean_q: 3.166861, mean_eps: 0.168902
 4156488/6000000: episode: 5369, duration: 14.427s, episode steps: 627, steps per second:  43, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.809 [0.000, 5.000],  loss: 0.014706, mae: 2.633546, mean_q: 3.166527, mean_eps: 0.168765
 4157238/6000000: episode: 5370, duration: 16.744s, episode steps: 750, steps per second:  45, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.016002, mae: 2.604955, mean_q: 3.133038, mean_eps: 0.168628
 4158073/6000000: episode: 5371, duration: 18.560s, episode steps: 835, steps per second:  45, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.237 [0.000, 5.000],  loss: 0.014342, mae: 2.626455, mean_q: 3.158589, mean_eps: 0.168469
 4158562/6000000: episode: 5372, duration: 10.848s, episode steps: 489, steps per second:  45, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.013918, mae: 2.618482, mean_q: 3.149960, mean_eps: 0.168336
 4159553/6000000: episode: 5373, duration: 22.650s, episode steps: 991, steps per second:  44, episode reward: 30.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.141 [0.000, 5.000],  loss: 0.014709, mae: 2.622562, mean_q: 3.154732, mean_eps: 0.168188
 4160325/6000000: episode: 5374, duration: 18.614s, episode steps: 772, steps per second:  41, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.014473, mae: 2.630120, mean_q: 3.163271, mean_eps: 0.168012
 4160809/6000000: episode: 5375, duration: 11.753s, episode steps: 484, steps per second:  41, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.599 [0.000, 5.000],  loss: 0.013653, mae: 2.652945, mean_q: 3.192404, mean_eps: 0.167886
 4161471/6000000: episode: 5376, duration: 15.861s, episode steps: 662, steps per second:  42, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.941 [0.000, 5.000],  loss: 0.015601, mae: 2.645258, mean_q: 3.181971, mean_eps: 0.167772
 4162567/6000000: episode: 5377, duration: 24.279s, episode steps: 1096, steps per second:  45, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.903 [0.000, 5.000],  loss: 0.014981, mae: 2.651032, mean_q: 3.188322, mean_eps: 0.167596
 4163478/6000000: episode: 5378, duration: 20.821s, episode steps: 911, steps per second:  44, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.681 [0.000, 5.000],  loss: 0.014925, mae: 2.646335, mean_q: 3.182423, mean_eps: 0.167396
 4164214/6000000: episode: 5379, duration: 16.998s, episode steps: 736, steps per second:  43, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.099 [0.000, 5.000],  loss: 0.016219, mae: 2.666714, mean_q: 3.205105, mean_eps: 0.167231
 4164880/6000000: episode: 5380, duration: 15.133s, episode steps: 666, steps per second:  44, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.194 [0.000, 5.000],  loss: 0.015662, mae: 2.644266, mean_q: 3.181165, mean_eps: 0.167091
 4165473/6000000: episode: 5381, duration: 13.489s, episode steps: 593, steps per second:  44, episode reward: 15.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.189 [0.000, 5.000],  loss: 0.014634, mae: 2.627082, mean_q: 3.159780, mean_eps: 0.166965
 4166498/6000000: episode: 5382, duration: 24.905s, episode steps: 1025, steps per second:  41, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.110 [0.000, 5.000],  loss: 0.014796, mae: 2.654835, mean_q: 3.192669, mean_eps: 0.166803
 4167313/6000000: episode: 5383, duration: 17.895s, episode steps: 815, steps per second:  46, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.918 [0.000, 5.000],  loss: 0.014493, mae: 2.656670, mean_q: 3.195207, mean_eps: 0.166619
 4168113/6000000: episode: 5384, duration: 17.447s, episode steps: 800, steps per second:  46, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.206 [0.000, 5.000],  loss: 0.015602, mae: 2.631855, mean_q: 3.164310, mean_eps: 0.166457
 4169062/6000000: episode: 5385, duration: 20.693s, episode steps: 949, steps per second:  46, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.773 [0.000, 5.000],  loss: 0.015022, mae: 2.647512, mean_q: 3.185420, mean_eps: 0.166282
 4169882/6000000: episode: 5386, duration: 17.310s, episode steps: 820, steps per second:  47, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.013716, mae: 2.664265, mean_q: 3.205958, mean_eps: 0.166106
 4170513/6000000: episode: 5387, duration: 13.274s, episode steps: 631, steps per second:  48, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.837 [0.000, 5.000],  loss: 0.016293, mae: 2.665511, mean_q: 3.206981, mean_eps: 0.165960
 4171727/6000000: episode: 5388, duration: 25.761s, episode steps: 1214, steps per second:  47, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.909 [0.000, 5.000],  loss: 0.015677, mae: 2.645681, mean_q: 3.183693, mean_eps: 0.165776
 4172627/6000000: episode: 5389, duration: 19.655s, episode steps: 900, steps per second:  46, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.012894, mae: 2.642000, mean_q: 3.179767, mean_eps: 0.165565
 4173597/6000000: episode: 5390, duration: 21.778s, episode steps: 970, steps per second:  45, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.028 [0.000, 5.000],  loss: 0.015477, mae: 2.651088, mean_q: 3.188495, mean_eps: 0.165378
 4174369/6000000: episode: 5391, duration: 17.353s, episode steps: 772, steps per second:  44, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.313 [0.000, 5.000],  loss: 0.013820, mae: 2.625716, mean_q: 3.159860, mean_eps: 0.165203
 4175389/6000000: episode: 5392, duration: 22.618s, episode steps: 1020, steps per second:  45, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.992 [0.000, 5.000],  loss: 0.013840, mae: 2.643133, mean_q: 3.178459, mean_eps: 0.165024
 4176039/6000000: episode: 5393, duration: 14.682s, episode steps: 650, steps per second:  44, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.623 [0.000, 5.000],  loss: 0.013839, mae: 2.646833, mean_q: 3.183853, mean_eps: 0.164857
 4176837/6000000: episode: 5394, duration: 19.601s, episode steps: 798, steps per second:  41, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.630 [0.000, 5.000],  loss: 0.015036, mae: 2.619039, mean_q: 3.150165, mean_eps: 0.164712
 4178168/6000000: episode: 5395, duration: 30.412s, episode steps: 1331, steps per second:  44, episode reward: 34.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.014215, mae: 2.642647, mean_q: 3.178457, mean_eps: 0.164500
 4179079/6000000: episode: 5396, duration: 20.083s, episode steps: 911, steps per second:  45, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.903 [0.000, 5.000],  loss: 0.015244, mae: 2.634447, mean_q: 3.168348, mean_eps: 0.164276
 4179573/6000000: episode: 5397, duration: 11.236s, episode steps: 494, steps per second:  44, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.296 [0.000, 5.000],  loss: 0.014603, mae: 2.639390, mean_q: 3.173953, mean_eps: 0.164135
 4180512/6000000: episode: 5398, duration: 20.372s, episode steps: 939, steps per second:  46, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.776 [0.000, 5.000],  loss: 0.014335, mae: 2.655765, mean_q: 3.194788, mean_eps: 0.163992
 4181498/6000000: episode: 5399, duration: 22.026s, episode steps: 986, steps per second:  45, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.164 [0.000, 5.000],  loss: 0.014563, mae: 2.660165, mean_q: 3.198717, mean_eps: 0.163799
 4182322/6000000: episode: 5400, duration: 18.345s, episode steps: 824, steps per second:  45, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.271 [0.000, 5.000],  loss: 0.014767, mae: 2.662986, mean_q: 3.203694, mean_eps: 0.163618
 4183360/6000000: episode: 5401, duration: 23.380s, episode steps: 1038, steps per second:  44, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.890 [0.000, 5.000],  loss: 0.015572, mae: 2.658306, mean_q: 3.197993, mean_eps: 0.163432
 4184639/6000000: episode: 5402, duration: 28.926s, episode steps: 1279, steps per second:  44, episode reward: 33.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.668 [0.000, 5.000],  loss: 0.014880, mae: 2.654564, mean_q: 3.194407, mean_eps: 0.163200
 4185176/6000000: episode: 5403, duration: 12.058s, episode steps: 537, steps per second:  45, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.912 [0.000, 5.000],  loss: 0.015289, mae: 2.658888, mean_q: 3.198403, mean_eps: 0.163019
 4186319/6000000: episode: 5404, duration: 24.046s, episode steps: 1143, steps per second:  48, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.927 [0.000, 5.000],  loss: 0.015328, mae: 2.658454, mean_q: 3.197503, mean_eps: 0.162851
 4187294/6000000: episode: 5405, duration: 20.599s, episode steps: 975, steps per second:  47, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.014804, mae: 2.645349, mean_q: 3.183329, mean_eps: 0.162639
 4188631/6000000: episode: 5406, duration: 29.288s, episode steps: 1337, steps per second:  46, episode reward: 33.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.071 [0.000, 5.000],  loss: 0.015091, mae: 2.659494, mean_q: 3.199892, mean_eps: 0.162408
 4189375/6000000: episode: 5407, duration: 16.054s, episode steps: 744, steps per second:  46, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.055 [0.000, 5.000],  loss: 0.014114, mae: 2.661542, mean_q: 3.202244, mean_eps: 0.162200
 4190252/6000000: episode: 5408, duration: 18.858s, episode steps: 877, steps per second:  47, episode reward: 27.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.918 [0.000, 5.000],  loss: 0.014009, mae: 2.643348, mean_q: 3.181609, mean_eps: 0.162038
 4191237/6000000: episode: 5409, duration: 21.732s, episode steps: 985, steps per second:  45, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.889 [0.000, 5.000],  loss: 0.014396, mae: 2.633755, mean_q: 3.168525, mean_eps: 0.161851
 4192112/6000000: episode: 5410, duration: 20.025s, episode steps: 875, steps per second:  44, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.013882, mae: 2.642994, mean_q: 3.180362, mean_eps: 0.161665
 4193141/6000000: episode: 5411, duration: 26.676s, episode steps: 1029, steps per second:  39, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.013756, mae: 2.631833, mean_q: 3.165206, mean_eps: 0.161475
 4194067/6000000: episode: 5412, duration: 22.530s, episode steps: 926, steps per second:  41, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.960 [0.000, 5.000],  loss: 0.015077, mae: 2.648558, mean_q: 3.185754, mean_eps: 0.161279
 4195086/6000000: episode: 5413, duration: 23.311s, episode steps: 1019, steps per second:  44, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.085 [0.000, 5.000],  loss: 0.014306, mae: 2.626767, mean_q: 3.161429, mean_eps: 0.161085
 4196269/6000000: episode: 5414, duration: 26.970s, episode steps: 1183, steps per second:  44, episode reward: 32.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.182 [0.000, 5.000],  loss: 0.014790, mae: 2.627605, mean_q: 3.161129, mean_eps: 0.160864
 4197191/6000000: episode: 5415, duration: 19.861s, episode steps: 922, steps per second:  46, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.653 [0.000, 5.000],  loss: 0.014778, mae: 2.617433, mean_q: 3.149945, mean_eps: 0.160654
 4198441/6000000: episode: 5416, duration: 29.045s, episode steps: 1250, steps per second:  43, episode reward: 32.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.981 [0.000, 5.000],  loss: 0.014831, mae: 2.642242, mean_q: 3.178309, mean_eps: 0.160437
 4199172/6000000: episode: 5417, duration: 15.851s, episode steps: 731, steps per second:  46, episode reward: 22.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.013835, mae: 2.603761, mean_q: 3.133218, mean_eps: 0.160239
 4200122/6000000: episode: 5418, duration: 20.567s, episode steps: 950, steps per second:  46, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.741 [0.000, 5.000],  loss: 0.015927, mae: 2.628846, mean_q: 3.165484, mean_eps: 0.160071
 4200762/6000000: episode: 5419, duration: 13.586s, episode steps: 640, steps per second:  47, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.889 [0.000, 5.000],  loss: 0.015751, mae: 2.629286, mean_q: 3.164632, mean_eps: 0.159912
 4201353/6000000: episode: 5420, duration: 13.004s, episode steps: 591, steps per second:  45, episode reward: 18.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.235 [0.000, 5.000],  loss: 0.017222, mae: 2.600669, mean_q: 3.127357, mean_eps: 0.159788
 4202411/6000000: episode: 5421, duration: 22.517s, episode steps: 1058, steps per second:  47, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.756 [0.000, 5.000],  loss: 0.015006, mae: 2.616257, mean_q: 3.147964, mean_eps: 0.159624
 4203176/6000000: episode: 5422, duration: 16.581s, episode steps: 765, steps per second:  46, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.005 [0.000, 5.000],  loss: 0.015383, mae: 2.600028, mean_q: 3.129149, mean_eps: 0.159442
 4203786/6000000: episode: 5423, duration: 13.309s, episode steps: 610, steps per second:  46, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.833 [0.000, 5.000],  loss: 0.015475, mae: 2.611236, mean_q: 3.139847, mean_eps: 0.159304
 4204675/6000000: episode: 5424, duration: 19.586s, episode steps: 889, steps per second:  45, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.015145, mae: 2.612348, mean_q: 3.142100, mean_eps: 0.159154
 4205882/6000000: episode: 5425, duration: 26.296s, episode steps: 1207, steps per second:  46, episode reward: 34.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.115 [0.000, 5.000],  loss: 0.016405, mae: 2.614023, mean_q: 3.143906, mean_eps: 0.158944
 4206474/6000000: episode: 5426, duration: 13.014s, episode steps: 592, steps per second:  45, episode reward: 15.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.066 [0.000, 5.000],  loss: 0.015482, mae: 2.606375, mean_q: 3.137442, mean_eps: 0.158764
 4207350/6000000: episode: 5427, duration: 19.805s, episode steps: 876, steps per second:  44, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.200 [0.000, 5.000],  loss: 0.014999, mae: 2.626563, mean_q: 3.159749, mean_eps: 0.158618
 4208393/6000000: episode: 5428, duration: 24.187s, episode steps: 1043, steps per second:  43, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.822 [0.000, 5.000],  loss: 0.014418, mae: 2.626520, mean_q: 3.162448, mean_eps: 0.158426
 4209213/6000000: episode: 5429, duration: 19.125s, episode steps: 820, steps per second:  43, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.017638, mae: 2.621798, mean_q: 3.153253, mean_eps: 0.158239
 4209857/6000000: episode: 5430, duration: 15.303s, episode steps: 644, steps per second:  42, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.051 [0.000, 5.000],  loss: 0.014238, mae: 2.616065, mean_q: 3.147785, mean_eps: 0.158093
 4210704/6000000: episode: 5431, duration: 18.682s, episode steps: 847, steps per second:  45, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.014950, mae: 2.601769, mean_q: 3.132502, mean_eps: 0.157944
 4212319/6000000: episode: 5432, duration: 37.036s, episode steps: 1615, steps per second:  44, episode reward: 40.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.196 [0.000, 5.000],  loss: 0.013053, mae: 2.611724, mean_q: 3.143876, mean_eps: 0.157698
 4213175/6000000: episode: 5433, duration: 18.760s, episode steps: 856, steps per second:  46, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.750 [0.000, 5.000],  loss: 0.014646, mae: 2.610640, mean_q: 3.140530, mean_eps: 0.157451
 4214045/6000000: episode: 5434, duration: 19.161s, episode steps: 870, steps per second:  45, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.016250, mae: 2.620144, mean_q: 3.152138, mean_eps: 0.157278
 4215386/6000000: episode: 5435, duration: 30.114s, episode steps: 1341, steps per second:  45, episode reward: 31.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.134 [0.000, 5.000],  loss: 0.014531, mae: 2.616920, mean_q: 3.149189, mean_eps: 0.157057
 4216295/6000000: episode: 5436, duration: 19.788s, episode steps: 909, steps per second:  46, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.098 [0.000, 5.000],  loss: 0.013172, mae: 2.604215, mean_q: 3.132931, mean_eps: 0.156832
 4217078/6000000: episode: 5437, duration: 17.039s, episode steps: 783, steps per second:  46, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.727 [0.000, 5.000],  loss: 0.014925, mae: 2.635063, mean_q: 3.169446, mean_eps: 0.156663
 4217943/6000000: episode: 5438, duration: 18.304s, episode steps: 865, steps per second:  47, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.014657, mae: 2.634828, mean_q: 3.169275, mean_eps: 0.156498
 4218866/6000000: episode: 5439, duration: 19.185s, episode steps: 923, steps per second:  48, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.111 [0.000, 5.000],  loss: 0.014912, mae: 2.626714, mean_q: 3.159148, mean_eps: 0.156319
 4219770/6000000: episode: 5440, duration: 19.462s, episode steps: 904, steps per second:  46, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: 0.014303, mae: 2.610981, mean_q: 3.141726, mean_eps: 0.156136
 4220592/6000000: episode: 5441, duration: 17.608s, episode steps: 822, steps per second:  47, episode reward: 24.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.182 [0.000, 5.000],  loss: 0.012790, mae: 2.647585, mean_q: 3.184693, mean_eps: 0.155964
 4221647/6000000: episode: 5442, duration: 22.993s, episode steps: 1055, steps per second:  46, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.966 [0.000, 5.000],  loss: 0.013133, mae: 2.640454, mean_q: 3.178423, mean_eps: 0.155776
 4223051/6000000: episode: 5443, duration: 30.482s, episode steps: 1404, steps per second:  46, episode reward: 33.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.014272, mae: 2.642840, mean_q: 3.179630, mean_eps: 0.155530
 4223850/6000000: episode: 5444, duration: 18.919s, episode steps: 799, steps per second:  42, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.160 [0.000, 5.000],  loss: 0.017006, mae: 2.650295, mean_q: 3.187384, mean_eps: 0.155310
 4224283/6000000: episode: 5445, duration: 9.767s, episode steps: 433, steps per second:  44, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.857 [0.000, 5.000],  loss: 0.013869, mae: 2.664063, mean_q: 3.205223, mean_eps: 0.155187
 4225385/6000000: episode: 5446, duration: 27.542s, episode steps: 1102, steps per second:  40, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.918 [0.000, 5.000],  loss: 0.013915, mae: 2.644616, mean_q: 3.181819, mean_eps: 0.155033
 4226493/6000000: episode: 5447, duration: 25.358s, episode steps: 1108, steps per second:  44, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.705 [0.000, 5.000],  loss: 0.015345, mae: 2.643799, mean_q: 3.181957, mean_eps: 0.154812
 4227248/6000000: episode: 5448, duration: 16.797s, episode steps: 755, steps per second:  45, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.013071, mae: 2.656479, mean_q: 3.197130, mean_eps: 0.154626
 4228509/6000000: episode: 5449, duration: 28.404s, episode steps: 1261, steps per second:  44, episode reward: 32.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.215 [0.000, 5.000],  loss: 0.013629, mae: 2.639812, mean_q: 3.175936, mean_eps: 0.154424
 4229642/6000000: episode: 5450, duration: 24.979s, episode steps: 1133, steps per second:  45, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.013355, mae: 2.640462, mean_q: 3.177701, mean_eps: 0.154185
 4230726/6000000: episode: 5451, duration: 25.247s, episode steps: 1084, steps per second:  43, episode reward: 31.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.991 [0.000, 5.000],  loss: 0.014744, mae: 2.643898, mean_q: 3.180757, mean_eps: 0.153963
 4231548/6000000: episode: 5452, duration: 18.286s, episode steps: 822, steps per second:  45, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.263 [0.000, 5.000],  loss: 0.013410, mae: 2.650980, mean_q: 3.189478, mean_eps: 0.153773
 4232805/6000000: episode: 5453, duration: 27.386s, episode steps: 1257, steps per second:  46, episode reward: 30.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.014154, mae: 2.647290, mean_q: 3.184855, mean_eps: 0.153565
 4233650/6000000: episode: 5454, duration: 18.718s, episode steps: 845, steps per second:  45, episode reward: 25.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.262 [0.000, 5.000],  loss: 0.016351, mae: 2.639628, mean_q: 3.174456, mean_eps: 0.153354
 4234405/6000000: episode: 5455, duration: 16.288s, episode steps: 755, steps per second:  46, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.014590, mae: 2.619650, mean_q: 3.152298, mean_eps: 0.153194
 4235062/6000000: episode: 5456, duration: 13.691s, episode steps: 657, steps per second:  48, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.686 [0.000, 5.000],  loss: 0.013256, mae: 2.616963, mean_q: 3.149294, mean_eps: 0.153053
 4235546/6000000: episode: 5457, duration: 10.670s, episode steps: 484, steps per second:  45, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.012 [0.000, 5.000],  loss: 0.014730, mae: 2.651010, mean_q: 3.192181, mean_eps: 0.152939
 4236423/6000000: episode: 5458, duration: 19.706s, episode steps: 877, steps per second:  45, episode reward: 27.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.647 [0.000, 5.000],  loss: 0.014045, mae: 2.614983, mean_q: 3.148708, mean_eps: 0.152803
 4237303/6000000: episode: 5459, duration: 18.694s, episode steps: 880, steps per second:  47, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.015163, mae: 2.650102, mean_q: 3.189347, mean_eps: 0.152628
 4238197/6000000: episode: 5460, duration: 19.009s, episode steps: 894, steps per second:  47, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.015631, mae: 2.629845, mean_q: 3.164627, mean_eps: 0.152450
 4239642/6000000: episode: 5461, duration: 31.371s, episode steps: 1445, steps per second:  46, episode reward: 30.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.014079, mae: 2.627140, mean_q: 3.159928, mean_eps: 0.152216
 4240851/6000000: episode: 5462, duration: 28.204s, episode steps: 1209, steps per second:  43, episode reward: 30.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.990 [0.000, 5.000],  loss: 0.014145, mae: 2.615921, mean_q: 3.146113, mean_eps: 0.151951
 4241572/6000000: episode: 5463, duration: 17.785s, episode steps: 721, steps per second:  41, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.895 [0.000, 5.000],  loss: 0.013928, mae: 2.590630, mean_q: 3.116720, mean_eps: 0.151758
 4242153/6000000: episode: 5464, duration: 13.459s, episode steps: 581, steps per second:  43, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.707 [0.000, 5.000],  loss: 0.013418, mae: 2.619048, mean_q: 3.151015, mean_eps: 0.151628
 4243063/6000000: episode: 5465, duration: 20.055s, episode steps: 910, steps per second:  45, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.004 [0.000, 5.000],  loss: 0.014000, mae: 2.636567, mean_q: 3.172881, mean_eps: 0.151478
 4243799/6000000: episode: 5466, duration: 15.993s, episode steps: 736, steps per second:  46, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.655 [0.000, 5.000],  loss: 0.013374, mae: 2.610402, mean_q: 3.141729, mean_eps: 0.151314
 4244584/6000000: episode: 5467, duration: 17.253s, episode steps: 785, steps per second:  45, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.014160, mae: 2.630302, mean_q: 3.163595, mean_eps: 0.151162
 4245295/6000000: episode: 5468, duration: 15.057s, episode steps: 711, steps per second:  47, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.014610, mae: 2.615591, mean_q: 3.145843, mean_eps: 0.151012
 4246276/6000000: episode: 5469, duration: 21.822s, episode steps: 981, steps per second:  45, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.942 [0.000, 5.000],  loss: 0.014041, mae: 2.605963, mean_q: 3.135961, mean_eps: 0.150843
 4246821/6000000: episode: 5470, duration: 12.536s, episode steps: 545, steps per second:  43, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.881 [0.000, 5.000],  loss: 0.014638, mae: 2.592127, mean_q: 3.118123, mean_eps: 0.150690
 4247558/6000000: episode: 5471, duration: 16.575s, episode steps: 737, steps per second:  44, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.630 [0.000, 5.000],  loss: 0.014446, mae: 2.606592, mean_q: 3.136395, mean_eps: 0.150562
 4248270/6000000: episode: 5472, duration: 15.914s, episode steps: 712, steps per second:  45, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.910 [0.000, 5.000],  loss: 0.016283, mae: 2.600557, mean_q: 3.127494, mean_eps: 0.150417
 4249046/6000000: episode: 5473, duration: 17.084s, episode steps: 776, steps per second:  45, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.776 [0.000, 5.000],  loss: 0.013254, mae: 2.583723, mean_q: 3.109929, mean_eps: 0.150268
 4249908/6000000: episode: 5474, duration: 18.996s, episode steps: 862, steps per second:  45, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.079 [0.000, 5.000],  loss: 0.014360, mae: 2.607713, mean_q: 3.137288, mean_eps: 0.150105
 4250964/6000000: episode: 5475, duration: 21.678s, episode steps: 1056, steps per second:  49, episode reward: 30.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.784 [0.000, 5.000],  loss: 0.014305, mae: 2.605148, mean_q: 3.134684, mean_eps: 0.149913
 4251645/6000000: episode: 5476, duration: 14.315s, episode steps: 681, steps per second:  48, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.643 [0.000, 5.000],  loss: 0.013331, mae: 2.601384, mean_q: 3.130832, mean_eps: 0.149739
 4252925/6000000: episode: 5477, duration: 27.488s, episode steps: 1280, steps per second:  47, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.621 [0.000, 5.000],  loss: 0.014652, mae: 2.605040, mean_q: 3.134660, mean_eps: 0.149543
 4253875/6000000: episode: 5478, duration: 19.960s, episode steps: 950, steps per second:  48, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.729 [0.000, 5.000],  loss: 0.015236, mae: 2.584058, mean_q: 3.109328, mean_eps: 0.149320
 4254793/6000000: episode: 5479, duration: 19.339s, episode steps: 918, steps per second:  47, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.164 [0.000, 5.000],  loss: 0.014324, mae: 2.597033, mean_q: 3.125783, mean_eps: 0.149133
 4255867/6000000: episode: 5480, duration: 22.674s, episode steps: 1074, steps per second:  47, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.079 [0.000, 5.000],  loss: 0.014136, mae: 2.598103, mean_q: 3.127511, mean_eps: 0.148934
 4257184/6000000: episode: 5481, duration: 31.001s, episode steps: 1317, steps per second:  42, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.110 [0.000, 5.000],  loss: 0.014605, mae: 2.564087, mean_q: 3.086013, mean_eps: 0.148695
 4258100/6000000: episode: 5482, duration: 23.657s, episode steps: 916, steps per second:  39, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.929 [0.000, 5.000],  loss: 0.015187, mae: 2.578841, mean_q: 3.102156, mean_eps: 0.148472
 4259261/6000000: episode: 5483, duration: 26.528s, episode steps: 1161, steps per second:  44, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.025 [0.000, 5.000],  loss: 0.015702, mae: 2.559759, mean_q: 3.079668, mean_eps: 0.148264
 4260051/6000000: episode: 5484, duration: 19.267s, episode steps: 790, steps per second:  41, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.013057, mae: 2.568484, mean_q: 3.091580, mean_eps: 0.148069
 4261086/6000000: episode: 5485, duration: 24.536s, episode steps: 1035, steps per second:  42, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.979 [0.000, 5.000],  loss: 0.013100, mae: 2.553032, mean_q: 3.071691, mean_eps: 0.147886
 4262097/6000000: episode: 5486, duration: 21.916s, episode steps: 1011, steps per second:  46, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.936 [0.000, 5.000],  loss: 0.014441, mae: 2.554709, mean_q: 3.075475, mean_eps: 0.147682
 4263030/6000000: episode: 5487, duration: 21.937s, episode steps: 933, steps per second:  43, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.779 [0.000, 5.000],  loss: 0.014974, mae: 2.539177, mean_q: 3.056489, mean_eps: 0.147487
 4264066/6000000: episode: 5488, duration: 23.927s, episode steps: 1036, steps per second:  43, episode reward: 31.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.025 [0.000, 5.000],  loss: 0.014926, mae: 2.563123, mean_q: 3.084243, mean_eps: 0.147290
 4265181/6000000: episode: 5489, duration: 25.368s, episode steps: 1115, steps per second:  44, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.882 [0.000, 5.000],  loss: 0.014263, mae: 2.554684, mean_q: 3.073444, mean_eps: 0.147075
 4266240/6000000: episode: 5490, duration: 23.910s, episode steps: 1059, steps per second:  44, episode reward: 33.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.077 [0.000, 5.000],  loss: 0.015284, mae: 2.562862, mean_q: 3.082668, mean_eps: 0.146858
 4267096/6000000: episode: 5491, duration: 17.579s, episode steps: 856, steps per second:  49, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.772 [0.000, 5.000],  loss: 0.014646, mae: 2.570923, mean_q: 3.094008, mean_eps: 0.146667
 4268043/6000000: episode: 5492, duration: 20.990s, episode steps: 947, steps per second:  45, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.812 [0.000, 5.000],  loss: 0.015145, mae: 2.564050, mean_q: 3.085764, mean_eps: 0.146486
 4269035/6000000: episode: 5493, duration: 22.711s, episode steps: 992, steps per second:  44, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.890 [0.000, 5.000],  loss: 0.014125, mae: 2.570338, mean_q: 3.092249, mean_eps: 0.146292
 4269698/6000000: episode: 5494, duration: 13.718s, episode steps: 663, steps per second:  48, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.882 [0.000, 5.000],  loss: 0.014656, mae: 2.546920, mean_q: 3.064875, mean_eps: 0.146127
 4270703/6000000: episode: 5495, duration: 20.816s, episode steps: 1005, steps per second:  48, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.051 [0.000, 5.000],  loss: 0.015451, mae: 2.565277, mean_q: 3.085644, mean_eps: 0.145960
 4271541/6000000: episode: 5496, duration: 17.753s, episode steps: 838, steps per second:  47, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.013419, mae: 2.544332, mean_q: 3.060303, mean_eps: 0.145776
 4272071/6000000: episode: 5497, duration: 11.912s, episode steps: 530, steps per second:  44, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.651 [0.000, 5.000],  loss: 0.014456, mae: 2.555272, mean_q: 3.073677, mean_eps: 0.145639
 4272801/6000000: episode: 5498, duration: 16.085s, episode steps: 730, steps per second:  45, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.023 [0.000, 5.000],  loss: 0.014072, mae: 2.516606, mean_q: 3.028259, mean_eps: 0.145513
 4273511/6000000: episode: 5499, duration: 16.840s, episode steps: 710, steps per second:  42, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.672 [0.000, 5.000],  loss: 0.013922, mae: 2.544479, mean_q: 3.061420, mean_eps: 0.145369
 4274027/6000000: episode: 5500, duration: 12.376s, episode steps: 516, steps per second:  42, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.570 [0.000, 5.000],  loss: 0.015689, mae: 2.535730, mean_q: 3.051389, mean_eps: 0.145246
 4275226/6000000: episode: 5501, duration: 25.587s, episode steps: 1199, steps per second:  47, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.847 [0.000, 5.000],  loss: 0.013687, mae: 2.554538, mean_q: 3.074456, mean_eps: 0.145075
 4276313/6000000: episode: 5502, duration: 23.333s, episode steps: 1087, steps per second:  47, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.800 [0.000, 5.000],  loss: 0.014660, mae: 2.565775, mean_q: 3.085836, mean_eps: 0.144846
 4277185/6000000: episode: 5503, duration: 19.058s, episode steps: 872, steps per second:  46, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.149 [0.000, 5.000],  loss: 0.012577, mae: 2.574253, mean_q: 3.096192, mean_eps: 0.144650
 4277891/6000000: episode: 5504, duration: 14.664s, episode steps: 706, steps per second:  48, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.907 [0.000, 5.000],  loss: 0.013435, mae: 2.569507, mean_q: 3.090788, mean_eps: 0.144492
 4278342/6000000: episode: 5505, duration: 9.469s, episode steps: 451, steps per second:  48, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.525 [0.000, 5.000],  loss: 0.017058, mae: 2.558245, mean_q: 3.079714, mean_eps: 0.144377
 4279107/6000000: episode: 5506, duration: 17.337s, episode steps: 765, steps per second:  44, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.881 [0.000, 5.000],  loss: 0.014529, mae: 2.564527, mean_q: 3.085920, mean_eps: 0.144255
 4280069/6000000: episode: 5507, duration: 22.003s, episode steps: 962, steps per second:  44, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.840 [0.000, 5.000],  loss: 0.013167, mae: 2.561207, mean_q: 3.081955, mean_eps: 0.144082
 4280780/6000000: episode: 5508, duration: 16.084s, episode steps: 711, steps per second:  44, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.101 [0.000, 5.000],  loss: 0.015167, mae: 2.592208, mean_q: 3.118917, mean_eps: 0.143915
 4281904/6000000: episode: 5509, duration: 25.685s, episode steps: 1124, steps per second:  44, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.004 [0.000, 5.000],  loss: 0.014338, mae: 2.583192, mean_q: 3.107919, mean_eps: 0.143732
 4282776/6000000: episode: 5510, duration: 19.215s, episode steps: 872, steps per second:  45, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.938 [0.000, 5.000],  loss: 0.013225, mae: 2.585179, mean_q: 3.110940, mean_eps: 0.143532
 4283838/6000000: episode: 5511, duration: 21.943s, episode steps: 1062, steps per second:  48, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.015209, mae: 2.600623, mean_q: 3.128602, mean_eps: 0.143339
 4284774/6000000: episode: 5512, duration: 20.499s, episode steps: 936, steps per second:  46, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.245 [0.000, 5.000],  loss: 0.013744, mae: 2.591495, mean_q: 3.118046, mean_eps: 0.143139
 4285597/6000000: episode: 5513, duration: 17.298s, episode steps: 823, steps per second:  48, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.072 [0.000, 5.000],  loss: 0.013731, mae: 2.577392, mean_q: 3.102422, mean_eps: 0.142963
 4286829/6000000: episode: 5514, duration: 26.044s, episode steps: 1232, steps per second:  47, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.865 [0.000, 5.000],  loss: 0.013894, mae: 2.571113, mean_q: 3.095426, mean_eps: 0.142757
 4287509/6000000: episode: 5515, duration: 14.690s, episode steps: 680, steps per second:  46, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.015366, mae: 2.562785, mean_q: 3.084933, mean_eps: 0.142566
 4288552/6000000: episode: 5516, duration: 22.276s, episode steps: 1043, steps per second:  47, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.157 [0.000, 5.000],  loss: 0.014465, mae: 2.591886, mean_q: 3.119310, mean_eps: 0.142394
 4289317/6000000: episode: 5517, duration: 17.572s, episode steps: 765, steps per second:  44, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.779 [0.000, 5.000],  loss: 0.014596, mae: 2.555584, mean_q: 3.074328, mean_eps: 0.142213
 4290152/6000000: episode: 5518, duration: 20.047s, episode steps: 835, steps per second:  42, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.015175, mae: 2.568730, mean_q: 3.089943, mean_eps: 0.142053
 4291024/6000000: episode: 5519, duration: 20.460s, episode steps: 872, steps per second:  43, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.064 [0.000, 5.000],  loss: 0.013232, mae: 2.599563, mean_q: 3.127868, mean_eps: 0.141883
 4291923/6000000: episode: 5520, duration: 20.089s, episode steps: 899, steps per second:  45, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.656 [0.000, 5.000],  loss: 0.015529, mae: 2.594417, mean_q: 3.121569, mean_eps: 0.141706
 4292503/6000000: episode: 5521, duration: 13.053s, episode steps: 580, steps per second:  44, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.209 [0.000, 5.000],  loss: 0.013611, mae: 2.571999, mean_q: 3.093700, mean_eps: 0.141558
 4293391/6000000: episode: 5522, duration: 20.153s, episode steps: 888, steps per second:  44, episode reward: 27.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.975 [0.000, 5.000],  loss: 0.015280, mae: 2.606503, mean_q: 3.135256, mean_eps: 0.141411
 4294206/6000000: episode: 5523, duration: 17.283s, episode steps: 815, steps per second:  47, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.013876, mae: 2.590710, mean_q: 3.117002, mean_eps: 0.141240
 4295355/6000000: episode: 5524, duration: 25.673s, episode steps: 1149, steps per second:  45, episode reward: 33.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.000 [0.000, 5.000],  loss: 0.014309, mae: 2.591232, mean_q: 3.118305, mean_eps: 0.141044
 4296199/6000000: episode: 5525, duration: 19.483s, episode steps: 844, steps per second:  43, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.818 [0.000, 5.000],  loss: 0.014649, mae: 2.610130, mean_q: 3.140091, mean_eps: 0.140845
 4297092/6000000: episode: 5526, duration: 19.173s, episode steps: 893, steps per second:  47, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.013610, mae: 2.586694, mean_q: 3.112784, mean_eps: 0.140671
 4297746/6000000: episode: 5527, duration: 14.331s, episode steps: 654, steps per second:  46, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.205 [0.000, 5.000],  loss: 0.013552, mae: 2.581906, mean_q: 3.106731, mean_eps: 0.140516
 4298802/6000000: episode: 5528, duration: 23.739s, episode steps: 1056, steps per second:  44, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.008 [0.000, 5.000],  loss: 0.013634, mae: 2.605223, mean_q: 3.135523, mean_eps: 0.140345
 4299507/6000000: episode: 5529, duration: 14.373s, episode steps: 705, steps per second:  49, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.933 [0.000, 5.000],  loss: 0.015293, mae: 2.578282, mean_q: 3.101291, mean_eps: 0.140169
 4300338/6000000: episode: 5530, duration: 17.778s, episode steps: 831, steps per second:  47, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.992 [0.000, 5.000],  loss: 0.014399, mae: 2.593587, mean_q: 3.120939, mean_eps: 0.140016
 4301141/6000000: episode: 5531, duration: 17.615s, episode steps: 803, steps per second:  46, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.013653, mae: 2.572444, mean_q: 3.095282, mean_eps: 0.139852
 4302429/6000000: episode: 5532, duration: 27.663s, episode steps: 1288, steps per second:  47, episode reward: 31.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.771 [0.000, 5.000],  loss: 0.013390, mae: 2.571815, mean_q: 3.094730, mean_eps: 0.139643
 4303407/6000000: episode: 5533, duration: 21.105s, episode steps: 978, steps per second:  46, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.056 [0.000, 5.000],  loss: 0.013612, mae: 2.585848, mean_q: 3.112063, mean_eps: 0.139416
 4304448/6000000: episode: 5534, duration: 23.266s, episode steps: 1041, steps per second:  45, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.727 [0.000, 5.000],  loss: 0.013579, mae: 2.581970, mean_q: 3.106863, mean_eps: 0.139215
 4305279/6000000: episode: 5535, duration: 18.458s, episode steps: 831, steps per second:  45, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.875 [0.000, 5.000],  loss: 0.012741, mae: 2.581238, mean_q: 3.107142, mean_eps: 0.139028
 4306389/6000000: episode: 5536, duration: 27.914s, episode steps: 1110, steps per second:  40, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.558 [0.000, 5.000],  loss: 0.014177, mae: 2.599333, mean_q: 3.128078, mean_eps: 0.138833
 4307334/6000000: episode: 5537, duration: 22.491s, episode steps: 945, steps per second:  42, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.467 [0.000, 5.000],  loss: 0.014294, mae: 2.600221, mean_q: 3.129037, mean_eps: 0.138628
 4308200/6000000: episode: 5538, duration: 19.102s, episode steps: 866, steps per second:  45, episode reward: 26.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.771 [0.000, 5.000],  loss: 0.014208, mae: 2.566062, mean_q: 3.088064, mean_eps: 0.138447
 4309151/6000000: episode: 5539, duration: 21.426s, episode steps: 951, steps per second:  44, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.924 [0.000, 5.000],  loss: 0.014622, mae: 2.592899, mean_q: 3.117773, mean_eps: 0.138265
 4310184/6000000: episode: 5540, duration: 22.306s, episode steps: 1033, steps per second:  46, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.186 [0.000, 5.000],  loss: 0.014592, mae: 2.591407, mean_q: 3.117910, mean_eps: 0.138067
 4311056/6000000: episode: 5541, duration: 20.119s, episode steps: 872, steps per second:  43, episode reward: 27.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.950 [0.000, 5.000],  loss: 0.014956, mae: 2.561975, mean_q: 3.083837, mean_eps: 0.137876
 4312084/6000000: episode: 5542, duration: 25.066s, episode steps: 1028, steps per second:  41, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.015293, mae: 2.585453, mean_q: 3.110910, mean_eps: 0.137686
 4312634/6000000: episode: 5543, duration: 12.511s, episode steps: 550, steps per second:  44, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.027 [0.000, 5.000],  loss: 0.014330, mae: 2.573923, mean_q: 3.097815, mean_eps: 0.137528
 4313658/6000000: episode: 5544, duration: 23.496s, episode steps: 1024, steps per second:  44, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.211 [0.000, 5.000],  loss: 0.014563, mae: 2.561218, mean_q: 3.080875, mean_eps: 0.137371
 4314288/6000000: episode: 5545, duration: 14.396s, episode steps: 630, steps per second:  44, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.013289, mae: 2.560222, mean_q: 3.082862, mean_eps: 0.137206
 4315144/6000000: episode: 5546, duration: 18.576s, episode steps: 856, steps per second:  46, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.112 [0.000, 5.000],  loss: 0.014684, mae: 2.554609, mean_q: 3.075492, mean_eps: 0.137057
 4315973/6000000: episode: 5547, duration: 17.441s, episode steps: 829, steps per second:  48, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.516 [0.000, 5.000],  loss: 0.014710, mae: 2.597618, mean_q: 3.125905, mean_eps: 0.136888
 4316865/6000000: episode: 5548, duration: 19.734s, episode steps: 892, steps per second:  45, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.136 [0.000, 5.000],  loss: 0.013422, mae: 2.608387, mean_q: 3.140218, mean_eps: 0.136716
 4318253/6000000: episode: 5549, duration: 29.371s, episode steps: 1388, steps per second:  47, episode reward: 32.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.014145, mae: 2.599478, mean_q: 3.129496, mean_eps: 0.136488
 4318888/6000000: episode: 5550, duration: 13.995s, episode steps: 635, steps per second:  45, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.202 [0.000, 5.000],  loss: 0.014087, mae: 2.592660, mean_q: 3.120903, mean_eps: 0.136286
 4319873/6000000: episode: 5551, duration: 21.369s, episode steps: 985, steps per second:  46, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.798 [0.000, 5.000],  loss: 0.014764, mae: 2.597454, mean_q: 3.127382, mean_eps: 0.136124
 4320693/6000000: episode: 5552, duration: 19.215s, episode steps: 820, steps per second:  43, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.861 [0.000, 5.000],  loss: 0.015034, mae: 2.607533, mean_q: 3.136336, mean_eps: 0.135943
 4321640/6000000: episode: 5553, duration: 22.928s, episode steps: 947, steps per second:  41, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.855 [0.000, 5.000],  loss: 0.015334, mae: 2.607915, mean_q: 3.137825, mean_eps: 0.135767
 4322810/6000000: episode: 5554, duration: 29.777s, episode steps: 1170, steps per second:  39, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.827 [0.000, 5.000],  loss: 0.015140, mae: 2.597674, mean_q: 3.125740, mean_eps: 0.135555
 4323704/6000000: episode: 5555, duration: 19.659s, episode steps: 894, steps per second:  45, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.674 [0.000, 5.000],  loss: 0.014149, mae: 2.610725, mean_q: 3.142522, mean_eps: 0.135349
 4324997/6000000: episode: 5556, duration: 29.212s, episode steps: 1293, steps per second:  44, episode reward: 29.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.097 [0.000, 5.000],  loss: 0.014730, mae: 2.602089, mean_q: 3.131689, mean_eps: 0.135130
 4325903/6000000: episode: 5557, duration: 20.838s, episode steps: 906, steps per second:  43, episode reward: 27.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.634 [0.000, 5.000],  loss: 0.012906, mae: 2.610850, mean_q: 3.144332, mean_eps: 0.134910
 4326405/6000000: episode: 5558, duration: 10.588s, episode steps: 502, steps per second:  47, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.014848, mae: 2.603465, mean_q: 3.133337, mean_eps: 0.134769
 4327492/6000000: episode: 5559, duration: 26.108s, episode steps: 1087, steps per second:  42, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.601 [0.000, 5.000],  loss: 0.015197, mae: 2.616122, mean_q: 3.147136, mean_eps: 0.134610
 4328436/6000000: episode: 5560, duration: 21.186s, episode steps: 944, steps per second:  45, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.831 [0.000, 5.000],  loss: 0.013456, mae: 2.613616, mean_q: 3.146147, mean_eps: 0.134408
 4329251/6000000: episode: 5561, duration: 17.744s, episode steps: 815, steps per second:  46, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.014354, mae: 2.612608, mean_q: 3.144534, mean_eps: 0.134232
 4330141/6000000: episode: 5562, duration: 19.742s, episode steps: 890, steps per second:  45, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.783 [0.000, 5.000],  loss: 0.015256, mae: 2.598087, mean_q: 3.126734, mean_eps: 0.134061
 4331197/6000000: episode: 5563, duration: 22.613s, episode steps: 1056, steps per second:  47, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.822 [0.000, 5.000],  loss: 0.014756, mae: 2.605612, mean_q: 3.135831, mean_eps: 0.133866
 4332470/6000000: episode: 5564, duration: 26.704s, episode steps: 1273, steps per second:  48, episode reward: 31.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.551 [0.000, 5.000],  loss: 0.014108, mae: 2.606522, mean_q: 3.137348, mean_eps: 0.133633
 4333567/6000000: episode: 5565, duration: 24.259s, episode steps: 1097, steps per second:  45, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.791 [0.000, 5.000],  loss: 0.015393, mae: 2.604497, mean_q: 3.134166, mean_eps: 0.133396
 4334216/6000000: episode: 5566, duration: 14.051s, episode steps: 649, steps per second:  46, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.208 [0.000, 5.000],  loss: 0.014211, mae: 2.612010, mean_q: 3.143597, mean_eps: 0.133222
 4335504/6000000: episode: 5567, duration: 28.315s, episode steps: 1288, steps per second:  45, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.014243, mae: 2.604102, mean_q: 3.132818, mean_eps: 0.133028
 4336581/6000000: episode: 5568, duration: 23.922s, episode steps: 1077, steps per second:  45, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.108 [0.000, 5.000],  loss: 0.014331, mae: 2.627216, mean_q: 3.161408, mean_eps: 0.132792
 4337461/6000000: episode: 5569, duration: 20.255s, episode steps: 880, steps per second:  43, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.876 [0.000, 5.000],  loss: 0.013892, mae: 2.611576, mean_q: 3.143364, mean_eps: 0.132596
 4338101/6000000: episode: 5570, duration: 15.896s, episode steps: 640, steps per second:  40, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.722 [0.000, 5.000],  loss: 0.013597, mae: 2.650890, mean_q: 3.192203, mean_eps: 0.132444
 4338949/6000000: episode: 5571, duration: 21.475s, episode steps: 848, steps per second:  39, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.123 [0.000, 5.000],  loss: 0.014688, mae: 2.608699, mean_q: 3.139197, mean_eps: 0.132295
 4339847/6000000: episode: 5572, duration: 20.618s, episode steps: 898, steps per second:  44, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.010 [0.000, 5.000],  loss: 0.014944, mae: 2.621214, mean_q: 3.153758, mean_eps: 0.132120
 4340356/6000000: episode: 5573, duration: 11.739s, episode steps: 509, steps per second:  43, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.022 [0.000, 5.000],  loss: 0.013743, mae: 2.631714, mean_q: 3.166176, mean_eps: 0.131980
 4341253/6000000: episode: 5574, duration: 19.523s, episode steps: 897, steps per second:  46, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.013702, mae: 2.633958, mean_q: 3.169136, mean_eps: 0.131839
 4342180/6000000: episode: 5575, duration: 21.167s, episode steps: 927, steps per second:  44, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.222 [0.000, 5.000],  loss: 0.014456, mae: 2.650656, mean_q: 3.188706, mean_eps: 0.131657
 4342655/6000000: episode: 5576, duration: 10.834s, episode steps: 475, steps per second:  44, episode reward: 12.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.947 [0.000, 5.000],  loss: 0.012839, mae: 2.638078, mean_q: 3.174056, mean_eps: 0.131517
 4343993/6000000: episode: 5577, duration: 32.829s, episode steps: 1338, steps per second:  41, episode reward: 28.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.088 [0.000, 5.000],  loss: 0.012918, mae: 2.639418, mean_q: 3.177399, mean_eps: 0.131335
 4344474/6000000: episode: 5578, duration: 11.026s, episode steps: 481, steps per second:  44, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 3.058 [0.000, 5.000],  loss: 0.013926, mae: 2.646104, mean_q: 3.185648, mean_eps: 0.131153
 4345420/6000000: episode: 5579, duration: 20.812s, episode steps: 946, steps per second:  45, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.013480, mae: 2.629286, mean_q: 3.165809, mean_eps: 0.131011
 4346435/6000000: episode: 5580, duration: 23.201s, episode steps: 1015, steps per second:  44, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.147 [0.000, 5.000],  loss: 0.013014, mae: 2.620516, mean_q: 3.157101, mean_eps: 0.130815
 4347484/6000000: episode: 5581, duration: 21.598s, episode steps: 1049, steps per second:  49, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.136 [0.000, 5.000],  loss: 0.014606, mae: 2.617318, mean_q: 3.149765, mean_eps: 0.130608
 4348376/6000000: episode: 5582, duration: 19.221s, episode steps: 892, steps per second:  46, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.992 [0.000, 5.000],  loss: 0.013813, mae: 2.631713, mean_q: 3.166675, mean_eps: 0.130414
 4349116/6000000: episode: 5583, duration: 16.398s, episode steps: 740, steps per second:  45, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.636 [0.000, 5.000],  loss: 0.014153, mae: 2.612939, mean_q: 3.144621, mean_eps: 0.130251
 4349912/6000000: episode: 5584, duration: 17.607s, episode steps: 796, steps per second:  45, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.633 [0.000, 5.000],  loss: 0.013579, mae: 2.628086, mean_q: 3.163240, mean_eps: 0.130098
 4351077/6000000: episode: 5585, duration: 25.765s, episode steps: 1165, steps per second:  45, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.955 [0.000, 5.000],  loss: 0.015188, mae: 2.608592, mean_q: 3.140380, mean_eps: 0.129901
 4352257/6000000: episode: 5586, duration: 26.916s, episode steps: 1180, steps per second:  44, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.013716, mae: 2.617330, mean_q: 3.149007, mean_eps: 0.129666
 4353342/6000000: episode: 5587, duration: 26.500s, episode steps: 1085, steps per second:  41, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.013576, mae: 2.607329, mean_q: 3.137107, mean_eps: 0.129440
 4354262/6000000: episode: 5588, duration: 22.848s, episode steps: 920, steps per second:  40, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.989 [0.000, 5.000],  loss: 0.014839, mae: 2.620532, mean_q: 3.154154, mean_eps: 0.129240
 4355092/6000000: episode: 5589, duration: 18.357s, episode steps: 830, steps per second:  45, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.015087, mae: 2.624062, mean_q: 3.156824, mean_eps: 0.129065
 4356077/6000000: episode: 5590, duration: 21.680s, episode steps: 985, steps per second:  45, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.807 [0.000, 5.000],  loss: 0.012917, mae: 2.643166, mean_q: 3.182132, mean_eps: 0.128883
 4356936/6000000: episode: 5591, duration: 19.584s, episode steps: 859, steps per second:  44, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.013329, mae: 2.634286, mean_q: 3.170146, mean_eps: 0.128699
 4357897/6000000: episode: 5592, duration: 21.144s, episode steps: 961, steps per second:  45, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.323 [0.000, 5.000],  loss: 0.013965, mae: 2.640577, mean_q: 3.177611, mean_eps: 0.128517
 4358736/6000000: episode: 5593, duration: 18.965s, episode steps: 839, steps per second:  44, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.057 [0.000, 5.000],  loss: 0.014135, mae: 2.638582, mean_q: 3.175080, mean_eps: 0.128337
 4359678/6000000: episode: 5594, duration: 22.626s, episode steps: 942, steps per second:  42, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.120 [0.000, 5.000],  loss: 0.014497, mae: 2.628220, mean_q: 3.164778, mean_eps: 0.128159
 4360741/6000000: episode: 5595, duration: 24.383s, episode steps: 1063, steps per second:  44, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.561 [0.000, 5.000],  loss: 0.014639, mae: 2.628766, mean_q: 3.163082, mean_eps: 0.127958
 4361542/6000000: episode: 5596, duration: 18.102s, episode steps: 801, steps per second:  44, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.296 [0.000, 5.000],  loss: 0.012755, mae: 2.631516, mean_q: 3.169798, mean_eps: 0.127772
 4362571/6000000: episode: 5597, duration: 23.647s, episode steps: 1029, steps per second:  44, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.828 [0.000, 5.000],  loss: 0.015001, mae: 2.636409, mean_q: 3.172880, mean_eps: 0.127589
 4363850/6000000: episode: 5598, duration: 26.240s, episode steps: 1279, steps per second:  49, episode reward: 28.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.822 [0.000, 5.000],  loss: 0.014215, mae: 2.625510, mean_q: 3.161138, mean_eps: 0.127358
 4364767/6000000: episode: 5599, duration: 20.429s, episode steps: 917, steps per second:  45, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.955 [0.000, 5.000],  loss: 0.014917, mae: 2.643411, mean_q: 3.179374, mean_eps: 0.127138
 4365858/6000000: episode: 5600, duration: 23.699s, episode steps: 1091, steps per second:  46, episode reward: 31.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.053 [0.000, 5.000],  loss: 0.013154, mae: 2.601627, mean_q: 3.131110, mean_eps: 0.126938
 4366536/6000000: episode: 5601, duration: 14.011s, episode steps: 678, steps per second:  48, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.745 [0.000, 5.000],  loss: 0.014832, mae: 2.611531, mean_q: 3.142397, mean_eps: 0.126761
 4367171/6000000: episode: 5602, duration: 13.340s, episode steps: 635, steps per second:  48, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.015324, mae: 2.591500, mean_q: 3.118641, mean_eps: 0.126630
 4368161/6000000: episode: 5603, duration: 21.037s, episode steps: 990, steps per second:  47, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.804 [0.000, 5.000],  loss: 0.014619, mae: 2.609696, mean_q: 3.141695, mean_eps: 0.126467
 4368714/6000000: episode: 5604, duration: 12.864s, episode steps: 553, steps per second:  43, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.920 [0.000, 5.000],  loss: 0.016225, mae: 2.595734, mean_q: 3.125386, mean_eps: 0.126312
 4369760/6000000: episode: 5605, duration: 25.594s, episode steps: 1046, steps per second:  41, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.959 [0.000, 5.000],  loss: 0.014959, mae: 2.604019, mean_q: 3.134200, mean_eps: 0.126153
 4370298/6000000: episode: 5606, duration: 13.190s, episode steps: 538, steps per second:  41, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.967 [0.000, 5.000],  loss: 0.014950, mae: 2.628116, mean_q: 3.163236, mean_eps: 0.125994
 4371602/6000000: episode: 5607, duration: 29.900s, episode steps: 1304, steps per second:  44, episode reward: 27.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.910 [0.000, 5.000],  loss: 0.013493, mae: 2.634876, mean_q: 3.171748, mean_eps: 0.125810
 4372461/6000000: episode: 5608, duration: 22.376s, episode steps: 859, steps per second:  38, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.206 [0.000, 5.000],  loss: 0.013623, mae: 2.632889, mean_q: 3.167938, mean_eps: 0.125594
 4373548/6000000: episode: 5609, duration: 24.352s, episode steps: 1087, steps per second:  45, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.014299, mae: 2.605805, mean_q: 3.135439, mean_eps: 0.125399
 4374409/6000000: episode: 5610, duration: 20.157s, episode steps: 861, steps per second:  43, episode reward: 26.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.749 [0.000, 5.000],  loss: 0.014214, mae: 2.629290, mean_q: 3.164353, mean_eps: 0.125204
 4375314/6000000: episode: 5611, duration: 21.316s, episode steps: 905, steps per second:  42, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.014192, mae: 2.635996, mean_q: 3.171982, mean_eps: 0.125028
 4376230/6000000: episode: 5612, duration: 20.699s, episode steps: 916, steps per second:  44, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.014833, mae: 2.624724, mean_q: 3.158508, mean_eps: 0.124846
 4377410/6000000: episode: 5613, duration: 26.154s, episode steps: 1180, steps per second:  45, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.731 [0.000, 5.000],  loss: 0.014276, mae: 2.613417, mean_q: 3.145756, mean_eps: 0.124636
 4378508/6000000: episode: 5614, duration: 24.772s, episode steps: 1098, steps per second:  44, episode reward: 32.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.212 [0.000, 5.000],  loss: 0.012198, mae: 2.620621, mean_q: 3.153956, mean_eps: 0.124408
 4379328/6000000: episode: 5615, duration: 18.728s, episode steps: 820, steps per second:  44, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.413 [0.000, 5.000],  loss: 0.013342, mae: 2.620182, mean_q: 3.152615, mean_eps: 0.124217
 4380141/6000000: episode: 5616, duration: 19.386s, episode steps: 813, steps per second:  42, episode reward: 24.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.154 [0.000, 5.000],  loss: 0.014351, mae: 2.608296, mean_q: 3.138672, mean_eps: 0.124053
 4380970/6000000: episode: 5617, duration: 17.762s, episode steps: 829, steps per second:  47, episode reward: 28.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.084 [0.000, 5.000],  loss: 0.012299, mae: 2.591960, mean_q: 3.119375, mean_eps: 0.123889
 4382075/6000000: episode: 5618, duration: 24.177s, episode steps: 1105, steps per second:  46, episode reward: 30.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.134 [0.000, 5.000],  loss: 0.014378, mae: 2.591679, mean_q: 3.119048, mean_eps: 0.123696
 4382971/6000000: episode: 5619, duration: 21.753s, episode steps: 896, steps per second:  41, episode reward: 27.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.068 [0.000, 5.000],  loss: 0.014080, mae: 2.618806, mean_q: 3.150925, mean_eps: 0.123496
 4383718/6000000: episode: 5620, duration: 18.865s, episode steps: 747, steps per second:  40, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.636 [0.000, 5.000],  loss: 0.015018, mae: 2.609059, mean_q: 3.139033, mean_eps: 0.123331
 4384322/6000000: episode: 5621, duration: 13.418s, episode steps: 604, steps per second:  45, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 0.957 [0.000, 5.000],  loss: 0.015121, mae: 2.588977, mean_q: 3.115052, mean_eps: 0.123196
 4384884/6000000: episode: 5622, duration: 12.469s, episode steps: 562, steps per second:  45, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.167 [0.000, 5.000],  loss: 0.013065, mae: 2.618737, mean_q: 3.151365, mean_eps: 0.123080
 4385582/6000000: episode: 5623, duration: 17.788s, episode steps: 698, steps per second:  39, episode reward: 21.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.268 [0.000, 5.000],  loss: 0.012818, mae: 2.617199, mean_q: 3.149006, mean_eps: 0.122954
 4386304/6000000: episode: 5624, duration: 17.202s, episode steps: 722, steps per second:  42, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.821 [0.000, 5.000],  loss: 0.014284, mae: 2.640800, mean_q: 3.178457, mean_eps: 0.122812
 4387486/6000000: episode: 5625, duration: 26.784s, episode steps: 1182, steps per second:  44, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.013162, mae: 2.630715, mean_q: 3.167672, mean_eps: 0.122621
 4388479/6000000: episode: 5626, duration: 23.022s, episode steps: 993, steps per second:  43, episode reward: 30.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.612 [0.000, 5.000],  loss: 0.014463, mae: 2.633997, mean_q: 3.169184, mean_eps: 0.122404
 4389001/6000000: episode: 5627, duration: 11.537s, episode steps: 522, steps per second:  45, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.016021, mae: 2.611215, mean_q: 3.142005, mean_eps: 0.122252
 4389715/6000000: episode: 5628, duration: 15.514s, episode steps: 714, steps per second:  46, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.139 [0.000, 5.000],  loss: 0.013886, mae: 2.612042, mean_q: 3.144492, mean_eps: 0.122128
 4390922/6000000: episode: 5629, duration: 28.989s, episode steps: 1207, steps per second:  42, episode reward: 35.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.013685, mae: 2.611737, mean_q: 3.143577, mean_eps: 0.121936
 4392244/6000000: episode: 5630, duration: 29.500s, episode steps: 1322, steps per second:  45, episode reward: 31.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.156 [0.000, 5.000],  loss: 0.013473, mae: 2.621801, mean_q: 3.155227, mean_eps: 0.121684
 4392908/6000000: episode: 5631, duration: 14.796s, episode steps: 664, steps per second:  45, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.013372, mae: 2.646752, mean_q: 3.185631, mean_eps: 0.121485
 4393454/6000000: episode: 5632, duration: 12.371s, episode steps: 546, steps per second:  44, episode reward: 14.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.930 [0.000, 5.000],  loss: 0.014114, mae: 2.623363, mean_q: 3.154964, mean_eps: 0.121364
 4394701/6000000: episode: 5633, duration: 26.079s, episode steps: 1247, steps per second:  48, episode reward: 35.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.013986, mae: 2.622729, mean_q: 3.155062, mean_eps: 0.121184
 4395647/6000000: episode: 5634, duration: 19.606s, episode steps: 946, steps per second:  48, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.066 [0.000, 5.000],  loss: 0.013018, mae: 2.630990, mean_q: 3.165843, mean_eps: 0.120965
 4396419/6000000: episode: 5635, duration: 17.624s, episode steps: 772, steps per second:  44, episode reward: 23.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.012818, mae: 2.637683, mean_q: 3.174338, mean_eps: 0.120794
 4397354/6000000: episode: 5636, duration: 20.102s, episode steps: 935, steps per second:  47, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.664 [0.000, 5.000],  loss: 0.013059, mae: 2.639781, mean_q: 3.175741, mean_eps: 0.120623
 4398221/6000000: episode: 5637, duration: 17.985s, episode steps: 867, steps per second:  48, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.768 [0.000, 5.000],  loss: 0.013531, mae: 2.636556, mean_q: 3.172387, mean_eps: 0.120442
 4399022/6000000: episode: 5638, duration: 17.176s, episode steps: 801, steps per second:  47, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.763 [0.000, 5.000],  loss: 0.014449, mae: 2.643265, mean_q: 3.181343, mean_eps: 0.120276
 4399967/6000000: episode: 5639, duration: 20.213s, episode steps: 945, steps per second:  47, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.138 [0.000, 5.000],  loss: 0.013767, mae: 2.640137, mean_q: 3.176798, mean_eps: 0.120101
 4400972/6000000: episode: 5640, duration: 22.606s, episode steps: 1005, steps per second:  44, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.943 [0.000, 5.000],  loss: 0.013494, mae: 2.626781, mean_q: 3.162942, mean_eps: 0.119906
 4401539/6000000: episode: 5641, duration: 13.674s, episode steps: 567, steps per second:  41, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.013884, mae: 2.622956, mean_q: 3.155290, mean_eps: 0.119749
 4402612/6000000: episode: 5642, duration: 25.835s, episode steps: 1073, steps per second:  42, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.136 [0.000, 5.000],  loss: 0.014975, mae: 2.612104, mean_q: 3.142604, mean_eps: 0.119585
 4403441/6000000: episode: 5643, duration: 18.952s, episode steps: 829, steps per second:  44, episode reward: 24.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.192 [0.000, 5.000],  loss: 0.013853, mae: 2.637374, mean_q: 3.173773, mean_eps: 0.119395
 4404230/6000000: episode: 5644, duration: 16.651s, episode steps: 789, steps per second:  47, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.949 [0.000, 5.000],  loss: 0.013546, mae: 2.598353, mean_q: 3.128008, mean_eps: 0.119233
 4405454/6000000: episode: 5645, duration: 28.266s, episode steps: 1224, steps per second:  43, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.921 [0.000, 5.000],  loss: 0.014792, mae: 2.624007, mean_q: 3.159019, mean_eps: 0.119032
 4406237/6000000: episode: 5646, duration: 17.194s, episode steps: 783, steps per second:  46, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.857 [0.000, 5.000],  loss: 0.013997, mae: 2.625153, mean_q: 3.159374, mean_eps: 0.118831
 4406879/6000000: episode: 5647, duration: 14.151s, episode steps: 642, steps per second:  45, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.774 [0.000, 5.000],  loss: 0.014080, mae: 2.655688, mean_q: 3.195891, mean_eps: 0.118688
 4407821/6000000: episode: 5648, duration: 21.664s, episode steps: 942, steps per second:  43, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.808 [0.000, 5.000],  loss: 0.014052, mae: 2.641910, mean_q: 3.180084, mean_eps: 0.118530
 4408971/6000000: episode: 5649, duration: 25.439s, episode steps: 1150, steps per second:  45, episode reward: 31.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.013 [0.000, 5.000],  loss: 0.013051, mae: 2.642356, mean_q: 3.181013, mean_eps: 0.118321
 4409855/6000000: episode: 5650, duration: 19.968s, episode steps: 884, steps per second:  44, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.129 [0.000, 5.000],  loss: 0.015474, mae: 2.629833, mean_q: 3.164081, mean_eps: 0.118118
 4411144/6000000: episode: 5651, duration: 27.311s, episode steps: 1289, steps per second:  47, episode reward: 28.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.306 [0.000, 5.000],  loss: 0.013779, mae: 2.631622, mean_q: 3.166321, mean_eps: 0.117900
 4412245/6000000: episode: 5652, duration: 23.249s, episode steps: 1101, steps per second:  47, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.728 [0.000, 5.000],  loss: 0.014999, mae: 2.639628, mean_q: 3.175840, mean_eps: 0.117661
 4413506/6000000: episode: 5653, duration: 27.017s, episode steps: 1261, steps per second:  47, episode reward: 28.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.113 [0.000, 5.000],  loss: 0.014499, mae: 2.609713, mean_q: 3.141234, mean_eps: 0.117425
 4414257/6000000: episode: 5654, duration: 15.975s, episode steps: 751, steps per second:  47, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.013801, mae: 2.624309, mean_q: 3.156807, mean_eps: 0.117224
 4414908/6000000: episode: 5655, duration: 13.524s, episode steps: 651, steps per second:  48, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.680 [0.000, 5.000],  loss: 0.014649, mae: 2.635781, mean_q: 3.170634, mean_eps: 0.117084
 4416113/6000000: episode: 5656, duration: 25.973s, episode steps: 1205, steps per second:  46, episode reward: 32.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.012628, mae: 2.645641, mean_q: 3.183219, mean_eps: 0.116898
 4416936/6000000: episode: 5657, duration: 18.422s, episode steps: 823, steps per second:  45, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.751 [0.000, 5.000],  loss: 0.014241, mae: 2.618618, mean_q: 3.152595, mean_eps: 0.116695
 4418096/6000000: episode: 5658, duration: 28.144s, episode steps: 1160, steps per second:  41, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.013410, mae: 2.639205, mean_q: 3.177007, mean_eps: 0.116497
 4419361/6000000: episode: 5659, duration: 29.344s, episode steps: 1265, steps per second:  43, episode reward: 30.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.014590, mae: 2.636758, mean_q: 3.171959, mean_eps: 0.116254
 4420622/6000000: episode: 5660, duration: 27.429s, episode steps: 1261, steps per second:  46, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.074 [0.000, 5.000],  loss: 0.013611, mae: 2.635607, mean_q: 3.175595, mean_eps: 0.116002
 4421267/6000000: episode: 5661, duration: 13.654s, episode steps: 645, steps per second:  47, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.992 [0.000, 5.000],  loss: 0.013757, mae: 2.629688, mean_q: 3.165496, mean_eps: 0.115811
 4422222/6000000: episode: 5662, duration: 20.907s, episode steps: 955, steps per second:  46, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.945 [0.000, 5.000],  loss: 0.013116, mae: 2.635226, mean_q: 3.171340, mean_eps: 0.115651
 4422980/6000000: episode: 5663, duration: 17.180s, episode steps: 758, steps per second:  44, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.106 [0.000, 5.000],  loss: 0.014798, mae: 2.623698, mean_q: 3.156975, mean_eps: 0.115480
 4424093/6000000: episode: 5664, duration: 26.494s, episode steps: 1113, steps per second:  42, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.131 [0.000, 5.000],  loss: 0.014686, mae: 2.627105, mean_q: 3.161073, mean_eps: 0.115293
 4425308/6000000: episode: 5665, duration: 26.555s, episode steps: 1215, steps per second:  46, episode reward: 33.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.815 [0.000, 5.000],  loss: 0.014372, mae: 2.646581, mean_q: 3.185225, mean_eps: 0.115060
 4426117/6000000: episode: 5666, duration: 18.491s, episode steps: 809, steps per second:  44, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.114 [0.000, 5.000],  loss: 0.015032, mae: 2.670977, mean_q: 3.212801, mean_eps: 0.114858
 4427356/6000000: episode: 5667, duration: 25.979s, episode steps: 1239, steps per second:  48, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.869 [0.000, 5.000],  loss: 0.014531, mae: 2.650626, mean_q: 3.189172, mean_eps: 0.114653
 4428336/6000000: episode: 5668, duration: 20.339s, episode steps: 980, steps per second:  48, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.167 [0.000, 5.000],  loss: 0.014576, mae: 2.637085, mean_q: 3.172533, mean_eps: 0.114431
 4429246/6000000: episode: 5669, duration: 19.856s, episode steps: 910, steps per second:  46, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.005 [0.000, 5.000],  loss: 0.013596, mae: 2.638650, mean_q: 3.174833, mean_eps: 0.114242
 4430581/6000000: episode: 5670, duration: 28.624s, episode steps: 1335, steps per second:  47, episode reward: 30.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.209 [0.000, 5.000],  loss: 0.013236, mae: 2.634214, mean_q: 3.170272, mean_eps: 0.114017
 4431073/6000000: episode: 5671, duration: 10.461s, episode steps: 492, steps per second:  47, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.013018, mae: 2.623348, mean_q: 3.156739, mean_eps: 0.113834
 4431817/6000000: episode: 5672, duration: 16.136s, episode steps: 744, steps per second:  46, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.016 [0.000, 5.000],  loss: 0.013814, mae: 2.633733, mean_q: 3.169964, mean_eps: 0.113711
 4432762/6000000: episode: 5673, duration: 21.341s, episode steps: 945, steps per second:  44, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.078 [0.000, 5.000],  loss: 0.014584, mae: 2.637854, mean_q: 3.174084, mean_eps: 0.113542
 4433590/6000000: episode: 5674, duration: 20.278s, episode steps: 828, steps per second:  41, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.750 [0.000, 5.000],  loss: 0.013326, mae: 2.626424, mean_q: 3.161539, mean_eps: 0.113365
 4434694/6000000: episode: 5675, duration: 26.260s, episode steps: 1104, steps per second:  42, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.114 [0.000, 5.000],  loss: 0.015080, mae: 2.641942, mean_q: 3.178064, mean_eps: 0.113172
 4435537/6000000: episode: 5676, duration: 18.940s, episode steps: 843, steps per second:  45, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.012582, mae: 2.595413, mean_q: 3.124215, mean_eps: 0.112977
 4436307/6000000: episode: 5677, duration: 16.863s, episode steps: 770, steps per second:  46, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.013071, mae: 2.621460, mean_q: 3.156553, mean_eps: 0.112816
 4436884/6000000: episode: 5678, duration: 12.928s, episode steps: 577, steps per second:  45, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.704 [0.000, 5.000],  loss: 0.013732, mae: 2.604160, mean_q: 3.135207, mean_eps: 0.112681
 4437960/6000000: episode: 5679, duration: 23.679s, episode steps: 1076, steps per second:  45, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.886 [0.000, 5.000],  loss: 0.013090, mae: 2.612753, mean_q: 3.144633, mean_eps: 0.112516
 4438773/6000000: episode: 5680, duration: 17.309s, episode steps: 813, steps per second:  47, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.100 [0.000, 5.000],  loss: 0.015134, mae: 2.602594, mean_q: 3.132388, mean_eps: 0.112327
 4439471/6000000: episode: 5681, duration: 15.533s, episode steps: 698, steps per second:  45, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.655 [0.000, 5.000],  loss: 0.014912, mae: 2.633610, mean_q: 3.168408, mean_eps: 0.112176
 4440485/6000000: episode: 5682, duration: 22.126s, episode steps: 1014, steps per second:  46, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.014115, mae: 2.620943, mean_q: 3.153168, mean_eps: 0.112004
 4441410/6000000: episode: 5683, duration: 20.008s, episode steps: 925, steps per second:  46, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.114 [0.000, 5.000],  loss: 0.016179, mae: 2.632734, mean_q: 3.167689, mean_eps: 0.111810
 4442671/6000000: episode: 5684, duration: 27.819s, episode steps: 1261, steps per second:  45, episode reward: 32.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.201 [0.000, 5.000],  loss: 0.014959, mae: 2.623002, mean_q: 3.155830, mean_eps: 0.111592
 4443456/6000000: episode: 5685, duration: 15.973s, episode steps: 785, steps per second:  49, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.865 [0.000, 5.000],  loss: 0.014059, mae: 2.609576, mean_q: 3.141381, mean_eps: 0.111388
 4444268/6000000: episode: 5686, duration: 16.909s, episode steps: 812, steps per second:  48, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.080 [0.000, 5.000],  loss: 0.014326, mae: 2.627998, mean_q: 3.161934, mean_eps: 0.111228
 4444970/6000000: episode: 5687, duration: 15.464s, episode steps: 702, steps per second:  45, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.826 [0.000, 5.000],  loss: 0.012694, mae: 2.620112, mean_q: 3.153028, mean_eps: 0.111076
 4445779/6000000: episode: 5688, duration: 18.022s, episode steps: 809, steps per second:  45, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.126 [0.000, 5.000],  loss: 0.013252, mae: 2.598917, mean_q: 3.127268, mean_eps: 0.110925
 4446348/6000000: episode: 5689, duration: 13.080s, episode steps: 569, steps per second:  44, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.178 [0.000, 5.000],  loss: 0.015015, mae: 2.596984, mean_q: 3.125537, mean_eps: 0.110788
 4447166/6000000: episode: 5690, duration: 18.480s, episode steps: 818, steps per second:  44, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.982 [0.000, 5.000],  loss: 0.012520, mae: 2.608153, mean_q: 3.138337, mean_eps: 0.110649
 4447876/6000000: episode: 5691, duration: 16.523s, episode steps: 710, steps per second:  43, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.769 [0.000, 5.000],  loss: 0.014820, mae: 2.622819, mean_q: 3.156271, mean_eps: 0.110496
 4449289/6000000: episode: 5692, duration: 34.333s, episode steps: 1413, steps per second:  41, episode reward: 30.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.182 [0.000, 5.000],  loss: 0.013247, mae: 2.594417, mean_q: 3.122035, mean_eps: 0.110284
 4450159/6000000: episode: 5693, duration: 21.183s, episode steps: 870, steps per second:  41, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.972 [0.000, 5.000],  loss: 0.013761, mae: 2.608848, mean_q: 3.139416, mean_eps: 0.110055
 4451487/6000000: episode: 5694, duration: 30.676s, episode steps: 1328, steps per second:  43, episode reward: 28.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.636 [0.000, 5.000],  loss: 0.013659, mae: 2.590416, mean_q: 3.117451, mean_eps: 0.109836
 4452055/6000000: episode: 5695, duration: 12.494s, episode steps: 568, steps per second:  45, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.014862, mae: 2.585595, mean_q: 3.112870, mean_eps: 0.109646
 4452896/6000000: episode: 5696, duration: 18.133s, episode steps: 841, steps per second:  46, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.013502, mae: 2.597588, mean_q: 3.127713, mean_eps: 0.109505
 4453805/6000000: episode: 5697, duration: 19.680s, episode steps: 909, steps per second:  46, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.013716, mae: 2.583178, mean_q: 3.108284, mean_eps: 0.109330
 4454610/6000000: episode: 5698, duration: 16.900s, episode steps: 805, steps per second:  48, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.963 [0.000, 5.000],  loss: 0.014054, mae: 2.588745, mean_q: 3.117024, mean_eps: 0.109158
 4455480/6000000: episode: 5699, duration: 18.921s, episode steps: 870, steps per second:  46, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.190 [0.000, 5.000],  loss: 0.012545, mae: 2.569227, mean_q: 3.093635, mean_eps: 0.108991
 4456113/6000000: episode: 5700, duration: 14.051s, episode steps: 633, steps per second:  45, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.014575, mae: 2.590944, mean_q: 3.118625, mean_eps: 0.108841
 4457467/6000000: episode: 5701, duration: 29.675s, episode steps: 1354, steps per second:  46, episode reward: 31.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.152 [0.000, 5.000],  loss: 0.014316, mae: 2.594319, mean_q: 3.121311, mean_eps: 0.108642
 4458836/6000000: episode: 5702, duration: 30.160s, episode steps: 1369, steps per second:  45, episode reward: 33.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.056 [0.000, 5.000],  loss: 0.014485, mae: 2.596321, mean_q: 3.122791, mean_eps: 0.108370
 4459711/6000000: episode: 5703, duration: 18.419s, episode steps: 875, steps per second:  48, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.254 [0.000, 5.000],  loss: 0.012298, mae: 2.582724, mean_q: 3.108787, mean_eps: 0.108146
 4460344/6000000: episode: 5704, duration: 13.638s, episode steps: 633, steps per second:  46, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.777 [0.000, 5.000],  loss: 0.013586, mae: 2.601729, mean_q: 3.132043, mean_eps: 0.107995
 4461108/6000000: episode: 5705, duration: 16.421s, episode steps: 764, steps per second:  47, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.306 [0.000, 5.000],  loss: 0.012953, mae: 2.603476, mean_q: 3.133695, mean_eps: 0.107855
 4461770/6000000: episode: 5706, duration: 15.879s, episode steps: 662, steps per second:  42, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.012171, mae: 2.599667, mean_q: 3.129977, mean_eps: 0.107712
 4462980/6000000: episode: 5707, duration: 25.561s, episode steps: 1210, steps per second:  47, episode reward: 32.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.964 [0.000, 5.000],  loss: 0.013738, mae: 2.604877, mean_q: 3.135279, mean_eps: 0.107525
 4464004/6000000: episode: 5708, duration: 22.541s, episode steps: 1024, steps per second:  45, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.992 [0.000, 5.000],  loss: 0.013874, mae: 2.593387, mean_q: 3.121833, mean_eps: 0.107302
 4464652/6000000: episode: 5709, duration: 13.871s, episode steps: 648, steps per second:  47, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.860 [0.000, 5.000],  loss: 0.015697, mae: 2.612487, mean_q: 3.144889, mean_eps: 0.107135
 4465588/6000000: episode: 5710, duration: 21.549s, episode steps: 936, steps per second:  43, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.028 [0.000, 5.000],  loss: 0.013031, mae: 2.597713, mean_q: 3.127241, mean_eps: 0.106976
 4466380/6000000: episode: 5711, duration: 19.204s, episode steps: 792, steps per second:  41, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.184 [0.000, 5.000],  loss: 0.013006, mae: 2.585184, mean_q: 3.112700, mean_eps: 0.106804
 4466913/6000000: episode: 5712, duration: 13.709s, episode steps: 533, steps per second:  39, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.013022, mae: 2.620469, mean_q: 3.154177, mean_eps: 0.106671
 4468559/6000000: episode: 5713, duration: 37.825s, episode steps: 1646, steps per second:  44, episode reward: 35.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.134 [0.000, 5.000],  loss: 0.013571, mae: 2.591638, mean_q: 3.119168, mean_eps: 0.106453
 4469609/6000000: episode: 5714, duration: 24.328s, episode steps: 1050, steps per second:  43, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.061 [0.000, 5.000],  loss: 0.014191, mae: 2.606449, mean_q: 3.136199, mean_eps: 0.106183
 4470602/6000000: episode: 5715, duration: 20.967s, episode steps: 993, steps per second:  47, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.070 [0.000, 5.000],  loss: 0.013982, mae: 2.579613, mean_q: 3.104165, mean_eps: 0.105979
 4471461/6000000: episode: 5716, duration: 19.088s, episode steps: 859, steps per second:  45, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.874 [0.000, 5.000],  loss: 0.013414, mae: 2.589990, mean_q: 3.116421, mean_eps: 0.105794
 4472951/6000000: episode: 5717, duration: 34.465s, episode steps: 1490, steps per second:  43, episode reward: 34.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.187 [0.000, 5.000],  loss: 0.013616, mae: 2.592813, mean_q: 3.120074, mean_eps: 0.105559
 4473705/6000000: episode: 5718, duration: 16.327s, episode steps: 754, steps per second:  46, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.809 [0.000, 5.000],  loss: 0.012073, mae: 2.592746, mean_q: 3.121144, mean_eps: 0.105334
 4474330/6000000: episode: 5719, duration: 14.337s, episode steps: 625, steps per second:  44, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.012793, mae: 2.588387, mean_q: 3.115863, mean_eps: 0.105196
 4475197/6000000: episode: 5720, duration: 18.876s, episode steps: 867, steps per second:  46, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.012283, mae: 2.576929, mean_q: 3.101779, mean_eps: 0.105047
 4475950/6000000: episode: 5721, duration: 15.421s, episode steps: 753, steps per second:  49, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.013422, mae: 2.608379, mean_q: 3.139435, mean_eps: 0.104885
 4476930/6000000: episode: 5722, duration: 20.764s, episode steps: 980, steps per second:  47, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.014456, mae: 2.596869, mean_q: 3.123252, mean_eps: 0.104712
 4477434/6000000: episode: 5723, duration: 11.258s, episode steps: 504, steps per second:  45, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.821 [0.000, 5.000],  loss: 0.014145, mae: 2.617016, mean_q: 3.149495, mean_eps: 0.104564
 4478467/6000000: episode: 5724, duration: 22.848s, episode steps: 1033, steps per second:  45, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.014997, mae: 2.599321, mean_q: 3.126885, mean_eps: 0.104410
 4479824/6000000: episode: 5725, duration: 29.312s, episode steps: 1357, steps per second:  46, episode reward: 35.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.055 [0.000, 5.000],  loss: 0.014112, mae: 2.582252, mean_q: 3.107333, mean_eps: 0.104171
 4480802/6000000: episode: 5726, duration: 21.843s, episode steps: 978, steps per second:  45, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.962 [0.000, 5.000],  loss: 0.014832, mae: 2.619672, mean_q: 3.152332, mean_eps: 0.103938
 4481709/6000000: episode: 5727, duration: 21.393s, episode steps: 907, steps per second:  42, episode reward: 27.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.000 [0.000, 5.000],  loss: 0.013413, mae: 2.620117, mean_q: 3.151788, mean_eps: 0.103749
 4482553/6000000: episode: 5728, duration: 20.962s, episode steps: 844, steps per second:  40, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.922 [0.000, 5.000],  loss: 0.014466, mae: 2.611634, mean_q: 3.141831, mean_eps: 0.103574
 4483265/6000000: episode: 5729, duration: 16.544s, episode steps: 712, steps per second:  43, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.992 [0.000, 5.000],  loss: 0.013115, mae: 2.603131, mean_q: 3.133428, mean_eps: 0.103418
 4483969/6000000: episode: 5730, duration: 15.478s, episode steps: 704, steps per second:  45, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.273 [0.000, 5.000],  loss: 0.014144, mae: 2.619851, mean_q: 3.151660, mean_eps: 0.103276
 4484875/6000000: episode: 5731, duration: 20.173s, episode steps: 906, steps per second:  45, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.050 [0.000, 5.000],  loss: 0.012873, mae: 2.608789, mean_q: 3.139601, mean_eps: 0.103116
 4485699/6000000: episode: 5732, duration: 18.666s, episode steps: 824, steps per second:  44, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.131 [0.000, 5.000],  loss: 0.014806, mae: 2.630649, mean_q: 3.164686, mean_eps: 0.102943
 4486784/6000000: episode: 5733, duration: 23.901s, episode steps: 1085, steps per second:  45, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.901 [0.000, 5.000],  loss: 0.013598, mae: 2.616738, mean_q: 3.151351, mean_eps: 0.102752
 4487448/6000000: episode: 5734, duration: 15.383s, episode steps: 664, steps per second:  43, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.675 [0.000, 5.000],  loss: 0.015283, mae: 2.636403, mean_q: 3.173151, mean_eps: 0.102577
 4488266/6000000: episode: 5735, duration: 19.215s, episode steps: 818, steps per second:  43, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.322 [0.000, 5.000],  loss: 0.012374, mae: 2.609308, mean_q: 3.143529, mean_eps: 0.102429
 4489566/6000000: episode: 5736, duration: 29.505s, episode steps: 1300, steps per second:  44, episode reward: 32.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.013708, mae: 2.616180, mean_q: 3.147437, mean_eps: 0.102217
 4490193/6000000: episode: 5737, duration: 13.648s, episode steps: 627, steps per second:  46, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.981 [0.000, 5.000],  loss: 0.014919, mae: 2.635813, mean_q: 3.172307, mean_eps: 0.102024
 4491200/6000000: episode: 5738, duration: 22.358s, episode steps: 1007, steps per second:  45, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.013934, mae: 2.628454, mean_q: 3.164456, mean_eps: 0.101861
 4491993/6000000: episode: 5739, duration: 16.057s, episode steps: 793, steps per second:  49, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.416 [0.000, 5.000],  loss: 0.012997, mae: 2.616328, mean_q: 3.149162, mean_eps: 0.101681
 4493112/6000000: episode: 5740, duration: 23.136s, episode steps: 1119, steps per second:  48, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.256 [0.000, 5.000],  loss: 0.014419, mae: 2.629091, mean_q: 3.163664, mean_eps: 0.101490
 4493917/6000000: episode: 5741, duration: 17.409s, episode steps: 805, steps per second:  46, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.865 [0.000, 5.000],  loss: 0.015056, mae: 2.632196, mean_q: 3.167662, mean_eps: 0.101297
 4495073/6000000: episode: 5742, duration: 24.196s, episode steps: 1156, steps per second:  48, episode reward: 31.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.014203, mae: 2.625419, mean_q: 3.159734, mean_eps: 0.101101
 4496205/6000000: episode: 5743, duration: 23.309s, episode steps: 1132, steps per second:  49, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.014093, mae: 2.644031, mean_q: 3.183127, mean_eps: 0.100872
 4497396/6000000: episode: 5744, duration: 25.380s, episode steps: 1191, steps per second:  47, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.115 [0.000, 5.000],  loss: 0.012891, mae: 2.647151, mean_q: 3.185427, mean_eps: 0.100640
 4498201/6000000: episode: 5745, duration: 17.807s, episode steps: 805, steps per second:  45, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.309 [0.000, 5.000],  loss: 0.015860, mae: 2.642134, mean_q: 3.179755, mean_eps: 0.100440
 4499230/6000000: episode: 5746, duration: 24.999s, episode steps: 1029, steps per second:  41, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.243 [0.000, 5.000],  loss: 0.013582, mae: 2.632758, mean_q: 3.168985, mean_eps: 0.100257
 4500339/6000000: episode: 5747, duration: 25.451s, episode steps: 1109, steps per second:  44, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.573 [0.000, 5.000],  loss: 0.013889, mae: 2.632410, mean_q: 3.169262, mean_eps: 0.100054
 4501686/6000000: episode: 5748, duration: 30.701s, episode steps: 1347, steps per second:  44, episode reward: 26.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.684 [0.000, 5.000],  loss: 0.013303, mae: 2.635766, mean_q: 3.172967, mean_eps: 0.100000
 4502434/6000000: episode: 5749, duration: 16.492s, episode steps: 748, steps per second:  45, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.025 [0.000, 5.000],  loss: 0.014441, mae: 2.624437, mean_q: 3.158675, mean_eps: 0.100000
 4503336/6000000: episode: 5750, duration: 19.632s, episode steps: 902, steps per second:  46, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.456 [0.000, 5.000],  loss: 0.013659, mae: 2.628260, mean_q: 3.163220, mean_eps: 0.100000
 4504060/6000000: episode: 5751, duration: 17.484s, episode steps: 724, steps per second:  41, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.054 [0.000, 5.000],  loss: 0.014879, mae: 2.627335, mean_q: 3.162345, mean_eps: 0.100000
 4505076/6000000: episode: 5752, duration: 23.795s, episode steps: 1016, steps per second:  43, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.730 [0.000, 5.000],  loss: 0.016999, mae: 2.637502, mean_q: 3.172977, mean_eps: 0.100000
 4506167/6000000: episode: 5753, duration: 24.330s, episode steps: 1091, steps per second:  45, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.885 [0.000, 5.000],  loss: 0.013817, mae: 2.664254, mean_q: 3.206884, mean_eps: 0.100000
 4507033/6000000: episode: 5754, duration: 20.370s, episode steps: 866, steps per second:  43, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.387 [0.000, 5.000],  loss: 0.015206, mae: 2.663328, mean_q: 3.204386, mean_eps: 0.100000
 4508154/6000000: episode: 5755, duration: 23.805s, episode steps: 1121, steps per second:  47, episode reward: 31.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.771 [0.000, 5.000],  loss: 0.013495, mae: 2.667822, mean_q: 3.211236, mean_eps: 0.100000
 4509079/6000000: episode: 5756, duration: 19.608s, episode steps: 925, steps per second:  47, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.128 [0.000, 5.000],  loss: 0.014164, mae: 2.660730, mean_q: 3.201419, mean_eps: 0.100000
 4509729/6000000: episode: 5757, duration: 13.841s, episode steps: 650, steps per second:  47, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.254 [0.000, 5.000],  loss: 0.015281, mae: 2.663175, mean_q: 3.207399, mean_eps: 0.100000
 4510795/6000000: episode: 5758, duration: 22.276s, episode steps: 1066, steps per second:  48, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.054 [0.000, 5.000],  loss: 0.014366, mae: 2.654374, mean_q: 3.194936, mean_eps: 0.100000
 4511707/6000000: episode: 5759, duration: 18.713s, episode steps: 912, steps per second:  49, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.955 [0.000, 5.000],  loss: 0.013422, mae: 2.655428, mean_q: 3.198902, mean_eps: 0.100000
 4512788/6000000: episode: 5760, duration: 22.608s, episode steps: 1081, steps per second:  48, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.135 [0.000, 5.000],  loss: 0.014894, mae: 2.649063, mean_q: 3.188547, mean_eps: 0.100000
 4513997/6000000: episode: 5761, duration: 26.464s, episode steps: 1209, steps per second:  46, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.254 [0.000, 5.000],  loss: 0.012766, mae: 2.653526, mean_q: 3.194044, mean_eps: 0.100000
 4514791/6000000: episode: 5762, duration: 18.130s, episode steps: 794, steps per second:  44, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.869 [0.000, 5.000],  loss: 0.014571, mae: 2.637994, mean_q: 3.176140, mean_eps: 0.100000
 4515872/6000000: episode: 5763, duration: 26.612s, episode steps: 1081, steps per second:  41, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.804 [0.000, 5.000],  loss: 0.014799, mae: 2.671587, mean_q: 3.216852, mean_eps: 0.100000
 4516925/6000000: episode: 5764, duration: 23.089s, episode steps: 1053, steps per second:  46, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.227 [0.000, 5.000],  loss: 0.013563, mae: 2.638414, mean_q: 3.176997, mean_eps: 0.100000
 4518217/6000000: episode: 5765, duration: 28.575s, episode steps: 1292, steps per second:  45, episode reward: 33.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.014672, mae: 2.645375, mean_q: 3.184115, mean_eps: 0.100000
 4519149/6000000: episode: 5766, duration: 20.435s, episode steps: 932, steps per second:  46, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.161 [0.000, 5.000],  loss: 0.014040, mae: 2.637608, mean_q: 3.175532, mean_eps: 0.100000
 4520146/6000000: episode: 5767, duration: 21.390s, episode steps: 997, steps per second:  47, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.015060, mae: 2.659205, mean_q: 3.200428, mean_eps: 0.100000
 4520756/6000000: episode: 5768, duration: 13.613s, episode steps: 610, steps per second:  45, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.207 [0.000, 5.000],  loss: 0.013276, mae: 2.613599, mean_q: 3.146112, mean_eps: 0.100000
 4521783/6000000: episode: 5769, duration: 22.502s, episode steps: 1027, steps per second:  46, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.724 [0.000, 5.000],  loss: 0.013833, mae: 2.627753, mean_q: 3.163913, mean_eps: 0.100000
 4523009/6000000: episode: 5770, duration: 26.137s, episode steps: 1226, steps per second:  47, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.012194, mae: 2.627759, mean_q: 3.163670, mean_eps: 0.100000
 4523828/6000000: episode: 5771, duration: 17.793s, episode steps: 819, steps per second:  46, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.980 [0.000, 5.000],  loss: 0.014679, mae: 2.638063, mean_q: 3.175201, mean_eps: 0.100000
 4524801/6000000: episode: 5772, duration: 20.182s, episode steps: 973, steps per second:  48, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.254 [0.000, 5.000],  loss: 0.012873, mae: 2.637698, mean_q: 3.174582, mean_eps: 0.100000
 4525821/6000000: episode: 5773, duration: 21.244s, episode steps: 1020, steps per second:  48, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.093 [0.000, 5.000],  loss: 0.013800, mae: 2.619946, mean_q: 3.154017, mean_eps: 0.100000
 4527058/6000000: episode: 5774, duration: 26.081s, episode steps: 1237, steps per second:  47, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.682 [0.000, 5.000],  loss: 0.013538, mae: 2.626624, mean_q: 3.164682, mean_eps: 0.100000
 4528202/6000000: episode: 5775, duration: 23.532s, episode steps: 1144, steps per second:  49, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.035 [0.000, 5.000],  loss: 0.014844, mae: 2.615686, mean_q: 3.149579, mean_eps: 0.100000
 4529190/6000000: episode: 5776, duration: 21.113s, episode steps: 988, steps per second:  47, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.557 [0.000, 5.000],  loss: 0.013447, mae: 2.606351, mean_q: 3.138058, mean_eps: 0.100000
 4530596/6000000: episode: 5777, duration: 30.827s, episode steps: 1406, steps per second:  46, episode reward: 34.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.137 [0.000, 5.000],  loss: 0.014306, mae: 2.611412, mean_q: 3.142684, mean_eps: 0.100000
 4531221/6000000: episode: 5778, duration: 14.241s, episode steps: 625, steps per second:  44, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.874 [0.000, 5.000],  loss: 0.012221, mae: 2.631077, mean_q: 3.166201, mean_eps: 0.100000
 4531968/6000000: episode: 5779, duration: 17.416s, episode steps: 747, steps per second:  43, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.199 [0.000, 5.000],  loss: 0.012975, mae: 2.634794, mean_q: 3.171090, mean_eps: 0.100000
 4532905/6000000: episode: 5780, duration: 21.420s, episode steps: 937, steps per second:  44, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.014745, mae: 2.641912, mean_q: 3.180439, mean_eps: 0.100000
 4533778/6000000: episode: 5781, duration: 19.957s, episode steps: 873, steps per second:  44, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.733 [0.000, 5.000],  loss: 0.013581, mae: 2.646772, mean_q: 3.185837, mean_eps: 0.100000
 4534506/6000000: episode: 5782, duration: 17.018s, episode steps: 728, steps per second:  43, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.069 [0.000, 5.000],  loss: 0.012853, mae: 2.661226, mean_q: 3.203471, mean_eps: 0.100000
 4535519/6000000: episode: 5783, duration: 22.355s, episode steps: 1013, steps per second:  45, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.087 [0.000, 5.000],  loss: 0.012614, mae: 2.628196, mean_q: 3.164015, mean_eps: 0.100000
 4536526/6000000: episode: 5784, duration: 22.394s, episode steps: 1007, steps per second:  45, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.129 [0.000, 5.000],  loss: 0.012114, mae: 2.644235, mean_q: 3.183484, mean_eps: 0.100000
 4537322/6000000: episode: 5785, duration: 18.481s, episode steps: 796, steps per second:  43, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.015839, mae: 2.644029, mean_q: 3.181827, mean_eps: 0.100000
 4538313/6000000: episode: 5786, duration: 22.579s, episode steps: 991, steps per second:  44, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.076 [0.000, 5.000],  loss: 0.013959, mae: 2.645174, mean_q: 3.183020, mean_eps: 0.100000
 4539102/6000000: episode: 5787, duration: 17.592s, episode steps: 789, steps per second:  45, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.013264, mae: 2.625120, mean_q: 3.160204, mean_eps: 0.100000
 4540276/6000000: episode: 5788, duration: 26.473s, episode steps: 1174, steps per second:  44, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.012379, mae: 2.652007, mean_q: 3.190332, mean_eps: 0.100000
 4540648/6000000: episode: 5789, duration: 7.905s, episode steps: 372, steps per second:  47, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.774 [0.000, 5.000],  loss: 0.012968, mae: 2.622590, mean_q: 3.156463, mean_eps: 0.100000
 4541351/6000000: episode: 5790, duration: 14.535s, episode steps: 703, steps per second:  48, episode reward: 20.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.012952, mae: 2.652424, mean_q: 3.192312, mean_eps: 0.100000
 4542246/6000000: episode: 5791, duration: 18.796s, episode steps: 895, steps per second:  48, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.859 [0.000, 5.000],  loss: 0.013954, mae: 2.657268, mean_q: 3.197420, mean_eps: 0.100000
 4543103/6000000: episode: 5792, duration: 18.684s, episode steps: 857, steps per second:  46, episode reward: 26.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.790 [0.000, 5.000],  loss: 0.015212, mae: 2.636951, mean_q: 3.173872, mean_eps: 0.100000
 4544006/6000000: episode: 5793, duration: 19.288s, episode steps: 903, steps per second:  47, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.775 [0.000, 5.000],  loss: 0.014115, mae: 2.651936, mean_q: 3.191205, mean_eps: 0.100000
 4545094/6000000: episode: 5794, duration: 24.949s, episode steps: 1088, steps per second:  44, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.582 [0.000, 5.000],  loss: 0.013465, mae: 2.641664, mean_q: 3.180823, mean_eps: 0.100000
 4545812/6000000: episode: 5795, duration: 16.935s, episode steps: 718, steps per second:  42, episode reward: 21.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.908 [0.000, 5.000],  loss: 0.014671, mae: 2.659351, mean_q: 3.199585, mean_eps: 0.100000
 4547168/6000000: episode: 5796, duration: 31.012s, episode steps: 1356, steps per second:  44, episode reward: 33.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.271 [0.000, 5.000],  loss: 0.013282, mae: 2.672229, mean_q: 3.216468, mean_eps: 0.100000
 4548182/6000000: episode: 5797, duration: 25.852s, episode steps: 1014, steps per second:  39, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.013709, mae: 2.676498, mean_q: 3.223140, mean_eps: 0.100000
 4549370/6000000: episode: 5798, duration: 26.845s, episode steps: 1188, steps per second:  44, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.145 [0.000, 5.000],  loss: 0.014514, mae: 2.664721, mean_q: 3.208551, mean_eps: 0.100000
 4550463/6000000: episode: 5799, duration: 23.964s, episode steps: 1093, steps per second:  46, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.294 [0.000, 5.000],  loss: 0.014176, mae: 2.660845, mean_q: 3.201070, mean_eps: 0.100000
 4551449/6000000: episode: 5800, duration: 21.254s, episode steps: 986, steps per second:  46, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.015686, mae: 2.655018, mean_q: 3.194903, mean_eps: 0.100000
 4552095/6000000: episode: 5801, duration: 13.701s, episode steps: 646, steps per second:  47, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 0.014874, mae: 2.647503, mean_q: 3.186742, mean_eps: 0.100000
 4552937/6000000: episode: 5802, duration: 18.734s, episode steps: 842, steps per second:  45, episode reward: 25.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.681 [0.000, 5.000],  loss: 0.015273, mae: 2.654288, mean_q: 3.192958, mean_eps: 0.100000
 4554028/6000000: episode: 5803, duration: 24.109s, episode steps: 1091, steps per second:  45, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.013765, mae: 2.651693, mean_q: 3.190461, mean_eps: 0.100000
 4555133/6000000: episode: 5804, duration: 25.003s, episode steps: 1105, steps per second:  44, episode reward: 31.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.196 [0.000, 5.000],  loss: 0.013953, mae: 2.653503, mean_q: 3.193124, mean_eps: 0.100000
 4556196/6000000: episode: 5805, duration: 24.417s, episode steps: 1063, steps per second:  44, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.325 [0.000, 5.000],  loss: 0.013192, mae: 2.647772, mean_q: 3.186775, mean_eps: 0.100000
 4556814/6000000: episode: 5806, duration: 13.026s, episode steps: 618, steps per second:  47, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.010 [0.000, 5.000],  loss: 0.013380, mae: 2.641956, mean_q: 3.180217, mean_eps: 0.100000
 4558026/6000000: episode: 5807, duration: 25.600s, episode steps: 1212, steps per second:  47, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.013651, mae: 2.632880, mean_q: 3.168334, mean_eps: 0.100000
 4559475/6000000: episode: 5808, duration: 32.255s, episode steps: 1449, steps per second:  45, episode reward: 32.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.164 [0.000, 5.000],  loss: 0.013625, mae: 2.641813, mean_q: 3.179272, mean_eps: 0.100000
 4560237/6000000: episode: 5809, duration: 15.873s, episode steps: 762, steps per second:  48, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.015923, mae: 2.652036, mean_q: 3.190039, mean_eps: 0.100000
 4561531/6000000: episode: 5810, duration: 27.450s, episode steps: 1294, steps per second:  47, episode reward: 31.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.097 [0.000, 5.000],  loss: 0.013823, mae: 2.619850, mean_q: 3.153119, mean_eps: 0.100000
 4562288/6000000: episode: 5811, duration: 16.686s, episode steps: 757, steps per second:  45, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.950 [0.000, 5.000],  loss: 0.015735, mae: 2.628279, mean_q: 3.162808, mean_eps: 0.100000
 4563433/6000000: episode: 5812, duration: 25.133s, episode steps: 1145, steps per second:  46, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.014756, mae: 2.647457, mean_q: 3.185860, mean_eps: 0.100000
 4563968/6000000: episode: 5813, duration: 13.049s, episode steps: 535, steps per second:  41, episode reward: 16.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.890 [0.000, 5.000],  loss: 0.015360, mae: 2.627308, mean_q: 3.161143, mean_eps: 0.100000
 4564717/6000000: episode: 5814, duration: 18.611s, episode steps: 749, steps per second:  40, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.014410, mae: 2.633207, mean_q: 3.169410, mean_eps: 0.100000
 4565666/6000000: episode: 5815, duration: 20.527s, episode steps: 949, steps per second:  46, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.271 [0.000, 5.000],  loss: 0.013686, mae: 2.632956, mean_q: 3.168698, mean_eps: 0.100000
 4566426/6000000: episode: 5816, duration: 16.958s, episode steps: 760, steps per second:  45, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.203 [0.000, 5.000],  loss: 0.013059, mae: 2.611043, mean_q: 3.142133, mean_eps: 0.100000
 4567238/6000000: episode: 5817, duration: 18.487s, episode steps: 812, steps per second:  44, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.097 [0.000, 5.000],  loss: 0.012624, mae: 2.606733, mean_q: 3.138163, mean_eps: 0.100000
 4568321/6000000: episode: 5818, duration: 24.773s, episode steps: 1083, steps per second:  44, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.197 [0.000, 5.000],  loss: 0.014218, mae: 2.616554, mean_q: 3.148815, mean_eps: 0.100000
 4569457/6000000: episode: 5819, duration: 26.185s, episode steps: 1136, steps per second:  43, episode reward: 31.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.015056, mae: 2.600932, mean_q: 3.129025, mean_eps: 0.100000
 4569797/6000000: episode: 5820, duration: 8.041s, episode steps: 340, steps per second:  42, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.726 [0.000, 5.000],  loss: 0.015209, mae: 2.608436, mean_q: 3.137292, mean_eps: 0.100000
 4570690/6000000: episode: 5821, duration: 19.586s, episode steps: 893, steps per second:  46, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.066 [0.000, 5.000],  loss: 0.014469, mae: 2.631963, mean_q: 3.165891, mean_eps: 0.100000
 4571679/6000000: episode: 5822, duration: 22.107s, episode steps: 989, steps per second:  45, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.266 [0.000, 5.000],  loss: 0.012899, mae: 2.615433, mean_q: 3.147198, mean_eps: 0.100000
 4572601/6000000: episode: 5823, duration: 20.544s, episode steps: 922, steps per second:  45, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.629 [0.000, 5.000],  loss: 0.015772, mae: 2.597028, mean_q: 3.124226, mean_eps: 0.100000
 4573385/6000000: episode: 5824, duration: 16.349s, episode steps: 784, steps per second:  48, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.756 [0.000, 5.000],  loss: 0.015216, mae: 2.622984, mean_q: 3.155525, mean_eps: 0.100000
 4574480/6000000: episode: 5825, duration: 23.753s, episode steps: 1095, steps per second:  46, episode reward: 31.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.013651, mae: 2.612800, mean_q: 3.143693, mean_eps: 0.100000
 4575400/6000000: episode: 5826, duration: 20.299s, episode steps: 920, steps per second:  45, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.821 [0.000, 5.000],  loss: 0.012758, mae: 2.608952, mean_q: 3.139210, mean_eps: 0.100000
 4576180/6000000: episode: 5827, duration: 16.394s, episode steps: 780, steps per second:  48, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.015831, mae: 2.613920, mean_q: 3.143409, mean_eps: 0.100000
 4577608/6000000: episode: 5828, duration: 30.510s, episode steps: 1428, steps per second:  47, episode reward: 32.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.838 [0.000, 5.000],  loss: 0.013863, mae: 2.613763, mean_q: 3.145926, mean_eps: 0.100000
 4578856/6000000: episode: 5829, duration: 27.959s, episode steps: 1248, steps per second:  45, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.842 [0.000, 5.000],  loss: 0.013647, mae: 2.596745, mean_q: 3.125113, mean_eps: 0.100000
 4579745/6000000: episode: 5830, duration: 20.873s, episode steps: 889, steps per second:  43, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.694 [0.000, 5.000],  loss: 0.013578, mae: 2.596402, mean_q: 3.124916, mean_eps: 0.100000
 4581249/6000000: episode: 5831, duration: 34.342s, episode steps: 1504, steps per second:  44, episode reward: 33.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.964 [0.000, 5.000],  loss: 0.013486, mae: 2.611760, mean_q: 3.143402, mean_eps: 0.100000
 4582516/6000000: episode: 5832, duration: 28.986s, episode steps: 1267, steps per second:  44, episode reward: 35.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.756 [0.000, 5.000],  loss: 0.013420, mae: 2.615078, mean_q: 3.148135, mean_eps: 0.100000
 4583205/6000000: episode: 5833, duration: 16.058s, episode steps: 689, steps per second:  43, episode reward: 22.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.010 [0.000, 5.000],  loss: 0.012730, mae: 2.596914, mean_q: 3.126398, mean_eps: 0.100000
 4584460/6000000: episode: 5834, duration: 27.565s, episode steps: 1255, steps per second:  46, episode reward: 28.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.143 [0.000, 5.000],  loss: 0.013855, mae: 2.594180, mean_q: 3.121528, mean_eps: 0.100000
 4585690/6000000: episode: 5835, duration: 27.014s, episode steps: 1230, steps per second:  46, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.306 [0.000, 5.000],  loss: 0.014689, mae: 2.599354, mean_q: 3.127254, mean_eps: 0.100000
 4586557/6000000: episode: 5836, duration: 19.433s, episode steps: 867, steps per second:  45, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.040 [0.000, 5.000],  loss: 0.014209, mae: 2.589856, mean_q: 3.116936, mean_eps: 0.100000
 4588023/6000000: episode: 5837, duration: 31.558s, episode steps: 1466, steps per second:  46, episode reward: 36.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.866 [0.000, 5.000],  loss: 0.014381, mae: 2.602454, mean_q: 3.134001, mean_eps: 0.100000
 4589290/6000000: episode: 5838, duration: 27.054s, episode steps: 1267, steps per second:  47, episode reward: 31.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.779 [0.000, 5.000],  loss: 0.013536, mae: 2.595677, mean_q: 3.124652, mean_eps: 0.100000
 4590240/6000000: episode: 5839, duration: 19.210s, episode steps: 950, steps per second:  49, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.245 [0.000, 5.000],  loss: 0.014147, mae: 2.586414, mean_q: 3.112781, mean_eps: 0.100000
 4591062/6000000: episode: 5840, duration: 18.026s, episode steps: 822, steps per second:  46, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.013932, mae: 2.598370, mean_q: 3.128002, mean_eps: 0.100000
 4592108/6000000: episode: 5841, duration: 22.663s, episode steps: 1046, steps per second:  46, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.930 [0.000, 5.000],  loss: 0.014315, mae: 2.603145, mean_q: 3.133119, mean_eps: 0.100000
 4593304/6000000: episode: 5842, duration: 24.745s, episode steps: 1196, steps per second:  48, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.015461, mae: 2.598815, mean_q: 3.129491, mean_eps: 0.100000
 4594017/6000000: episode: 5843, duration: 15.043s, episode steps: 713, steps per second:  47, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.013669, mae: 2.600141, mean_q: 3.130294, mean_eps: 0.100000
 4595089/6000000: episode: 5844, duration: 23.253s, episode steps: 1072, steps per second:  46, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.899 [0.000, 5.000],  loss: 0.014933, mae: 2.602112, mean_q: 3.131960, mean_eps: 0.100000
 4596010/6000000: episode: 5845, duration: 20.681s, episode steps: 921, steps per second:  45, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.922 [0.000, 5.000],  loss: 0.015735, mae: 2.602789, mean_q: 3.132596, mean_eps: 0.100000
 4596367/6000000: episode: 5846, duration: 8.517s, episode steps: 357, steps per second:  42, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.036 [0.000, 5.000],  loss: 0.015108, mae: 2.622447, mean_q: 3.155327, mean_eps: 0.100000
 4597244/6000000: episode: 5847, duration: 20.410s, episode steps: 877, steps per second:  43, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.079 [0.000, 5.000],  loss: 0.013983, mae: 2.624531, mean_q: 3.157922, mean_eps: 0.100000
 4598357/6000000: episode: 5848, duration: 23.834s, episode steps: 1113, steps per second:  47, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.015393, mae: 2.626873, mean_q: 3.160960, mean_eps: 0.100000
 4599314/6000000: episode: 5849, duration: 20.617s, episode steps: 957, steps per second:  46, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.096 [0.000, 5.000],  loss: 0.015321, mae: 2.643613, mean_q: 3.181810, mean_eps: 0.100000
 4599845/6000000: episode: 5850, duration: 12.213s, episode steps: 531, steps per second:  43, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.299 [0.000, 5.000],  loss: 0.013512, mae: 2.625595, mean_q: 3.159490, mean_eps: 0.100000
 4600583/6000000: episode: 5851, duration: 15.444s, episode steps: 738, steps per second:  48, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.869 [0.000, 5.000],  loss: 0.015409, mae: 2.598677, mean_q: 3.126165, mean_eps: 0.100000
 4601218/6000000: episode: 5852, duration: 12.940s, episode steps: 635, steps per second:  49, episode reward: 18.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.013938, mae: 2.604373, mean_q: 3.134524, mean_eps: 0.100000
 4602007/6000000: episode: 5853, duration: 17.858s, episode steps: 789, steps per second:  44, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.015507, mae: 2.610605, mean_q: 3.141674, mean_eps: 0.100000
 4603063/6000000: episode: 5854, duration: 23.102s, episode steps: 1056, steps per second:  46, episode reward: 34.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.015303, mae: 2.594068, mean_q: 3.120305, mean_eps: 0.100000
 4604032/6000000: episode: 5855, duration: 21.511s, episode steps: 969, steps per second:  45, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.981 [0.000, 5.000],  loss: 0.014481, mae: 2.594710, mean_q: 3.121963, mean_eps: 0.100000
 4604838/6000000: episode: 5856, duration: 17.991s, episode steps: 806, steps per second:  45, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.014572, mae: 2.580939, mean_q: 3.105426, mean_eps: 0.100000
 4605615/6000000: episode: 5857, duration: 16.870s, episode steps: 777, steps per second:  46, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.936 [0.000, 5.000],  loss: 0.013000, mae: 2.599880, mean_q: 3.128239, mean_eps: 0.100000
 4606570/6000000: episode: 5858, duration: 19.724s, episode steps: 955, steps per second:  48, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.096 [0.000, 5.000],  loss: 0.015120, mae: 2.573728, mean_q: 3.096476, mean_eps: 0.100000
 4607650/6000000: episode: 5859, duration: 22.788s, episode steps: 1080, steps per second:  47, episode reward: 30.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.989 [0.000, 5.000],  loss: 0.014894, mae: 2.584276, mean_q: 3.109566, mean_eps: 0.100000
 4609001/6000000: episode: 5860, duration: 27.829s, episode steps: 1351, steps per second:  49, episode reward: 35.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.976 [0.000, 5.000],  loss: 0.014002, mae: 2.576012, mean_q: 3.100007, mean_eps: 0.100000
 4610460/6000000: episode: 5861, duration: 30.851s, episode steps: 1459, steps per second:  47, episode reward: 36.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.948 [0.000, 5.000],  loss: 0.014522, mae: 2.567189, mean_q: 3.088188, mean_eps: 0.100000
 4611448/6000000: episode: 5862, duration: 22.313s, episode steps: 988, steps per second:  44, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.010 [0.000, 5.000],  loss: 0.014091, mae: 2.593144, mean_q: 3.121299, mean_eps: 0.100000
 4612527/6000000: episode: 5863, duration: 24.268s, episode steps: 1079, steps per second:  44, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.883 [0.000, 5.000],  loss: 0.013576, mae: 2.574315, mean_q: 3.097776, mean_eps: 0.100000
 4613651/6000000: episode: 5864, duration: 29.830s, episode steps: 1124, steps per second:  38, episode reward: 30.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.130 [0.000, 5.000],  loss: 0.014436, mae: 2.570958, mean_q: 3.094071, mean_eps: 0.100000
 4614564/6000000: episode: 5865, duration: 19.896s, episode steps: 913, steps per second:  46, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.079 [0.000, 5.000],  loss: 0.013750, mae: 2.596745, mean_q: 3.124320, mean_eps: 0.100000
 4615439/6000000: episode: 5866, duration: 18.787s, episode steps: 875, steps per second:  47, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.637 [0.000, 5.000],  loss: 0.013868, mae: 2.581242, mean_q: 3.107348, mean_eps: 0.100000
 4616419/6000000: episode: 5867, duration: 21.994s, episode steps: 980, steps per second:  45, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.788 [0.000, 5.000],  loss: 0.013841, mae: 2.574402, mean_q: 3.099555, mean_eps: 0.100000
 4617515/6000000: episode: 5868, duration: 24.059s, episode steps: 1096, steps per second:  46, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.783 [0.000, 5.000],  loss: 0.014612, mae: 2.575948, mean_q: 3.099891, mean_eps: 0.100000
 4618327/6000000: episode: 5869, duration: 19.778s, episode steps: 812, steps per second:  41, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.060 [0.000, 5.000],  loss: 0.014466, mae: 2.590762, mean_q: 3.117672, mean_eps: 0.100000
 4619177/6000000: episode: 5870, duration: 19.654s, episode steps: 850, steps per second:  43, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.014312, mae: 2.594075, mean_q: 3.121690, mean_eps: 0.100000
 4620074/6000000: episode: 5871, duration: 19.195s, episode steps: 897, steps per second:  47, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.594 [0.000, 5.000],  loss: 0.012504, mae: 2.575809, mean_q: 3.100751, mean_eps: 0.100000
 4620670/6000000: episode: 5872, duration: 13.171s, episode steps: 596, steps per second:  45, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.767 [0.000, 5.000],  loss: 0.014532, mae: 2.606491, mean_q: 3.137182, mean_eps: 0.100000
 4621550/6000000: episode: 5873, duration: 19.093s, episode steps: 880, steps per second:  46, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.210 [0.000, 5.000],  loss: 0.013016, mae: 2.590682, mean_q: 3.117460, mean_eps: 0.100000
 4622516/6000000: episode: 5874, duration: 19.643s, episode steps: 966, steps per second:  49, episode reward: 29.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.291 [0.000, 5.000],  loss: 0.015339, mae: 2.564564, mean_q: 3.086095, mean_eps: 0.100000
 4623688/6000000: episode: 5875, duration: 23.888s, episode steps: 1172, steps per second:  49, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.912 [0.000, 5.000],  loss: 0.014009, mae: 2.559822, mean_q: 3.081461, mean_eps: 0.100000
 4624558/6000000: episode: 5876, duration: 18.528s, episode steps: 870, steps per second:  47, episode reward: 26.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.014250, mae: 2.577099, mean_q: 3.100518, mean_eps: 0.100000
 4625311/6000000: episode: 5877, duration: 15.656s, episode steps: 753, steps per second:  48, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.014108, mae: 2.568999, mean_q: 3.092230, mean_eps: 0.100000
 4626624/6000000: episode: 5878, duration: 27.710s, episode steps: 1313, steps per second:  47, episode reward: 29.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.013341, mae: 2.559241, mean_q: 3.080008, mean_eps: 0.100000
 4627346/6000000: episode: 5879, duration: 15.273s, episode steps: 722, steps per second:  47, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: 0.013384, mae: 2.552276, mean_q: 3.072638, mean_eps: 0.100000
 4628268/6000000: episode: 5880, duration: 20.674s, episode steps: 922, steps per second:  45, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.799 [0.000, 5.000],  loss: 0.013191, mae: 2.554865, mean_q: 3.077526, mean_eps: 0.100000
 4629291/6000000: episode: 5881, duration: 22.840s, episode steps: 1023, steps per second:  45, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.247 [0.000, 5.000],  loss: 0.014929, mae: 2.574665, mean_q: 3.097592, mean_eps: 0.100000
 4630620/6000000: episode: 5882, duration: 30.580s, episode steps: 1329, steps per second:  43, episode reward: 34.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.065 [0.000, 5.000],  loss: 0.014820, mae: 2.574638, mean_q: 3.098801, mean_eps: 0.100000
 4631362/6000000: episode: 5883, duration: 16.937s, episode steps: 742, steps per second:  44, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.972 [0.000, 5.000],  loss: 0.015335, mae: 2.563594, mean_q: 3.085057, mean_eps: 0.100000
 4632361/6000000: episode: 5884, duration: 22.142s, episode steps: 999, steps per second:  45, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.928 [0.000, 5.000],  loss: 0.015575, mae: 2.568097, mean_q: 3.092392, mean_eps: 0.100000
 4633486/6000000: episode: 5885, duration: 24.180s, episode steps: 1125, steps per second:  47, episode reward: 32.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.996 [0.000, 5.000],  loss: 0.014943, mae: 2.572272, mean_q: 3.096170, mean_eps: 0.100000
 4634202/6000000: episode: 5886, duration: 15.120s, episode steps: 716, steps per second:  47, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.013994, mae: 2.564696, mean_q: 3.088282, mean_eps: 0.100000
 4634921/6000000: episode: 5887, duration: 16.737s, episode steps: 719, steps per second:  43, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.108 [0.000, 5.000],  loss: 0.014741, mae: 2.575105, mean_q: 3.101321, mean_eps: 0.100000
 4635873/6000000: episode: 5888, duration: 21.651s, episode steps: 952, steps per second:  44, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.959 [0.000, 5.000],  loss: 0.012155, mae: 2.581534, mean_q: 3.110344, mean_eps: 0.100000
 4636981/6000000: episode: 5889, duration: 24.204s, episode steps: 1108, steps per second:  46, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.098 [0.000, 5.000],  loss: 0.015352, mae: 2.575821, mean_q: 3.100192, mean_eps: 0.100000
 4638055/6000000: episode: 5890, duration: 23.739s, episode steps: 1074, steps per second:  45, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.013190, mae: 2.583926, mean_q: 3.110577, mean_eps: 0.100000
 4638868/6000000: episode: 5891, duration: 16.973s, episode steps: 813, steps per second:  48, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.736 [0.000, 5.000],  loss: 0.015851, mae: 2.574253, mean_q: 3.099102, mean_eps: 0.100000
 4640096/6000000: episode: 5892, duration: 25.686s, episode steps: 1228, steps per second:  48, episode reward: 30.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.884 [0.000, 5.000],  loss: 0.013663, mae: 2.583513, mean_q: 3.112504, mean_eps: 0.100000
 4640958/6000000: episode: 5893, duration: 18.568s, episode steps: 862, steps per second:  46, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.317 [0.000, 5.000],  loss: 0.013998, mae: 2.591973, mean_q: 3.121717, mean_eps: 0.100000
 4641804/6000000: episode: 5894, duration: 18.103s, episode steps: 846, steps per second:  47, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.936 [0.000, 5.000],  loss: 0.014594, mae: 2.598545, mean_q: 3.128228, mean_eps: 0.100000
 4642557/6000000: episode: 5895, duration: 16.033s, episode steps: 753, steps per second:  47, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.013853, mae: 2.586952, mean_q: 3.115330, mean_eps: 0.100000
 4643367/6000000: episode: 5896, duration: 18.385s, episode steps: 810, steps per second:  44, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.033 [0.000, 5.000],  loss: 0.013332, mae: 2.610236, mean_q: 3.142752, mean_eps: 0.100000
 4644458/6000000: episode: 5897, duration: 25.184s, episode steps: 1091, steps per second:  43, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.975 [0.000, 5.000],  loss: 0.013493, mae: 2.597120, mean_q: 3.126735, mean_eps: 0.100000
 4645153/6000000: episode: 5898, duration: 17.712s, episode steps: 695, steps per second:  39, episode reward: 20.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.014181, mae: 2.607415, mean_q: 3.138179, mean_eps: 0.100000
 4646138/6000000: episode: 5899, duration: 25.013s, episode steps: 985, steps per second:  39, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.712 [0.000, 5.000],  loss: 0.013834, mae: 2.598206, mean_q: 3.127022, mean_eps: 0.100000
 4647092/6000000: episode: 5900, duration: 22.435s, episode steps: 954, steps per second:  43, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.014192, mae: 2.585227, mean_q: 3.110789, mean_eps: 0.100000
 4648336/6000000: episode: 5901, duration: 28.633s, episode steps: 1244, steps per second:  43, episode reward: 34.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.181 [0.000, 5.000],  loss: 0.013872, mae: 2.601851, mean_q: 3.131800, mean_eps: 0.100000
 4649218/6000000: episode: 5902, duration: 18.983s, episode steps: 882, steps per second:  46, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.880 [0.000, 5.000],  loss: 0.015187, mae: 2.591656, mean_q: 3.119841, mean_eps: 0.100000
 4650227/6000000: episode: 5903, duration: 22.137s, episode steps: 1009, steps per second:  46, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.924 [0.000, 5.000],  loss: 0.013677, mae: 2.588548, mean_q: 3.116940, mean_eps: 0.100000
 4651038/6000000: episode: 5904, duration: 17.972s, episode steps: 811, steps per second:  45, episode reward: 25.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.015330, mae: 2.581200, mean_q: 3.107206, mean_eps: 0.100000
 4651822/6000000: episode: 5905, duration: 17.198s, episode steps: 784, steps per second:  46, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.686 [0.000, 5.000],  loss: 0.013526, mae: 2.575665, mean_q: 3.100532, mean_eps: 0.100000
 4652688/6000000: episode: 5906, duration: 18.823s, episode steps: 866, steps per second:  46, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.932 [0.000, 5.000],  loss: 0.014009, mae: 2.584436, mean_q: 3.112716, mean_eps: 0.100000
 4653349/6000000: episode: 5907, duration: 14.199s, episode steps: 661, steps per second:  47, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.204 [0.000, 5.000],  loss: 0.014684, mae: 2.577804, mean_q: 3.101984, mean_eps: 0.100000
 4654342/6000000: episode: 5908, duration: 21.495s, episode steps: 993, steps per second:  46, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.180 [0.000, 5.000],  loss: 0.013481, mae: 2.586063, mean_q: 3.112494, mean_eps: 0.100000
 4655314/6000000: episode: 5909, duration: 19.889s, episode steps: 972, steps per second:  49, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.014128, mae: 2.568298, mean_q: 3.090710, mean_eps: 0.100000
 4656405/6000000: episode: 5910, duration: 23.072s, episode steps: 1091, steps per second:  47, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.590 [0.000, 5.000],  loss: 0.016126, mae: 2.577182, mean_q: 3.102759, mean_eps: 0.100000
 4657273/6000000: episode: 5911, duration: 18.245s, episode steps: 868, steps per second:  48, episode reward: 28.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 1.894 [0.000, 5.000],  loss: 0.015859, mae: 2.575604, mean_q: 3.099450, mean_eps: 0.100000
 4658180/6000000: episode: 5912, duration: 19.963s, episode steps: 907, steps per second:  45, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.830 [0.000, 5.000],  loss: 0.014001, mae: 2.576931, mean_q: 3.100978, mean_eps: 0.100000
 4659062/6000000: episode: 5913, duration: 18.590s, episode steps: 882, steps per second:  47, episode reward: 28.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.121 [0.000, 5.000],  loss: 0.014281, mae: 2.573145, mean_q: 3.096949, mean_eps: 0.100000
 4660060/6000000: episode: 5914, duration: 20.623s, episode steps: 998, steps per second:  48, episode reward: 30.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.248 [0.000, 5.000],  loss: 0.014336, mae: 2.567327, mean_q: 3.090342, mean_eps: 0.100000
 4661089/6000000: episode: 5915, duration: 22.273s, episode steps: 1029, steps per second:  46, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.751 [0.000, 5.000],  loss: 0.014499, mae: 2.586277, mean_q: 3.113612, mean_eps: 0.100000
 4662009/6000000: episode: 5916, duration: 20.827s, episode steps: 920, steps per second:  44, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.874 [0.000, 5.000],  loss: 0.015190, mae: 2.584138, mean_q: 3.110615, mean_eps: 0.100000
 4662749/6000000: episode: 5917, duration: 18.353s, episode steps: 740, steps per second:  40, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.123 [0.000, 5.000],  loss: 0.013723, mae: 2.584919, mean_q: 3.110874, mean_eps: 0.100000
 4663586/6000000: episode: 5918, duration: 18.707s, episode steps: 837, steps per second:  45, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.141 [0.000, 5.000],  loss: 0.014015, mae: 2.577065, mean_q: 3.103239, mean_eps: 0.100000
 4664395/6000000: episode: 5919, duration: 19.523s, episode steps: 809, steps per second:  41, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.555 [0.000, 5.000],  loss: 0.015062, mae: 2.580067, mean_q: 3.106205, mean_eps: 0.100000
 4664894/6000000: episode: 5920, duration: 12.189s, episode steps: 499, steps per second:  41, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.013137, mae: 2.560101, mean_q: 3.082261, mean_eps: 0.100000
 4666068/6000000: episode: 5921, duration: 26.170s, episode steps: 1174, steps per second:  45, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.971 [0.000, 5.000],  loss: 0.014199, mae: 2.582704, mean_q: 3.109823, mean_eps: 0.100000
 4666866/6000000: episode: 5922, duration: 18.390s, episode steps: 798, steps per second:  43, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.837 [0.000, 5.000],  loss: 0.014550, mae: 2.574476, mean_q: 3.099324, mean_eps: 0.100000
 4667661/6000000: episode: 5923, duration: 18.748s, episode steps: 795, steps per second:  42, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.070 [0.000, 5.000],  loss: 0.013679, mae: 2.597756, mean_q: 3.126308, mean_eps: 0.100000
 4668905/6000000: episode: 5924, duration: 27.401s, episode steps: 1244, steps per second:  45, episode reward: 35.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.859 [0.000, 5.000],  loss: 0.014492, mae: 2.584981, mean_q: 3.111113, mean_eps: 0.100000
 4670173/6000000: episode: 5925, duration: 27.423s, episode steps: 1268, steps per second:  46, episode reward: 32.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.972 [0.000, 5.000],  loss: 0.013748, mae: 2.592492, mean_q: 3.120093, mean_eps: 0.100000
 4670949/6000000: episode: 5926, duration: 16.096s, episode steps: 776, steps per second:  48, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.678 [0.000, 5.000],  loss: 0.014383, mae: 2.587956, mean_q: 3.114022, mean_eps: 0.100000
 4671972/6000000: episode: 5927, duration: 21.367s, episode steps: 1023, steps per second:  48, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.649 [0.000, 5.000],  loss: 0.013722, mae: 2.588514, mean_q: 3.115302, mean_eps: 0.100000
 4673147/6000000: episode: 5928, duration: 25.625s, episode steps: 1175, steps per second:  46, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.283 [0.000, 5.000],  loss: 0.013791, mae: 2.578904, mean_q: 3.103112, mean_eps: 0.100000
 4673858/6000000: episode: 5929, duration: 15.269s, episode steps: 711, steps per second:  47, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.018 [0.000, 5.000],  loss: 0.013041, mae: 2.589726, mean_q: 3.116650, mean_eps: 0.100000
 4674634/6000000: episode: 5930, duration: 16.164s, episode steps: 776, steps per second:  48, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.701 [0.000, 5.000],  loss: 0.014873, mae: 2.583559, mean_q: 3.108763, mean_eps: 0.100000
 4675839/6000000: episode: 5931, duration: 25.697s, episode steps: 1205, steps per second:  47, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.066 [0.000, 5.000],  loss: 0.015291, mae: 2.593648, mean_q: 3.119761, mean_eps: 0.100000
 4676327/6000000: episode: 5932, duration: 10.542s, episode steps: 488, steps per second:  46, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.244 [0.000, 5.000],  loss: 0.012950, mae: 2.562233, mean_q: 3.084034, mean_eps: 0.100000
 4677213/6000000: episode: 5933, duration: 20.168s, episode steps: 886, steps per second:  44, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.015967, mae: 2.580371, mean_q: 3.105976, mean_eps: 0.100000
 4677917/6000000: episode: 5934, duration: 16.411s, episode steps: 704, steps per second:  43, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.977 [0.000, 5.000],  loss: 0.015315, mae: 2.584539, mean_q: 3.109858, mean_eps: 0.100000
 4679186/6000000: episode: 5935, duration: 29.146s, episode steps: 1269, steps per second:  44, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.755 [0.000, 5.000],  loss: 0.015655, mae: 2.576074, mean_q: 3.100332, mean_eps: 0.100000
 4680323/6000000: episode: 5936, duration: 25.244s, episode steps: 1137, steps per second:  45, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.208 [0.000, 5.000],  loss: 0.013971, mae: 2.585023, mean_q: 3.111004, mean_eps: 0.100000
 4681128/6000000: episode: 5937, duration: 18.747s, episode steps: 805, steps per second:  43, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.940 [0.000, 5.000],  loss: 0.015384, mae: 2.593250, mean_q: 3.121169, mean_eps: 0.100000
 4682185/6000000: episode: 5938, duration: 22.189s, episode steps: 1057, steps per second:  48, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.015577, mae: 2.595979, mean_q: 3.123253, mean_eps: 0.100000
 4683133/6000000: episode: 5939, duration: 19.801s, episode steps: 948, steps per second:  48, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.184 [0.000, 5.000],  loss: 0.014536, mae: 2.592778, mean_q: 3.121072, mean_eps: 0.100000
 4683847/6000000: episode: 5940, duration: 16.567s, episode steps: 714, steps per second:  43, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.014100, mae: 2.597386, mean_q: 3.126595, mean_eps: 0.100000
 4685093/6000000: episode: 5941, duration: 28.170s, episode steps: 1246, steps per second:  44, episode reward: 33.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 0.013373, mae: 2.587394, mean_q: 3.114830, mean_eps: 0.100000
 4686521/6000000: episode: 5942, duration: 31.410s, episode steps: 1428, steps per second:  45, episode reward: 33.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.884 [0.000, 5.000],  loss: 0.014092, mae: 2.584188, mean_q: 3.110697, mean_eps: 0.100000
 4687427/6000000: episode: 5943, duration: 19.443s, episode steps: 906, steps per second:  47, episode reward: 28.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.127 [0.000, 5.000],  loss: 0.013064, mae: 2.571995, mean_q: 3.096678, mean_eps: 0.100000
 4688932/6000000: episode: 5944, duration: 30.757s, episode steps: 1505, steps per second:  49, episode reward: 30.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.200 [0.000, 5.000],  loss: 0.014838, mae: 2.568071, mean_q: 3.091077, mean_eps: 0.100000
 4690274/6000000: episode: 5945, duration: 28.837s, episode steps: 1342, steps per second:  47, episode reward: 33.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.176 [0.000, 5.000],  loss: 0.013953, mae: 2.594019, mean_q: 3.121898, mean_eps: 0.100000
 4691734/6000000: episode: 5946, duration: 31.651s, episode steps: 1460, steps per second:  46, episode reward: 32.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.013949, mae: 2.560636, mean_q: 3.081716, mean_eps: 0.100000
 4692775/6000000: episode: 5947, duration: 24.240s, episode steps: 1041, steps per second:  43, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.007 [0.000, 5.000],  loss: 0.014833, mae: 2.549933, mean_q: 3.067991, mean_eps: 0.100000
 4693645/6000000: episode: 5948, duration: 19.837s, episode steps: 870, steps per second:  44, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.220 [0.000, 5.000],  loss: 0.014472, mae: 2.560460, mean_q: 3.080970, mean_eps: 0.100000
 4694854/6000000: episode: 5949, duration: 28.775s, episode steps: 1209, steps per second:  42, episode reward: 35.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.318 [0.000, 5.000],  loss: 0.014194, mae: 2.547566, mean_q: 3.065402, mean_eps: 0.100000
 4695535/6000000: episode: 5950, duration: 17.795s, episode steps: 681, steps per second:  38, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.015282, mae: 2.568933, mean_q: 3.092613, mean_eps: 0.100000
 4696425/6000000: episode: 5951, duration: 23.947s, episode steps: 890, steps per second:  37, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.043 [0.000, 5.000],  loss: 0.013045, mae: 2.542925, mean_q: 3.062994, mean_eps: 0.100000
 4697465/6000000: episode: 5952, duration: 24.869s, episode steps: 1040, steps per second:  42, episode reward: 29.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.013923, mae: 2.554919, mean_q: 3.075051, mean_eps: 0.100000
 4698145/6000000: episode: 5953, duration: 16.209s, episode steps: 680, steps per second:  42, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.887 [0.000, 5.000],  loss: 0.013431, mae: 2.564821, mean_q: 3.087928, mean_eps: 0.100000
 4699535/6000000: episode: 5954, duration: 31.645s, episode steps: 1390, steps per second:  44, episode reward: 34.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.865 [0.000, 5.000],  loss: 0.013734, mae: 2.548838, mean_q: 3.068816, mean_eps: 0.100000
 4700325/6000000: episode: 5955, duration: 18.537s, episode steps: 790, steps per second:  43, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.010 [0.000, 5.000],  loss: 0.012896, mae: 2.565697, mean_q: 3.087945, mean_eps: 0.100000
 4701684/6000000: episode: 5956, duration: 31.105s, episode steps: 1359, steps per second:  44, episode reward: 30.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.948 [0.000, 5.000],  loss: 0.012668, mae: 2.568967, mean_q: 3.093200, mean_eps: 0.100000
 4702902/6000000: episode: 5957, duration: 29.460s, episode steps: 1218, steps per second:  41, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.530 [0.000, 5.000],  loss: 0.014433, mae: 2.562031, mean_q: 3.085019, mean_eps: 0.100000
 4703762/6000000: episode: 5958, duration: 20.744s, episode steps: 860, steps per second:  41, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.222 [0.000, 5.000],  loss: 0.015422, mae: 2.576635, mean_q: 3.100211, mean_eps: 0.100000
 4704871/6000000: episode: 5959, duration: 26.790s, episode steps: 1109, steps per second:  41, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.096 [0.000, 5.000],  loss: 0.015746, mae: 2.596045, mean_q: 3.122341, mean_eps: 0.100000
 4705664/6000000: episode: 5960, duration: 18.488s, episode steps: 793, steps per second:  43, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.035 [0.000, 5.000],  loss: 0.013680, mae: 2.556363, mean_q: 3.076892, mean_eps: 0.100000
 4706583/6000000: episode: 5961, duration: 21.733s, episode steps: 919, steps per second:  42, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.852 [0.000, 5.000],  loss: 0.014177, mae: 2.572218, mean_q: 3.096057, mean_eps: 0.100000
 4707337/6000000: episode: 5962, duration: 18.255s, episode steps: 754, steps per second:  41, episode reward: 23.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.599 [0.000, 5.000],  loss: 0.013554, mae: 2.545800, mean_q: 3.064017, mean_eps: 0.100000
 4708400/6000000: episode: 5963, duration: 25.779s, episode steps: 1063, steps per second:  41, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.014088, mae: 2.586952, mean_q: 3.113452, mean_eps: 0.100000
 4709538/6000000: episode: 5964, duration: 28.103s, episode steps: 1138, steps per second:  40, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.891 [0.000, 5.000],  loss: 0.015130, mae: 2.578926, mean_q: 3.104000, mean_eps: 0.100000
 4710431/6000000: episode: 5965, duration: 21.411s, episode steps: 893, steps per second:  42, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.046 [0.000, 5.000],  loss: 0.014816, mae: 2.562740, mean_q: 3.083449, mean_eps: 0.100000
 4711563/6000000: episode: 5966, duration: 27.213s, episode steps: 1132, steps per second:  42, episode reward: 30.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.837 [0.000, 5.000],  loss: 0.014202, mae: 2.548356, mean_q: 3.067612, mean_eps: 0.100000
 4712264/6000000: episode: 5967, duration: 18.182s, episode steps: 701, steps per second:  39, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.183 [0.000, 5.000],  loss: 0.014607, mae: 2.540823, mean_q: 3.058226, mean_eps: 0.100000
 4713294/6000000: episode: 5968, duration: 25.467s, episode steps: 1030, steps per second:  40, episode reward: 29.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.869 [0.000, 5.000],  loss: 0.012722, mae: 2.548986, mean_q: 3.069584, mean_eps: 0.100000
 4714378/6000000: episode: 5969, duration: 29.070s, episode steps: 1084, steps per second:  37, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.611 [0.000, 5.000],  loss: 0.014923, mae: 2.542493, mean_q: 3.060498, mean_eps: 0.100000
 4715244/6000000: episode: 5970, duration: 31.885s, episode steps: 866, steps per second:  27, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.957 [0.000, 5.000],  loss: 0.013079, mae: 2.528680, mean_q: 3.044720, mean_eps: 0.100000
 4715947/6000000: episode: 5971, duration: 29.583s, episode steps: 703, steps per second:  24, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.257 [0.000, 5.000],  loss: 0.015337, mae: 2.542363, mean_q: 3.059996, mean_eps: 0.100000
 4716655/6000000: episode: 5972, duration: 29.438s, episode steps: 708, steps per second:  24, episode reward: 22.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.226 [0.000, 5.000],  loss: 0.014706, mae: 2.543387, mean_q: 3.061238, mean_eps: 0.100000
 4717690/6000000: episode: 5973, duration: 41.149s, episode steps: 1035, steps per second:  25, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.729 [0.000, 5.000],  loss: 0.013142, mae: 2.537145, mean_q: 3.054682, mean_eps: 0.100000
 4718349/6000000: episode: 5974, duration: 33.406s, episode steps: 659, steps per second:  20, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.209 [0.000, 5.000],  loss: 0.014978, mae: 2.530908, mean_q: 3.045434, mean_eps: 0.100000
 4718782/6000000: episode: 5975, duration: 22.850s, episode steps: 433, steps per second:  19, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.917 [0.000, 5.000],  loss: 0.013964, mae: 2.551622, mean_q: 3.071109, mean_eps: 0.100000
 4719591/6000000: episode: 5976, duration: 38.024s, episode steps: 809, steps per second:  21, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.574 [0.000, 5.000],  loss: 0.014950, mae: 2.550109, mean_q: 3.069808, mean_eps: 0.100000
 4720235/6000000: episode: 5977, duration: 28.842s, episode steps: 644, steps per second:  22, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.582 [0.000, 5.000],  loss: 0.013846, mae: 2.555669, mean_q: 3.075043, mean_eps: 0.100000
 4720986/6000000: episode: 5978, duration: 34.743s, episode steps: 751, steps per second:  22, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.119 [0.000, 5.000],  loss: 0.015263, mae: 2.550394, mean_q: 3.068631, mean_eps: 0.100000
 4722103/6000000: episode: 5979, duration: 48.902s, episode steps: 1117, steps per second:  23, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.650 [0.000, 5.000],  loss: 0.014163, mae: 2.520988, mean_q: 3.034756, mean_eps: 0.100000
 4723011/6000000: episode: 5980, duration: 33.357s, episode steps: 908, steps per second:  27, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.557 [0.000, 5.000],  loss: 0.015701, mae: 2.537438, mean_q: 3.055794, mean_eps: 0.100000
 4723916/6000000: episode: 5981, duration: 49.844s, episode steps: 905, steps per second:  18, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.914 [0.000, 5.000],  loss: 0.013734, mae: 2.535240, mean_q: 3.052159, mean_eps: 0.100000
 4724703/6000000: episode: 5982, duration: 44.386s, episode steps: 787, steps per second:  18, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.977 [0.000, 5.000],  loss: 0.016356, mae: 2.520014, mean_q: 3.031004, mean_eps: 0.100000
 4725688/6000000: episode: 5983, duration: 56.291s, episode steps: 985, steps per second:  17, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.790 [0.000, 5.000],  loss: 0.014308, mae: 2.530936, mean_q: 3.046599, mean_eps: 0.100000
 4726524/6000000: episode: 5984, duration: 47.417s, episode steps: 836, steps per second:  18, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.013632, mae: 2.520910, mean_q: 3.033912, mean_eps: 0.100000
 4727161/6000000: episode: 5985, duration: 34.054s, episode steps: 637, steps per second:  19, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.857 [0.000, 5.000],  loss: 0.013798, mae: 2.515915, mean_q: 3.029758, mean_eps: 0.100000
 4727868/6000000: episode: 5986, duration: 37.937s, episode steps: 707, steps per second:  19, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.011520, mae: 2.520975, mean_q: 3.036673, mean_eps: 0.100000
 4728950/6000000: episode: 5987, duration: 50.615s, episode steps: 1082, steps per second:  21, episode reward: 30.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.904 [0.000, 5.000],  loss: 0.014974, mae: 2.530324, mean_q: 3.044615, mean_eps: 0.100000
 4729630/6000000: episode: 5988, duration: 34.810s, episode steps: 680, steps per second:  20, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.013776, mae: 2.510442, mean_q: 3.021925, mean_eps: 0.100000
 4730576/6000000: episode: 5989, duration: 48.017s, episode steps: 946, steps per second:  20, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.033 [0.000, 5.000],  loss: 0.013019, mae: 2.521155, mean_q: 3.033662, mean_eps: 0.100000
 4731377/6000000: episode: 5990, duration: 42.402s, episode steps: 801, steps per second:  19, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.918 [0.000, 5.000],  loss: 0.012714, mae: 2.536507, mean_q: 3.053991, mean_eps: 0.100000
 4732259/6000000: episode: 5991, duration: 46.541s, episode steps: 882, steps per second:  19, episode reward: 28.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.712 [0.000, 5.000],  loss: 0.014024, mae: 2.508841, mean_q: 3.018143, mean_eps: 0.100000
 4733090/6000000: episode: 5992, duration: 43.677s, episode steps: 831, steps per second:  19, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.171 [0.000, 5.000],  loss: 0.016109, mae: 2.524279, mean_q: 3.035805, mean_eps: 0.100000
 4733930/6000000: episode: 5993, duration: 44.827s, episode steps: 840, steps per second:  19, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.012 [0.000, 5.000],  loss: 0.013267, mae: 2.524012, mean_q: 3.038252, mean_eps: 0.100000
 4734951/6000000: episode: 5994, duration: 56.363s, episode steps: 1021, steps per second:  18, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.907 [0.000, 5.000],  loss: 0.014756, mae: 2.523315, mean_q: 3.036503, mean_eps: 0.100000
 4735430/6000000: episode: 5995, duration: 27.009s, episode steps: 479, steps per second:  18, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.812 [0.000, 5.000],  loss: 0.014979, mae: 2.493482, mean_q: 3.003273, mean_eps: 0.100000
 4736424/6000000: episode: 5996, duration: 53.213s, episode steps: 994, steps per second:  19, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.724 [0.000, 5.000],  loss: 0.013029, mae: 2.499135, mean_q: 3.010658, mean_eps: 0.100000
 4737858/6000000: episode: 5997, duration: 78.728s, episode steps: 1434, steps per second:  18, episode reward: 31.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.351 [0.000, 5.000],  loss: 0.014851, mae: 2.500079, mean_q: 3.010630, mean_eps: 0.100000
 4739005/6000000: episode: 5998, duration: 64.287s, episode steps: 1147, steps per second:  18, episode reward: 34.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.592 [0.000, 5.000],  loss: 0.014990, mae: 2.512537, mean_q: 3.024028, mean_eps: 0.100000
 4739836/6000000: episode: 5999, duration: 46.917s, episode steps: 831, steps per second:  18, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.694 [0.000, 5.000],  loss: 0.013727, mae: 2.492718, mean_q: 3.002094, mean_eps: 0.100000
 4740591/6000000: episode: 6000, duration: 41.033s, episode steps: 755, steps per second:  18, episode reward: 21.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.012765, mae: 2.485757, mean_q: 2.991979, mean_eps: 0.100000
 4741857/6000000: episode: 6001, duration: 68.702s, episode steps: 1266, steps per second:  18, episode reward: 30.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.875 [0.000, 5.000],  loss: 0.014238, mae: 2.490238, mean_q: 2.997185, mean_eps: 0.100000
 4742596/6000000: episode: 6002, duration: 41.359s, episode steps: 739, steps per second:  18, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.015291, mae: 2.478746, mean_q: 2.983706, mean_eps: 0.100000
 4743604/6000000: episode: 6003, duration: 56.456s, episode steps: 1008, steps per second:  18, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.169 [0.000, 5.000],  loss: 0.013199, mae: 2.525367, mean_q: 3.039159, mean_eps: 0.100000
 4744282/6000000: episode: 6004, duration: 37.897s, episode steps: 678, steps per second:  18, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.014705, mae: 2.506993, mean_q: 3.017451, mean_eps: 0.100000
 4744979/6000000: episode: 6005, duration: 37.134s, episode steps: 697, steps per second:  19, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.778 [0.000, 5.000],  loss: 0.013491, mae: 2.481089, mean_q: 2.985900, mean_eps: 0.100000
 4746024/6000000: episode: 6006, duration: 57.045s, episode steps: 1045, steps per second:  18, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.138 [0.000, 5.000],  loss: 0.014218, mae: 2.499418, mean_q: 3.007358, mean_eps: 0.100000
 4747018/6000000: episode: 6007, duration: 55.526s, episode steps: 994, steps per second:  18, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.014249, mae: 2.499151, mean_q: 3.006524, mean_eps: 0.100000
 4748343/6000000: episode: 6008, duration: 74.495s, episode steps: 1325, steps per second:  18, episode reward: 29.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.795 [0.000, 5.000],  loss: 0.015310, mae: 2.512830, mean_q: 3.024016, mean_eps: 0.100000
 4749432/6000000: episode: 6009, duration: 58.897s, episode steps: 1089, steps per second:  18, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.054 [0.000, 5.000],  loss: 0.014327, mae: 2.500986, mean_q: 3.010542, mean_eps: 0.100000
 4750145/6000000: episode: 6010, duration: 39.306s, episode steps: 713, steps per second:  18, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.938 [0.000, 5.000],  loss: 0.012535, mae: 2.518711, mean_q: 3.032843, mean_eps: 0.100000
 4750772/6000000: episode: 6011, duration: 34.148s, episode steps: 627, steps per second:  18, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.014040, mae: 2.509533, mean_q: 3.021962, mean_eps: 0.100000
 4751890/6000000: episode: 6012, duration: 61.648s, episode steps: 1118, steps per second:  18, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.147 [0.000, 5.000],  loss: 0.014609, mae: 2.535934, mean_q: 3.051917, mean_eps: 0.100000
 4752934/6000000: episode: 6013, duration: 58.725s, episode steps: 1044, steps per second:  18, episode reward: 29.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.016 [0.000, 5.000],  loss: 0.014563, mae: 2.532900, mean_q: 3.047252, mean_eps: 0.100000
 4754187/6000000: episode: 6014, duration: 67.398s, episode steps: 1253, steps per second:  19, episode reward: 35.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.015 [0.000, 5.000],  loss: 0.014309, mae: 2.510650, mean_q: 3.020567, mean_eps: 0.100000
 4755249/6000000: episode: 6015, duration: 58.448s, episode steps: 1062, steps per second:  18, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.180 [0.000, 5.000],  loss: 0.013841, mae: 2.511940, mean_q: 3.023157, mean_eps: 0.100000
 4756339/6000000: episode: 6016, duration: 60.238s, episode steps: 1090, steps per second:  18, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.680 [0.000, 5.000],  loss: 0.014147, mae: 2.493164, mean_q: 2.999465, mean_eps: 0.100000
 4757446/6000000: episode: 6017, duration: 61.804s, episode steps: 1107, steps per second:  18, episode reward: 32.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.014358, mae: 2.514440, mean_q: 3.026157, mean_eps: 0.100000
 4758258/6000000: episode: 6018, duration: 43.830s, episode steps: 812, steps per second:  19, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.181 [0.000, 5.000],  loss: 0.014028, mae: 2.495537, mean_q: 3.003647, mean_eps: 0.100000
 4759182/6000000: episode: 6019, duration: 51.616s, episode steps: 924, steps per second:  18, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.095 [0.000, 5.000],  loss: 0.014668, mae: 2.497566, mean_q: 3.005612, mean_eps: 0.100000
 4760508/6000000: episode: 6020, duration: 75.558s, episode steps: 1326, steps per second:  18, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.944 [0.000, 5.000],  loss: 0.013826, mae: 2.507231, mean_q: 3.020341, mean_eps: 0.100000
 4761136/6000000: episode: 6021, duration: 36.044s, episode steps: 628, steps per second:  17, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.713 [0.000, 5.000],  loss: 0.014373, mae: 2.533706, mean_q: 3.050483, mean_eps: 0.100000
 4762069/6000000: episode: 6022, duration: 51.760s, episode steps: 933, steps per second:  18, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.800 [0.000, 5.000],  loss: 0.014735, mae: 2.511685, mean_q: 3.023422, mean_eps: 0.100000
 4762812/6000000: episode: 6023, duration: 40.188s, episode steps: 743, steps per second:  18, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.014502, mae: 2.536783, mean_q: 3.053352, mean_eps: 0.100000
 4763712/6000000: episode: 6024, duration: 50.756s, episode steps: 900, steps per second:  18, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.067 [0.000, 5.000],  loss: 0.013419, mae: 2.516304, mean_q: 3.030968, mean_eps: 0.100000
 4764253/6000000: episode: 6025, duration: 30.073s, episode steps: 541, steps per second:  18, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.070 [0.000, 5.000],  loss: 0.014099, mae: 2.517448, mean_q: 3.030474, mean_eps: 0.100000
 4765086/6000000: episode: 6026, duration: 46.378s, episode steps: 833, steps per second:  18, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.052 [0.000, 5.000],  loss: 0.014358, mae: 2.537844, mean_q: 3.055004, mean_eps: 0.100000
 4765715/6000000: episode: 6027, duration: 35.428s, episode steps: 629, steps per second:  18, episode reward: 18.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.013735, mae: 2.537450, mean_q: 3.052944, mean_eps: 0.100000
 4767087/6000000: episode: 6028, duration: 50.446s, episode steps: 1372, steps per second:  27, episode reward: 33.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.786 [0.000, 5.000],  loss: 0.012861, mae: 2.507531, mean_q: 3.018972, mean_eps: 0.100000
 4768120/6000000: episode: 6029, duration: 23.665s, episode steps: 1033, steps per second:  44, episode reward: 32.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.014409, mae: 2.508569, mean_q: 3.019458, mean_eps: 0.100000
 4769224/6000000: episode: 6030, duration: 24.482s, episode steps: 1104, steps per second:  45, episode reward: 32.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.948 [0.000, 5.000],  loss: 0.013251, mae: 2.530570, mean_q: 3.046452, mean_eps: 0.100000
 4769703/6000000: episode: 6031, duration: 10.681s, episode steps: 479, steps per second:  45, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.073 [0.000, 5.000],  loss: 0.014447, mae: 2.529231, mean_q: 3.043985, mean_eps: 0.100000
 4771033/6000000: episode: 6032, duration: 29.413s, episode steps: 1330, steps per second:  45, episode reward: 29.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.009 [0.000, 5.000],  loss: 0.013856, mae: 2.516725, mean_q: 3.028582, mean_eps: 0.100000
 4772239/6000000: episode: 6033, duration: 26.263s, episode steps: 1206, steps per second:  46, episode reward: 32.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.087 [0.000, 5.000],  loss: 0.013781, mae: 2.506187, mean_q: 3.015874, mean_eps: 0.100000
 4773205/6000000: episode: 6034, duration: 21.446s, episode steps: 966, steps per second:  45, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.961 [0.000, 5.000],  loss: 0.014718, mae: 2.508044, mean_q: 3.018413, mean_eps: 0.100000
 4774048/6000000: episode: 6035, duration: 17.996s, episode steps: 843, steps per second:  47, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.014735, mae: 2.508441, mean_q: 3.018398, mean_eps: 0.100000
 4774831/6000000: episode: 6036, duration: 16.704s, episode steps: 783, steps per second:  47, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.946 [0.000, 5.000],  loss: 0.013010, mae: 2.504655, mean_q: 3.014520, mean_eps: 0.100000
 4775944/6000000: episode: 6037, duration: 23.902s, episode steps: 1113, steps per second:  47, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.857 [0.000, 5.000],  loss: 0.012418, mae: 2.509727, mean_q: 3.022258, mean_eps: 0.100000
 4777108/6000000: episode: 6038, duration: 23.825s, episode steps: 1164, steps per second:  49, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.843 [0.000, 5.000],  loss: 0.014409, mae: 2.486981, mean_q: 2.993928, mean_eps: 0.100000
 4778076/6000000: episode: 6039, duration: 20.346s, episode steps: 968, steps per second:  48, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.048 [0.000, 5.000],  loss: 0.013408, mae: 2.502305, mean_q: 3.012871, mean_eps: 0.100000
 4778763/6000000: episode: 6040, duration: 14.722s, episode steps: 687, steps per second:  47, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.721 [0.000, 5.000],  loss: 0.014517, mae: 2.511801, mean_q: 3.024603, mean_eps: 0.100000
 4779722/6000000: episode: 6041, duration: 22.149s, episode steps: 959, steps per second:  43, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.380 [0.000, 5.000],  loss: 0.014318, mae: 2.503700, mean_q: 3.014850, mean_eps: 0.100000
 4780824/6000000: episode: 6042, duration: 25.648s, episode steps: 1102, steps per second:  43, episode reward: 30.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.765 [0.000, 5.000],  loss: 0.014959, mae: 2.498839, mean_q: 3.007354, mean_eps: 0.100000
 4782191/6000000: episode: 6043, duration: 30.469s, episode steps: 1367, steps per second:  45, episode reward: 32.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.019 [0.000, 5.000],  loss: 0.015317, mae: 2.505481, mean_q: 3.014277, mean_eps: 0.100000
 4783204/6000000: episode: 6044, duration: 23.186s, episode steps: 1013, steps per second:  44, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.114 [0.000, 5.000],  loss: 0.014819, mae: 2.507629, mean_q: 3.017653, mean_eps: 0.100000
 4784631/6000000: episode: 6045, duration: 33.241s, episode steps: 1427, steps per second:  43, episode reward: 32.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.765 [0.000, 5.000],  loss: 0.013480, mae: 2.503170, mean_q: 3.012332, mean_eps: 0.100000
 4785438/6000000: episode: 6046, duration: 18.051s, episode steps: 807, steps per second:  45, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.985 [0.000, 5.000],  loss: 0.014894, mae: 2.514330, mean_q: 3.026770, mean_eps: 0.100000
 4786116/6000000: episode: 6047, duration: 16.266s, episode steps: 678, steps per second:  42, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.861 [0.000, 5.000],  loss: 0.015210, mae: 2.501406, mean_q: 3.012812, mean_eps: 0.100000
 4787402/6000000: episode: 6048, duration: 28.706s, episode steps: 1286, steps per second:  45, episode reward: 33.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.984 [0.000, 5.000],  loss: 0.013641, mae: 2.501805, mean_q: 3.012728, mean_eps: 0.100000
 4788496/6000000: episode: 6049, duration: 25.217s, episode steps: 1094, steps per second:  43, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.669 [0.000, 5.000],  loss: 0.013249, mae: 2.508632, mean_q: 3.020480, mean_eps: 0.100000
 4789385/6000000: episode: 6050, duration: 20.683s, episode steps: 889, steps per second:  43, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.492 [0.000, 5.000],  loss: 0.014870, mae: 2.510099, mean_q: 3.022180, mean_eps: 0.100000
 4790141/6000000: episode: 6051, duration: 16.384s, episode steps: 756, steps per second:  46, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.017 [0.000, 5.000],  loss: 0.014414, mae: 2.502866, mean_q: 3.014429, mean_eps: 0.100000
 4791042/6000000: episode: 6052, duration: 20.129s, episode steps: 901, steps per second:  45, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.782 [0.000, 5.000],  loss: 0.015095, mae: 2.553544, mean_q: 3.074086, mean_eps: 0.100000
 4792000/6000000: episode: 6053, duration: 21.592s, episode steps: 958, steps per second:  44, episode reward: 28.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.584 [0.000, 5.000],  loss: 0.014618, mae: 2.535410, mean_q: 3.053714, mean_eps: 0.100000
 4792978/6000000: episode: 6054, duration: 20.151s, episode steps: 978, steps per second:  49, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.921 [0.000, 5.000],  loss: 0.014114, mae: 2.531092, mean_q: 3.047797, mean_eps: 0.100000
 4793852/6000000: episode: 6055, duration: 18.934s, episode steps: 874, steps per second:  46, episode reward: 30.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.013675, mae: 2.555675, mean_q: 3.075567, mean_eps: 0.100000
 4794668/6000000: episode: 6056, duration: 18.346s, episode steps: 816, steps per second:  44, episode reward: 27.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 1.925 [0.000, 5.000],  loss: 0.013866, mae: 2.542923, mean_q: 3.060996, mean_eps: 0.100000
 4795542/6000000: episode: 6057, duration: 19.186s, episode steps: 874, steps per second:  46, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.174 [0.000, 5.000],  loss: 0.014914, mae: 2.550452, mean_q: 3.070564, mean_eps: 0.100000
 4796617/6000000: episode: 6058, duration: 23.099s, episode steps: 1075, steps per second:  47, episode reward: 32.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.996 [0.000, 5.000],  loss: 0.014261, mae: 2.535096, mean_q: 3.050613, mean_eps: 0.100000
 4798057/6000000: episode: 6059, duration: 32.395s, episode steps: 1440, steps per second:  44, episode reward: 29.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.878 [0.000, 5.000],  loss: 0.016257, mae: 2.537206, mean_q: 3.054429, mean_eps: 0.100000
 4798760/6000000: episode: 6060, duration: 16.665s, episode steps: 703, steps per second:  42, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.013733, mae: 2.529662, mean_q: 3.044803, mean_eps: 0.100000
 4799834/6000000: episode: 6061, duration: 26.550s, episode steps: 1074, steps per second:  40, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.803 [0.000, 5.000],  loss: 0.013511, mae: 2.530619, mean_q: 3.048420, mean_eps: 0.100000
 4800691/6000000: episode: 6062, duration: 20.510s, episode steps: 857, steps per second:  42, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.776 [0.000, 5.000],  loss: 0.013213, mae: 2.539359, mean_q: 3.057395, mean_eps: 0.100000
 4801794/6000000: episode: 6063, duration: 25.730s, episode steps: 1103, steps per second:  43, episode reward: 30.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.573 [0.000, 5.000],  loss: 0.013570, mae: 2.536484, mean_q: 3.054592, mean_eps: 0.100000
 4802698/6000000: episode: 6064, duration: 20.562s, episode steps: 904, steps per second:  44, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.662 [0.000, 5.000],  loss: 0.014367, mae: 2.515969, mean_q: 3.028715, mean_eps: 0.100000
 4803679/6000000: episode: 6065, duration: 22.370s, episode steps: 981, steps per second:  44, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.228 [0.000, 5.000],  loss: 0.013837, mae: 2.532357, mean_q: 3.048629, mean_eps: 0.100000
 4805016/6000000: episode: 6066, duration: 31.803s, episode steps: 1337, steps per second:  42, episode reward: 32.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.014973, mae: 2.541866, mean_q: 3.059271, mean_eps: 0.100000
 4806343/6000000: episode: 6067, duration: 29.390s, episode steps: 1327, steps per second:  45, episode reward: 31.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.014663, mae: 2.507954, mean_q: 3.019094, mean_eps: 0.100000
 4806932/6000000: episode: 6068, duration: 13.382s, episode steps: 589, steps per second:  44, episode reward: 15.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.015324, mae: 2.540608, mean_q: 3.057593, mean_eps: 0.100000
 4807718/6000000: episode: 6069, duration: 17.118s, episode steps: 786, steps per second:  46, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.015906, mae: 2.508204, mean_q: 3.018817, mean_eps: 0.100000
 4808442/6000000: episode: 6070, duration: 15.014s, episode steps: 724, steps per second:  48, episode reward: 20.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 0.014409, mae: 2.506373, mean_q: 3.016659, mean_eps: 0.100000
 4809366/6000000: episode: 6071, duration: 18.846s, episode steps: 924, steps per second:  49, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.334 [0.000, 5.000],  loss: 0.013749, mae: 2.514833, mean_q: 3.027647, mean_eps: 0.100000
 4810405/6000000: episode: 6072, duration: 22.685s, episode steps: 1039, steps per second:  46, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.955 [0.000, 5.000],  loss: 0.013749, mae: 2.519764, mean_q: 3.033312, mean_eps: 0.100000
 4811220/6000000: episode: 6073, duration: 17.662s, episode steps: 815, steps per second:  46, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.189 [0.000, 5.000],  loss: 0.014429, mae: 2.541535, mean_q: 3.058773, mean_eps: 0.100000
 4812238/6000000: episode: 6074, duration: 21.694s, episode steps: 1018, steps per second:  47, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.901 [0.000, 5.000],  loss: 0.013569, mae: 2.510815, mean_q: 3.022627, mean_eps: 0.100000
 4813080/6000000: episode: 6075, duration: 18.169s, episode steps: 842, steps per second:  46, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.753 [0.000, 5.000],  loss: 0.015272, mae: 2.526595, mean_q: 3.038526, mean_eps: 0.100000
 4814008/6000000: episode: 6076, duration: 21.357s, episode steps: 928, steps per second:  43, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.255 [0.000, 5.000],  loss: 0.013788, mae: 2.531867, mean_q: 3.046908, mean_eps: 0.100000
 4814958/6000000: episode: 6077, duration: 21.986s, episode steps: 950, steps per second:  43, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.955 [0.000, 5.000],  loss: 0.013115, mae: 2.535025, mean_q: 3.051222, mean_eps: 0.100000
 4816021/6000000: episode: 6078, duration: 24.756s, episode steps: 1063, steps per second:  43, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: 0.014113, mae: 2.516318, mean_q: 3.028654, mean_eps: 0.100000
 4816634/6000000: episode: 6079, duration: 13.476s, episode steps: 613, steps per second:  45, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.013508, mae: 2.513272, mean_q: 3.025064, mean_eps: 0.100000
 4817434/6000000: episode: 6080, duration: 17.532s, episode steps: 800, steps per second:  46, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.671 [0.000, 5.000],  loss: 0.013107, mae: 2.498377, mean_q: 3.008400, mean_eps: 0.100000
 4818237/6000000: episode: 6081, duration: 18.147s, episode steps: 803, steps per second:  44, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.127 [0.000, 5.000],  loss: 0.015468, mae: 2.523657, mean_q: 3.038678, mean_eps: 0.100000
 4819288/6000000: episode: 6082, duration: 22.105s, episode steps: 1051, steps per second:  48, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.021 [0.000, 5.000],  loss: 0.013404, mae: 2.514445, mean_q: 3.026610, mean_eps: 0.100000
 4820344/6000000: episode: 6083, duration: 23.109s, episode steps: 1056, steps per second:  46, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.296 [0.000, 5.000],  loss: 0.015716, mae: 2.509878, mean_q: 3.022683, mean_eps: 0.100000
 4821401/6000000: episode: 6084, duration: 23.812s, episode steps: 1057, steps per second:  44, episode reward: 31.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.176 [0.000, 5.000],  loss: 0.013045, mae: 2.522688, mean_q: 3.037183, mean_eps: 0.100000
 4822284/6000000: episode: 6085, duration: 19.451s, episode steps: 883, steps per second:  45, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.715 [0.000, 5.000],  loss: 0.015681, mae: 2.536874, mean_q: 3.054555, mean_eps: 0.100000
 4823507/6000000: episode: 6086, duration: 25.768s, episode steps: 1223, steps per second:  47, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.291 [0.000, 5.000],  loss: 0.014098, mae: 2.504287, mean_q: 3.016253, mean_eps: 0.100000
 4824645/6000000: episode: 6087, duration: 23.898s, episode steps: 1138, steps per second:  48, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.313 [0.000, 5.000],  loss: 0.014204, mae: 2.519209, mean_q: 3.032728, mean_eps: 0.100000
 4825422/6000000: episode: 6088, duration: 15.754s, episode steps: 777, steps per second:  49, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.932 [0.000, 5.000],  loss: 0.013549, mae: 2.525760, mean_q: 3.040628, mean_eps: 0.100000
 4826223/6000000: episode: 6089, duration: 17.769s, episode steps: 801, steps per second:  45, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.014561, mae: 2.544307, mean_q: 3.061982, mean_eps: 0.100000
 4827194/6000000: episode: 6090, duration: 21.260s, episode steps: 971, steps per second:  46, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.107 [0.000, 5.000],  loss: 0.013169, mae: 2.529696, mean_q: 3.046809, mean_eps: 0.100000
 4827628/6000000: episode: 6091, duration: 9.410s, episode steps: 434, steps per second:  46, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.652 [0.000, 5.000],  loss: 0.012465, mae: 2.494538, mean_q: 3.005127, mean_eps: 0.100000
 4828255/6000000: episode: 6092, duration: 13.536s, episode steps: 627, steps per second:  46, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.879 [0.000, 5.000],  loss: 0.014166, mae: 2.537598, mean_q: 3.052960, mean_eps: 0.100000
 4829003/6000000: episode: 6093, duration: 15.823s, episode steps: 748, steps per second:  47, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.822 [0.000, 5.000],  loss: 0.014228, mae: 2.537251, mean_q: 3.053164, mean_eps: 0.100000
 4830367/6000000: episode: 6094, duration: 29.341s, episode steps: 1364, steps per second:  46, episode reward: 33.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.052 [0.000, 5.000],  loss: 0.012929, mae: 2.532973, mean_q: 3.051281, mean_eps: 0.100000
 4831245/6000000: episode: 6095, duration: 20.015s, episode steps: 878, steps per second:  44, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.982 [0.000, 5.000],  loss: 0.014167, mae: 2.562805, mean_q: 3.085548, mean_eps: 0.100000
 4832427/6000000: episode: 6096, duration: 29.811s, episode steps: 1182, steps per second:  40, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.066 [0.000, 5.000],  loss: 0.012876, mae: 2.538844, mean_q: 3.058865, mean_eps: 0.100000
 4833735/6000000: episode: 6097, duration: 28.747s, episode steps: 1308, steps per second:  46, episode reward: 32.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.918 [0.000, 5.000],  loss: 0.013418, mae: 2.557567, mean_q: 3.079322, mean_eps: 0.100000
 4834455/6000000: episode: 6098, duration: 15.872s, episode steps: 720, steps per second:  45, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.013052, mae: 2.536996, mean_q: 3.054537, mean_eps: 0.100000
 4835476/6000000: episode: 6099, duration: 22.079s, episode steps: 1021, steps per second:  46, episode reward: 29.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.766 [0.000, 5.000],  loss: 0.014099, mae: 2.542040, mean_q: 3.059109, mean_eps: 0.100000
 4836354/6000000: episode: 6100, duration: 19.318s, episode steps: 878, steps per second:  45, episode reward: 29.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.014536, mae: 2.528970, mean_q: 3.044794, mean_eps: 0.100000
 4837196/6000000: episode: 6101, duration: 19.542s, episode steps: 842, steps per second:  43, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.985 [0.000, 5.000],  loss: 0.015321, mae: 2.535926, mean_q: 3.052720, mean_eps: 0.100000
 4838010/6000000: episode: 6102, duration: 17.882s, episode steps: 814, steps per second:  46, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.258 [0.000, 5.000],  loss: 0.013632, mae: 2.527520, mean_q: 3.044280, mean_eps: 0.100000
 4838874/6000000: episode: 6103, duration: 19.196s, episode steps: 864, steps per second:  45, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.874 [0.000, 5.000],  loss: 0.015098, mae: 2.543128, mean_q: 3.058761, mean_eps: 0.100000
 4840057/6000000: episode: 6104, duration: 25.309s, episode steps: 1183, steps per second:  47, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.518 [0.000, 5.000],  loss: 0.013765, mae: 2.532030, mean_q: 3.047679, mean_eps: 0.100000
 4841189/6000000: episode: 6105, duration: 23.363s, episode steps: 1132, steps per second:  48, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.872 [0.000, 5.000],  loss: 0.013478, mae: 2.534239, mean_q: 3.051909, mean_eps: 0.100000
 4842108/6000000: episode: 6106, duration: 18.652s, episode steps: 919, steps per second:  49, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.118 [0.000, 5.000],  loss: 0.014264, mae: 2.516270, mean_q: 3.027640, mean_eps: 0.100000
 4842953/6000000: episode: 6107, duration: 18.247s, episode steps: 845, steps per second:  46, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.973 [0.000, 5.000],  loss: 0.014413, mae: 2.533820, mean_q: 3.049209, mean_eps: 0.100000
 4843880/6000000: episode: 6108, duration: 19.536s, episode steps: 927, steps per second:  47, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.722 [0.000, 5.000],  loss: 0.015162, mae: 2.518856, mean_q: 3.031276, mean_eps: 0.100000
 4845031/6000000: episode: 6109, duration: 24.483s, episode steps: 1151, steps per second:  47, episode reward: 31.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.691 [0.000, 5.000],  loss: 0.013604, mae: 2.535468, mean_q: 3.053442, mean_eps: 0.100000
 4846383/6000000: episode: 6110, duration: 30.365s, episode steps: 1352, steps per second:  45, episode reward: 35.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.200 [0.000, 5.000],  loss: 0.014038, mae: 2.557640, mean_q: 3.078611, mean_eps: 0.100000
 4847220/6000000: episode: 6111, duration: 19.123s, episode steps: 837, steps per second:  44, episode reward: 24.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.832 [0.000, 5.000],  loss: 0.014484, mae: 2.543555, mean_q: 3.062520, mean_eps: 0.100000
 4847975/6000000: episode: 6112, duration: 17.994s, episode steps: 755, steps per second:  42, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.244 [0.000, 5.000],  loss: 0.013719, mae: 2.521519, mean_q: 3.036204, mean_eps: 0.100000
 4848525/6000000: episode: 6113, duration: 13.693s, episode steps: 550, steps per second:  40, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.904 [0.000, 5.000],  loss: 0.013963, mae: 2.539603, mean_q: 3.058483, mean_eps: 0.100000
 4849357/6000000: episode: 6114, duration: 18.550s, episode steps: 832, steps per second:  45, episode reward: 24.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.677 [0.000, 5.000],  loss: 0.012668, mae: 2.539396, mean_q: 3.057071, mean_eps: 0.100000
 4850294/6000000: episode: 6115, duration: 21.132s, episode steps: 937, steps per second:  44, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.244 [0.000, 5.000],  loss: 0.014676, mae: 2.550847, mean_q: 3.071355, mean_eps: 0.100000
 4851100/6000000: episode: 6116, duration: 20.258s, episode steps: 806, steps per second:  40, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.912 [0.000, 5.000],  loss: 0.013678, mae: 2.548620, mean_q: 3.068564, mean_eps: 0.100000
 4852278/6000000: episode: 6117, duration: 26.012s, episode steps: 1178, steps per second:  45, episode reward: 32.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.747 [0.000, 5.000],  loss: 0.014831, mae: 2.549534, mean_q: 3.069301, mean_eps: 0.100000
 4852977/6000000: episode: 6118, duration: 15.560s, episode steps: 699, steps per second:  45, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.273 [0.000, 5.000],  loss: 0.013637, mae: 2.543625, mean_q: 3.060510, mean_eps: 0.100000
 4854070/6000000: episode: 6119, duration: 24.396s, episode steps: 1093, steps per second:  45, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.037 [0.000, 5.000],  loss: 0.013358, mae: 2.543237, mean_q: 3.061496, mean_eps: 0.100000
 4854905/6000000: episode: 6120, duration: 18.041s, episode steps: 835, steps per second:  46, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.691 [0.000, 5.000],  loss: 0.013470, mae: 2.562749, mean_q: 3.086403, mean_eps: 0.100000
 4855409/6000000: episode: 6121, duration: 11.405s, episode steps: 504, steps per second:  44, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.016554, mae: 2.541564, mean_q: 3.061106, mean_eps: 0.100000
 4856367/6000000: episode: 6122, duration: 21.190s, episode steps: 958, steps per second:  45, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.862 [0.000, 5.000],  loss: 0.015528, mae: 2.545076, mean_q: 3.060997, mean_eps: 0.100000
 4857483/6000000: episode: 6123, duration: 23.797s, episode steps: 1116, steps per second:  47, episode reward: 32.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.977 [0.000, 5.000],  loss: 0.013341, mae: 2.535018, mean_q: 3.050525, mean_eps: 0.100000
 4858243/6000000: episode: 6124, duration: 15.927s, episode steps: 760, steps per second:  48, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.179 [0.000, 5.000],  loss: 0.013592, mae: 2.545654, mean_q: 3.064846, mean_eps: 0.100000
 4859128/6000000: episode: 6125, duration: 19.180s, episode steps: 885, steps per second:  46, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.014249, mae: 2.530259, mean_q: 3.046184, mean_eps: 0.100000
 4859894/6000000: episode: 6126, duration: 16.710s, episode steps: 766, steps per second:  46, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.192 [0.000, 5.000],  loss: 0.014187, mae: 2.538745, mean_q: 3.056351, mean_eps: 0.100000
 4860661/6000000: episode: 6127, duration: 16.697s, episode steps: 767, steps per second:  46, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.015278, mae: 2.542808, mean_q: 3.062777, mean_eps: 0.100000
 4861521/6000000: episode: 6128, duration: 18.817s, episode steps: 860, steps per second:  46, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.823 [0.000, 5.000],  loss: 0.014563, mae: 2.564235, mean_q: 3.089753, mean_eps: 0.100000
 4862649/6000000: episode: 6129, duration: 25.558s, episode steps: 1128, steps per second:  44, episode reward: 32.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.601 [0.000, 5.000],  loss: 0.013724, mae: 2.535576, mean_q: 3.053196, mean_eps: 0.100000
 4863541/6000000: episode: 6130, duration: 21.647s, episode steps: 892, steps per second:  41, episode reward: 27.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.213 [0.000, 5.000],  loss: 0.013322, mae: 2.538844, mean_q: 3.057499, mean_eps: 0.100000
 4864173/6000000: episode: 6131, duration: 15.937s, episode steps: 632, steps per second:  40, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.676 [0.000, 5.000],  loss: 0.013472, mae: 2.524418, mean_q: 3.041256, mean_eps: 0.100000
 4865066/6000000: episode: 6132, duration: 21.473s, episode steps: 893, steps per second:  42, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.786 [0.000, 5.000],  loss: 0.016179, mae: 2.555660, mean_q: 3.076058, mean_eps: 0.100000
 4865745/6000000: episode: 6133, duration: 15.616s, episode steps: 679, steps per second:  43, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.014931, mae: 2.552169, mean_q: 3.073203, mean_eps: 0.100000
 4866666/6000000: episode: 6134, duration: 21.906s, episode steps: 921, steps per second:  42, episode reward: 32.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 1.657 [0.000, 5.000],  loss: 0.013480, mae: 2.551564, mean_q: 3.072191, mean_eps: 0.100000
 4867400/6000000: episode: 6135, duration: 17.127s, episode steps: 734, steps per second:  43, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.263 [0.000, 5.000],  loss: 0.014478, mae: 2.537069, mean_q: 3.055903, mean_eps: 0.100000
 4868654/6000000: episode: 6136, duration: 29.306s, episode steps: 1254, steps per second:  43, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.817 [0.000, 5.000],  loss: 0.014097, mae: 2.558876, mean_q: 3.080023, mean_eps: 0.100000
 4869471/6000000: episode: 6137, duration: 19.690s, episode steps: 817, steps per second:  41, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.197 [0.000, 5.000],  loss: 0.013347, mae: 2.552430, mean_q: 3.073482, mean_eps: 0.100000
 4870470/6000000: episode: 6138, duration: 23.092s, episode steps: 999, steps per second:  43, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.300 [0.000, 5.000],  loss: 0.014949, mae: 2.538360, mean_q: 3.054646, mean_eps: 0.100000
 4871643/6000000: episode: 6139, duration: 26.507s, episode steps: 1173, steps per second:  44, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.972 [0.000, 5.000],  loss: 0.013002, mae: 2.547607, mean_q: 3.067385, mean_eps: 0.100000
 4872432/6000000: episode: 6140, duration: 18.705s, episode steps: 789, steps per second:  42, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.958 [0.000, 5.000],  loss: 0.012272, mae: 2.566530, mean_q: 3.089690, mean_eps: 0.100000
 4873443/6000000: episode: 6141, duration: 20.967s, episode steps: 1011, steps per second:  48, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.188 [0.000, 5.000],  loss: 0.013784, mae: 2.552401, mean_q: 3.071820, mean_eps: 0.100000
 4874516/6000000: episode: 6142, duration: 24.063s, episode steps: 1073, steps per second:  45, episode reward: 31.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.993 [0.000, 5.000],  loss: 0.014158, mae: 2.540001, mean_q: 3.057107, mean_eps: 0.100000
 4875733/6000000: episode: 6143, duration: 27.180s, episode steps: 1217, steps per second:  45, episode reward: 34.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.080 [0.000, 5.000],  loss: 0.014343, mae: 2.536783, mean_q: 3.053913, mean_eps: 0.100000
 4876737/6000000: episode: 6144, duration: 22.007s, episode steps: 1004, steps per second:  46, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.107 [0.000, 5.000],  loss: 0.014392, mae: 2.519729, mean_q: 3.035329, mean_eps: 0.100000
 4877283/6000000: episode: 6145, duration: 12.813s, episode steps: 546, steps per second:  43, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.194 [0.000, 5.000],  loss: 0.013766, mae: 2.533864, mean_q: 3.049686, mean_eps: 0.100000
 4877808/6000000: episode: 6146, duration: 12.833s, episode steps: 525, steps per second:  41, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.286 [0.000, 5.000],  loss: 0.013319, mae: 2.519458, mean_q: 3.033630, mean_eps: 0.100000
 4878621/6000000: episode: 6147, duration: 19.573s, episode steps: 813, steps per second:  42, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.015155, mae: 2.527582, mean_q: 3.041993, mean_eps: 0.100000
 4879674/6000000: episode: 6148, duration: 25.746s, episode steps: 1053, steps per second:  41, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.011 [0.000, 5.000],  loss: 0.015332, mae: 2.536730, mean_q: 3.052089, mean_eps: 0.100000
 4880970/6000000: episode: 6149, duration: 29.601s, episode steps: 1296, steps per second:  44, episode reward: 33.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.013200, mae: 2.553934, mean_q: 3.075192, mean_eps: 0.100000
 4882186/6000000: episode: 6150, duration: 27.147s, episode steps: 1216, steps per second:  45, episode reward: 32.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.206 [0.000, 5.000],  loss: 0.014610, mae: 2.537318, mean_q: 3.053957, mean_eps: 0.100000
 4883093/6000000: episode: 6151, duration: 21.084s, episode steps: 907, steps per second:  43, episode reward: 27.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.015058, mae: 2.541487, mean_q: 3.059295, mean_eps: 0.100000
 4884369/6000000: episode: 6152, duration: 27.988s, episode steps: 1276, steps per second:  46, episode reward: 33.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.013661, mae: 2.550287, mean_q: 3.069456, mean_eps: 0.100000
 4885312/6000000: episode: 6153, duration: 21.553s, episode steps: 943, steps per second:  44, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.100 [0.000, 5.000],  loss: 0.013832, mae: 2.536175, mean_q: 3.052563, mean_eps: 0.100000
 4886066/6000000: episode: 6154, duration: 16.829s, episode steps: 754, steps per second:  45, episode reward: 23.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.014641, mae: 2.530241, mean_q: 3.046198, mean_eps: 0.100000
 4887292/6000000: episode: 6155, duration: 26.997s, episode steps: 1226, steps per second:  45, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.104 [0.000, 5.000],  loss: 0.013522, mae: 2.535203, mean_q: 3.051487, mean_eps: 0.100000
 4887959/6000000: episode: 6156, duration: 14.769s, episode steps: 667, steps per second:  45, episode reward: 22.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.014386, mae: 2.540339, mean_q: 3.056788, mean_eps: 0.100000
 4889114/6000000: episode: 6157, duration: 24.086s, episode steps: 1155, steps per second:  48, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.621 [0.000, 5.000],  loss: 0.013889, mae: 2.547979, mean_q: 3.066057, mean_eps: 0.100000
 4890386/6000000: episode: 6158, duration: 26.038s, episode steps: 1272, steps per second:  49, episode reward: 33.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.069 [0.000, 5.000],  loss: 0.013745, mae: 2.530651, mean_q: 3.046895, mean_eps: 0.100000
 4891279/6000000: episode: 6159, duration: 19.339s, episode steps: 893, steps per second:  46, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.016005, mae: 2.516730, mean_q: 3.028877, mean_eps: 0.100000
 4892047/6000000: episode: 6160, duration: 16.472s, episode steps: 768, steps per second:  47, episode reward: 23.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.120 [0.000, 5.000],  loss: 0.014371, mae: 2.552397, mean_q: 3.072445, mean_eps: 0.100000
 4893313/6000000: episode: 6161, duration: 27.635s, episode steps: 1266, steps per second:  46, episode reward: 34.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.968 [0.000, 5.000],  loss: 0.013457, mae: 2.520393, mean_q: 3.033685, mean_eps: 0.100000
 4894479/6000000: episode: 6162, duration: 25.311s, episode steps: 1166, steps per second:  46, episode reward: 31.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.732 [0.000, 5.000],  loss: 0.014836, mae: 2.527747, mean_q: 3.042736, mean_eps: 0.100000
 4895463/6000000: episode: 6163, duration: 24.174s, episode steps: 984, steps per second:  41, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.750 [0.000, 5.000],  loss: 0.013031, mae: 2.545802, mean_q: 3.066423, mean_eps: 0.100000
 4896249/6000000: episode: 6164, duration: 20.343s, episode steps: 786, steps per second:  39, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.887 [0.000, 5.000],  loss: 0.015483, mae: 2.549251, mean_q: 3.069262, mean_eps: 0.100000
 4897442/6000000: episode: 6165, duration: 27.726s, episode steps: 1193, steps per second:  43, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.926 [0.000, 5.000],  loss: 0.014834, mae: 2.521316, mean_q: 3.036416, mean_eps: 0.100000
 4898239/6000000: episode: 6166, duration: 18.321s, episode steps: 797, steps per second:  44, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.832 [0.000, 5.000],  loss: 0.014957, mae: 2.535265, mean_q: 3.052082, mean_eps: 0.100000
 4898975/6000000: episode: 6167, duration: 16.349s, episode steps: 736, steps per second:  45, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.014076, mae: 2.532691, mean_q: 3.050237, mean_eps: 0.100000
 4899962/6000000: episode: 6168, duration: 20.970s, episode steps: 987, steps per second:  47, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.564 [0.000, 5.000],  loss: 0.014890, mae: 2.528625, mean_q: 3.046337, mean_eps: 0.100000
 4900736/6000000: episode: 6169, duration: 16.679s, episode steps: 774, steps per second:  46, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.105 [0.000, 5.000],  loss: 0.012904, mae: 2.550307, mean_q: 3.071187, mean_eps: 0.100000
 4901645/6000000: episode: 6170, duration: 20.436s, episode steps: 909, steps per second:  44, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.836 [0.000, 5.000],  loss: 0.013317, mae: 2.517607, mean_q: 3.030205, mean_eps: 0.100000
 4902195/6000000: episode: 6171, duration: 12.292s, episode steps: 550, steps per second:  45, episode reward: 14.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.111 [0.000, 5.000],  loss: 0.015713, mae: 2.527246, mean_q: 3.043080, mean_eps: 0.100000
 4903184/6000000: episode: 6172, duration: 21.580s, episode steps: 989, steps per second:  46, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.902 [0.000, 5.000],  loss: 0.013829, mae: 2.535742, mean_q: 3.051747, mean_eps: 0.100000
 4904228/6000000: episode: 6173, duration: 22.895s, episode steps: 1044, steps per second:  46, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.745 [0.000, 5.000],  loss: 0.013280, mae: 2.533805, mean_q: 3.051623, mean_eps: 0.100000
 4905261/6000000: episode: 6174, duration: 21.475s, episode steps: 1033, steps per second:  48, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.155 [0.000, 5.000],  loss: 0.013217, mae: 2.525135, mean_q: 3.039731, mean_eps: 0.100000
 4905839/6000000: episode: 6175, duration: 11.795s, episode steps: 578, steps per second:  49, episode reward: 17.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.050 [0.000, 5.000],  loss: 0.012858, mae: 2.475547, mean_q: 2.981436, mean_eps: 0.100000
 4906622/6000000: episode: 6176, duration: 17.163s, episode steps: 783, steps per second:  46, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.727 [0.000, 5.000],  loss: 0.013725, mae: 2.512378, mean_q: 3.026675, mean_eps: 0.100000
 4907314/6000000: episode: 6177, duration: 15.183s, episode steps: 692, steps per second:  46, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.965 [0.000, 5.000],  loss: 0.013380, mae: 2.484889, mean_q: 2.993462, mean_eps: 0.100000
 4907733/6000000: episode: 6178, duration: 9.352s, episode steps: 419, steps per second:  45, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.461 [0.000, 5.000],  loss: 0.012781, mae: 2.502678, mean_q: 3.016922, mean_eps: 0.100000
 4908686/6000000: episode: 6179, duration: 20.678s, episode steps: 953, steps per second:  46, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.122 [0.000, 5.000],  loss: 0.014328, mae: 2.500623, mean_q: 3.011210, mean_eps: 0.100000
 4909397/6000000: episode: 6180, duration: 15.282s, episode steps: 711, steps per second:  47, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.010 [0.000, 5.000],  loss: 0.013137, mae: 2.516866, mean_q: 3.032642, mean_eps: 0.100000
 4910238/6000000: episode: 6181, duration: 18.928s, episode steps: 841, steps per second:  44, episode reward: 26.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 0.014240, mae: 2.499754, mean_q: 3.009328, mean_eps: 0.100000
 4911251/6000000: episode: 6182, duration: 23.432s, episode steps: 1013, steps per second:  43, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.232 [0.000, 5.000],  loss: 0.015996, mae: 2.505425, mean_q: 3.014940, mean_eps: 0.100000
 4912641/6000000: episode: 6183, duration: 33.589s, episode steps: 1390, steps per second:  41, episode reward: 33.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.013414, mae: 2.505466, mean_q: 3.017146, mean_eps: 0.100000
 4913685/6000000: episode: 6184, duration: 23.522s, episode steps: 1044, steps per second:  44, episode reward: 30.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.831 [0.000, 5.000],  loss: 0.013764, mae: 2.507506, mean_q: 3.018475, mean_eps: 0.100000
 4914211/6000000: episode: 6185, duration: 12.299s, episode steps: 526, steps per second:  43, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.939 [0.000, 5.000],  loss: 0.015285, mae: 2.518692, mean_q: 3.032246, mean_eps: 0.100000
 4915451/6000000: episode: 6186, duration: 28.773s, episode steps: 1240, steps per second:  43, episode reward: 32.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.999 [0.000, 5.000],  loss: 0.013677, mae: 2.512099, mean_q: 3.023964, mean_eps: 0.100000
 4916955/6000000: episode: 6187, duration: 33.238s, episode steps: 1504, steps per second:  45, episode reward: 30.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.246 [0.000, 5.000],  loss: 0.014228, mae: 2.506993, mean_q: 3.017135, mean_eps: 0.100000
 4917799/6000000: episode: 6188, duration: 19.832s, episode steps: 844, steps per second:  43, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.705 [0.000, 5.000],  loss: 0.015520, mae: 2.506403, mean_q: 3.016039, mean_eps: 0.100000
 4918716/6000000: episode: 6189, duration: 20.548s, episode steps: 917, steps per second:  45, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.058 [0.000, 5.000],  loss: 0.013407, mae: 2.498824, mean_q: 3.008588, mean_eps: 0.100000
 4919425/6000000: episode: 6190, duration: 16.316s, episode steps: 709, steps per second:  43, episode reward: 20.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.850 [0.000, 5.000],  loss: 0.013188, mae: 2.494556, mean_q: 3.003307, mean_eps: 0.100000
 4920832/6000000: episode: 6191, duration: 30.707s, episode steps: 1407, steps per second:  46, episode reward: 29.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.043 [0.000, 5.000],  loss: 0.014268, mae: 2.479460, mean_q: 2.985502, mean_eps: 0.100000
 4921833/6000000: episode: 6192, duration: 20.610s, episode steps: 1001, steps per second:  49, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.662 [0.000, 5.000],  loss: 0.013552, mae: 2.468644, mean_q: 2.972360, mean_eps: 0.100000
 4922640/6000000: episode: 6193, duration: 18.324s, episode steps: 807, steps per second:  44, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.532 [0.000, 5.000],  loss: 0.014278, mae: 2.477645, mean_q: 2.983666, mean_eps: 0.100000
 4923636/6000000: episode: 6194, duration: 21.770s, episode steps: 996, steps per second:  46, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.015 [0.000, 5.000],  loss: 0.014511, mae: 2.485049, mean_q: 2.993844, mean_eps: 0.100000
 4924665/6000000: episode: 6195, duration: 22.803s, episode steps: 1029, steps per second:  45, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.654 [0.000, 5.000],  loss: 0.013577, mae: 2.467594, mean_q: 2.971817, mean_eps: 0.100000
 4925434/6000000: episode: 6196, duration: 17.572s, episode steps: 769, steps per second:  44, episode reward: 22.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.017 [0.000, 5.000],  loss: 0.015030, mae: 2.494390, mean_q: 3.004176, mean_eps: 0.100000
 4926462/6000000: episode: 6197, duration: 23.765s, episode steps: 1028, steps per second:  43, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.946 [0.000, 5.000],  loss: 0.014041, mae: 2.491135, mean_q: 2.997942, mean_eps: 0.100000
 4927537/6000000: episode: 6198, duration: 26.749s, episode steps: 1075, steps per second:  40, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.015219, mae: 2.499609, mean_q: 3.007613, mean_eps: 0.100000
 4928813/6000000: episode: 6199, duration: 30.541s, episode steps: 1276, steps per second:  42, episode reward: 31.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.746 [0.000, 5.000],  loss: 0.014212, mae: 2.482567, mean_q: 2.989765, mean_eps: 0.100000
 4930204/6000000: episode: 6200, duration: 31.447s, episode steps: 1391, steps per second:  44, episode reward: 34.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.015275, mae: 2.485074, mean_q: 2.992298, mean_eps: 0.100000
 4931490/6000000: episode: 6201, duration: 29.898s, episode steps: 1286, steps per second:  43, episode reward: 32.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.219 [0.000, 5.000],  loss: 0.013536, mae: 2.497865, mean_q: 3.008496, mean_eps: 0.100000
 4932395/6000000: episode: 6202, duration: 20.438s, episode steps: 905, steps per second:  44, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.933 [0.000, 5.000],  loss: 0.014740, mae: 2.478616, mean_q: 2.985528, mean_eps: 0.100000
 4933621/6000000: episode: 6203, duration: 28.925s, episode steps: 1226, steps per second:  42, episode reward: 30.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.189 [0.000, 5.000],  loss: 0.014194, mae: 2.483892, mean_q: 2.990370, mean_eps: 0.100000
 4934516/6000000: episode: 6204, duration: 20.404s, episode steps: 895, steps per second:  44, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.013 [0.000, 5.000],  loss: 0.014313, mae: 2.500832, mean_q: 3.011568, mean_eps: 0.100000
 4935354/6000000: episode: 6205, duration: 18.974s, episode steps: 838, steps per second:  44, episode reward: 25.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.021 [0.000, 5.000],  loss: 0.013397, mae: 2.509621, mean_q: 3.021710, mean_eps: 0.100000
 4936702/6000000: episode: 6206, duration: 29.094s, episode steps: 1348, steps per second:  46, episode reward: 29.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.031 [0.000, 5.000],  loss: 0.014142, mae: 2.506608, mean_q: 3.017328, mean_eps: 0.100000
 4937666/6000000: episode: 6207, duration: 19.743s, episode steps: 964, steps per second:  49, episode reward: 29.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.140 [0.000, 5.000],  loss: 0.014020, mae: 2.508503, mean_q: 3.019847, mean_eps: 0.100000
 4939038/6000000: episode: 6208, duration: 29.450s, episode steps: 1372, steps per second:  47, episode reward: 33.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.158 [0.000, 5.000],  loss: 0.013807, mae: 2.493626, mean_q: 3.004441, mean_eps: 0.100000
 4939644/6000000: episode: 6209, duration: 13.300s, episode steps: 606, steps per second:  46, episode reward: 17.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.850 [0.000, 5.000],  loss: 0.013592, mae: 2.494844, mean_q: 3.005423, mean_eps: 0.100000
 4940854/6000000: episode: 6210, duration: 26.880s, episode steps: 1210, steps per second:  45, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.014110, mae: 2.500319, mean_q: 3.012063, mean_eps: 0.100000
 4941521/6000000: episode: 6211, duration: 14.941s, episode steps: 667, steps per second:  45, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.204 [0.000, 5.000],  loss: 0.013095, mae: 2.471307, mean_q: 2.974442, mean_eps: 0.100000
 4942829/6000000: episode: 6212, duration: 29.993s, episode steps: 1308, steps per second:  44, episode reward: 34.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.138 [0.000, 5.000],  loss: 0.014417, mae: 2.498708, mean_q: 3.008339, mean_eps: 0.100000
 4943706/6000000: episode: 6213, duration: 21.418s, episode steps: 877, steps per second:  41, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.652 [0.000, 5.000],  loss: 0.015595, mae: 2.497932, mean_q: 3.007206, mean_eps: 0.100000
 4944799/6000000: episode: 6214, duration: 26.240s, episode steps: 1093, steps per second:  42, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.992 [0.000, 5.000],  loss: 0.015457, mae: 2.500858, mean_q: 3.011442, mean_eps: 0.100000
 4945879/6000000: episode: 6215, duration: 24.751s, episode steps: 1080, steps per second:  44, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.212 [0.000, 5.000],  loss: 0.014870, mae: 2.505943, mean_q: 3.017614, mean_eps: 0.100000
 4946734/6000000: episode: 6216, duration: 19.963s, episode steps: 855, steps per second:  43, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.332 [0.000, 5.000],  loss: 0.014418, mae: 2.514421, mean_q: 3.029983, mean_eps: 0.100000
 4947570/6000000: episode: 6217, duration: 19.863s, episode steps: 836, steps per second:  42, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.733 [0.000, 5.000],  loss: 0.014608, mae: 2.506487, mean_q: 3.019255, mean_eps: 0.100000
 4948461/6000000: episode: 6218, duration: 20.971s, episode steps: 891, steps per second:  42, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.772 [0.000, 5.000],  loss: 0.014806, mae: 2.527098, mean_q: 3.042780, mean_eps: 0.100000
 4949431/6000000: episode: 6219, duration: 21.940s, episode steps: 970, steps per second:  44, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.704 [0.000, 5.000],  loss: 0.013980, mae: 2.519956, mean_q: 3.034411, mean_eps: 0.100000
 4950797/6000000: episode: 6220, duration: 30.863s, episode steps: 1366, steps per second:  44, episode reward: 28.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.072 [0.000, 5.000],  loss: 0.013693, mae: 2.512965, mean_q: 3.025759, mean_eps: 0.100000
 4951342/6000000: episode: 6221, duration: 12.986s, episode steps: 545, steps per second:  42, episode reward: 15.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.620 [0.000, 5.000],  loss: 0.014148, mae: 2.516914, mean_q: 3.030983, mean_eps: 0.100000
 4952249/6000000: episode: 6222, duration: 20.837s, episode steps: 907, steps per second:  44, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.804 [0.000, 5.000],  loss: 0.012491, mae: 2.511739, mean_q: 3.026448, mean_eps: 0.100000
 4952994/6000000: episode: 6223, duration: 15.485s, episode steps: 745, steps per second:  48, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.826 [0.000, 5.000],  loss: 0.014462, mae: 2.508477, mean_q: 3.020304, mean_eps: 0.100000
 4953710/6000000: episode: 6224, duration: 14.663s, episode steps: 716, steps per second:  49, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.807 [0.000, 5.000],  loss: 0.014862, mae: 2.491121, mean_q: 2.998548, mean_eps: 0.100000
 4954879/6000000: episode: 6225, duration: 25.468s, episode steps: 1169, steps per second:  46, episode reward: 33.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.139 [0.000, 5.000],  loss: 0.013061, mae: 2.499974, mean_q: 3.010714, mean_eps: 0.100000
 4955633/6000000: episode: 6226, duration: 16.566s, episode steps: 754, steps per second:  46, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.846 [0.000, 5.000],  loss: 0.013667, mae: 2.529667, mean_q: 3.046571, mean_eps: 0.100000
 4956875/6000000: episode: 6227, duration: 26.887s, episode steps: 1242, steps per second:  46, episode reward: 34.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.013575, mae: 2.502671, mean_q: 3.014693, mean_eps: 0.100000
 4957520/6000000: episode: 6228, duration: 14.745s, episode steps: 645, steps per second:  44, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.012989, mae: 2.502541, mean_q: 3.013903, mean_eps: 0.100000
 4958529/6000000: episode: 6229, duration: 32.731s, episode steps: 1009, steps per second:  31, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.887 [0.000, 5.000],  loss: 0.014645, mae: 2.503014, mean_q: 3.014294, mean_eps: 0.100000
 4959569/6000000: episode: 6230, duration: 39.595s, episode steps: 1040, steps per second:  26, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.869 [0.000, 5.000],  loss: 0.013795, mae: 2.525804, mean_q: 3.040537, mean_eps: 0.100000
 4960639/6000000: episode: 6231, duration: 23.098s, episode steps: 1070, steps per second:  46, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.920 [0.000, 5.000],  loss: 0.014443, mae: 2.489324, mean_q: 2.995685, mean_eps: 0.100000
 4961965/6000000: episode: 6232, duration: 29.589s, episode steps: 1326, steps per second:  45, episode reward: 35.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.256 [0.000, 5.000],  loss: 0.013537, mae: 2.511858, mean_q: 3.023913, mean_eps: 0.100000
 4962864/6000000: episode: 6233, duration: 19.260s, episode steps: 899, steps per second:  47, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.682 [0.000, 5.000],  loss: 0.013427, mae: 2.525007, mean_q: 3.041196, mean_eps: 0.100000
 4963641/6000000: episode: 6234, duration: 17.146s, episode steps: 777, steps per second:  45, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.825 [0.000, 5.000],  loss: 0.014062, mae: 2.509219, mean_q: 3.020808, mean_eps: 0.100000
 4964772/6000000: episode: 6235, duration: 25.076s, episode steps: 1131, steps per second:  45, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.012872, mae: 2.511172, mean_q: 3.024036, mean_eps: 0.100000
 4965577/6000000: episode: 6236, duration: 17.683s, episode steps: 805, steps per second:  46, episode reward: 24.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.012369, mae: 2.484626, mean_q: 2.992465, mean_eps: 0.100000
 4966491/6000000: episode: 6237, duration: 19.872s, episode steps: 914, steps per second:  46, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.488 [0.000, 5.000],  loss: 0.014117, mae: 2.491731, mean_q: 2.999390, mean_eps: 0.100000
 4967520/6000000: episode: 6238, duration: 22.153s, episode steps: 1029, steps per second:  46, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.013800, mae: 2.505304, mean_q: 3.015956, mean_eps: 0.100000
 4968327/6000000: episode: 6239, duration: 16.607s, episode steps: 807, steps per second:  49, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.013654, mae: 2.489588, mean_q: 2.997392, mean_eps: 0.100000
 4969137/6000000: episode: 6240, duration: 17.107s, episode steps: 810, steps per second:  47, episode reward: 24.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.927 [0.000, 5.000],  loss: 0.014704, mae: 2.504412, mean_q: 3.012905, mean_eps: 0.100000
 4970013/6000000: episode: 6241, duration: 24.459s, episode steps: 876, steps per second:  36, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.013352, mae: 2.507851, mean_q: 3.020780, mean_eps: 0.100000
 4971190/6000000: episode: 6242, duration: 65.301s, episode steps: 1177, steps per second:  18, episode reward: 32.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.195 [0.000, 5.000],  loss: 0.013934, mae: 2.489280, mean_q: 2.996925, mean_eps: 0.100000
 4971965/6000000: episode: 6243, duration: 42.665s, episode steps: 775, steps per second:  18, episode reward: 24.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.911 [0.000, 5.000],  loss: 0.015672, mae: 2.483088, mean_q: 2.990247, mean_eps: 0.100000
 4972885/6000000: episode: 6244, duration: 52.634s, episode steps: 920, steps per second:  17, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.761 [0.000, 5.000],  loss: 0.013794, mae: 2.491997, mean_q: 3.000461, mean_eps: 0.100000
 4973938/6000000: episode: 6245, duration: 58.382s, episode steps: 1053, steps per second:  18, episode reward: 30.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.852 [0.000, 5.000],  loss: 0.014540, mae: 2.481683, mean_q: 2.987945, mean_eps: 0.100000
 4974749/6000000: episode: 6246, duration: 44.074s, episode steps: 811, steps per second:  18, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.131 [0.000, 5.000],  loss: 0.013789, mae: 2.477344, mean_q: 2.983834, mean_eps: 0.100000
 4975759/6000000: episode: 6247, duration: 52.109s, episode steps: 1010, steps per second:  19, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.962 [0.000, 5.000],  loss: 0.012648, mae: 2.474181, mean_q: 2.979717, mean_eps: 0.100000
 4976723/6000000: episode: 6248, duration: 37.818s, episode steps: 964, steps per second:  25, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.914 [0.000, 5.000],  loss: 0.013706, mae: 2.465992, mean_q: 2.969905, mean_eps: 0.100000
 4977626/6000000: episode: 6249, duration: 21.593s, episode steps: 903, steps per second:  42, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.911 [0.000, 5.000],  loss: 0.013675, mae: 2.470928, mean_q: 2.975849, mean_eps: 0.100000
 4978501/6000000: episode: 6250, duration: 20.833s, episode steps: 875, steps per second:  42, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.969 [0.000, 5.000],  loss: 0.013917, mae: 2.491611, mean_q: 2.999886, mean_eps: 0.100000
 4979492/6000000: episode: 6251, duration: 25.075s, episode steps: 991, steps per second:  40, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.561 [0.000, 5.000],  loss: 0.012817, mae: 2.472963, mean_q: 2.978778, mean_eps: 0.100000
 4980360/6000000: episode: 6252, duration: 42.012s, episode steps: 868, steps per second:  21, episode reward: 26.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.323 [0.000, 5.000],  loss: 0.015044, mae: 2.486102, mean_q: 2.992908, mean_eps: 0.100000
 4981382/6000000: episode: 6253, duration: 54.555s, episode steps: 1022, steps per second:  19, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.936 [0.000, 5.000],  loss: 0.015091, mae: 2.523752, mean_q: 3.037160, mean_eps: 0.100000
 4982183/6000000: episode: 6254, duration: 44.345s, episode steps: 801, steps per second:  18, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.013981, mae: 2.491554, mean_q: 2.999507, mean_eps: 0.100000
 4983395/6000000: episode: 6255, duration: 44.306s, episode steps: 1212, steps per second:  27, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.014130, mae: 2.505981, mean_q: 3.015130, mean_eps: 0.100000
 4984404/6000000: episode: 6256, duration: 35.736s, episode steps: 1009, steps per second:  28, episode reward: 29.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.167 [0.000, 5.000],  loss: 0.014427, mae: 2.510039, mean_q: 3.020107, mean_eps: 0.100000
 4985302/6000000: episode: 6257, duration: 46.985s, episode steps: 898, steps per second:  19, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.140 [0.000, 5.000],  loss: 0.013849, mae: 2.489163, mean_q: 2.994755, mean_eps: 0.100000
 4985923/6000000: episode: 6258, duration: 30.344s, episode steps: 621, steps per second:  20, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.802 [0.000, 5.000],  loss: 0.014867, mae: 2.480269, mean_q: 2.987118, mean_eps: 0.100000
 4986674/6000000: episode: 6259, duration: 37.777s, episode steps: 751, steps per second:  20, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.694 [0.000, 5.000],  loss: 0.012633, mae: 2.491807, mean_q: 2.999728, mean_eps: 0.100000
 4987763/6000000: episode: 6260, duration: 55.403s, episode steps: 1089, steps per second:  20, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.605 [0.000, 5.000],  loss: 0.014746, mae: 2.499245, mean_q: 3.009139, mean_eps: 0.100000
 4988574/6000000: episode: 6261, duration: 42.186s, episode steps: 811, steps per second:  19, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.326 [0.000, 5.000],  loss: 0.014111, mae: 2.494901, mean_q: 3.001514, mean_eps: 0.100000
 4989729/6000000: episode: 6262, duration: 51.109s, episode steps: 1155, steps per second:  23, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.013646, mae: 2.491647, mean_q: 2.999272, mean_eps: 0.100000
 4990598/6000000: episode: 6263, duration: 27.851s, episode steps: 869, steps per second:  31, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.144 [0.000, 5.000],  loss: 0.013554, mae: 2.487531, mean_q: 2.993920, mean_eps: 0.100000
 4991543/6000000: episode: 6264, duration: 45.557s, episode steps: 945, steps per second:  21, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.912 [0.000, 5.000],  loss: 0.015050, mae: 2.482355, mean_q: 2.986779, mean_eps: 0.100000
 4992966/6000000: episode: 6265, duration: 71.302s, episode steps: 1423, steps per second:  20, episode reward: 30.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.122 [0.000, 5.000],  loss: 0.014459, mae: 2.490143, mean_q: 2.997000, mean_eps: 0.100000
 4993825/6000000: episode: 6266, duration: 44.687s, episode steps: 859, steps per second:  19, episode reward: 28.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.071 [0.000, 5.000],  loss: 0.013289, mae: 2.485054, mean_q: 2.991273, mean_eps: 0.100000
 4994657/6000000: episode: 6267, duration: 25.739s, episode steps: 832, steps per second:  32, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.012620, mae: 2.474973, mean_q: 2.979138, mean_eps: 0.100000
 4995632/6000000: episode: 6268, duration: 20.711s, episode steps: 975, steps per second:  47, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.666 [0.000, 5.000],  loss: 0.013920, mae: 2.473887, mean_q: 2.979441, mean_eps: 0.100000
 4997009/6000000: episode: 6269, duration: 31.090s, episode steps: 1377, steps per second:  44, episode reward: 35.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.300 [0.000, 5.000],  loss: 0.013864, mae: 2.478341, mean_q: 2.984818, mean_eps: 0.100000
 4997973/6000000: episode: 6270, duration: 22.755s, episode steps: 964, steps per second:  42, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.809 [0.000, 5.000],  loss: 0.015570, mae: 2.495700, mean_q: 3.003922, mean_eps: 0.100000
 4998807/6000000: episode: 6271, duration: 18.912s, episode steps: 834, steps per second:  44, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.332 [0.000, 5.000],  loss: 0.015256, mae: 2.473669, mean_q: 2.978370, mean_eps: 0.100000
 4999562/6000000: episode: 6272, duration: 16.411s, episode steps: 755, steps per second:  46, episode reward: 22.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.690 [0.000, 5.000],  loss: 0.014264, mae: 2.461653, mean_q: 2.964738, mean_eps: 0.100000
 5000386/6000000: episode: 6273, duration: 18.343s, episode steps: 824, steps per second:  45, episode reward: 27.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 1.555 [0.000, 5.000],  loss: 0.013720, mae: 2.471612, mean_q: 2.976649, mean_eps: 0.100000
 5001363/6000000: episode: 6274, duration: 21.249s, episode steps: 977, steps per second:  46, episode reward: 30.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.875 [0.000, 5.000],  loss: 0.013293, mae: 2.504715, mean_q: 3.016602, mean_eps: 0.100000
 5002069/6000000: episode: 6275, duration: 15.337s, episode steps: 706, steps per second:  46, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.657 [0.000, 5.000],  loss: 0.013929, mae: 2.507937, mean_q: 3.020046, mean_eps: 0.100000
 5003441/6000000: episode: 6276, duration: 30.485s, episode steps: 1372, steps per second:  45, episode reward: 32.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.009 [0.000, 5.000],  loss: 0.012950, mae: 2.503036, mean_q: 3.014195, mean_eps: 0.100000
 5004172/6000000: episode: 6277, duration: 15.949s, episode steps: 731, steps per second:  46, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.130 [0.000, 5.000],  loss: 0.013468, mae: 2.502347, mean_q: 3.011967, mean_eps: 0.100000
 5005389/6000000: episode: 6278, duration: 26.347s, episode steps: 1217, steps per second:  46, episode reward: 35.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.789 [0.000, 5.000],  loss: 0.013807, mae: 2.512234, mean_q: 3.025819, mean_eps: 0.100000
 5006059/6000000: episode: 6279, duration: 15.048s, episode steps: 670, steps per second:  45, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.712 [0.000, 5.000],  loss: 0.013012, mae: 2.530128, mean_q: 3.047444, mean_eps: 0.100000
 5007018/6000000: episode: 6280, duration: 20.402s, episode steps: 959, steps per second:  47, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.943 [0.000, 5.000],  loss: 0.012917, mae: 2.509263, mean_q: 3.023371, mean_eps: 0.100000
 5007981/6000000: episode: 6281, duration: 20.063s, episode steps: 963, steps per second:  48, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.066 [0.000, 5.000],  loss: 0.012484, mae: 2.514188, mean_q: 3.028903, mean_eps: 0.100000
 5009076/6000000: episode: 6282, duration: 25.639s, episode steps: 1095, steps per second:  43, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.913 [0.000, 5.000],  loss: 0.012840, mae: 2.521896, mean_q: 3.036353, mean_eps: 0.100000
 5009779/6000000: episode: 6283, duration: 34.790s, episode steps: 703, steps per second:  20, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.013682, mae: 2.535508, mean_q: 3.053260, mean_eps: 0.100000
 5010799/6000000: episode: 6284, duration: 50.823s, episode steps: 1020, steps per second:  20, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.012712, mae: 2.505889, mean_q: 3.018492, mean_eps: 0.100000
 5011485/6000000: episode: 6285, duration: 34.387s, episode steps: 686, steps per second:  20, episode reward: 20.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.013389, mae: 2.519633, mean_q: 3.034921, mean_eps: 0.100000
 5012087/6000000: episode: 6286, duration: 29.611s, episode steps: 602, steps per second:  20, episode reward: 17.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.018 [0.000, 5.000],  loss: 0.014269, mae: 2.536317, mean_q: 3.053141, mean_eps: 0.100000
 5012951/6000000: episode: 6287, duration: 19.430s, episode steps: 864, steps per second:  44, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.184 [0.000, 5.000],  loss: 0.013954, mae: 2.535196, mean_q: 3.052146, mean_eps: 0.100000
 5013974/6000000: episode: 6288, duration: 22.617s, episode steps: 1023, steps per second:  45, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.991 [0.000, 5.000],  loss: 0.014757, mae: 2.538904, mean_q: 3.057733, mean_eps: 0.100000
 5015203/6000000: episode: 6289, duration: 26.542s, episode steps: 1229, steps per second:  46, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.581 [0.000, 5.000],  loss: 0.014625, mae: 2.516579, mean_q: 3.031576, mean_eps: 0.100000
 5016062/6000000: episode: 6290, duration: 19.521s, episode steps: 859, steps per second:  44, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.795 [0.000, 5.000],  loss: 0.014506, mae: 2.537254, mean_q: 3.057418, mean_eps: 0.100000
 5016900/6000000: episode: 6291, duration: 27.215s, episode steps: 838, steps per second:  31, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.959 [0.000, 5.000],  loss: 0.015142, mae: 2.538669, mean_q: 3.057128, mean_eps: 0.100000
 5017742/6000000: episode: 6292, duration: 39.925s, episode steps: 842, steps per second:  21, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.599 [0.000, 5.000],  loss: 0.013963, mae: 2.519962, mean_q: 3.035600, mean_eps: 0.100000
 5018270/6000000: episode: 6293, duration: 10.830s, episode steps: 528, steps per second:  49, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.699 [0.000, 5.000],  loss: 0.016349, mae: 2.505733, mean_q: 3.018831, mean_eps: 0.100000
 5018810/6000000: episode: 6294, duration: 11.081s, episode steps: 540, steps per second:  49, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.237 [0.000, 5.000],  loss: 0.015203, mae: 2.533678, mean_q: 3.049844, mean_eps: 0.100000
 5019654/6000000: episode: 6295, duration: 18.413s, episode steps: 844, steps per second:  46, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.050 [0.000, 5.000],  loss: 0.013084, mae: 2.529439, mean_q: 3.046216, mean_eps: 0.100000
 5020555/6000000: episode: 6296, duration: 19.958s, episode steps: 901, steps per second:  45, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.351 [0.000, 5.000],  loss: 0.012885, mae: 2.511660, mean_q: 3.025369, mean_eps: 0.100000
 5022096/6000000: episode: 6297, duration: 33.921s, episode steps: 1541, steps per second:  45, episode reward: 35.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.898 [0.000, 5.000],  loss: 0.014580, mae: 2.532132, mean_q: 3.049728, mean_eps: 0.100000
 5023535/6000000: episode: 6298, duration: 33.186s, episode steps: 1439, steps per second:  43, episode reward: 35.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.186 [0.000, 5.000],  loss: 0.012819, mae: 2.528581, mean_q: 3.045892, mean_eps: 0.100000
 5024230/6000000: episode: 6299, duration: 15.723s, episode steps: 695, steps per second:  44, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.955 [0.000, 5.000],  loss: 0.014332, mae: 2.523026, mean_q: 3.039558, mean_eps: 0.100000
 5025373/6000000: episode: 6300, duration: 28.211s, episode steps: 1143, steps per second:  41, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.953 [0.000, 5.000],  loss: 0.014056, mae: 2.515297, mean_q: 3.029831, mean_eps: 0.100000
 5026457/6000000: episode: 6301, duration: 25.148s, episode steps: 1084, steps per second:  43, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.913 [0.000, 5.000],  loss: 0.013105, mae: 2.525950, mean_q: 3.042718, mean_eps: 0.100000
 5027153/6000000: episode: 6302, duration: 15.724s, episode steps: 696, steps per second:  44, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.675 [0.000, 5.000],  loss: 0.014088, mae: 2.519731, mean_q: 3.034698, mean_eps: 0.100000
 5027990/6000000: episode: 6303, duration: 18.804s, episode steps: 837, steps per second:  45, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.907 [0.000, 5.000],  loss: 0.012347, mae: 2.511495, mean_q: 3.024753, mean_eps: 0.100000
 5029274/6000000: episode: 6304, duration: 29.562s, episode steps: 1284, steps per second:  43, episode reward: 33.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.637 [0.000, 5.000],  loss: 0.014297, mae: 2.526582, mean_q: 3.043153, mean_eps: 0.100000
 5030321/6000000: episode: 6305, duration: 24.028s, episode steps: 1047, steps per second:  44, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.140 [0.000, 5.000],  loss: 0.013464, mae: 2.523957, mean_q: 3.039437, mean_eps: 0.100000
 5031404/6000000: episode: 6306, duration: 24.661s, episode steps: 1083, steps per second:  44, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.014256, mae: 2.508247, mean_q: 3.020808, mean_eps: 0.100000
 5032172/6000000: episode: 6307, duration: 16.862s, episode steps: 768, steps per second:  46, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.507 [0.000, 5.000],  loss: 0.014217, mae: 2.511792, mean_q: 3.024365, mean_eps: 0.100000
 5033094/6000000: episode: 6308, duration: 20.435s, episode steps: 922, steps per second:  45, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 0.893 [0.000, 5.000],  loss: 0.013064, mae: 2.523678, mean_q: 3.041695, mean_eps: 0.100000
 5033969/6000000: episode: 6309, duration: 18.484s, episode steps: 875, steps per second:  47, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.647 [0.000, 5.000],  loss: 0.014405, mae: 2.503259, mean_q: 3.015832, mean_eps: 0.100000
 5034902/6000000: episode: 6310, duration: 18.812s, episode steps: 933, steps per second:  50, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.013803, mae: 2.516416, mean_q: 3.030419, mean_eps: 0.100000
 5036320/6000000: episode: 6311, duration: 30.009s, episode steps: 1418, steps per second:  47, episode reward: 33.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.782 [0.000, 5.000],  loss: 0.013623, mae: 2.500507, mean_q: 3.012087, mean_eps: 0.100000
 5037411/6000000: episode: 6312, duration: 23.260s, episode steps: 1091, steps per second:  47, episode reward: 32.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.071 [0.000, 5.000],  loss: 0.012598, mae: 2.506515, mean_q: 3.019249, mean_eps: 0.100000
 5038027/6000000: episode: 6313, duration: 13.354s, episode steps: 616, steps per second:  46, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.950 [0.000, 5.000],  loss: 0.015748, mae: 2.505561, mean_q: 3.015139, mean_eps: 0.100000
 5038966/6000000: episode: 6314, duration: 20.770s, episode steps: 939, steps per second:  45, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.921 [0.000, 5.000],  loss: 0.014316, mae: 2.500550, mean_q: 3.011728, mean_eps: 0.100000
 5039997/6000000: episode: 6315, duration: 22.445s, episode steps: 1031, steps per second:  46, episode reward: 31.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.092 [0.000, 5.000],  loss: 0.013301, mae: 2.503959, mean_q: 3.016194, mean_eps: 0.100000
 5041100/6000000: episode: 6316, duration: 26.605s, episode steps: 1103, steps per second:  41, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.299 [0.000, 5.000],  loss: 0.014098, mae: 2.497510, mean_q: 3.007509, mean_eps: 0.100000
 5042043/6000000: episode: 6317, duration: 21.588s, episode steps: 943, steps per second:  44, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.533 [0.000, 5.000],  loss: 0.013341, mae: 2.495473, mean_q: 3.004855, mean_eps: 0.100000
 5042694/6000000: episode: 6318, duration: 14.621s, episode steps: 651, steps per second:  45, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.014025, mae: 2.498023, mean_q: 3.007480, mean_eps: 0.100000
 5043839/6000000: episode: 6319, duration: 25.053s, episode steps: 1145, steps per second:  46, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.072 [0.000, 5.000],  loss: 0.013078, mae: 2.520360, mean_q: 3.034595, mean_eps: 0.100000
 5044711/6000000: episode: 6320, duration: 19.884s, episode steps: 872, steps per second:  44, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.992 [0.000, 5.000],  loss: 0.015412, mae: 2.521700, mean_q: 3.036664, mean_eps: 0.100000
 5045771/6000000: episode: 6321, duration: 22.959s, episode steps: 1060, steps per second:  46, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.013898, mae: 2.498540, mean_q: 3.007533, mean_eps: 0.100000
 5046608/6000000: episode: 6322, duration: 20.057s, episode steps: 837, steps per second:  42, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.458 [0.000, 5.000],  loss: 0.013740, mae: 2.486040, mean_q: 2.994523, mean_eps: 0.100000
 5047348/6000000: episode: 6323, duration: 17.398s, episode steps: 740, steps per second:  43, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.693 [0.000, 5.000],  loss: 0.014046, mae: 2.511836, mean_q: 3.024580, mean_eps: 0.100000
 5048558/6000000: episode: 6324, duration: 26.773s, episode steps: 1210, steps per second:  45, episode reward: 35.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.898 [0.000, 5.000],  loss: 0.014685, mae: 2.502155, mean_q: 3.012026, mean_eps: 0.100000
 5049495/6000000: episode: 6325, duration: 21.054s, episode steps: 937, steps per second:  45, episode reward: 28.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.201 [0.000, 5.000],  loss: 0.014581, mae: 2.475396, mean_q: 2.979613, mean_eps: 0.100000
 5050721/6000000: episode: 6326, duration: 25.586s, episode steps: 1226, steps per second:  48, episode reward: 36.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.287 [0.000, 5.000],  loss: 0.014272, mae: 2.499277, mean_q: 3.009290, mean_eps: 0.100000
 5051794/6000000: episode: 6327, duration: 23.012s, episode steps: 1073, steps per second:  47, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.013726, mae: 2.484141, mean_q: 2.990947, mean_eps: 0.100000
 5052683/6000000: episode: 6328, duration: 19.568s, episode steps: 889, steps per second:  45, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.014942, mae: 2.491853, mean_q: 2.999198, mean_eps: 0.100000
 5053798/6000000: episode: 6329, duration: 23.593s, episode steps: 1115, steps per second:  47, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.875 [0.000, 5.000],  loss: 0.015126, mae: 2.485959, mean_q: 2.994093, mean_eps: 0.100000
 5054988/6000000: episode: 6330, duration: 25.295s, episode steps: 1190, steps per second:  47, episode reward: 36.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.012653, mae: 2.482671, mean_q: 2.989999, mean_eps: 0.100000
 5055788/6000000: episode: 6331, duration: 18.438s, episode steps: 800, steps per second:  43, episode reward: 26.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.299 [0.000, 5.000],  loss: 0.012305, mae: 2.486323, mean_q: 2.994849, mean_eps: 0.100000
 5056802/6000000: episode: 6332, duration: 23.688s, episode steps: 1014, steps per second:  43, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.110 [0.000, 5.000],  loss: 0.013290, mae: 2.471393, mean_q: 2.975968, mean_eps: 0.100000
 5057363/6000000: episode: 6333, duration: 13.558s, episode steps: 561, steps per second:  41, episode reward: 15.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.013233, mae: 2.439688, mean_q: 2.938159, mean_eps: 0.100000
 5058390/6000000: episode: 6334, duration: 24.712s, episode steps: 1027, steps per second:  42, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.013152, mae: 2.454842, mean_q: 2.956847, mean_eps: 0.100000
 5059437/6000000: episode: 6335, duration: 23.279s, episode steps: 1047, steps per second:  45, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.164 [0.000, 5.000],  loss: 0.013340, mae: 2.464159, mean_q: 2.966404, mean_eps: 0.100000
 5060255/6000000: episode: 6336, duration: 18.082s, episode steps: 818, steps per second:  45, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.120 [0.000, 5.000],  loss: 0.013686, mae: 2.465266, mean_q: 2.970363, mean_eps: 0.100000
 5061044/6000000: episode: 6337, duration: 17.305s, episode steps: 789, steps per second:  46, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.281 [0.000, 5.000],  loss: 0.016120, mae: 2.465636, mean_q: 2.970316, mean_eps: 0.100000
 5061939/6000000: episode: 6338, duration: 19.090s, episode steps: 895, steps per second:  47, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.013235, mae: 2.468324, mean_q: 2.975616, mean_eps: 0.100000
 5062867/6000000: episode: 6339, duration: 22.665s, episode steps: 928, steps per second:  41, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.862 [0.000, 5.000],  loss: 0.012540, mae: 2.468045, mean_q: 2.973691, mean_eps: 0.100000
 5063952/6000000: episode: 6340, duration: 26.009s, episode steps: 1085, steps per second:  42, episode reward: 35.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.046 [0.000, 5.000],  loss: 0.014364, mae: 2.449911, mean_q: 2.950968, mean_eps: 0.100000
 5064581/6000000: episode: 6341, duration: 14.438s, episode steps: 629, steps per second:  44, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.118 [0.000, 5.000],  loss: 0.014523, mae: 2.462152, mean_q: 2.965367, mean_eps: 0.100000
 5065581/6000000: episode: 6342, duration: 22.358s, episode steps: 1000, steps per second:  45, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.932 [0.000, 5.000],  loss: 0.013702, mae: 2.472934, mean_q: 2.979504, mean_eps: 0.100000
 5066143/6000000: episode: 6343, duration: 11.842s, episode steps: 562, steps per second:  47, episode reward: 15.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.520 [0.000, 5.000],  loss: 0.013665, mae: 2.483308, mean_q: 2.991409, mean_eps: 0.100000
 5067039/6000000: episode: 6344, duration: 18.600s, episode steps: 896, steps per second:  48, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.781 [0.000, 5.000],  loss: 0.013981, mae: 2.488489, mean_q: 2.998201, mean_eps: 0.100000
 5068098/6000000: episode: 6345, duration: 24.765s, episode steps: 1059, steps per second:  43, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.987 [0.000, 5.000],  loss: 0.014243, mae: 2.482578, mean_q: 2.991125, mean_eps: 0.100000
 5068975/6000000: episode: 6346, duration: 20.590s, episode steps: 877, steps per second:  43, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.049 [0.000, 5.000],  loss: 0.013844, mae: 2.465575, mean_q: 2.968617, mean_eps: 0.100000
 5069854/6000000: episode: 6347, duration: 19.614s, episode steps: 879, steps per second:  45, episode reward: 28.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 1.810 [0.000, 5.000],  loss: 0.012961, mae: 2.480069, mean_q: 2.987260, mean_eps: 0.100000
 5070997/6000000: episode: 6348, duration: 26.285s, episode steps: 1143, steps per second:  43, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.056 [0.000, 5.000],  loss: 0.014645, mae: 2.457895, mean_q: 2.961192, mean_eps: 0.100000
 5071608/6000000: episode: 6349, duration: 14.222s, episode steps: 611, steps per second:  43, episode reward: 15.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.061 [0.000, 5.000],  loss: 0.014200, mae: 2.448450, mean_q: 2.947347, mean_eps: 0.100000
 5072543/6000000: episode: 6350, duration: 21.369s, episode steps: 935, steps per second:  44, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.550 [0.000, 5.000],  loss: 0.013631, mae: 2.415167, mean_q: 2.909119, mean_eps: 0.100000
 5073433/6000000: episode: 6351, duration: 20.798s, episode steps: 890, steps per second:  43, episode reward: 27.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.665 [0.000, 5.000],  loss: 0.013911, mae: 2.432837, mean_q: 2.929754, mean_eps: 0.100000
 5074153/6000000: episode: 6352, duration: 17.876s, episode steps: 720, steps per second:  40, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.119 [0.000, 5.000],  loss: 0.013738, mae: 2.425142, mean_q: 2.921106, mean_eps: 0.100000
 5075090/6000000: episode: 6353, duration: 20.840s, episode steps: 937, steps per second:  45, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.066 [0.000, 5.000],  loss: 0.013328, mae: 2.436753, mean_q: 2.933679, mean_eps: 0.100000
 5075762/6000000: episode: 6354, duration: 15.170s, episode steps: 672, steps per second:  44, episode reward: 19.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.917 [0.000, 5.000],  loss: 0.014443, mae: 2.448122, mean_q: 2.947918, mean_eps: 0.100000
 5076610/6000000: episode: 6355, duration: 18.686s, episode steps: 848, steps per second:  45, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.233 [0.000, 5.000],  loss: 0.014749, mae: 2.422469, mean_q: 2.916541, mean_eps: 0.100000
 5077613/6000000: episode: 6356, duration: 21.621s, episode steps: 1003, steps per second:  46, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.013357, mae: 2.411949, mean_q: 2.904183, mean_eps: 0.100000
 5079019/6000000: episode: 6357, duration: 31.778s, episode steps: 1406, steps per second:  44, episode reward: 35.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.968 [0.000, 5.000],  loss: 0.014393, mae: 2.439575, mean_q: 2.937174, mean_eps: 0.100000
 5079497/6000000: episode: 6358, duration: 10.519s, episode steps: 478, steps per second:  45, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.749 [0.000, 5.000],  loss: 0.012353, mae: 2.436196, mean_q: 2.934157, mean_eps: 0.100000
 5080275/6000000: episode: 6359, duration: 16.888s, episode steps: 778, steps per second:  46, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.680 [0.000, 5.000],  loss: 0.013625, mae: 2.442971, mean_q: 2.942896, mean_eps: 0.100000
 5081191/6000000: episode: 6360, duration: 20.389s, episode steps: 916, steps per second:  45, episode reward: 28.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.986 [0.000, 5.000],  loss: 0.014685, mae: 2.430237, mean_q: 2.926158, mean_eps: 0.100000
 5082015/6000000: episode: 6361, duration: 17.913s, episode steps: 824, steps per second:  46, episode reward: 25.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.723 [0.000, 5.000],  loss: 0.013897, mae: 2.434361, mean_q: 2.930953, mean_eps: 0.100000
 5083236/6000000: episode: 6362, duration: 24.783s, episode steps: 1221, steps per second:  49, episode reward: 33.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.103 [0.000, 5.000],  loss: 0.014703, mae: 2.415118, mean_q: 2.908085, mean_eps: 0.100000
 5084651/6000000: episode: 6363, duration: 29.965s, episode steps: 1415, steps per second:  47, episode reward: 43.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.013058, mae: 2.426034, mean_q: 2.920778, mean_eps: 0.100000
 5085468/6000000: episode: 6364, duration: 17.501s, episode steps: 817, steps per second:  47, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.065 [0.000, 5.000],  loss: 0.012146, mae: 2.425391, mean_q: 2.920141, mean_eps: 0.100000
 5086143/6000000: episode: 6365, duration: 15.551s, episode steps: 675, steps per second:  43, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.293 [0.000, 5.000],  loss: 0.014309, mae: 2.433145, mean_q: 2.929734, mean_eps: 0.100000
 5086897/6000000: episode: 6366, duration: 17.027s, episode steps: 754, steps per second:  44, episode reward: 21.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.759 [0.000, 5.000],  loss: 0.014026, mae: 2.429159, mean_q: 2.924948, mean_eps: 0.100000
 5087794/6000000: episode: 6367, duration: 19.898s, episode steps: 897, steps per second:  45, episode reward: 29.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.162 [0.000, 5.000],  loss: 0.013366, mae: 2.442831, mean_q: 2.940937, mean_eps: 0.100000
 5088521/6000000: episode: 6368, duration: 16.382s, episode steps: 727, steps per second:  44, episode reward: 22.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.193 [0.000, 5.000],  loss: 0.013504, mae: 2.435553, mean_q: 2.933469, mean_eps: 0.100000
 5089764/6000000: episode: 6369, duration: 30.432s, episode steps: 1243, steps per second:  41, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.895 [0.000, 5.000],  loss: 0.013128, mae: 2.431740, mean_q: 2.927314, mean_eps: 0.100000
 5090196/6000000: episode: 6370, duration: 10.064s, episode steps: 432, steps per second:  43, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.278 [0.000, 5.000],  loss: 0.014925, mae: 2.449535, mean_q: 2.951187, mean_eps: 0.100000
 5091146/6000000: episode: 6371, duration: 21.793s, episode steps: 950, steps per second:  44, episode reward: 32.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 1.875 [0.000, 5.000],  loss: 0.013773, mae: 2.420664, mean_q: 2.915421, mean_eps: 0.100000
 5091744/6000000: episode: 6372, duration: 13.307s, episode steps: 598, steps per second:  45, episode reward: 15.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.012646, mae: 2.435415, mean_q: 2.931913, mean_eps: 0.100000
 5092813/6000000: episode: 6373, duration: 25.137s, episode steps: 1069, steps per second:  43, episode reward: 32.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.976 [0.000, 5.000],  loss: 0.013607, mae: 2.431903, mean_q: 2.928056, mean_eps: 0.100000
 5093627/6000000: episode: 6374, duration: 17.815s, episode steps: 814, steps per second:  46, episode reward: 25.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.760 [0.000, 5.000],  loss: 0.014225, mae: 2.427433, mean_q: 2.923528, mean_eps: 0.100000
 5094456/6000000: episode: 6375, duration: 19.065s, episode steps: 829, steps per second:  43, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.893 [0.000, 5.000],  loss: 0.013549, mae: 2.431142, mean_q: 2.927121, mean_eps: 0.100000
 5095461/6000000: episode: 6376, duration: 23.317s, episode steps: 1005, steps per second:  43, episode reward: 29.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.753 [0.000, 5.000],  loss: 0.012973, mae: 2.423399, mean_q: 2.919222, mean_eps: 0.100000
 5096507/6000000: episode: 6377, duration: 22.769s, episode steps: 1046, steps per second:  46, episode reward: 32.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.766 [0.000, 5.000],  loss: 0.013173, mae: 2.427621, mean_q: 2.923791, mean_eps: 0.100000
 5097928/6000000: episode: 6378, duration: 32.006s, episode steps: 1421, steps per second:  44, episode reward: 35.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.145 [0.000, 5.000],  loss: 0.014368, mae: 2.428677, mean_q: 2.923427, mean_eps: 0.100000
 5098597/6000000: episode: 6379, duration: 14.173s, episode steps: 669, steps per second:  47, episode reward: 19.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.969 [0.000, 5.000],  loss: 0.014426, mae: 2.403849, mean_q: 2.894143, mean_eps: 0.100000
 5099263/6000000: episode: 6380, duration: 13.790s, episode steps: 666, steps per second:  48, episode reward: 20.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.014019, mae: 2.405186, mean_q: 2.894935, mean_eps: 0.100000
 5100098/6000000: episode: 6381, duration: 17.746s, episode steps: 835, steps per second:  47, episode reward: 24.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.996 [0.000, 5.000],  loss: 0.014191, mae: 2.423783, mean_q: 2.918698, mean_eps: 0.100000
 5100709/6000000: episode: 6382, duration: 13.139s, episode steps: 611, steps per second:  47, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.095 [0.000, 5.000],  loss: 0.014183, mae: 2.431056, mean_q: 2.926619, mean_eps: 0.100000
 5101604/6000000: episode: 6383, duration: 18.900s, episode steps: 895, steps per second:  47, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.288 [0.000, 5.000],  loss: 0.014108, mae: 2.428050, mean_q: 2.923583, mean_eps: 0.100000
 5102372/6000000: episode: 6384, duration: 17.523s, episode steps: 768, steps per second:  44, episode reward: 22.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.137 [0.000, 5.000],  loss: 0.015692, mae: 2.430476, mean_q: 2.923934, mean_eps: 0.100000
 5103403/6000000: episode: 6385, duration: 22.673s, episode steps: 1031, steps per second:  45, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.805 [0.000, 5.000],  loss: 0.013161, mae: 2.431771, mean_q: 2.928647, mean_eps: 0.100000
 5104173/6000000: episode: 6386, duration: 18.176s, episode steps: 770, steps per second:  42, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.738 [0.000, 5.000],  loss: 0.013837, mae: 2.419401, mean_q: 2.913798, mean_eps: 0.100000
 5105158/6000000: episode: 6387, duration: 23.741s, episode steps: 985, steps per second:  41, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.745 [0.000, 5.000],  loss: 0.014435, mae: 2.417584, mean_q: 2.911457, mean_eps: 0.100000
 5106144/6000000: episode: 6388, duration: 24.279s, episode steps: 986, steps per second:  41, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.079 [0.000, 5.000],  loss: 0.014480, mae: 2.455020, mean_q: 2.954907, mean_eps: 0.100000
 5107134/6000000: episode: 6389, duration: 23.297s, episode steps: 990, steps per second:  42, episode reward: 32.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.177 [0.000, 5.000],  loss: 0.014428, mae: 2.430633, mean_q: 2.927030, mean_eps: 0.100000
 5108250/6000000: episode: 6390, duration: 25.338s, episode steps: 1116, steps per second:  44, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.798 [0.000, 5.000],  loss: 0.014980, mae: 2.440770, mean_q: 2.938595, mean_eps: 0.100000
 5109086/6000000: episode: 6391, duration: 18.777s, episode steps: 836, steps per second:  45, episode reward: 24.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.013325, mae: 2.436402, mean_q: 2.934208, mean_eps: 0.100000
 5110576/6000000: episode: 6392, duration: 34.139s, episode steps: 1490, steps per second:  44, episode reward: 35.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.012935, mae: 2.449536, mean_q: 2.949468, mean_eps: 0.100000
 5111435/6000000: episode: 6393, duration: 20.356s, episode steps: 859, steps per second:  42, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.140 [0.000, 5.000],  loss: 0.012440, mae: 2.452269, mean_q: 2.953535, mean_eps: 0.100000
 5112493/6000000: episode: 6394, duration: 24.175s, episode steps: 1058, steps per second:  44, episode reward: 31.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.968 [0.000, 5.000],  loss: 0.014247, mae: 2.459591, mean_q: 2.962123, mean_eps: 0.100000
 5113523/6000000: episode: 6395, duration: 23.435s, episode steps: 1030, steps per second:  44, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.194 [0.000, 5.000],  loss: 0.013517, mae: 2.458788, mean_q: 2.960930, mean_eps: 0.100000
 5114765/6000000: episode: 6396, duration: 26.592s, episode steps: 1242, steps per second:  47, episode reward: 34.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.804 [0.000, 5.000],  loss: 0.013789, mae: 2.451092, mean_q: 2.950814, mean_eps: 0.100000
 5115925/6000000: episode: 6397, duration: 24.443s, episode steps: 1160, steps per second:  47, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.157 [0.000, 5.000],  loss: 0.013769, mae: 2.463958, mean_q: 2.967448, mean_eps: 0.100000
 5116755/6000000: episode: 6398, duration: 17.937s, episode steps: 830, steps per second:  46, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.012858, mae: 2.455969, mean_q: 2.957856, mean_eps: 0.100000
 5117913/6000000: episode: 6399, duration: 24.932s, episode steps: 1158, steps per second:  46, episode reward: 33.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.215 [0.000, 5.000],  loss: 0.013821, mae: 2.447889, mean_q: 2.947190, mean_eps: 0.100000
 5118416/6000000: episode: 6400, duration: 10.829s, episode steps: 503, steps per second:  46, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.922 [0.000, 5.000],  loss: 0.014821, mae: 2.468902, mean_q: 2.972374, mean_eps: 0.100000
 5119385/6000000: episode: 6401, duration: 20.898s, episode steps: 969, steps per second:  46, episode reward: 31.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.173 [0.000, 5.000],  loss: 0.012925, mae: 2.442225, mean_q: 2.942081, mean_eps: 0.100000
 5120088/6000000: episode: 6402, duration: 15.950s, episode steps: 703, steps per second:  44, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.014470, mae: 2.456408, mean_q: 2.958359, mean_eps: 0.100000
 5121089/6000000: episode: 6403, duration: 24.444s, episode steps: 1001, steps per second:  41, episode reward: 32.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.024 [0.000, 5.000],  loss: 0.014001, mae: 2.484165, mean_q: 2.992129, mean_eps: 0.100000
 5121911/6000000: episode: 6404, duration: 21.326s, episode steps: 822, steps per second:  39, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.200 [0.000, 5.000],  loss: 0.012162, mae: 2.456086, mean_q: 2.960607, mean_eps: 0.100000
 5122575/6000000: episode: 6405, duration: 15.861s, episode steps: 664, steps per second:  42, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.013040, mae: 2.469240, mean_q: 2.972708, mean_eps: 0.100000
 5123217/6000000: episode: 6406, duration: 15.948s, episode steps: 642, steps per second:  40, episode reward: 20.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.318 [0.000, 5.000],  loss: 0.013909, mae: 2.484019, mean_q: 2.991688, mean_eps: 0.100000
 5124392/6000000: episode: 6407, duration: 27.769s, episode steps: 1175, steps per second:  42, episode reward: 33.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.723 [0.000, 5.000],  loss: 0.014895, mae: 2.464994, mean_q: 2.969294, mean_eps: 0.100000
 5124796/6000000: episode: 6408, duration: 8.919s, episode steps: 404, steps per second:  45, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.109 [0.000, 5.000],  loss: 0.010883, mae: 2.466888, mean_q: 2.972725, mean_eps: 0.100000
 5125818/6000000: episode: 6409, duration: 22.638s, episode steps: 1022, steps per second:  45, episode reward: 30.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.286 [0.000, 5.000],  loss: 0.013623, mae: 2.493759, mean_q: 3.002309, mean_eps: 0.100000
 5126733/6000000: episode: 6410, duration: 21.731s, episode steps: 915, steps per second:  42, episode reward: 27.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.952 [0.000, 5.000],  loss: 0.015010, mae: 2.452553, mean_q: 2.953609, mean_eps: 0.100000
 5128015/6000000: episode: 6411, duration: 28.640s, episode steps: 1282, steps per second:  45, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.909 [0.000, 5.000],  loss: 0.014260, mae: 2.486573, mean_q: 2.994284, mean_eps: 0.100000
 5128617/6000000: episode: 6412, duration: 13.361s, episode steps: 602, steps per second:  45, episode reward: 16.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.957 [0.000, 5.000],  loss: 0.012411, mae: 2.456461, mean_q: 2.960463, mean_eps: 0.100000
 5129496/6000000: episode: 6413, duration: 19.132s, episode steps: 879, steps per second:  46, episode reward: 27.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.620 [0.000, 5.000],  loss: 0.013400, mae: 2.467507, mean_q: 2.974104, mean_eps: 0.100000
 5130371/6000000: episode: 6414, duration: 18.459s, episode steps: 875, steps per second:  47, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.673 [0.000, 5.000],  loss: 0.012129, mae: 2.469807, mean_q: 2.974741, mean_eps: 0.100000
 5131035/6000000: episode: 6415, duration: 13.742s, episode steps: 664, steps per second:  48, episode reward: 19.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.833 [0.000, 5.000],  loss: 0.014494, mae: 2.457480, mean_q: 2.958671, mean_eps: 0.100000
 5131829/6000000: episode: 6416, duration: 17.418s, episode steps: 794, steps per second:  46, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.705 [0.000, 5.000],  loss: 0.014046, mae: 2.435171, mean_q: 2.933107, mean_eps: 0.100000
 5132938/6000000: episode: 6417, duration: 24.742s, episode steps: 1109, steps per second:  45, episode reward: 33.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.813 [0.000, 5.000],  loss: 0.013668, mae: 2.453271, mean_q: 2.954880, mean_eps: 0.100000
 5134181/6000000: episode: 6418, duration: 28.125s, episode steps: 1243, steps per second:  44, episode reward: 35.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.013718, mae: 2.454678, mean_q: 2.955347, mean_eps: 0.100000
 5134559/6000000: episode: 6419, duration: 8.570s, episode steps: 378, steps per second:  44, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.968 [0.000, 5.000],  loss: 0.013018, mae: 2.434536, mean_q: 2.930660, mean_eps: 0.100000
 5135369/6000000: episode: 6420, duration: 17.965s, episode steps: 810, steps per second:  45, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.996 [0.000, 5.000],  loss: 0.013624, mae: 2.459361, mean_q: 2.963109, mean_eps: 0.100000
 5135911/6000000: episode: 6421, duration: 12.010s, episode steps: 542, steps per second:  45, episode reward: 15.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.705 [0.000, 5.000],  loss: 0.013832, mae: 2.447812, mean_q: 2.945621, mean_eps: 0.100000
 5136663/6000000: episode: 6422, duration: 17.680s, episode steps: 752, steps per second:  43, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.211 [0.000, 5.000],  loss: 0.013519, mae: 2.457056, mean_q: 2.957196, mean_eps: 0.100000
 5137528/6000000: episode: 6423, duration: 22.109s, episode steps: 865, steps per second:  39, episode reward: 26.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.946 [0.000, 5.000],  loss: 0.013401, mae: 2.451270, mean_q: 2.951011, mean_eps: 0.100000
 5138343/6000000: episode: 6424, duration: 19.228s, episode steps: 815, steps per second:  42, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.014699, mae: 2.434815, mean_q: 2.931802, mean_eps: 0.100000
 5139072/6000000: episode: 6425, duration: 16.396s, episode steps: 729, steps per second:  44, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.014743, mae: 2.449207, mean_q: 2.948008, mean_eps: 0.100000
 5140070/6000000: episode: 6426, duration: 22.682s, episode steps: 998, steps per second:  44, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.030 [0.000, 5.000],  loss: 0.013849, mae: 2.434955, mean_q: 2.931060, mean_eps: 0.100000
 5140749/6000000: episode: 6427, duration: 15.159s, episode steps: 679, steps per second:  45, episode reward: 23.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.013742, mae: 2.423800, mean_q: 2.918535, mean_eps: 0.100000
 5141713/6000000: episode: 6428, duration: 21.565s, episode steps: 964, steps per second:  45, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.670 [0.000, 5.000],  loss: 0.012249, mae: 2.429595, mean_q: 2.924551, mean_eps: 0.100000
 5142829/6000000: episode: 6429, duration: 27.451s, episode steps: 1116, steps per second:  41, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.588 [0.000, 5.000],  loss: 0.012341, mae: 2.421849, mean_q: 2.917053, mean_eps: 0.100000
 5143642/6000000: episode: 6430, duration: 19.182s, episode steps: 813, steps per second:  42, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.850 [0.000, 5.000],  loss: 0.014110, mae: 2.428330, mean_q: 2.923922, mean_eps: 0.100000
 5144865/6000000: episode: 6431, duration: 28.865s, episode steps: 1223, steps per second:  42, episode reward: 32.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.890 [0.000, 5.000],  loss: 0.014327, mae: 2.450771, mean_q: 2.951963, mean_eps: 0.100000
 5145830/6000000: episode: 6432, duration: 21.329s, episode steps: 965, steps per second:  45, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.188 [0.000, 5.000],  loss: 0.014399, mae: 2.425054, mean_q: 2.920130, mean_eps: 0.100000
 5147048/6000000: episode: 6433, duration: 26.181s, episode steps: 1218, steps per second:  47, episode reward: 34.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: 0.012768, mae: 2.456459, mean_q: 2.959419, mean_eps: 0.100000
 5148067/6000000: episode: 6434, duration: 22.081s, episode steps: 1019, steps per second:  46, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.012866, mae: 2.444751, mean_q: 2.944869, mean_eps: 0.100000
 5148992/6000000: episode: 6435, duration: 20.506s, episode steps: 925, steps per second:  45, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.022 [0.000, 5.000],  loss: 0.014069, mae: 2.453019, mean_q: 2.954008, mean_eps: 0.100000
 5149736/6000000: episode: 6436, duration: 16.014s, episode steps: 744, steps per second:  46, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.582 [0.000, 5.000],  loss: 0.013360, mae: 2.425365, mean_q: 2.921000, mean_eps: 0.100000
 5150570/6000000: episode: 6437, duration: 18.580s, episode steps: 834, steps per second:  45, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.969 [0.000, 5.000],  loss: 0.012657, mae: 2.458783, mean_q: 2.962668, mean_eps: 0.100000
 5151830/6000000: episode: 6438, duration: 30.448s, episode steps: 1260, steps per second:  41, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.884 [0.000, 5.000],  loss: 0.013878, mae: 2.448144, mean_q: 2.950336, mean_eps: 0.100000
 5152537/6000000: episode: 6439, duration: 19.589s, episode steps: 707, steps per second:  36, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.840 [0.000, 5.000],  loss: 0.013839, mae: 2.466841, mean_q: 2.973016, mean_eps: 0.100000
 5153215/6000000: episode: 6440, duration: 18.886s, episode steps: 678, steps per second:  36, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.068 [0.000, 5.000],  loss: 0.015147, mae: 2.453014, mean_q: 2.954143, mean_eps: 0.100000
 5154179/6000000: episode: 6441, duration: 21.632s, episode steps: 964, steps per second:  45, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.157 [0.000, 5.000],  loss: 0.014169, mae: 2.453618, mean_q: 2.954920, mean_eps: 0.100000
 5155366/6000000: episode: 6442, duration: 26.999s, episode steps: 1187, steps per second:  44, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.083 [0.000, 5.000],  loss: 0.013334, mae: 2.435323, mean_q: 2.934216, mean_eps: 0.100000
 5156180/6000000: episode: 6443, duration: 18.118s, episode steps: 814, steps per second:  45, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.079 [0.000, 5.000],  loss: 0.012626, mae: 2.461119, mean_q: 2.964029, mean_eps: 0.100000
 5157076/6000000: episode: 6444, duration: 19.499s, episode steps: 896, steps per second:  46, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.004 [0.000, 5.000],  loss: 0.013130, mae: 2.454916, mean_q: 2.957291, mean_eps: 0.100000
 5158674/6000000: episode: 6445, duration: 37.080s, episode steps: 1598, steps per second:  43, episode reward: 43.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.097 [0.000, 5.000],  loss: 0.014402, mae: 2.471798, mean_q: 2.977079, mean_eps: 0.100000
 5159439/6000000: episode: 6446, duration: 17.279s, episode steps: 765, steps per second:  44, episode reward: 23.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 0.014408, mae: 2.452817, mean_q: 2.954869, mean_eps: 0.100000
 5160637/6000000: episode: 6447, duration: 26.678s, episode steps: 1198, steps per second:  45, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.013152, mae: 2.449592, mean_q: 2.950611, mean_eps: 0.100000
 5161451/6000000: episode: 6448, duration: 17.562s, episode steps: 814, steps per second:  46, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.039 [0.000, 5.000],  loss: 0.014606, mae: 2.469813, mean_q: 2.973942, mean_eps: 0.100000
 5162302/6000000: episode: 6449, duration: 17.427s, episode steps: 851, steps per second:  49, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.014827, mae: 2.478995, mean_q: 2.984796, mean_eps: 0.100000
 5163580/6000000: episode: 6450, duration: 27.211s, episode steps: 1278, steps per second:  47, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.014816, mae: 2.462239, mean_q: 2.964495, mean_eps: 0.100000
 5164770/6000000: episode: 6451, duration: 25.493s, episode steps: 1190, steps per second:  47, episode reward: 32.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.011896, mae: 2.452628, mean_q: 2.953198, mean_eps: 0.100000
 5165398/6000000: episode: 6452, duration: 13.477s, episode steps: 628, steps per second:  47, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.306 [0.000, 5.000],  loss: 0.012401, mae: 2.457751, mean_q: 2.961368, mean_eps: 0.100000
 5166293/6000000: episode: 6453, duration: 19.964s, episode steps: 895, steps per second:  45, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.773 [0.000, 5.000],  loss: 0.014753, mae: 2.473437, mean_q: 2.979302, mean_eps: 0.100000
 5167325/6000000: episode: 6454, duration: 23.034s, episode steps: 1032, steps per second:  45, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.093 [0.000, 5.000],  loss: 0.013518, mae: 2.444208, mean_q: 2.944535, mean_eps: 0.100000
 5168529/6000000: episode: 6455, duration: 28.274s, episode steps: 1204, steps per second:  43, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: 0.014640, mae: 2.463268, mean_q: 2.966566, mean_eps: 0.100000
 5169797/6000000: episode: 6456, duration: 30.518s, episode steps: 1268, steps per second:  42, episode reward: 35.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.014574, mae: 2.445776, mean_q: 2.946735, mean_eps: 0.100000
 5170567/6000000: episode: 6457, duration: 17.400s, episode steps: 770, steps per second:  44, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.013022, mae: 2.497823, mean_q: 3.009414, mean_eps: 0.100000
 5171416/6000000: episode: 6458, duration: 19.518s, episode steps: 849, steps per second:  43, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.012402, mae: 2.501976, mean_q: 3.013968, mean_eps: 0.100000
 5172086/6000000: episode: 6459, duration: 15.166s, episode steps: 670, steps per second:  44, episode reward: 20.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.014676, mae: 2.488790, mean_q: 2.998355, mean_eps: 0.100000
 5173131/6000000: episode: 6460, duration: 24.015s, episode steps: 1045, steps per second:  44, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.005 [0.000, 5.000],  loss: 0.014642, mae: 2.497051, mean_q: 3.007646, mean_eps: 0.100000
 5174616/6000000: episode: 6461, duration: 34.923s, episode steps: 1485, steps per second:  43, episode reward: 33.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.220 [0.000, 5.000],  loss: 0.013457, mae: 2.499678, mean_q: 3.010831, mean_eps: 0.100000
 5175678/6000000: episode: 6462, duration: 24.122s, episode steps: 1062, steps per second:  44, episode reward: 31.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.014939, mae: 2.501740, mean_q: 3.012664, mean_eps: 0.100000
 5176512/6000000: episode: 6463, duration: 19.146s, episode steps: 834, steps per second:  44, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 0.014571, mae: 2.503875, mean_q: 3.017595, mean_eps: 0.100000
 5177617/6000000: episode: 6464, duration: 25.133s, episode steps: 1105, steps per second:  44, episode reward: 30.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.830 [0.000, 5.000],  loss: 0.013416, mae: 2.515564, mean_q: 3.028942, mean_eps: 0.100000
 5178812/6000000: episode: 6465, duration: 25.679s, episode steps: 1195, steps per second:  47, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.233 [0.000, 5.000],  loss: 0.014111, mae: 2.516433, mean_q: 3.029270, mean_eps: 0.100000
 5179905/6000000: episode: 6466, duration: 24.759s, episode steps: 1093, steps per second:  44, episode reward: 31.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.197 [0.000, 5.000],  loss: 0.013960, mae: 2.527875, mean_q: 3.043955, mean_eps: 0.100000
 5180972/6000000: episode: 6467, duration: 23.667s, episode steps: 1067, steps per second:  45, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.896 [0.000, 5.000],  loss: 0.012749, mae: 2.499003, mean_q: 3.010534, mean_eps: 0.100000
 5182231/6000000: episode: 6468, duration: 28.613s, episode steps: 1259, steps per second:  44, episode reward: 33.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.291 [0.000, 5.000],  loss: 0.014188, mae: 2.515127, mean_q: 3.028961, mean_eps: 0.100000
 5183083/6000000: episode: 6469, duration: 19.573s, episode steps: 852, steps per second:  44, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.048 [0.000, 5.000],  loss: 0.014518, mae: 2.525464, mean_q: 3.041839, mean_eps: 0.100000
 5184300/6000000: episode: 6470, duration: 28.881s, episode steps: 1217, steps per second:  42, episode reward: 33.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.191 [0.000, 5.000],  loss: 0.012971, mae: 2.514306, mean_q: 3.028468, mean_eps: 0.100000
 5185024/6000000: episode: 6471, duration: 17.430s, episode steps: 724, steps per second:  42, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.965 [0.000, 5.000],  loss: 0.013465, mae: 2.504858, mean_q: 3.015369, mean_eps: 0.100000
 5185902/6000000: episode: 6472, duration: 21.001s, episode steps: 878, steps per second:  42, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.014196, mae: 2.493128, mean_q: 3.003041, mean_eps: 0.100000
 5186753/6000000: episode: 6473, duration: 19.778s, episode steps: 851, steps per second:  43, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.972 [0.000, 5.000],  loss: 0.013612, mae: 2.485279, mean_q: 2.993509, mean_eps: 0.100000
 5187367/6000000: episode: 6474, duration: 13.854s, episode steps: 614, steps per second:  44, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.015 [0.000, 5.000],  loss: 0.013225, mae: 2.513694, mean_q: 3.028626, mean_eps: 0.100000
 5188013/6000000: episode: 6475, duration: 14.636s, episode steps: 646, steps per second:  44, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.014418, mae: 2.494106, mean_q: 3.003572, mean_eps: 0.100000
 5188742/6000000: episode: 6476, duration: 16.003s, episode steps: 729, steps per second:  46, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.687 [0.000, 5.000],  loss: 0.013864, mae: 2.515623, mean_q: 3.028312, mean_eps: 0.100000
 5189360/6000000: episode: 6477, duration: 13.907s, episode steps: 618, steps per second:  44, episode reward: 18.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.014249, mae: 2.480760, mean_q: 2.985653, mean_eps: 0.100000
 5190194/6000000: episode: 6478, duration: 20.147s, episode steps: 834, steps per second:  41, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: 0.014307, mae: 2.511359, mean_q: 3.023425, mean_eps: 0.100000
 5190984/6000000: episode: 6479, duration: 18.465s, episode steps: 790, steps per second:  43, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.062 [0.000, 5.000],  loss: 0.012339, mae: 2.501925, mean_q: 3.014377, mean_eps: 0.100000
 5191869/6000000: episode: 6480, duration: 20.418s, episode steps: 885, steps per second:  43, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.589 [0.000, 5.000],  loss: 0.014600, mae: 2.516112, mean_q: 3.030247, mean_eps: 0.100000
 5192661/6000000: episode: 6481, duration: 18.618s, episode steps: 792, steps per second:  43, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.074 [0.000, 5.000],  loss: 0.013951, mae: 2.531509, mean_q: 3.049311, mean_eps: 0.100000
 5193559/6000000: episode: 6482, duration: 19.279s, episode steps: 898, steps per second:  47, episode reward: 27.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.739 [0.000, 5.000],  loss: 0.013755, mae: 2.504248, mean_q: 3.017326, mean_eps: 0.100000
 5194379/6000000: episode: 6483, duration: 17.106s, episode steps: 820, steps per second:  48, episode reward: 27.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 1.938 [0.000, 5.000],  loss: 0.013473, mae: 2.509096, mean_q: 3.023536, mean_eps: 0.100000
 5195190/6000000: episode: 6484, duration: 17.200s, episode steps: 811, steps per second:  47, episode reward: 26.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 1.815 [0.000, 5.000],  loss: 0.012445, mae: 2.491650, mean_q: 3.003879, mean_eps: 0.100000
 5195994/6000000: episode: 6485, duration: 17.136s, episode steps: 804, steps per second:  47, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.634 [0.000, 5.000],  loss: 0.012242, mae: 2.460091, mean_q: 2.963573, mean_eps: 0.100000
 5197332/6000000: episode: 6486, duration: 27.573s, episode steps: 1338, steps per second:  49, episode reward: 34.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.684 [0.000, 5.000],  loss: 0.014467, mae: 2.472703, mean_q: 2.978689, mean_eps: 0.100000
 5198230/6000000: episode: 6487, duration: 19.268s, episode steps: 898, steps per second:  47, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.915 [0.000, 5.000],  loss: 0.014774, mae: 2.492382, mean_q: 3.003495, mean_eps: 0.100000
 5199048/6000000: episode: 6488, duration: 18.204s, episode steps: 818, steps per second:  45, episode reward: 24.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.923 [0.000, 5.000],  loss: 0.013284, mae: 2.464620, mean_q: 2.969911, mean_eps: 0.100000
 5199735/6000000: episode: 6489, duration: 15.670s, episode steps: 687, steps per second:  44, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.654 [0.000, 5.000],  loss: 0.013977, mae: 2.493853, mean_q: 3.002729, mean_eps: 0.100000
 5200703/6000000: episode: 6490, duration: 23.550s, episode steps: 968, steps per second:  41, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.749 [0.000, 5.000],  loss: 0.014007, mae: 2.493971, mean_q: 3.004423, mean_eps: 0.100000
 5201417/6000000: episode: 6491, duration: 16.802s, episode steps: 714, steps per second:  42, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.013353, mae: 2.483524, mean_q: 2.991865, mean_eps: 0.100000
 5202261/6000000: episode: 6492, duration: 19.466s, episode steps: 844, steps per second:  43, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.319 [0.000, 5.000],  loss: 0.014180, mae: 2.480036, mean_q: 2.987261, mean_eps: 0.100000
 5202957/6000000: episode: 6493, duration: 16.090s, episode steps: 696, steps per second:  43, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.931 [0.000, 5.000],  loss: 0.016477, mae: 2.473043, mean_q: 2.979560, mean_eps: 0.100000
 5204299/6000000: episode: 6494, duration: 29.919s, episode steps: 1342, steps per second:  45, episode reward: 34.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.892 [0.000, 5.000],  loss: 0.014314, mae: 2.492271, mean_q: 3.002075, mean_eps: 0.100000
 5205704/6000000: episode: 6495, duration: 33.533s, episode steps: 1405, steps per second:  42, episode reward: 32.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.202 [0.000, 5.000],  loss: 0.014352, mae: 2.450293, mean_q: 2.952614, mean_eps: 0.100000
 5206560/6000000: episode: 6496, duration: 20.671s, episode steps: 856, steps per second:  41, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.757 [0.000, 5.000],  loss: 0.012593, mae: 2.465286, mean_q: 2.970583, mean_eps: 0.100000
 5207723/6000000: episode: 6497, duration: 25.098s, episode steps: 1163, steps per second:  46, episode reward: 32.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.805 [0.000, 5.000],  loss: 0.013207, mae: 2.430395, mean_q: 2.928788, mean_eps: 0.100000
 5208655/6000000: episode: 6498, duration: 20.779s, episode steps: 932, steps per second:  45, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.461 [0.000, 5.000],  loss: 0.013054, mae: 2.445966, mean_q: 2.948264, mean_eps: 0.100000
 5209535/6000000: episode: 6499, duration: 18.903s, episode steps: 880, steps per second:  47, episode reward: 26.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.985 [0.000, 5.000],  loss: 0.014294, mae: 2.461633, mean_q: 2.965304, mean_eps: 0.100000
 5210448/6000000: episode: 6500, duration: 18.503s, episode steps: 913, steps per second:  49, episode reward: 28.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.676 [0.000, 5.000],  loss: 0.013640, mae: 2.443536, mean_q: 2.944160, mean_eps: 0.100000
 5211477/6000000: episode: 6501, duration: 22.754s, episode steps: 1029, steps per second:  45, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.771 [0.000, 5.000],  loss: 0.014156, mae: 2.451963, mean_q: 2.952641, mean_eps: 0.100000
 5212791/6000000: episode: 6502, duration: 28.647s, episode steps: 1314, steps per second:  46, episode reward: 34.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.726 [0.000, 5.000],  loss: 0.012807, mae: 2.445546, mean_q: 2.947316, mean_eps: 0.100000
 5213879/6000000: episode: 6503, duration: 23.381s, episode steps: 1088, steps per second:  47, episode reward: 32.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.653 [0.000, 5.000],  loss: 0.015068, mae: 2.452550, mean_q: 2.953894, mean_eps: 0.100000
 5215139/6000000: episode: 6504, duration: 27.935s, episode steps: 1260, steps per second:  45, episode reward: 35.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.694 [0.000, 5.000],  loss: 0.014024, mae: 2.439326, mean_q: 2.939508, mean_eps: 0.100000
 5216608/6000000: episode: 6505, duration: 34.248s, episode steps: 1469, steps per second:  43, episode reward: 31.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.739 [0.000, 5.000],  loss: 0.014480, mae: 2.480012, mean_q: 2.987363, mean_eps: 0.100000
 5217619/6000000: episode: 6506, duration: 24.297s, episode steps: 1011, steps per second:  42, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.022 [0.000, 5.000],  loss: 0.013404, mae: 2.457384, mean_q: 2.961128, mean_eps: 0.100000
 5218880/6000000: episode: 6507, duration: 28.737s, episode steps: 1261, steps per second:  44, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.856 [0.000, 5.000],  loss: 0.015179, mae: 2.455019, mean_q: 2.958466, mean_eps: 0.100000
 5219615/6000000: episode: 6508, duration: 15.952s, episode steps: 735, steps per second:  46, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.833 [0.000, 5.000],  loss: 0.013902, mae: 2.463251, mean_q: 2.969023, mean_eps: 0.100000
 5220416/6000000: episode: 6509, duration: 17.524s, episode steps: 801, steps per second:  46, episode reward: 24.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.207 [0.000, 5.000],  loss: 0.014057, mae: 2.468446, mean_q: 2.972791, mean_eps: 0.100000
 5221247/6000000: episode: 6510, duration: 17.948s, episode steps: 831, steps per second:  46, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.153 [0.000, 5.000],  loss: 0.013805, mae: 2.457704, mean_q: 2.959011, mean_eps: 0.100000
 5221994/6000000: episode: 6511, duration: 17.519s, episode steps: 747, steps per second:  43, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.037 [0.000, 5.000],  loss: 0.013439, mae: 2.463715, mean_q: 2.967141, mean_eps: 0.100000
 5223236/6000000: episode: 6512, duration: 28.119s, episode steps: 1242, steps per second:  44, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.167 [0.000, 5.000],  loss: 0.013862, mae: 2.480820, mean_q: 2.987127, mean_eps: 0.100000
 5224118/6000000: episode: 6513, duration: 20.099s, episode steps: 882, steps per second:  44, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.013966, mae: 2.474881, mean_q: 2.980554, mean_eps: 0.100000
 5225571/6000000: episode: 6514, duration: 31.815s, episode steps: 1453, steps per second:  46, episode reward: 41.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.693 [0.000, 5.000],  loss: 0.014830, mae: 2.492034, mean_q: 3.002145, mean_eps: 0.100000
 5226533/6000000: episode: 6515, duration: 19.920s, episode steps: 962, steps per second:  48, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.178 [0.000, 5.000],  loss: 0.014197, mae: 2.482914, mean_q: 2.992177, mean_eps: 0.100000
 5227499/6000000: episode: 6516, duration: 21.536s, episode steps: 966, steps per second:  45, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.059 [0.000, 5.000],  loss: 0.014054, mae: 2.500979, mean_q: 3.013387, mean_eps: 0.100000
 5228564/6000000: episode: 6517, duration: 22.937s, episode steps: 1065, steps per second:  46, episode reward: 31.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 0.014273, mae: 2.485807, mean_q: 2.994549, mean_eps: 0.100000
 5229675/6000000: episode: 6518, duration: 23.426s, episode steps: 1111, steps per second:  47, episode reward: 33.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.014421, mae: 2.471868, mean_q: 2.976915, mean_eps: 0.100000
 5230556/6000000: episode: 6519, duration: 19.704s, episode steps: 881, steps per second:  45, episode reward: 26.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.042 [0.000, 5.000],  loss: 0.014143, mae: 2.484825, mean_q: 2.994520, mean_eps: 0.100000
 5231745/6000000: episode: 6520, duration: 28.519s, episode steps: 1189, steps per second:  42, episode reward: 34.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.013514, mae: 2.478173, mean_q: 2.985951, mean_eps: 0.100000
 5232316/6000000: episode: 6521, duration: 14.110s, episode steps: 571, steps per second:  40, episode reward: 16.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.014499, mae: 2.489551, mean_q: 2.999475, mean_eps: 0.100000
 5233733/6000000: episode: 6522, duration: 33.344s, episode steps: 1417, steps per second:  42, episode reward: 34.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.013719, mae: 2.483309, mean_q: 2.991854, mean_eps: 0.100000
 5234585/6000000: episode: 6523, duration: 19.193s, episode steps: 852, steps per second:  44, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: 0.014245, mae: 2.464236, mean_q: 2.968295, mean_eps: 0.100000
 5235837/6000000: episode: 6524, duration: 27.836s, episode steps: 1252, steps per second:  45, episode reward: 34.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.013924, mae: 2.479626, mean_q: 2.987365, mean_eps: 0.100000
 5236734/6000000: episode: 6525, duration: 19.591s, episode steps: 897, steps per second:  46, episode reward: 27.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.013746, mae: 2.473484, mean_q: 2.980047, mean_eps: 0.100000
 5237745/6000000: episode: 6526, duration: 22.316s, episode steps: 1011, steps per second:  45, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.013912, mae: 2.488790, mean_q: 2.998725, mean_eps: 0.100000
 5238738/6000000: episode: 6527, duration: 21.595s, episode steps: 993, steps per second:  46, episode reward: 30.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.705 [0.000, 5.000],  loss: 0.014125, mae: 2.500621, mean_q: 3.013722, mean_eps: 0.100000
 5239922/6000000: episode: 6528, duration: 25.450s, episode steps: 1184, steps per second:  47, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.033 [0.000, 5.000],  loss: 0.013854, mae: 2.476675, mean_q: 2.982386, mean_eps: 0.100000
 5241007/6000000: episode: 6529, duration: 23.982s, episode steps: 1085, steps per second:  45, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.220 [0.000, 5.000],  loss: 0.014674, mae: 2.478099, mean_q: 2.985479, mean_eps: 0.100000
 5241942/6000000: episode: 6530, duration: 20.458s, episode steps: 935, steps per second:  46, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.727 [0.000, 5.000],  loss: 0.012629, mae: 2.474699, mean_q: 2.981815, mean_eps: 0.100000
 5242725/6000000: episode: 6531, duration: 15.923s, episode steps: 783, steps per second:  49, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.914 [0.000, 5.000],  loss: 0.014178, mae: 2.485259, mean_q: 2.993483, mean_eps: 0.100000
 5243798/6000000: episode: 6532, duration: 23.797s, episode steps: 1073, steps per second:  45, episode reward: 31.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.156 [0.000, 5.000],  loss: 0.013883, mae: 2.481964, mean_q: 2.991055, mean_eps: 0.100000
 5244659/6000000: episode: 6533, duration: 18.440s, episode steps: 861, steps per second:  47, episode reward: 27.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.978 [0.000, 5.000],  loss: 0.014101, mae: 2.477269, mean_q: 2.984430, mean_eps: 0.100000
 5245645/6000000: episode: 6534, duration: 20.927s, episode steps: 986, steps per second:  47, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.504 [0.000, 5.000],  loss: 0.014132, mae: 2.499563, mean_q: 3.011043, mean_eps: 0.100000
 5246677/6000000: episode: 6535, duration: 22.197s, episode steps: 1032, steps per second:  46, episode reward: 30.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.033 [0.000, 5.000],  loss: 0.012817, mae: 2.504825, mean_q: 3.018035, mean_eps: 0.100000
 5247706/6000000: episode: 6536, duration: 23.448s, episode steps: 1029, steps per second:  44, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.771 [0.000, 5.000],  loss: 0.013238, mae: 2.498254, mean_q: 3.012333, mean_eps: 0.100000
 5248573/6000000: episode: 6537, duration: 21.311s, episode steps: 867, steps per second:  41, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.878 [0.000, 5.000],  loss: 0.014437, mae: 2.489107, mean_q: 2.999984, mean_eps: 0.100000
 5249718/6000000: episode: 6538, duration: 27.973s, episode steps: 1145, steps per second:  41, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.013857, mae: 2.504104, mean_q: 3.016284, mean_eps: 0.100000
 5250708/6000000: episode: 6539, duration: 23.371s, episode steps: 990, steps per second:  42, episode reward: 31.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.228 [0.000, 5.000],  loss: 0.013135, mae: 2.492912, mean_q: 3.003830, mean_eps: 0.100000
 5251813/6000000: episode: 6540, duration: 24.104s, episode steps: 1105, steps per second:  46, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.095 [0.000, 5.000],  loss: 0.014502, mae: 2.517061, mean_q: 3.032956, mean_eps: 0.100000
 5252377/6000000: episode: 6541, duration: 12.700s, episode steps: 564, steps per second:  44, episode reward: 17.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.133 [0.000, 5.000],  loss: 0.014690, mae: 2.512634, mean_q: 3.026584, mean_eps: 0.100000
 5253452/6000000: episode: 6542, duration: 23.157s, episode steps: 1075, steps per second:  46, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.958 [0.000, 5.000],  loss: 0.013962, mae: 2.527954, mean_q: 3.044766, mean_eps: 0.100000
 5254619/6000000: episode: 6543, duration: 26.757s, episode steps: 1167, steps per second:  44, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.968 [0.000, 5.000],  loss: 0.013867, mae: 2.520749, mean_q: 3.035672, mean_eps: 0.100000
 5255419/6000000: episode: 6544, duration: 17.793s, episode steps: 800, steps per second:  45, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.180 [0.000, 5.000],  loss: 0.013722, mae: 2.497141, mean_q: 3.008432, mean_eps: 0.100000
 5256929/6000000: episode: 6545, duration: 33.469s, episode steps: 1510, steps per second:  45, episode reward: 34.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.268 [0.000, 5.000],  loss: 0.013683, mae: 2.518772, mean_q: 3.033879, mean_eps: 0.100000
 5257698/6000000: episode: 6546, duration: 16.707s, episode steps: 769, steps per second:  46, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.013156, mae: 2.519993, mean_q: 3.036651, mean_eps: 0.100000
 5258454/6000000: episode: 6547, duration: 15.467s, episode steps: 756, steps per second:  49, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.012784, mae: 2.501938, mean_q: 3.012862, mean_eps: 0.100000
 5258976/6000000: episode: 6548, duration: 11.190s, episode steps: 522, steps per second:  47, episode reward: 15.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.014041, mae: 2.525189, mean_q: 3.041634, mean_eps: 0.100000
 5259742/6000000: episode: 6549, duration: 16.535s, episode steps: 766, steps per second:  46, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.014310, mae: 2.521973, mean_q: 3.036965, mean_eps: 0.100000
 5260696/6000000: episode: 6550, duration: 19.993s, episode steps: 954, steps per second:  48, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.012692, mae: 2.514797, mean_q: 3.031117, mean_eps: 0.100000
 5261435/6000000: episode: 6551, duration: 15.564s, episode steps: 739, steps per second:  47, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.053 [0.000, 5.000],  loss: 0.014294, mae: 2.502772, mean_q: 3.016115, mean_eps: 0.100000
 5262632/6000000: episode: 6552, duration: 26.739s, episode steps: 1197, steps per second:  45, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.077 [0.000, 5.000],  loss: 0.012629, mae: 2.485183, mean_q: 2.994104, mean_eps: 0.100000
 5263810/6000000: episode: 6553, duration: 26.150s, episode steps: 1178, steps per second:  45, episode reward: 32.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.729 [0.000, 5.000],  loss: 0.012929, mae: 2.499730, mean_q: 3.011627, mean_eps: 0.100000
 5264727/6000000: episode: 6554, duration: 20.920s, episode steps: 917, steps per second:  44, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.552 [0.000, 5.000],  loss: 0.012460, mae: 2.494412, mean_q: 3.006364, mean_eps: 0.100000
 5265432/6000000: episode: 6555, duration: 17.150s, episode steps: 705, steps per second:  41, episode reward: 20.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.641 [0.000, 5.000],  loss: 0.014570, mae: 2.501963, mean_q: 3.014838, mean_eps: 0.100000
 5266241/6000000: episode: 6556, duration: 18.629s, episode steps: 809, steps per second:  43, episode reward: 24.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.643 [0.000, 5.000],  loss: 0.013997, mae: 2.507183, mean_q: 3.020833, mean_eps: 0.100000
 5267203/6000000: episode: 6557, duration: 21.182s, episode steps: 962, steps per second:  45, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.232 [0.000, 5.000],  loss: 0.013024, mae: 2.504568, mean_q: 3.017510, mean_eps: 0.100000
 5267923/6000000: episode: 6558, duration: 16.088s, episode steps: 720, steps per second:  45, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.013824, mae: 2.488215, mean_q: 2.998461, mean_eps: 0.100000
 5268793/6000000: episode: 6559, duration: 18.599s, episode steps: 870, steps per second:  47, episode reward: 28.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 1.479 [0.000, 5.000],  loss: 0.012045, mae: 2.521802, mean_q: 3.039841, mean_eps: 0.100000
 5270020/6000000: episode: 6560, duration: 26.217s, episode steps: 1227, steps per second:  47, episode reward: 33.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.014593, mae: 2.497024, mean_q: 3.008014, mean_eps: 0.100000
 5270598/6000000: episode: 6561, duration: 13.286s, episode steps: 578, steps per second:  44, episode reward: 16.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.708 [0.000, 5.000],  loss: 0.013944, mae: 2.521908, mean_q: 3.037951, mean_eps: 0.100000
 5271959/6000000: episode: 6562, duration: 30.695s, episode steps: 1361, steps per second:  44, episode reward: 35.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.820 [0.000, 5.000],  loss: 0.015104, mae: 2.506226, mean_q: 3.019331, mean_eps: 0.100000
 5273319/6000000: episode: 6563, duration: 31.380s, episode steps: 1360, steps per second:  43, episode reward: 30.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.599 [0.000, 5.000],  loss: 0.013977, mae: 2.516527, mean_q: 3.032882, mean_eps: 0.100000
 5274431/6000000: episode: 6564, duration: 24.152s, episode steps: 1112, steps per second:  46, episode reward: 34.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.644 [0.000, 5.000],  loss: 0.013477, mae: 2.514172, mean_q: 3.029338, mean_eps: 0.100000
 5275373/6000000: episode: 6565, duration: 19.674s, episode steps: 942, steps per second:  48, episode reward: 28.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.851 [0.000, 5.000],  loss: 0.013373, mae: 2.493057, mean_q: 3.003724, mean_eps: 0.100000
 5276727/6000000: episode: 6566, duration: 29.443s, episode steps: 1354, steps per second:  46, episode reward: 32.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.031 [0.000, 5.000],  loss: 0.013377, mae: 2.511594, mean_q: 3.026956, mean_eps: 0.100000
 5277465/6000000: episode: 6567, duration: 15.508s, episode steps: 738, steps per second:  48, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.224 [0.000, 5.000],  loss: 0.012871, mae: 2.485744, mean_q: 2.995505, mean_eps: 0.100000
 5278567/6000000: episode: 6568, duration: 24.266s, episode steps: 1102, steps per second:  45, episode reward: 30.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.269 [0.000, 5.000],  loss: 0.013844, mae: 2.507220, mean_q: 3.021638, mean_eps: 0.100000
 5279569/6000000: episode: 6569, duration: 22.056s, episode steps: 1002, steps per second:  45, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.014574, mae: 2.491817, mean_q: 3.002999, mean_eps: 0.100000
 5280516/6000000: episode: 6570, duration: 22.197s, episode steps: 947, steps per second:  43, episode reward: 28.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.658 [0.000, 5.000],  loss: 0.013773, mae: 2.506502, mean_q: 3.020641, mean_eps: 0.100000
 5281410/6000000: episode: 6571, duration: 21.877s, episode steps: 894, steps per second:  41, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.124 [0.000, 5.000],  loss: 0.013442, mae: 2.497055, mean_q: 3.008582, mean_eps: 0.100000
 5282433/6000000: episode: 6572, duration: 24.471s, episode steps: 1023, steps per second:  42, episode reward: 30.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.012366, mae: 2.490732, mean_q: 3.001131, mean_eps: 0.100000
 5283805/6000000: episode: 6573, duration: 30.925s, episode steps: 1372, steps per second:  44, episode reward: 36.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.013338, mae: 2.489275, mean_q: 2.999958, mean_eps: 0.100000
 5284789/6000000: episode: 6574, duration: 21.921s, episode steps: 984, steps per second:  45, episode reward: 30.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.129 [0.000, 5.000],  loss: 0.013989, mae: 2.483760, mean_q: 2.992993, mean_eps: 0.100000
 5286108/6000000: episode: 6575, duration: 30.083s, episode steps: 1319, steps per second:  44, episode reward: 31.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.832 [0.000, 5.000],  loss: 0.012836, mae: 2.504548, mean_q: 3.018314, mean_eps: 0.100000
 5287262/6000000: episode: 6576, duration: 27.707s, episode steps: 1154, steps per second:  42, episode reward: 32.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.967 [0.000, 5.000],  loss: 0.013306, mae: 2.497340, mean_q: 3.008888, mean_eps: 0.100000
 5288635/6000000: episode: 6577, duration: 30.634s, episode steps: 1373, steps per second:  45, episode reward: 31.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.224 [0.000, 5.000],  loss: 0.013912, mae: 2.510758, mean_q: 3.024943, mean_eps: 0.100000
 5289189/6000000: episode: 6578, duration: 12.509s, episode steps: 554, steps per second:  44, episode reward: 15.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.191 [0.000, 5.000],  loss: 0.013816, mae: 2.518912, mean_q: 3.034577, mean_eps: 0.100000
 5289994/6000000: episode: 6579, duration: 17.280s, episode steps: 805, steps per second:  47, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.871 [0.000, 5.000],  loss: 0.013129, mae: 2.492240, mean_q: 3.003919, mean_eps: 0.100000
 5290716/6000000: episode: 6580, duration: 14.884s, episode steps: 722, steps per second:  49, episode reward: 20.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.119 [0.000, 5.000],  loss: 0.014269, mae: 2.495165, mean_q: 3.006652, mean_eps: 0.100000
 5291927/6000000: episode: 6581, duration: 26.752s, episode steps: 1211, steps per second:  45, episode reward: 34.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.909 [0.000, 5.000],  loss: 0.013834, mae: 2.472504, mean_q: 2.978962, mean_eps: 0.100000
 5292817/6000000: episode: 6582, duration: 19.669s, episode steps: 890, steps per second:  45, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.520 [0.000, 5.000],  loss: 0.014739, mae: 2.481898, mean_q: 2.991813, mean_eps: 0.100000
 5293706/6000000: episode: 6583, duration: 19.356s, episode steps: 889, steps per second:  46, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.489 [0.000, 5.000],  loss: 0.014220, mae: 2.468384, mean_q: 2.974520, mean_eps: 0.100000
 5294810/6000000: episode: 6584, duration: 23.497s, episode steps: 1104, steps per second:  47, episode reward: 32.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.909 [0.000, 5.000],  loss: 0.013958, mae: 2.492176, mean_q: 3.002223, mean_eps: 0.100000
 5295535/6000000: episode: 6585, duration: 15.761s, episode steps: 725, steps per second:  46, episode reward: 20.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.123 [0.000, 5.000],  loss: 0.013639, mae: 2.503744, mean_q: 3.016684, mean_eps: 0.100000
 5296603/6000000: episode: 6586, duration: 23.594s, episode steps: 1068, steps per second:  45, episode reward: 35.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 1.643 [0.000, 5.000],  loss: 0.012398, mae: 2.471631, mean_q: 2.979975, mean_eps: 0.100000
 5297457/6000000: episode: 6587, duration: 21.375s, episode steps: 854, steps per second:  40, episode reward: 27.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.067 [0.000, 5.000],  loss: 0.012126, mae: 2.495446, mean_q: 3.007945, mean_eps: 0.100000
 5298502/6000000: episode: 6588, duration: 24.025s, episode steps: 1045, steps per second:  43, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.013 [0.000, 5.000],  loss: 0.014282, mae: 2.498744, mean_q: 3.011334, mean_eps: 0.100000
 5299420/6000000: episode: 6589, duration: 20.699s, episode steps: 918, steps per second:  44, episode reward: 31.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 1.761 [0.000, 5.000],  loss: 0.013785, mae: 2.498152, mean_q: 3.010436, mean_eps: 0.100000
 5300199/6000000: episode: 6590, duration: 17.824s, episode steps: 779, steps per second:  44, episode reward: 25.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.137 [0.000, 5.000],  loss: 0.014044, mae: 2.500745, mean_q: 3.013997, mean_eps: 0.100000
 5301073/6000000: episode: 6591, duration: 19.328s, episode steps: 874, steps per second:  45, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.604 [0.000, 5.000],  loss: 0.013271, mae: 2.492845, mean_q: 3.005067, mean_eps: 0.100000
 5302088/6000000: episode: 6592, duration: 21.856s, episode steps: 1015, steps per second:  46, episode reward: 34.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.013321, mae: 2.496049, mean_q: 3.006806, mean_eps: 0.100000
 5303347/6000000: episode: 6593, duration: 28.731s, episode steps: 1259, steps per second:  44, episode reward: 30.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.013844, mae: 2.477029, mean_q: 2.984144, mean_eps: 0.100000
 5304808/6000000: episode: 6594, duration: 32.211s, episode steps: 1461, steps per second:  45, episode reward: 33.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.013266, mae: 2.478738, mean_q: 2.987430, mean_eps: 0.100000
 5305533/6000000: episode: 6595, duration: 16.229s, episode steps: 725, steps per second:  45, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.255 [0.000, 5.000],  loss: 0.013667, mae: 2.483270, mean_q: 2.992421, mean_eps: 0.100000
 5306804/6000000: episode: 6596, duration: 27.027s, episode steps: 1271, steps per second:  47, episode reward: 34.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.036 [0.000, 5.000],  loss: 0.015237, mae: 2.484864, mean_q: 2.993640, mean_eps: 0.100000
 5308089/6000000: episode: 6597, duration: 27.022s, episode steps: 1285, steps per second:  48, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.925 [0.000, 5.000],  loss: 0.013531, mae: 2.504937, mean_q: 3.018607, mean_eps: 0.100000
 5309271/6000000: episode: 6598, duration: 25.486s, episode steps: 1182, steps per second:  46, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.039 [0.000, 5.000],  loss: 0.013209, mae: 2.488645, mean_q: 2.998967, mean_eps: 0.100000
 5310111/6000000: episode: 6599, duration: 18.024s, episode steps: 840, steps per second:  47, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.949 [0.000, 5.000],  loss: 0.014139, mae: 2.483614, mean_q: 2.993699, mean_eps: 0.100000
 5311035/6000000: episode: 6600, duration: 19.919s, episode steps: 924, steps per second:  46, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.083 [0.000, 5.000],  loss: 0.013864, mae: 2.494713, mean_q: 3.006299, mean_eps: 0.100000
 5312033/6000000: episode: 6601, duration: 22.142s, episode steps: 998, steps per second:  45, episode reward: 29.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.882 [0.000, 5.000],  loss: 0.013135, mae: 2.485381, mean_q: 2.997016, mean_eps: 0.100000
 5312586/6000000: episode: 6602, duration: 12.583s, episode steps: 553, steps per second:  44, episode reward: 15.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.159 [0.000, 5.000],  loss: 0.013793, mae: 2.475941, mean_q: 2.983950, mean_eps: 0.100000
 5313322/6000000: episode: 6603, duration: 17.677s, episode steps: 736, steps per second:  42, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.863 [0.000, 5.000],  loss: 0.012676, mae: 2.480542, mean_q: 2.990268, mean_eps: 0.100000
 5314198/6000000: episode: 6604, duration: 20.941s, episode steps: 876, steps per second:  42, episode reward: 26.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.842 [0.000, 5.000],  loss: 0.013569, mae: 2.482613, mean_q: 2.991928, mean_eps: 0.100000
 5315072/6000000: episode: 6605, duration: 19.046s, episode steps: 874, steps per second:  46, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.212 [0.000, 5.000],  loss: 0.013719, mae: 2.501980, mean_q: 3.015396, mean_eps: 0.100000
 5315943/6000000: episode: 6606, duration: 20.096s, episode steps: 871, steps per second:  43, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.729 [0.000, 5.000],  loss: 0.013533, mae: 2.501896, mean_q: 3.015444, mean_eps: 0.100000
 5316594/6000000: episode: 6607, duration: 15.517s, episode steps: 651, steps per second:  42, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.618 [0.000, 5.000],  loss: 0.013266, mae: 2.528241, mean_q: 3.048869, mean_eps: 0.100000
 5317804/6000000: episode: 6608, duration: 26.538s, episode steps: 1210, steps per second:  46, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.890 [0.000, 5.000],  loss: 0.014373, mae: 2.530448, mean_q: 3.049204, mean_eps: 0.100000
 5318937/6000000: episode: 6609, duration: 27.085s, episode steps: 1133, steps per second:  42, episode reward: 33.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.085 [0.000, 5.000],  loss: 0.014297, mae: 2.524525, mean_q: 3.042105, mean_eps: 0.100000
 5319815/6000000: episode: 6610, duration: 19.925s, episode steps: 878, steps per second:  44, episode reward: 27.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.013217, mae: 2.516883, mean_q: 3.032760, mean_eps: 0.100000
 5320514/6000000: episode: 6611, duration: 15.708s, episode steps: 699, steps per second:  44, episode reward: 21.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.266 [0.000, 5.000],  loss: 0.014163, mae: 2.522400, mean_q: 3.039978, mean_eps: 0.100000
 5321641/6000000: episode: 6612, duration: 25.745s, episode steps: 1127, steps per second:  44, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.012849, mae: 2.518776, mean_q: 3.035884, mean_eps: 0.100000
 5323077/6000000: episode: 6613, duration: 30.715s, episode steps: 1436, steps per second:  47, episode reward: 35.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.014004, mae: 2.511150, mean_q: 3.026247, mean_eps: 0.100000
 5323801/6000000: episode: 6614, duration: 16.022s, episode steps: 724, steps per second:  45, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.896 [0.000, 5.000],  loss: 0.015393, mae: 2.531212, mean_q: 3.050573, mean_eps: 0.100000
 5324587/6000000: episode: 6615, duration: 17.035s, episode steps: 786, steps per second:  46, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.014075, mae: 2.509427, mean_q: 3.023866, mean_eps: 0.100000
 5325251/6000000: episode: 6616, duration: 14.413s, episode steps: 664, steps per second:  46, episode reward: 24.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.767 [0.000, 5.000],  loss: 0.012975, mae: 2.493314, mean_q: 3.006106, mean_eps: 0.100000
 5325825/6000000: episode: 6617, duration: 12.376s, episode steps: 574, steps per second:  46, episode reward: 17.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.338 [0.000, 5.000],  loss: 0.013150, mae: 2.506612, mean_q: 3.021990, mean_eps: 0.100000
 5326318/6000000: episode: 6618, duration: 10.979s, episode steps: 493, steps per second:  45, episode reward: 16.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 1.844 [0.000, 5.000],  loss: 0.012578, mae: 2.470885, mean_q: 2.980483, mean_eps: 0.100000
 5327491/6000000: episode: 6619, duration: 27.213s, episode steps: 1173, steps per second:  43, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.969 [0.000, 5.000],  loss: 0.012664, mae: 2.486653, mean_q: 2.996681, mean_eps: 0.100000
 5328445/6000000: episode: 6620, duration: 21.544s, episode steps: 954, steps per second:  44, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.996 [0.000, 5.000],  loss: 0.013608, mae: 2.507546, mean_q: 3.021940, mean_eps: 0.100000
 5329202/6000000: episode: 6621, duration: 18.134s, episode steps: 757, steps per second:  42, episode reward: 23.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.843 [0.000, 5.000],  loss: 0.014433, mae: 2.523997, mean_q: 3.040388, mean_eps: 0.100000
 5329904/6000000: episode: 6622, duration: 17.767s, episode steps: 702, steps per second:  40, episode reward: 20.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.015202, mae: 2.503350, mean_q: 3.016393, mean_eps: 0.100000
 5330554/6000000: episode: 6623, duration: 14.595s, episode steps: 650, steps per second:  45, episode reward: 19.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.260 [0.000, 5.000],  loss: 0.013442, mae: 2.469102, mean_q: 2.975328, mean_eps: 0.100000
 5331636/6000000: episode: 6624, duration: 23.798s, episode steps: 1082, steps per second:  45, episode reward: 31.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.013574, mae: 2.496477, mean_q: 3.008766, mean_eps: 0.100000
 5332530/6000000: episode: 6625, duration: 20.180s, episode steps: 894, steps per second:  44, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.791 [0.000, 5.000],  loss: 0.014010, mae: 2.492121, mean_q: 3.001705, mean_eps: 0.100000
 5333172/6000000: episode: 6626, duration: 14.169s, episode steps: 642, steps per second:  45, episode reward: 19.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.850 [0.000, 5.000],  loss: 0.015075, mae: 2.486575, mean_q: 2.995441, mean_eps: 0.100000
 5334227/6000000: episode: 6627, duration: 23.556s, episode steps: 1055, steps per second:  45, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 0.014857, mae: 2.497675, mean_q: 3.009289, mean_eps: 0.100000
 5335363/6000000: episode: 6628, duration: 28.416s, episode steps: 1136, steps per second:  40, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.169 [0.000, 5.000],  loss: 0.012063, mae: 2.495371, mean_q: 3.008953, mean_eps: 0.100000
 5336775/6000000: episode: 6629, duration: 31.169s, episode steps: 1412, steps per second:  45, episode reward: 34.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.014019, mae: 2.487452, mean_q: 2.997699, mean_eps: 0.100000
 5337992/6000000: episode: 6630, duration: 27.341s, episode steps: 1217, steps per second:  45, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.120 [0.000, 5.000],  loss: 0.014239, mae: 2.479192, mean_q: 2.987373, mean_eps: 0.100000
 5338903/6000000: episode: 6631, duration: 18.841s, episode steps: 911, steps per second:  48, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.986 [0.000, 5.000],  loss: 0.014398, mae: 2.477127, mean_q: 2.986432, mean_eps: 0.100000
 5340072/6000000: episode: 6632, duration: 26.422s, episode steps: 1169, steps per second:  44, episode reward: 31.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.945 [0.000, 5.000],  loss: 0.013626, mae: 2.466894, mean_q: 2.975333, mean_eps: 0.100000
 5340988/6000000: episode: 6633, duration: 20.261s, episode steps: 916, steps per second:  45, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.299 [0.000, 5.000],  loss: 0.014000, mae: 2.457812, mean_q: 2.962394, mean_eps: 0.100000
 5341926/6000000: episode: 6634, duration: 20.643s, episode steps: 938, steps per second:  45, episode reward: 28.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.965 [0.000, 5.000],  loss: 0.013202, mae: 2.449730, mean_q: 2.952150, mean_eps: 0.100000
 5342801/6000000: episode: 6635, duration: 18.967s, episode steps: 875, steps per second:  46, episode reward: 26.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.014681, mae: 2.451771, mean_q: 2.954440, mean_eps: 0.100000
 5343687/6000000: episode: 6636, duration: 19.516s, episode steps: 886, steps per second:  45, episode reward: 27.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.808 [0.000, 5.000],  loss: 0.013688, mae: 2.449570, mean_q: 2.951996, mean_eps: 0.100000
 5344871/6000000: episode: 6637, duration: 27.214s, episode steps: 1184, steps per second:  44, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.187 [0.000, 5.000],  loss: 0.012597, mae: 2.442783, mean_q: 2.945695, mean_eps: 0.100000
 5345880/6000000: episode: 6638, duration: 25.647s, episode steps: 1009, steps per second:  39, episode reward: 29.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.885 [0.000, 5.000],  loss: 0.014982, mae: 2.462365, mean_q: 2.967455, mean_eps: 0.100000
 5346767/6000000: episode: 6639, duration: 20.757s, episode steps: 887, steps per second:  43, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.015087, mae: 2.480734, mean_q: 2.988835, mean_eps: 0.100000
 5347911/6000000: episode: 6640, duration: 26.486s, episode steps: 1144, steps per second:  43, episode reward: 33.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.014967, mae: 2.452470, mean_q: 2.954610, mean_eps: 0.100000
 5348566/6000000: episode: 6641, duration: 14.974s, episode steps: 655, steps per second:  44, episode reward: 19.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.258 [0.000, 5.000],  loss: 0.013299, mae: 2.467648, mean_q: 2.973296, mean_eps: 0.100000
 5349570/6000000: episode: 6642, duration: 21.259s, episode steps: 1004, steps per second:  47, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.012548, mae: 2.466337, mean_q: 2.972887, mean_eps: 0.100000
 5350419/6000000: episode: 6643, duration: 19.398s, episode steps: 849, steps per second:  44, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.894 [0.000, 5.000],  loss: 0.014740, mae: 2.447528, mean_q: 2.949422, mean_eps: 0.100000
 5351067/6000000: episode: 6644, duration: 15.501s, episode steps: 648, steps per second:  42, episode reward: 18.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.013030, mae: 2.417621, mean_q: 2.914127, mean_eps: 0.100000
 5351783/6000000: episode: 6645, duration: 15.947s, episode steps: 716, steps per second:  45, episode reward: 23.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 1.807 [0.000, 5.000],  loss: 0.014528, mae: 2.419988, mean_q: 2.917972, mean_eps: 0.100000
 5352720/6000000: episode: 6646, duration: 21.087s, episode steps: 937, steps per second:  44, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.729 [0.000, 5.000],  loss: 0.013777, mae: 2.410585, mean_q: 2.905200, mean_eps: 0.100000
 5353703/6000000: episode: 6647, duration: 21.881s, episode steps: 983, steps per second:  45, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.318 [0.000, 5.000],  loss: 0.013447, mae: 2.423407, mean_q: 2.920438, mean_eps: 0.100000
 5354640/6000000: episode: 6648, duration: 19.387s, episode steps: 937, steps per second:  48, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.919 [0.000, 5.000],  loss: 0.011911, mae: 2.437959, mean_q: 2.938602, mean_eps: 0.100000
 5355427/6000000: episode: 6649, duration: 16.689s, episode steps: 787, steps per second:  47, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.673 [0.000, 5.000],  loss: 0.013274, mae: 2.413145, mean_q: 2.908410, mean_eps: 0.100000
 5356283/6000000: episode: 6650, duration: 18.901s, episode steps: 856, steps per second:  45, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.207 [0.000, 5.000],  loss: 0.012836, mae: 2.410002, mean_q: 2.905994, mean_eps: 0.100000
 5357079/6000000: episode: 6651, duration: 17.354s, episode steps: 796, steps per second:  46, episode reward: 27.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 1.984 [0.000, 5.000],  loss: 0.013237, mae: 2.419063, mean_q: 2.914616, mean_eps: 0.100000
 5357828/6000000: episode: 6652, duration: 16.373s, episode steps: 749, steps per second:  46, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.689 [0.000, 5.000],  loss: 0.013600, mae: 2.432306, mean_q: 2.931382, mean_eps: 0.100000
 5359065/6000000: episode: 6653, duration: 26.098s, episode steps: 1237, steps per second:  47, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.099 [0.000, 5.000],  loss: 0.014040, mae: 2.420785, mean_q: 2.916241, mean_eps: 0.100000
 5359951/6000000: episode: 6654, duration: 19.438s, episode steps: 886, steps per second:  46, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.915 [0.000, 5.000],  loss: 0.014250, mae: 2.414780, mean_q: 2.909104, mean_eps: 0.100000
 5361390/6000000: episode: 6655, duration: 34.020s, episode steps: 1439, steps per second:  42, episode reward: 32.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.085 [0.000, 5.000],  loss: 0.014605, mae: 2.442852, mean_q: 2.943897, mean_eps: 0.100000
 5362519/6000000: episode: 6656, duration: 26.512s, episode steps: 1129, steps per second:  43, episode reward: 31.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: 0.013262, mae: 2.447724, mean_q: 2.949042, mean_eps: 0.100000
 5363732/6000000: episode: 6657, duration: 28.227s, episode steps: 1213, steps per second:  43, episode reward: 34.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.026 [0.000, 5.000],  loss: 0.013560, mae: 2.436683, mean_q: 2.935659, mean_eps: 0.100000
 5364662/6000000: episode: 6658, duration: 21.215s, episode steps: 930, steps per second:  44, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.812 [0.000, 5.000],  loss: 0.014269, mae: 2.429966, mean_q: 2.927548, mean_eps: 0.100000
 5365515/6000000: episode: 6659, duration: 18.526s, episode steps: 853, steps per second:  46, episode reward: 26.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.183 [0.000, 5.000],  loss: 0.014085, mae: 2.434849, mean_q: 2.932773, mean_eps: 0.100000
 5366976/6000000: episode: 6660, duration: 32.844s, episode steps: 1461, steps per second:  44, episode reward: 42.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.097 [0.000, 5.000],  loss: 0.013891, mae: 2.419598, mean_q: 2.915000, mean_eps: 0.100000
 5368201/6000000: episode: 6661, duration: 28.267s, episode steps: 1225, steps per second:  43, episode reward: 35.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.103 [0.000, 5.000],  loss: 0.013176, mae: 2.417905, mean_q: 2.912503, mean_eps: 0.100000
 5369088/6000000: episode: 6662, duration: 20.426s, episode steps: 887, steps per second:  43, episode reward: 29.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.013299, mae: 2.440256, mean_q: 2.939501, mean_eps: 0.100000
 5369997/6000000: episode: 6663, duration: 20.688s, episode steps: 909, steps per second:  44, episode reward: 29.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.014519, mae: 2.444041, mean_q: 2.943402, mean_eps: 0.100000
 5371035/6000000: episode: 6664, duration: 21.973s, episode steps: 1038, steps per second:  47, episode reward: 30.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.983 [0.000, 5.000],  loss: 0.013764, mae: 2.425919, mean_q: 2.922476, mean_eps: 0.100000
 5371855/6000000: episode: 6665, duration: 17.831s, episode steps: 820, steps per second:  46, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.706 [0.000, 5.000],  loss: 0.013696, mae: 2.410536, mean_q: 2.904209, mean_eps: 0.100000
 5372649/6000000: episode: 6666, duration: 17.787s, episode steps: 794, steps per second:  45, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.073 [0.000, 5.000],  loss: 0.013350, mae: 2.411531, mean_q: 2.904663, mean_eps: 0.100000
 5373658/6000000: episode: 6667, duration: 21.708s, episode steps: 1009, steps per second:  46, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.752 [0.000, 5.000],  loss: 0.012803, mae: 2.398742, mean_q: 2.890358, mean_eps: 0.100000
 5374665/6000000: episode: 6668, duration: 21.649s, episode steps: 1007, steps per second:  47, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.097 [0.000, 5.000],  loss: 0.013614, mae: 2.404184, mean_q: 2.896623, mean_eps: 0.100000
 5375359/6000000: episode: 6669, duration: 14.495s, episode steps: 694, steps per second:  48, episode reward: 20.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.731 [0.000, 5.000],  loss: 0.013167, mae: 2.395080, mean_q: 2.886518, mean_eps: 0.100000
 5376423/6000000: episode: 6670, duration: 24.347s, episode steps: 1064, steps per second:  44, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.934 [0.000, 5.000],  loss: 0.013039, mae: 2.399598, mean_q: 2.891773, mean_eps: 0.100000
 5376958/6000000: episode: 6671, duration: 12.544s, episode steps: 535, steps per second:  43, episode reward: 14.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.034 [0.000, 5.000],  loss: 0.013975, mae: 2.414876, mean_q: 2.909857, mean_eps: 0.100000
 5377776/6000000: episode: 6672, duration: 19.834s, episode steps: 818, steps per second:  41, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.885 [0.000, 5.000],  loss: 0.013808, mae: 2.391385, mean_q: 2.881804, mean_eps: 0.100000
 5378387/6000000: episode: 6673, duration: 14.754s, episode steps: 611, steps per second:  41, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.678 [0.000, 5.000],  loss: 0.013580, mae: 2.417658, mean_q: 2.912775, mean_eps: 0.100000
 5380010/6000000: episode: 6674, duration: 38.506s, episode steps: 1623, steps per second:  42, episode reward: 41.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.062 [0.000, 5.000],  loss: 0.014117, mae: 2.410451, mean_q: 2.903555, mean_eps: 0.100000
 5381005/6000000: episode: 6675, duration: 23.055s, episode steps: 995, steps per second:  43, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.788 [0.000, 5.000],  loss: 0.013670, mae: 2.392608, mean_q: 2.883379, mean_eps: 0.100000
 5382145/6000000: episode: 6676, duration: 24.919s, episode steps: 1140, steps per second:  46, episode reward: 34.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.140 [0.000, 5.000],  loss: 0.013637, mae: 2.394311, mean_q: 2.886063, mean_eps: 0.100000
 5383109/6000000: episode: 6677, duration: 21.595s, episode steps: 964, steps per second:  45, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.014396, mae: 2.389139, mean_q: 2.879556, mean_eps: 0.100000
 5383835/6000000: episode: 6678, duration: 16.362s, episode steps: 726, steps per second:  44, episode reward: 27.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 1.880 [0.000, 5.000],  loss: 0.014088, mae: 2.403332, mean_q: 2.898234, mean_eps: 0.100000
 5384745/6000000: episode: 6679, duration: 20.938s, episode steps: 910, steps per second:  43, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.041 [0.000, 5.000],  loss: 0.014138, mae: 2.392847, mean_q: 2.883000, mean_eps: 0.100000
 5385711/6000000: episode: 6680, duration: 22.322s, episode steps: 966, steps per second:  43, episode reward: 28.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.744 [0.000, 5.000],  loss: 0.013209, mae: 2.411736, mean_q: 2.906297, mean_eps: 0.100000
 5386606/6000000: episode: 6681, duration: 19.005s, episode steps: 895, steps per second:  47, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.139 [0.000, 5.000],  loss: 0.014071, mae: 2.423944, mean_q: 2.921088, mean_eps: 0.100000
 5387474/6000000: episode: 6682, duration: 18.441s, episode steps: 868, steps per second:  47, episode reward: 27.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.015038, mae: 2.422873, mean_q: 2.917961, mean_eps: 0.100000
 5387959/6000000: episode: 6683, duration: 10.678s, episode steps: 485, steps per second:  45, episode reward: 13.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.827 [0.000, 5.000],  loss: 0.014788, mae: 2.442208, mean_q: 2.940234, mean_eps: 0.100000
 5389015/6000000: episode: 6684, duration: 23.227s, episode steps: 1056, steps per second:  45, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.082 [0.000, 5.000],  loss: 0.013773, mae: 2.421610, mean_q: 2.917621, mean_eps: 0.100000
 5389885/6000000: episode: 6685, duration: 17.787s, episode steps: 870, steps per second:  49, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.099 [0.000, 5.000],  loss: 0.014166, mae: 2.407415, mean_q: 2.899914, mean_eps: 0.100000
 5390638/6000000: episode: 6686, duration: 16.556s, episode steps: 753, steps per second:  45, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.778 [0.000, 5.000],  loss: 0.013523, mae: 2.424670, mean_q: 2.921943, mean_eps: 0.100000
 5391399/6000000: episode: 6687, duration: 16.917s, episode steps: 761, steps per second:  45, episode reward: 21.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.880 [0.000, 5.000],  loss: 0.012457, mae: 2.419861, mean_q: 2.916836, mean_eps: 0.100000
 5392591/6000000: episode: 6688, duration: 27.887s, episode steps: 1192, steps per second:  43, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.143 [0.000, 5.000],  loss: 0.012698, mae: 2.416182, mean_q: 2.912394, mean_eps: 0.100000
 5393846/6000000: episode: 6689, duration: 31.325s, episode steps: 1255, steps per second:  40, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.013302, mae: 2.419512, mean_q: 2.915813, mean_eps: 0.100000
 5394522/6000000: episode: 6690, duration: 15.814s, episode steps: 676, steps per second:  43, episode reward: 18.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.636 [0.000, 5.000],  loss: 0.013807, mae: 2.414082, mean_q: 2.908405, mean_eps: 0.100000
 5395245/6000000: episode: 6691, duration: 16.392s, episode steps: 723, steps per second:  44, episode reward: 20.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.692 [0.000, 5.000],  loss: 0.012677, mae: 2.431815, mean_q: 2.930552, mean_eps: 0.100000
 5396264/6000000: episode: 6692, duration: 23.975s, episode steps: 1019, steps per second:  43, episode reward: 30.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.012567, mae: 2.414782, mean_q: 2.911195, mean_eps: 0.100000
 5397234/6000000: episode: 6693, duration: 20.927s, episode steps: 970, steps per second:  46, episode reward: 29.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.870 [0.000, 5.000],  loss: 0.013554, mae: 2.407870, mean_q: 2.901577, mean_eps: 0.100000
 5397795/6000000: episode: 6694, duration: 12.000s, episode steps: 561, steps per second:  47, episode reward: 15.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.014038, mae: 2.400841, mean_q: 2.891916, mean_eps: 0.100000
 5398425/6000000: episode: 6695, duration: 14.278s, episode steps: 630, steps per second:  44, episode reward: 19.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.013639, mae: 2.381919, mean_q: 2.869343, mean_eps: 0.100000
 5398797/6000000: episode: 6696, duration: 8.462s, episode steps: 372, steps per second:  44, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.981 [0.000, 5.000],  loss: 0.014322, mae: 2.411982, mean_q: 2.905904, mean_eps: 0.100000
 5399852/6000000: episode: 6697, duration: 23.526s, episode steps: 1055, steps per second:  45, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.013858, mae: 2.414994, mean_q: 2.910458, mean_eps: 0.100000
 5400611/6000000: episode: 6698, duration: 16.706s, episode steps: 759, steps per second:  45, episode reward: 22.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.013247, mae: 2.402497, mean_q: 2.895009, mean_eps: 0.100000
 5401410/6000000: episode: 6699, duration: 18.552s, episode steps: 799, steps per second:  43, episode reward: 25.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 0.012544, mae: 2.386109, mean_q: 2.875518, mean_eps: 0.100000
 5402229/6000000: episode: 6700, duration: 17.790s, episode steps: 819, steps per second:  46, episode reward: 25.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.079 [0.000, 5.000],  loss: 0.012193, mae: 2.391766, mean_q: 2.883037, mean_eps: 0.100000
 5403169/6000000: episode: 6701, duration: 19.302s, episode steps: 940, steps per second:  49, episode reward: 28.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.171 [0.000, 5.000],  loss: 0.013583, mae: 2.411125, mean_q: 2.904419, mean_eps: 0.100000
 5403767/6000000: episode: 6702, duration: 12.906s, episode steps: 598, steps per second:  46, episode reward: 17.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.033 [0.000, 5.000],  loss: 0.014579, mae: 2.415423, mean_q: 2.911251, mean_eps: 0.100000
 5404627/6000000: episode: 6703, duration: 18.823s, episode steps: 860, steps per second:  46, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.497 [0.000, 5.000],  loss: 0.014906, mae: 2.385265, mean_q: 2.874462, mean_eps: 0.100000
 5405423/6000000: episode: 6704, duration: 16.534s, episode steps: 796, steps per second:  48, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.013343, mae: 2.387741, mean_q: 2.877666, mean_eps: 0.100000
 5406310/6000000: episode: 6705, duration: 18.771s, episode steps: 887, steps per second:  47, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.012812, mae: 2.372132, mean_q: 2.858026, mean_eps: 0.100000
 5406956/6000000: episode: 6706, duration: 14.340s, episode steps: 646, steps per second:  45, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.966 [0.000, 5.000],  loss: 0.013009, mae: 2.378205, mean_q: 2.865868, mean_eps: 0.100000
 5407595/6000000: episode: 6707, duration: 13.301s, episode steps: 639, steps per second:  48, episode reward: 18.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.479 [0.000, 5.000],  loss: 0.014798, mae: 2.390005, mean_q: 2.880007, mean_eps: 0.100000
 5408473/6000000: episode: 6708, duration: 19.230s, episode steps: 878, steps per second:  46, episode reward: 27.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.269 [0.000, 5.000],  loss: 0.013341, mae: 2.395214, mean_q: 2.885340, mean_eps: 0.100000
 5409371/6000000: episode: 6709, duration: 20.998s, episode steps: 898, steps per second:  43, episode reward: 29.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 1.962 [0.000, 5.000],  loss: 0.014219, mae: 2.375940, mean_q: 2.862983, mean_eps: 0.100000
 5410451/6000000: episode: 6710, duration: 26.099s, episode steps: 1080, steps per second:  41, episode reward: 32.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.169 [0.000, 5.000],  loss: 0.015120, mae: 2.404216, mean_q: 2.896728, mean_eps: 0.100000
 5411506/6000000: episode: 6711, duration: 23.589s, episode steps: 1055, steps per second:  45, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.208 [0.000, 5.000],  loss: 0.013753, mae: 2.402456, mean_q: 2.895701, mean_eps: 0.100000
 5412594/6000000: episode: 6712, duration: 24.103s, episode steps: 1088, steps per second:  45, episode reward: 30.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.013344, mae: 2.379441, mean_q: 2.867229, mean_eps: 0.100000
 5413522/6000000: episode: 6713, duration: 20.073s, episode steps: 928, steps per second:  46, episode reward: 31.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.267 [0.000, 5.000],  loss: 0.014223, mae: 2.411382, mean_q: 2.904837, mean_eps: 0.100000
 5414717/6000000: episode: 6714, duration: 26.162s, episode steps: 1195, steps per second:  46, episode reward: 30.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.126 [0.000, 5.000],  loss: 0.014354, mae: 2.389934, mean_q: 2.879174, mean_eps: 0.100000
 5416048/6000000: episode: 6715, duration: 30.735s, episode steps: 1331, steps per second:  43, episode reward: 35.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.923 [0.000, 5.000],  loss: 0.014643, mae: 2.387017, mean_q: 2.875280, mean_eps: 0.100000
 5416853/6000000: episode: 6716, duration: 17.878s, episode steps: 805, steps per second:  45, episode reward: 24.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.037 [0.000, 5.000],  loss: 0.013365, mae: 2.367754, mean_q: 2.852719, mean_eps: 0.100000
 5418047/6000000: episode: 6717, duration: 26.032s, episode steps: 1194, steps per second:  46, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.764 [0.000, 5.000],  loss: 0.014709, mae: 2.373555, mean_q: 2.860557, mean_eps: 0.100000
 5418875/6000000: episode: 6718, duration: 17.129s, episode steps: 828, steps per second:  48, episode reward: 27.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 1.295 [0.000, 5.000],  loss: 0.014426, mae: 2.376633, mean_q: 2.863751, mean_eps: 0.100000
 5419768/6000000: episode: 6719, duration: 18.449s, episode steps: 893, steps per second:  48, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.647 [0.000, 5.000],  loss: 0.014426, mae: 2.354844, mean_q: 2.837296, mean_eps: 0.100000
 5420674/6000000: episode: 6720, duration: 19.422s, episode steps: 906, steps per second:  47, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.841 [0.000, 5.000],  loss: 0.014688, mae: 2.387933, mean_q: 2.877149, mean_eps: 0.100000
 5421845/6000000: episode: 6721, duration: 26.155s, episode steps: 1171, steps per second:  45, episode reward: 34.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.087 [0.000, 5.000],  loss: 0.013622, mae: 2.397118, mean_q: 2.889162, mean_eps: 0.100000
 5422723/6000000: episode: 6722, duration: 18.977s, episode steps: 878, steps per second:  46, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.014858, mae: 2.371467, mean_q: 2.856279, mean_eps: 0.100000
 5423617/6000000: episode: 6723, duration: 20.072s, episode steps: 894, steps per second:  45, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.072 [0.000, 5.000],  loss: 0.013900, mae: 2.360143, mean_q: 2.844012, mean_eps: 0.100000
 5424472/6000000: episode: 6724, duration: 18.812s, episode steps: 855, steps per second:  45, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.013873, mae: 2.379825, mean_q: 2.867546, mean_eps: 0.100000
 5425350/6000000: episode: 6725, duration: 19.455s, episode steps: 878, steps per second:  45, episode reward: 28.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.080 [0.000, 5.000],  loss: 0.012753, mae: 2.383311, mean_q: 2.871390, mean_eps: 0.100000
 5425986/6000000: episode: 6726, duration: 16.013s, episode steps: 636, steps per second:  40, episode reward: 18.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.013954, mae: 2.407408, mean_q: 2.900218, mean_eps: 0.100000
 5426939/6000000: episode: 6727, duration: 21.747s, episode steps: 953, steps per second:  44, episode reward: 31.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.013839, mae: 2.387926, mean_q: 2.878151, mean_eps: 0.100000
 5427976/6000000: episode: 6728, duration: 23.415s, episode steps: 1037, steps per second:  44, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.014042, mae: 2.376273, mean_q: 2.864277, mean_eps: 0.100000
 5429182/6000000: episode: 6729, duration: 29.068s, episode steps: 1206, steps per second:  41, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.265 [0.000, 5.000],  loss: 0.015348, mae: 2.390783, mean_q: 2.879413, mean_eps: 0.100000
 5430096/6000000: episode: 6730, duration: 19.911s, episode steps: 914, steps per second:  46, episode reward: 27.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.013695, mae: 2.407173, mean_q: 2.899586, mean_eps: 0.100000
 5430838/6000000: episode: 6731, duration: 17.077s, episode steps: 742, steps per second:  43, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.013204, mae: 2.373084, mean_q: 2.860107, mean_eps: 0.100000
 5431558/6000000: episode: 6732, duration: 18.233s, episode steps: 720, steps per second:  39, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.036 [0.000, 5.000],  loss: 0.013108, mae: 2.373089, mean_q: 2.858950, mean_eps: 0.100000
 5432601/6000000: episode: 6733, duration: 23.891s, episode steps: 1043, steps per second:  44, episode reward: 30.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.908 [0.000, 5.000],  loss: 0.014267, mae: 2.379787, mean_q: 2.866032, mean_eps: 0.100000
 5433332/6000000: episode: 6734, duration: 17.010s, episode steps: 731, steps per second:  43, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.012904, mae: 2.374538, mean_q: 2.861591, mean_eps: 0.100000
 5434300/6000000: episode: 6735, duration: 22.249s, episode steps: 968, steps per second:  44, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.013324, mae: 2.365374, mean_q: 2.850317, mean_eps: 0.100000
 5434839/6000000: episode: 6736, duration: 11.299s, episode steps: 539, steps per second:  48, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.013933, mae: 2.390263, mean_q: 2.879101, mean_eps: 0.100000
 5435862/6000000: episode: 6737, duration: 21.652s, episode steps: 1023, steps per second:  47, episode reward: 30.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.757 [0.000, 5.000],  loss: 0.013982, mae: 2.391453, mean_q: 2.883200, mean_eps: 0.100000
 5437242/6000000: episode: 6738, duration: 30.133s, episode steps: 1380, steps per second:  46, episode reward: 34.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.765 [0.000, 5.000],  loss: 0.013011, mae: 2.385235, mean_q: 2.876781, mean_eps: 0.100000
 5438266/6000000: episode: 6739, duration: 21.620s, episode steps: 1024, steps per second:  47, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.112 [0.000, 5.000],  loss: 0.014350, mae: 2.361894, mean_q: 2.845804, mean_eps: 0.100000
 5439281/6000000: episode: 6740, duration: 20.856s, episode steps: 1015, steps per second:  49, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 0.013607, mae: 2.405265, mean_q: 2.897587, mean_eps: 0.100000
 5440443/6000000: episode: 6741, duration: 24.783s, episode steps: 1162, steps per second:  47, episode reward: 33.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.077 [0.000, 5.000],  loss: 0.014242, mae: 2.380828, mean_q: 2.868027, mean_eps: 0.100000
 5441666/6000000: episode: 6742, duration: 28.777s, episode steps: 1223, steps per second:  42, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.192 [0.000, 5.000],  loss: 0.013046, mae: 2.398818, mean_q: 2.890499, mean_eps: 0.100000
 5442682/6000000: episode: 6743, duration: 25.014s, episode steps: 1016, steps per second:  41, episode reward: 30.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.135 [0.000, 5.000],  loss: 0.013598, mae: 2.409162, mean_q: 2.903311, mean_eps: 0.100000
 5443917/6000000: episode: 6744, duration: 28.243s, episode steps: 1235, steps per second:  44, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.014077, mae: 2.376758, mean_q: 2.864589, mean_eps: 0.100000
 5444911/6000000: episode: 6745, duration: 22.880s, episode steps: 994, steps per second:  43, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.013064, mae: 2.381828, mean_q: 2.870883, mean_eps: 0.100000
 5445814/6000000: episode: 6746, duration: 19.289s, episode steps: 903, steps per second:  47, episode reward: 27.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.013962, mae: 2.395269, mean_q: 2.886046, mean_eps: 0.100000
 5446687/6000000: episode: 6747, duration: 19.586s, episode steps: 873, steps per second:  45, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.013606, mae: 2.406057, mean_q: 2.899743, mean_eps: 0.100000
 5447632/6000000: episode: 6748, duration: 21.715s, episode steps: 945, steps per second:  44, episode reward: 29.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.126 [0.000, 5.000],  loss: 0.012334, mae: 2.419903, mean_q: 2.916145, mean_eps: 0.100000
 5448481/6000000: episode: 6749, duration: 18.565s, episode steps: 849, steps per second:  46, episode reward: 28.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.013904, mae: 2.402057, mean_q: 2.893193, mean_eps: 0.100000
 5449410/6000000: episode: 6750, duration: 20.541s, episode steps: 929, steps per second:  45, episode reward: 28.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.272 [0.000, 5.000],  loss: 0.014068, mae: 2.395274, mean_q: 2.885733, mean_eps: 0.100000
 5450156/6000000: episode: 6751, duration: 16.742s, episode steps: 746, steps per second:  45, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.074 [0.000, 5.000],  loss: 0.013118, mae: 2.377675, mean_q: 2.863787, mean_eps: 0.100000
 5450929/6000000: episode: 6752, duration: 16.639s, episode steps: 773, steps per second:  46, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.717 [0.000, 5.000],  loss: 0.012395, mae: 2.379076, mean_q: 2.866780, mean_eps: 0.100000
 5452181/6000000: episode: 6753, duration: 27.041s, episode steps: 1252, steps per second:  46, episode reward: 33.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.013139, mae: 2.391156, mean_q: 2.880560, mean_eps: 0.100000
 5453020/6000000: episode: 6754, duration: 19.163s, episode steps: 839, steps per second:  44, episode reward: 24.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.939 [0.000, 5.000],  loss: 0.012449, mae: 2.373698, mean_q: 2.859223, mean_eps: 0.100000
 5453853/6000000: episode: 6755, duration: 18.159s, episode steps: 833, steps per second:  46, episode reward: 28.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.082 [0.000, 5.000],  loss: 0.014174, mae: 2.398598, mean_q: 2.890088, mean_eps: 0.100000
 5454444/6000000: episode: 6756, duration: 13.143s, episode steps: 591, steps per second:  45, episode reward: 18.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.013529, mae: 2.378176, mean_q: 2.864316, mean_eps: 0.100000
 5455364/6000000: episode: 6757, duration: 19.999s, episode steps: 920, steps per second:  46, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.014094, mae: 2.364744, mean_q: 2.849370, mean_eps: 0.100000
 5456112/6000000: episode: 6758, duration: 16.512s, episode steps: 748, steps per second:  45, episode reward: 22.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.032 [0.000, 5.000],  loss: 0.015686, mae: 2.350945, mean_q: 2.829678, mean_eps: 0.100000
 5456912/6000000: episode: 6759, duration: 19.440s, episode steps: 800, steps per second:  41, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.014464, mae: 2.366123, mean_q: 2.849668, mean_eps: 0.100000
 5457876/6000000: episode: 6760, duration: 23.723s, episode steps: 964, steps per second:  41, episode reward: 31.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.171 [0.000, 5.000],  loss: 0.013640, mae: 2.356467, mean_q: 2.838267, mean_eps: 0.100000
 5458696/6000000: episode: 6761, duration: 19.691s, episode steps: 820, steps per second:  42, episode reward: 25.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.085 [0.000, 5.000],  loss: 0.014154, mae: 2.349323, mean_q: 2.829996, mean_eps: 0.100000
 5459789/6000000: episode: 6762, duration: 25.828s, episode steps: 1093, steps per second:  42, episode reward: 31.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.906 [0.000, 5.000],  loss: 0.013703, mae: 2.370642, mean_q: 2.856945, mean_eps: 0.100000
 5460428/6000000: episode: 6763, duration: 13.976s, episode steps: 639, steps per second:  46, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.174 [0.000, 5.000],  loss: 0.012799, mae: 2.361574, mean_q: 2.848526, mean_eps: 0.100000
 5461167/6000000: episode: 6764, duration: 16.469s, episode steps: 739, steps per second:  45, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.945 [0.000, 5.000],  loss: 0.012942, mae: 2.401614, mean_q: 2.893558, mean_eps: 0.100000
 5461943/6000000: episode: 6765, duration: 16.718s, episode steps: 776, steps per second:  46, episode reward: 23.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.686 [0.000, 5.000],  loss: 0.012749, mae: 2.366464, mean_q: 2.853178, mean_eps: 0.100000
 5463247/6000000: episode: 6766, duration: 28.820s, episode steps: 1304, steps per second:  45, episode reward: 34.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.014391, mae: 2.374191, mean_q: 2.859626, mean_eps: 0.100000
 5464009/6000000: episode: 6767, duration: 16.766s, episode steps: 762, steps per second:  45, episode reward: 22.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.983 [0.000, 5.000],  loss: 0.014762, mae: 2.362678, mean_q: 2.844883, mean_eps: 0.100000
 5465121/6000000: episode: 6768, duration: 24.099s, episode steps: 1112, steps per second:  46, episode reward: 33.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.109 [0.000, 5.000],  loss: 0.014334, mae: 2.383404, mean_q: 2.870930, mean_eps: 0.100000
 5465993/6000000: episode: 6769, duration: 19.065s, episode steps: 872, steps per second:  46, episode reward: 26.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.999 [0.000, 5.000],  loss: 0.012637, mae: 2.386932, mean_q: 2.877786, mean_eps: 0.100000
 5466899/6000000: episode: 6770, duration: 19.259s, episode steps: 906, steps per second:  47, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.217 [0.000, 5.000],  loss: 0.014408, mae: 2.388780, mean_q: 2.877817, mean_eps: 0.100000
 5467549/6000000: episode: 6771, duration: 13.365s, episode steps: 650, steps per second:  49, episode reward: 20.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.723 [0.000, 5.000],  loss: 0.013086, mae: 2.382918, mean_q: 2.872134, mean_eps: 0.100000
 5468132/6000000: episode: 6772, duration: 12.192s, episode steps: 583, steps per second:  48, episode reward: 16.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.875 [0.000, 5.000],  loss: 0.014269, mae: 2.395615, mean_q: 2.887492, mean_eps: 0.100000
 5469497/6000000: episode: 6773, duration: 29.379s, episode steps: 1365, steps per second:  46, episode reward: 36.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.013301, mae: 2.376967, mean_q: 2.864619, mean_eps: 0.100000
 5470397/6000000: episode: 6774, duration: 18.991s, episode steps: 900, steps per second:  47, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.730 [0.000, 5.000],  loss: 0.013438, mae: 2.378854, mean_q: 2.867563, mean_eps: 0.100000
 5471301/6000000: episode: 6775, duration: 19.166s, episode steps: 904, steps per second:  47, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.015052, mae: 2.384008, mean_q: 2.871859, mean_eps: 0.100000
 5472003/6000000: episode: 6776, duration: 15.509s, episode steps: 702, steps per second:  45, episode reward: 23.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.674 [0.000, 5.000],  loss: 0.013448, mae: 2.396444, mean_q: 2.886719, mean_eps: 0.100000
 5472841/6000000: episode: 6777, duration: 18.822s, episode steps: 838, steps per second:  45, episode reward: 28.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.013136, mae: 2.385474, mean_q: 2.874020, mean_eps: 0.100000
 5473588/6000000: episode: 6778, duration: 16.840s, episode steps: 747, steps per second:  44, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.679 [0.000, 5.000],  loss: 0.015377, mae: 2.399050, mean_q: 2.889965, mean_eps: 0.100000
 5474442/6000000: episode: 6779, duration: 20.763s, episode steps: 854, steps per second:  41, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.513 [0.000, 5.000],  loss: 0.013969, mae: 2.411334, mean_q: 2.905625, mean_eps: 0.100000
 5475480/6000000: episode: 6780, duration: 24.335s, episode steps: 1038, steps per second:  43, episode reward: 30.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.803 [0.000, 5.000],  loss: 0.013813, mae: 2.398247, mean_q: 2.889043, mean_eps: 0.100000
 5476237/6000000: episode: 6781, duration: 16.431s, episode steps: 757, steps per second:  46, episode reward: 22.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.016 [0.000, 5.000],  loss: 0.014488, mae: 2.403875, mean_q: 2.896249, mean_eps: 0.100000
 5477784/6000000: episode: 6782, duration: 34.347s, episode steps: 1547, steps per second:  45, episode reward: 37.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.118 [0.000, 5.000],  loss: 0.013755, mae: 2.405352, mean_q: 2.899105, mean_eps: 0.100000
 5478866/6000000: episode: 6783, duration: 24.460s, episode steps: 1082, steps per second:  44, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.014917, mae: 2.411871, mean_q: 2.907356, mean_eps: 0.100000
 5479634/6000000: episode: 6784, duration: 17.114s, episode steps: 768, steps per second:  45, episode reward: 23.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.865 [0.000, 5.000],  loss: 0.013622, mae: 2.394647, mean_q: 2.884957, mean_eps: 0.100000
 5480727/6000000: episode: 6785, duration: 23.877s, episode steps: 1093, steps per second:  46, episode reward: 31.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.826 [0.000, 5.000],  loss: 0.014683, mae: 2.394832, mean_q: 2.885242, mean_eps: 0.100000
 5481716/6000000: episode: 6786, duration: 21.780s, episode steps: 989, steps per second:  45, episode reward: 29.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.013357, mae: 2.412609, mean_q: 2.905794, mean_eps: 0.100000
 5482830/6000000: episode: 6787, duration: 24.478s, episode steps: 1114, steps per second:  46, episode reward: 31.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.014083, mae: 2.392313, mean_q: 2.881410, mean_eps: 0.100000
 5483664/6000000: episode: 6788, duration: 16.928s, episode steps: 834, steps per second:  49, episode reward: 25.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.143 [0.000, 5.000],  loss: 0.013371, mae: 2.397141, mean_q: 2.887186, mean_eps: 0.100000
 5484489/6000000: episode: 6789, duration: 17.355s, episode steps: 825, steps per second:  48, episode reward: 26.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 1.851 [0.000, 5.000],  loss: 0.013451, mae: 2.399108, mean_q: 2.889132, mean_eps: 0.100000
 5485497/6000000: episode: 6790, duration: 20.947s, episode steps: 1008, steps per second:  48, episode reward: 33.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 1.959 [0.000, 5.000],  loss: 0.012988, mae: 2.408595, mean_q: 2.901464, mean_eps: 0.100000
 5486588/6000000: episode: 6791, duration: 23.183s, episode steps: 1091, steps per second:  47, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.012042, mae: 2.385929, mean_q: 2.874112, mean_eps: 0.100000
 5487320/6000000: episode: 6792, duration: 15.860s, episode steps: 732, steps per second:  46, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.014194, mae: 2.393841, mean_q: 2.882519, mean_eps: 0.100000
 5488249/6000000: episode: 6793, duration: 19.567s, episode steps: 929, steps per second:  47, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.605 [0.000, 5.000],  loss: 0.013433, mae: 2.407577, mean_q: 2.902275, mean_eps: 0.100000
 5489141/6000000: episode: 6794, duration: 19.074s, episode steps: 892, steps per second:  47, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.094 [0.000, 5.000],  loss: 0.014562, mae: 2.385870, mean_q: 2.873559, mean_eps: 0.100000
 5490243/6000000: episode: 6795, duration: 24.232s, episode steps: 1102, steps per second:  45, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.224 [0.000, 5.000],  loss: 0.014083, mae: 2.397621, mean_q: 2.888253, mean_eps: 0.100000
 5491176/6000000: episode: 6796, duration: 21.449s, episode steps: 933, steps per second:  43, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.043 [0.000, 5.000],  loss: 0.013354, mae: 2.394892, mean_q: 2.884071, mean_eps: 0.100000
 5492213/6000000: episode: 6797, duration: 24.106s, episode steps: 1037, steps per second:  43, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.047 [0.000, 5.000],  loss: 0.015035, mae: 2.410005, mean_q: 2.904161, mean_eps: 0.100000
 5492983/6000000: episode: 6798, duration: 16.925s, episode steps: 770, steps per second:  45, episode reward: 25.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.012591, mae: 2.369694, mean_q: 2.855250, mean_eps: 0.100000
 5494182/6000000: episode: 6799, duration: 25.776s, episode steps: 1199, steps per second:  47, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.291 [0.000, 5.000],  loss: 0.014204, mae: 2.399534, mean_q: 2.890779, mean_eps: 0.100000
 5495185/6000000: episode: 6800, duration: 21.272s, episode steps: 1003, steps per second:  47, episode reward: 29.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.714 [0.000, 5.000],  loss: 0.015448, mae: 2.403022, mean_q: 2.894299, mean_eps: 0.100000
 5495963/6000000: episode: 6801, duration: 17.441s, episode steps: 778, steps per second:  45, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.013224, mae: 2.391730, mean_q: 2.881999, mean_eps: 0.100000
 5496881/6000000: episode: 6802, duration: 20.465s, episode steps: 918, steps per second:  45, episode reward: 29.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.077 [0.000, 5.000],  loss: 0.015320, mae: 2.414009, mean_q: 2.906783, mean_eps: 0.100000
 5497706/6000000: episode: 6803, duration: 18.285s, episode steps: 825, steps per second:  45, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.247 [0.000, 5.000],  loss: 0.013092, mae: 2.410193, mean_q: 2.902903, mean_eps: 0.100000
 5498414/6000000: episode: 6804, duration: 15.388s, episode steps: 708, steps per second:  46, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: 0.014473, mae: 2.407182, mean_q: 2.898306, mean_eps: 0.100000
 5499015/6000000: episode: 6805, duration: 12.818s, episode steps: 601, steps per second:  47, episode reward: 18.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.013302, mae: 2.402741, mean_q: 2.893523, mean_eps: 0.100000
 5500202/6000000: episode: 6806, duration: 24.490s, episode steps: 1187, steps per second:  48, episode reward: 32.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.014317, mae: 2.412457, mean_q: 2.905576, mean_eps: 0.100000
 5500979/6000000: episode: 6807, duration: 16.268s, episode steps: 777, steps per second:  48, episode reward: 24.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.255 [0.000, 5.000],  loss: 0.013976, mae: 2.380421, mean_q: 2.868901, mean_eps: 0.100000
 5501718/6000000: episode: 6808, duration: 15.153s, episode steps: 739, steps per second:  49, episode reward: 24.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.096 [0.000, 5.000],  loss: 0.012993, mae: 2.374210, mean_q: 2.860224, mean_eps: 0.100000
 5502598/6000000: episode: 6809, duration: 18.510s, episode steps: 880, steps per second:  48, episode reward: 31.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.014300, mae: 2.382175, mean_q: 2.868828, mean_eps: 0.100000
 5503646/6000000: episode: 6810, duration: 22.244s, episode steps: 1048, steps per second:  47, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.171 [0.000, 5.000],  loss: 0.013684, mae: 2.366770, mean_q: 2.851265, mean_eps: 0.100000
 5504590/6000000: episode: 6811, duration: 19.523s, episode steps: 944, steps per second:  48, episode reward: 28.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.128 [0.000, 5.000],  loss: 0.014091, mae: 2.383781, mean_q: 2.871615, mean_eps: 0.100000
 5505663/6000000: episode: 6812, duration: 23.060s, episode steps: 1073, steps per second:  47, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.169 [0.000, 5.000],  loss: 0.013293, mae: 2.383259, mean_q: 2.871123, mean_eps: 0.100000
 5506584/6000000: episode: 6813, duration: 19.357s, episode steps: 921, steps per second:  48, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.472 [0.000, 5.000],  loss: 0.013303, mae: 2.394285, mean_q: 2.883584, mean_eps: 0.100000
 5507460/6000000: episode: 6814, duration: 19.804s, episode steps: 876, steps per second:  44, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.145 [0.000, 5.000],  loss: 0.013323, mae: 2.389364, mean_q: 2.879563, mean_eps: 0.100000
 5508225/6000000: episode: 6815, duration: 16.794s, episode steps: 765, steps per second:  46, episode reward: 23.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.068 [0.000, 5.000],  loss: 0.014111, mae: 2.416032, mean_q: 2.910345, mean_eps: 0.100000
 5509256/6000000: episode: 6816, duration: 22.363s, episode steps: 1031, steps per second:  46, episode reward: 32.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.013790, mae: 2.403334, mean_q: 2.893259, mean_eps: 0.100000
 5509905/6000000: episode: 6817, duration: 13.667s, episode steps: 649, steps per second:  47, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.753 [0.000, 5.000],  loss: 0.013289, mae: 2.387416, mean_q: 2.877229, mean_eps: 0.100000
 5511210/6000000: episode: 6818, duration: 28.315s, episode steps: 1305, steps per second:  46, episode reward: 33.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.818 [0.000, 5.000],  loss: 0.013364, mae: 2.406392, mean_q: 2.899597, mean_eps: 0.100000
 5511803/6000000: episode: 6819, duration: 12.284s, episode steps: 593, steps per second:  48, episode reward: 18.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.503 [0.000, 5.000],  loss: 0.014549, mae: 2.428763, mean_q: 2.928389, mean_eps: 0.100000
 5512723/6000000: episode: 6820, duration: 19.783s, episode steps: 920, steps per second:  47, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.655 [0.000, 5.000],  loss: 0.013851, mae: 2.399636, mean_q: 2.892911, mean_eps: 0.100000
 5513528/6000000: episode: 6821, duration: 18.330s, episode steps: 805, steps per second:  44, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.201 [0.000, 5.000],  loss: 0.012982, mae: 2.421558, mean_q: 2.917151, mean_eps: 0.100000
 5514499/6000000: episode: 6822, duration: 21.754s, episode steps: 971, steps per second:  45, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.872 [0.000, 5.000],  loss: 0.014531, mae: 2.393391, mean_q: 2.883884, mean_eps: 0.100000
 5515507/6000000: episode: 6823, duration: 21.956s, episode steps: 1008, steps per second:  46, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.925 [0.000, 5.000],  loss: 0.012943, mae: 2.391265, mean_q: 2.881919, mean_eps: 0.100000
 5516283/6000000: episode: 6824, duration: 17.319s, episode steps: 776, steps per second:  45, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.170 [0.000, 5.000],  loss: 0.013933, mae: 2.402787, mean_q: 2.895346, mean_eps: 0.100000
 5516923/6000000: episode: 6825, duration: 12.913s, episode steps: 640, steps per second:  50, episode reward: 20.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.030 [0.000, 5.000],  loss: 0.011982, mae: 2.407836, mean_q: 2.901183, mean_eps: 0.100000
 5517970/6000000: episode: 6826, duration: 20.901s, episode steps: 1047, steps per second:  50, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.024 [0.000, 5.000],  loss: 0.013715, mae: 2.387572, mean_q: 2.878021, mean_eps: 0.100000
 5518912/6000000: episode: 6827, duration: 19.954s, episode steps: 942, steps per second:  47, episode reward: 29.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.042 [0.000, 5.000],  loss: 0.013556, mae: 2.395161, mean_q: 2.885241, mean_eps: 0.100000
 5519628/6000000: episode: 6828, duration: 14.978s, episode steps: 716, steps per second:  48, episode reward: 21.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.256 [0.000, 5.000],  loss: 0.015216, mae: 2.379148, mean_q: 2.866062, mean_eps: 0.100000
 5520541/6000000: episode: 6829, duration: 18.738s, episode steps: 913, steps per second:  49, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.014288, mae: 2.390377, mean_q: 2.879261, mean_eps: 0.100000
 5521174/6000000: episode: 6830, duration: 13.378s, episode steps: 633, steps per second:  47, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.013878, mae: 2.384229, mean_q: 2.870795, mean_eps: 0.100000
 5521997/6000000: episode: 6831, duration: 16.777s, episode steps: 823, steps per second:  49, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.174 [0.000, 5.000],  loss: 0.014670, mae: 2.387263, mean_q: 2.874800, mean_eps: 0.100000
 5522780/6000000: episode: 6832, duration: 16.842s, episode steps: 783, steps per second:  46, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.014909, mae: 2.372758, mean_q: 2.858714, mean_eps: 0.100000
 5523527/6000000: episode: 6833, duration: 15.776s, episode steps: 747, steps per second:  47, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.014694, mae: 2.350711, mean_q: 2.830811, mean_eps: 0.100000
 5524256/6000000: episode: 6834, duration: 16.199s, episode steps: 729, steps per second:  45, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.669 [0.000, 5.000],  loss: 0.014597, mae: 2.351665, mean_q: 2.832622, mean_eps: 0.100000
 5525200/6000000: episode: 6835, duration: 21.128s, episode steps: 944, steps per second:  45, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.901 [0.000, 5.000],  loss: 0.014537, mae: 2.367981, mean_q: 2.852000, mean_eps: 0.100000
 5525896/6000000: episode: 6836, duration: 15.093s, episode steps: 696, steps per second:  46, episode reward: 20.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.170 [0.000, 5.000],  loss: 0.014227, mae: 2.370029, mean_q: 2.856361, mean_eps: 0.100000
 5526850/6000000: episode: 6837, duration: 20.868s, episode steps: 954, steps per second:  46, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.013847, mae: 2.347387, mean_q: 2.827593, mean_eps: 0.100000
 5527759/6000000: episode: 6838, duration: 19.536s, episode steps: 909, steps per second:  47, episode reward: 28.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.127 [0.000, 5.000],  loss: 0.013773, mae: 2.352462, mean_q: 2.833343, mean_eps: 0.100000
 5528430/6000000: episode: 6839, duration: 14.057s, episode steps: 671, steps per second:  48, episode reward: 21.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.021 [0.000, 5.000],  loss: 0.013677, mae: 2.345402, mean_q: 2.824738, mean_eps: 0.100000
 5529318/6000000: episode: 6840, duration: 19.282s, episode steps: 888, steps per second:  46, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.860 [0.000, 5.000],  loss: 0.015115, mae: 2.359055, mean_q: 2.842806, mean_eps: 0.100000
 5530027/6000000: episode: 6841, duration: 15.580s, episode steps: 709, steps per second:  46, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.012485, mae: 2.353838, mean_q: 2.836057, mean_eps: 0.100000
 5530845/6000000: episode: 6842, duration: 18.667s, episode steps: 818, steps per second:  44, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.164 [0.000, 5.000],  loss: 0.013643, mae: 2.360598, mean_q: 2.846251, mean_eps: 0.100000
 5531722/6000000: episode: 6843, duration: 18.718s, episode steps: 877, steps per second:  47, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.916 [0.000, 5.000],  loss: 0.014408, mae: 2.363252, mean_q: 2.847294, mean_eps: 0.100000
 5533087/6000000: episode: 6844, duration: 29.011s, episode steps: 1365, steps per second:  47, episode reward: 30.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.076 [0.000, 5.000],  loss: 0.013860, mae: 2.365488, mean_q: 2.850529, mean_eps: 0.100000
 5533913/6000000: episode: 6845, duration: 17.065s, episode steps: 826, steps per second:  48, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.025 [0.000, 5.000],  loss: 0.013501, mae: 2.333410, mean_q: 2.812645, mean_eps: 0.100000
 5535060/6000000: episode: 6846, duration: 23.920s, episode steps: 1147, steps per second:  48, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.151 [0.000, 5.000],  loss: 0.014363, mae: 2.360157, mean_q: 2.843386, mean_eps: 0.100000
 5536171/6000000: episode: 6847, duration: 23.241s, episode steps: 1111, steps per second:  48, episode reward: 35.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 1.827 [0.000, 5.000],  loss: 0.013251, mae: 2.379279, mean_q: 2.867820, mean_eps: 0.100000
 5537104/6000000: episode: 6848, duration: 19.255s, episode steps: 933, steps per second:  48, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.884 [0.000, 5.000],  loss: 0.013025, mae: 2.392417, mean_q: 2.883687, mean_eps: 0.100000
 5538364/6000000: episode: 6849, duration: 27.449s, episode steps: 1260, steps per second:  46, episode reward: 36.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.762 [0.000, 5.000],  loss: 0.013482, mae: 2.375035, mean_q: 2.861848, mean_eps: 0.100000
 5539225/6000000: episode: 6850, duration: 18.868s, episode steps: 861, steps per second:  46, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.777 [0.000, 5.000],  loss: 0.012780, mae: 2.374505, mean_q: 2.863093, mean_eps: 0.100000
 5540128/6000000: episode: 6851, duration: 19.505s, episode steps: 903, steps per second:  46, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.629 [0.000, 5.000],  loss: 0.013266, mae: 2.389226, mean_q: 2.879279, mean_eps: 0.100000
 5541017/6000000: episode: 6852, duration: 20.657s, episode steps: 889, steps per second:  43, episode reward: 29.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 1.812 [0.000, 5.000],  loss: 0.013679, mae: 2.373510, mean_q: 2.860433, mean_eps: 0.100000
 5541588/6000000: episode: 6853, duration: 12.950s, episode steps: 571, steps per second:  44, episode reward: 15.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.722 [0.000, 5.000],  loss: 0.014405, mae: 2.353780, mean_q: 2.838948, mean_eps: 0.100000
 5542768/6000000: episode: 6854, duration: 25.161s, episode steps: 1180, steps per second:  47, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.977 [0.000, 5.000],  loss: 0.014913, mae: 2.379396, mean_q: 2.866136, mean_eps: 0.100000
 5543738/6000000: episode: 6855, duration: 21.418s, episode steps: 970, steps per second:  45, episode reward: 30.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.177 [0.000, 5.000],  loss: 0.013884, mae: 2.391283, mean_q: 2.881928, mean_eps: 0.100000
 5544732/6000000: episode: 6856, duration: 22.291s, episode steps: 994, steps per second:  45, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.983 [0.000, 5.000],  loss: 0.014310, mae: 2.375718, mean_q: 2.862811, mean_eps: 0.100000
 5545531/6000000: episode: 6857, duration: 17.208s, episode steps: 799, steps per second:  46, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.049 [0.000, 5.000],  loss: 0.013457, mae: 2.405783, mean_q: 2.898156, mean_eps: 0.100000
 5546688/6000000: episode: 6858, duration: 24.952s, episode steps: 1157, steps per second:  46, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.232 [0.000, 5.000],  loss: 0.013536, mae: 2.390914, mean_q: 2.879873, mean_eps: 0.100000
 5547812/6000000: episode: 6859, duration: 24.581s, episode steps: 1124, steps per second:  46, episode reward: 33.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.278 [0.000, 5.000],  loss: 0.013708, mae: 2.382664, mean_q: 2.869807, mean_eps: 0.100000
 5548867/6000000: episode: 6860, duration: 23.071s, episode steps: 1055, steps per second:  46, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.275 [0.000, 5.000],  loss: 0.014214, mae: 2.385355, mean_q: 2.872528, mean_eps: 0.100000
 5549885/6000000: episode: 6861, duration: 21.921s, episode steps: 1018, steps per second:  46, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.199 [0.000, 5.000],  loss: 0.014683, mae: 2.391817, mean_q: 2.882031, mean_eps: 0.100000
 5550901/6000000: episode: 6862, duration: 20.437s, episode steps: 1016, steps per second:  50, episode reward: 30.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.963 [0.000, 5.000],  loss: 0.014473, mae: 2.399869, mean_q: 2.891109, mean_eps: 0.100000
 5551970/6000000: episode: 6863, duration: 22.432s, episode steps: 1069, steps per second:  48, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.667 [0.000, 5.000],  loss: 0.013807, mae: 2.401441, mean_q: 2.893467, mean_eps: 0.100000
 5552762/6000000: episode: 6864, duration: 18.700s, episode steps: 792, steps per second:  42, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.944 [0.000, 5.000],  loss: 0.013247, mae: 2.406163, mean_q: 2.899353, mean_eps: 0.100000
 5553707/6000000: episode: 6865, duration: 20.568s, episode steps: 945, steps per second:  46, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.496 [0.000, 5.000],  loss: 0.013688, mae: 2.406009, mean_q: 2.897866, mean_eps: 0.100000
 5554434/6000000: episode: 6866, duration: 15.241s, episode steps: 727, steps per second:  48, episode reward: 21.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.110 [0.000, 5.000],  loss: 0.012543, mae: 2.369234, mean_q: 2.855267, mean_eps: 0.100000
 5555427/6000000: episode: 6867, duration: 21.511s, episode steps: 993, steps per second:  46, episode reward: 31.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.013469, mae: 2.404584, mean_q: 2.895489, mean_eps: 0.100000
 5556284/6000000: episode: 6868, duration: 21.267s, episode steps: 857, steps per second:  40, episode reward: 28.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.156 [0.000, 5.000],  loss: 0.014335, mae: 2.410171, mean_q: 2.903024, mean_eps: 0.100000
 5557509/6000000: episode: 6869, duration: 35.315s, episode steps: 1225, steps per second:  35, episode reward: 34.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.176 [0.000, 5.000],  loss: 0.013232, mae: 2.407932, mean_q: 2.900278, mean_eps: 0.100000
 5558040/6000000: episode: 6870, duration: 12.030s, episode steps: 531, steps per second:  44, episode reward: 17.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.154 [0.000, 5.000],  loss: 0.013174, mae: 2.426898, mean_q: 2.923252, mean_eps: 0.100000
 5558947/6000000: episode: 6871, duration: 18.840s, episode steps: 907, steps per second:  48, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.782 [0.000, 5.000],  loss: 0.013034, mae: 2.401440, mean_q: 2.891573, mean_eps: 0.100000
 5559508/6000000: episode: 6872, duration: 14.003s, episode steps: 561, steps per second:  40, episode reward: 17.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.959 [0.000, 5.000],  loss: 0.012577, mae: 2.393183, mean_q: 2.884316, mean_eps: 0.100000
 5560802/6000000: episode: 6873, duration: 28.769s, episode steps: 1294, steps per second:  45, episode reward: 32.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.961 [0.000, 5.000],  loss: 0.014403, mae: 2.412900, mean_q: 2.906722, mean_eps: 0.100000
 5561825/6000000: episode: 6874, duration: 22.220s, episode steps: 1023, steps per second:  46, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.928 [0.000, 5.000],  loss: 0.012310, mae: 2.390102, mean_q: 2.880802, mean_eps: 0.100000
 5562730/6000000: episode: 6875, duration: 19.252s, episode steps: 905, steps per second:  47, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.014290, mae: 2.413329, mean_q: 2.906909, mean_eps: 0.100000
 5563656/6000000: episode: 6876, duration: 20.009s, episode steps: 926, steps per second:  46, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.855 [0.000, 5.000],  loss: 0.013524, mae: 2.423662, mean_q: 2.919532, mean_eps: 0.100000
 5564879/6000000: episode: 6877, duration: 26.665s, episode steps: 1223, steps per second:  46, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.014161, mae: 2.400271, mean_q: 2.892199, mean_eps: 0.100000
 5565717/6000000: episode: 6878, duration: 18.331s, episode steps: 838, steps per second:  46, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.525 [0.000, 5.000],  loss: 0.014117, mae: 2.388302, mean_q: 2.879902, mean_eps: 0.100000
 5566405/6000000: episode: 6879, duration: 13.986s, episode steps: 688, steps per second:  49, episode reward: 21.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.802 [0.000, 5.000],  loss: 0.014332, mae: 2.411043, mean_q: 2.904693, mean_eps: 0.100000
 5567137/6000000: episode: 6880, duration: 14.779s, episode steps: 732, steps per second:  50, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.936 [0.000, 5.000],  loss: 0.011735, mae: 2.387644, mean_q: 2.878354, mean_eps: 0.100000
 5568077/6000000: episode: 6881, duration: 20.389s, episode steps: 940, steps per second:  46, episode reward: 28.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.100 [0.000, 5.000],  loss: 0.014026, mae: 2.399636, mean_q: 2.891979, mean_eps: 0.100000
 5568942/6000000: episode: 6882, duration: 18.412s, episode steps: 865, steps per second:  47, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.966 [0.000, 5.000],  loss: 0.014115, mae: 2.394317, mean_q: 2.885489, mean_eps: 0.100000
 5569627/6000000: episode: 6883, duration: 14.380s, episode steps: 685, steps per second:  48, episode reward: 19.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.086 [0.000, 5.000],  loss: 0.012863, mae: 2.384940, mean_q: 2.875157, mean_eps: 0.100000
 5570487/6000000: episode: 6884, duration: 17.735s, episode steps: 860, steps per second:  48, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.945 [0.000, 5.000],  loss: 0.013891, mae: 2.399652, mean_q: 2.890784, mean_eps: 0.100000
 5571487/6000000: episode: 6885, duration: 21.086s, episode steps: 1000, steps per second:  47, episode reward: 33.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 1.971 [0.000, 5.000],  loss: 0.013400, mae: 2.400133, mean_q: 2.892602, mean_eps: 0.100000
 5572265/6000000: episode: 6886, duration: 18.889s, episode steps: 778, steps per second:  41, episode reward: 26.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.022 [0.000, 5.000],  loss: 0.013305, mae: 2.375652, mean_q: 2.862595, mean_eps: 0.100000
 5573136/6000000: episode: 6887, duration: 21.704s, episode steps: 871, steps per second:  40, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.772 [0.000, 5.000],  loss: 0.014727, mae: 2.384654, mean_q: 2.872351, mean_eps: 0.100000
 5574102/6000000: episode: 6888, duration: 24.112s, episode steps: 966, steps per second:  40, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.735 [0.000, 5.000],  loss: 0.013326, mae: 2.389813, mean_q: 2.880168, mean_eps: 0.100000
 5574941/6000000: episode: 6889, duration: 18.401s, episode steps: 839, steps per second:  46, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.865 [0.000, 5.000],  loss: 0.013440, mae: 2.398241, mean_q: 2.888690, mean_eps: 0.100000
 5576046/6000000: episode: 6890, duration: 23.892s, episode steps: 1105, steps per second:  46, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.878 [0.000, 5.000],  loss: 0.014540, mae: 2.397547, mean_q: 2.887957, mean_eps: 0.100000
 5576806/6000000: episode: 6891, duration: 16.095s, episode steps: 760, steps per second:  47, episode reward: 21.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.554 [0.000, 5.000],  loss: 0.013787, mae: 2.384885, mean_q: 2.872680, mean_eps: 0.100000
 5577954/6000000: episode: 6892, duration: 23.507s, episode steps: 1148, steps per second:  49, episode reward: 31.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.226 [0.000, 5.000],  loss: 0.014344, mae: 2.391509, mean_q: 2.880520, mean_eps: 0.100000
 5578824/6000000: episode: 6893, duration: 18.938s, episode steps: 870, steps per second:  46, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.847 [0.000, 5.000],  loss: 0.013182, mae: 2.389630, mean_q: 2.879940, mean_eps: 0.100000
 5580028/6000000: episode: 6894, duration: 26.075s, episode steps: 1204, steps per second:  46, episode reward: 33.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.013879, mae: 2.383713, mean_q: 2.871133, mean_eps: 0.100000
 5580847/6000000: episode: 6895, duration: 17.635s, episode steps: 819, steps per second:  46, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.962 [0.000, 5.000],  loss: 0.013699, mae: 2.413207, mean_q: 2.909256, mean_eps: 0.100000
 5581712/6000000: episode: 6896, duration: 18.507s, episode steps: 865, steps per second:  47, episode reward: 27.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.200 [0.000, 5.000],  loss: 0.014463, mae: 2.413425, mean_q: 2.907455, mean_eps: 0.100000
 5583241/6000000: episode: 6897, duration: 31.152s, episode steps: 1529, steps per second:  49, episode reward: 34.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.013479, mae: 2.401057, mean_q: 2.893419, mean_eps: 0.100000
 5584487/6000000: episode: 6898, duration: 25.554s, episode steps: 1246, steps per second:  49, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.233 [0.000, 5.000],  loss: 0.013383, mae: 2.412197, mean_q: 2.905869, mean_eps: 0.100000
 5585670/6000000: episode: 6899, duration: 24.824s, episode steps: 1183, steps per second:  48, episode reward: 33.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.013921, mae: 2.432521, mean_q: 2.930536, mean_eps: 0.100000
 5586859/6000000: episode: 6900, duration: 26.838s, episode steps: 1189, steps per second:  44, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.892 [0.000, 5.000],  loss: 0.013368, mae: 2.440826, mean_q: 2.939544, mean_eps: 0.100000
 5587602/6000000: episode: 6901, duration: 16.349s, episode steps: 743, steps per second:  45, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.249 [0.000, 5.000],  loss: 0.015515, mae: 2.434806, mean_q: 2.932515, mean_eps: 0.100000
 5588634/6000000: episode: 6902, duration: 23.726s, episode steps: 1032, steps per second:  43, episode reward: 33.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.018 [0.000, 5.000],  loss: 0.014465, mae: 2.456124, mean_q: 2.958677, mean_eps: 0.100000
 5589472/6000000: episode: 6903, duration: 20.836s, episode steps: 838, steps per second:  40, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.641 [0.000, 5.000],  loss: 0.011909, mae: 2.420573, mean_q: 2.917171, mean_eps: 0.100000
 5590547/6000000: episode: 6904, duration: 25.295s, episode steps: 1075, steps per second:  42, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.970 [0.000, 5.000],  loss: 0.013627, mae: 2.429048, mean_q: 2.927239, mean_eps: 0.100000
 5591086/6000000: episode: 6905, duration: 12.091s, episode steps: 539, steps per second:  45, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.237 [0.000, 5.000],  loss: 0.013535, mae: 2.411921, mean_q: 2.905463, mean_eps: 0.100000
 5591852/6000000: episode: 6906, duration: 15.653s, episode steps: 766, steps per second:  49, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.161 [0.000, 5.000],  loss: 0.012790, mae: 2.418604, mean_q: 2.913450, mean_eps: 0.100000
 5592708/6000000: episode: 6907, duration: 18.267s, episode steps: 856, steps per second:  47, episode reward: 26.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.958 [0.000, 5.000],  loss: 0.013459, mae: 2.454726, mean_q: 2.956864, mean_eps: 0.100000
 5593577/6000000: episode: 6908, duration: 18.492s, episode steps: 869, steps per second:  47, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.586 [0.000, 5.000],  loss: 0.013484, mae: 2.424846, mean_q: 2.921121, mean_eps: 0.100000
 5594920/6000000: episode: 6909, duration: 28.255s, episode steps: 1343, steps per second:  48, episode reward: 31.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.014296, mae: 2.416218, mean_q: 2.910578, mean_eps: 0.100000
 5595608/6000000: episode: 6910, duration: 15.763s, episode steps: 688, steps per second:  44, episode reward: 20.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.785 [0.000, 5.000],  loss: 0.011740, mae: 2.420041, mean_q: 2.916960, mean_eps: 0.100000
 5596440/6000000: episode: 6911, duration: 19.139s, episode steps: 832, steps per second:  43, episode reward: 27.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 1.966 [0.000, 5.000],  loss: 0.014209, mae: 2.411646, mean_q: 2.905436, mean_eps: 0.100000
 5597323/6000000: episode: 6912, duration: 19.985s, episode steps: 883, steps per second:  44, episode reward: 28.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 1.578 [0.000, 5.000],  loss: 0.014549, mae: 2.405792, mean_q: 2.899440, mean_eps: 0.100000
 5598291/6000000: episode: 6913, duration: 21.090s, episode steps: 968, steps per second:  46, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.600 [0.000, 5.000],  loss: 0.014042, mae: 2.421234, mean_q: 2.917853, mean_eps: 0.100000
 5599311/6000000: episode: 6914, duration: 20.831s, episode steps: 1020, steps per second:  49, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.048 [0.000, 5.000],  loss: 0.013229, mae: 2.417736, mean_q: 2.913636, mean_eps: 0.100000
 5600217/6000000: episode: 6915, duration: 18.730s, episode steps: 906, steps per second:  48, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.776 [0.000, 5.000],  loss: 0.015081, mae: 2.416437, mean_q: 2.911470, mean_eps: 0.100000
 5600859/6000000: episode: 6916, duration: 13.894s, episode steps: 642, steps per second:  46, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.693 [0.000, 5.000],  loss: 0.011584, mae: 2.474178, mean_q: 2.984568, mean_eps: 0.100000
 5602090/6000000: episode: 6917, duration: 26.658s, episode steps: 1231, steps per second:  46, episode reward: 30.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.977 [0.000, 5.000],  loss: 0.014755, mae: 2.467653, mean_q: 2.973284, mean_eps: 0.100000
 5602586/6000000: episode: 6918, duration: 10.743s, episode steps: 496, steps per second:  46, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.212 [0.000, 5.000],  loss: 0.013755, mae: 2.441165, mean_q: 2.941457, mean_eps: 0.100000
 5603669/6000000: episode: 6919, duration: 23.759s, episode steps: 1083, steps per second:  46, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.013645, mae: 2.449039, mean_q: 2.950171, mean_eps: 0.100000
 5604880/6000000: episode: 6920, duration: 27.689s, episode steps: 1211, steps per second:  44, episode reward: 33.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.050 [0.000, 5.000],  loss: 0.013739, mae: 2.434436, mean_q: 2.932458, mean_eps: 0.100000
 5605670/6000000: episode: 6921, duration: 18.723s, episode steps: 790, steps per second:  42, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.039 [0.000, 5.000],  loss: 0.014428, mae: 2.489743, mean_q: 2.998967, mean_eps: 0.100000
 5606843/6000000: episode: 6922, duration: 27.769s, episode steps: 1173, steps per second:  42, episode reward: 33.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.014159, mae: 2.485484, mean_q: 2.994479, mean_eps: 0.100000
 5607728/6000000: episode: 6923, duration: 21.063s, episode steps: 885, steps per second:  42, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.128 [0.000, 5.000],  loss: 0.014051, mae: 2.465908, mean_q: 2.970147, mean_eps: 0.100000
 5609005/6000000: episode: 6924, duration: 29.475s, episode steps: 1277, steps per second:  43, episode reward: 33.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.036 [0.000, 5.000],  loss: 0.014196, mae: 2.471414, mean_q: 2.977433, mean_eps: 0.100000
 5609670/6000000: episode: 6925, duration: 14.795s, episode steps: 665, steps per second:  45, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.111 [0.000, 5.000],  loss: 0.014012, mae: 2.473740, mean_q: 2.980208, mean_eps: 0.100000
 5611150/6000000: episode: 6926, duration: 31.686s, episode steps: 1480, steps per second:  47, episode reward: 34.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.820 [0.000, 5.000],  loss: 0.012739, mae: 2.484258, mean_q: 2.993818, mean_eps: 0.100000
 5611886/6000000: episode: 6927, duration: 16.662s, episode steps: 736, steps per second:  44, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.912 [0.000, 5.000],  loss: 0.013046, mae: 2.452825, mean_q: 2.956314, mean_eps: 0.100000
 5612509/6000000: episode: 6928, duration: 14.035s, episode steps: 623, steps per second:  44, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.014707, mae: 2.471197, mean_q: 2.979157, mean_eps: 0.100000
 5613360/6000000: episode: 6929, duration: 18.532s, episode steps: 851, steps per second:  46, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.013549, mae: 2.478073, mean_q: 2.985687, mean_eps: 0.100000
 5614193/6000000: episode: 6930, duration: 18.065s, episode steps: 833, steps per second:  46, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.079 [0.000, 5.000],  loss: 0.013740, mae: 2.466198, mean_q: 2.971680, mean_eps: 0.100000
 5614752/6000000: episode: 6931, duration: 12.024s, episode steps: 559, steps per second:  46, episode reward: 14.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.826 [0.000, 5.000],  loss: 0.014251, mae: 2.468045, mean_q: 2.974561, mean_eps: 0.100000
 5615649/6000000: episode: 6932, duration: 18.412s, episode steps: 897, steps per second:  49, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.271 [0.000, 5.000],  loss: 0.013596, mae: 2.466259, mean_q: 2.970860, mean_eps: 0.100000
 5616550/6000000: episode: 6933, duration: 19.358s, episode steps: 901, steps per second:  47, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.700 [0.000, 5.000],  loss: 0.014231, mae: 2.496851, mean_q: 3.006827, mean_eps: 0.100000
 5617468/6000000: episode: 6934, duration: 20.791s, episode steps: 918, steps per second:  44, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.880 [0.000, 5.000],  loss: 0.014566, mae: 2.464928, mean_q: 2.969403, mean_eps: 0.100000
 5617994/6000000: episode: 6935, duration: 11.468s, episode steps: 526, steps per second:  46, episode reward: 16.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.224 [0.000, 5.000],  loss: 0.013117, mae: 2.456185, mean_q: 2.957721, mean_eps: 0.100000
 5618801/6000000: episode: 6936, duration: 17.317s, episode steps: 807, steps per second:  47, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.015370, mae: 2.469156, mean_q: 2.973991, mean_eps: 0.100000
 5619963/6000000: episode: 6937, duration: 24.260s, episode steps: 1162, steps per second:  48, episode reward: 31.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.907 [0.000, 5.000],  loss: 0.015287, mae: 2.448231, mean_q: 2.948451, mean_eps: 0.100000
 5620962/6000000: episode: 6938, duration: 21.751s, episode steps: 999, steps per second:  46, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.105 [0.000, 5.000],  loss: 0.013405, mae: 2.472600, mean_q: 2.977615, mean_eps: 0.100000
 5621666/6000000: episode: 6939, duration: 15.794s, episode steps: 704, steps per second:  45, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.972 [0.000, 5.000],  loss: 0.014853, mae: 2.484330, mean_q: 2.992061, mean_eps: 0.100000
 5622818/6000000: episode: 6940, duration: 27.179s, episode steps: 1152, steps per second:  42, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.952 [0.000, 5.000],  loss: 0.014749, mae: 2.472807, mean_q: 2.978946, mean_eps: 0.100000
 5623859/6000000: episode: 6941, duration: 23.114s, episode steps: 1041, steps per second:  45, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.633 [0.000, 5.000],  loss: 0.013643, mae: 2.472352, mean_q: 2.978365, mean_eps: 0.100000
 5624803/6000000: episode: 6942, duration: 20.439s, episode steps: 944, steps per second:  46, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.013476, mae: 2.472769, mean_q: 2.978624, mean_eps: 0.100000
 5625580/6000000: episode: 6943, duration: 17.039s, episode steps: 777, steps per second:  46, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.668 [0.000, 5.000],  loss: 0.013595, mae: 2.489258, mean_q: 2.998916, mean_eps: 0.100000
 5626443/6000000: episode: 6944, duration: 17.893s, episode steps: 863, steps per second:  48, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.090 [0.000, 5.000],  loss: 0.014518, mae: 2.480281, mean_q: 2.988439, mean_eps: 0.100000
 5627414/6000000: episode: 6945, duration: 20.159s, episode steps: 971, steps per second:  48, episode reward: 28.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.090 [0.000, 5.000],  loss: 0.014265, mae: 2.469359, mean_q: 2.975779, mean_eps: 0.100000
 5628482/6000000: episode: 6946, duration: 24.086s, episode steps: 1068, steps per second:  44, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.018 [0.000, 5.000],  loss: 0.012811, mae: 2.455682, mean_q: 2.958509, mean_eps: 0.100000
 5629731/6000000: episode: 6947, duration: 26.600s, episode steps: 1249, steps per second:  47, episode reward: 32.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.013317, mae: 2.468437, mean_q: 2.975218, mean_eps: 0.100000
 5630873/6000000: episode: 6948, duration: 25.008s, episode steps: 1142, steps per second:  46, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.005 [0.000, 5.000],  loss: 0.014531, mae: 2.513038, mean_q: 3.029709, mean_eps: 0.100000
 5632165/6000000: episode: 6949, duration: 26.744s, episode steps: 1292, steps per second:  48, episode reward: 33.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.874 [0.000, 5.000],  loss: 0.013710, mae: 2.499199, mean_q: 3.012822, mean_eps: 0.100000
 5632977/6000000: episode: 6950, duration: 16.528s, episode steps: 812, steps per second:  49, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.574 [0.000, 5.000],  loss: 0.013620, mae: 2.517287, mean_q: 3.032646, mean_eps: 0.100000
 5634038/6000000: episode: 6951, duration: 22.824s, episode steps: 1061, steps per second:  46, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.479 [0.000, 5.000],  loss: 0.015053, mae: 2.520540, mean_q: 3.037586, mean_eps: 0.100000
 5634787/6000000: episode: 6952, duration: 15.538s, episode steps: 749, steps per second:  48, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.300 [0.000, 5.000],  loss: 0.012584, mae: 2.497826, mean_q: 3.009301, mean_eps: 0.100000
 5635526/6000000: episode: 6953, duration: 15.646s, episode steps: 739, steps per second:  47, episode reward: 21.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.014259, mae: 2.514444, mean_q: 3.029711, mean_eps: 0.100000
 5636755/6000000: episode: 6954, duration: 26.814s, episode steps: 1229, steps per second:  46, episode reward: 34.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.954 [0.000, 5.000],  loss: 0.013897, mae: 2.497631, mean_q: 3.009665, mean_eps: 0.100000
 5637878/6000000: episode: 6955, duration: 25.122s, episode steps: 1123, steps per second:  45, episode reward: 34.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.676 [0.000, 5.000],  loss: 0.013146, mae: 2.513006, mean_q: 3.027434, mean_eps: 0.100000
 5638701/6000000: episode: 6956, duration: 17.912s, episode steps: 823, steps per second:  46, episode reward: 24.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.780 [0.000, 5.000],  loss: 0.014084, mae: 2.495072, mean_q: 3.006773, mean_eps: 0.100000
 5639590/6000000: episode: 6957, duration: 20.615s, episode steps: 889, steps per second:  43, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.858 [0.000, 5.000],  loss: 0.014109, mae: 2.499695, mean_q: 3.011779, mean_eps: 0.100000
 5640439/6000000: episode: 6958, duration: 18.672s, episode steps: 849, steps per second:  45, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.572 [0.000, 5.000],  loss: 0.012867, mae: 2.477283, mean_q: 2.985477, mean_eps: 0.100000
 5641290/6000000: episode: 6959, duration: 18.097s, episode steps: 851, steps per second:  47, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.013934, mae: 2.476596, mean_q: 2.984128, mean_eps: 0.100000
 5642347/6000000: episode: 6960, duration: 22.212s, episode steps: 1057, steps per second:  48, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.291 [0.000, 5.000],  loss: 0.014726, mae: 2.507294, mean_q: 3.020537, mean_eps: 0.100000
 5643334/6000000: episode: 6961, duration: 20.762s, episode steps: 987, steps per second:  48, episode reward: 29.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.065 [0.000, 5.000],  loss: 0.013404, mae: 2.489678, mean_q: 2.999651, mean_eps: 0.100000
 5644567/6000000: episode: 6962, duration: 26.503s, episode steps: 1233, steps per second:  47, episode reward: 33.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.260 [0.000, 5.000],  loss: 0.015142, mae: 2.492822, mean_q: 3.001935, mean_eps: 0.100000
 5645933/6000000: episode: 6963, duration: 30.091s, episode steps: 1366, steps per second:  45, episode reward: 31.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.978 [0.000, 5.000],  loss: 0.013567, mae: 2.482150, mean_q: 2.990728, mean_eps: 0.100000
 5646899/6000000: episode: 6964, duration: 21.430s, episode steps: 966, steps per second:  45, episode reward: 31.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 1.889 [0.000, 5.000],  loss: 0.014311, mae: 2.500501, mean_q: 3.012950, mean_eps: 0.100000
 5648317/6000000: episode: 6965, duration: 30.136s, episode steps: 1418, steps per second:  47, episode reward: 38.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.626 [0.000, 5.000],  loss: 0.014557, mae: 2.500824, mean_q: 3.014647, mean_eps: 0.100000
 5648870/6000000: episode: 6966, duration: 11.040s, episode steps: 553, steps per second:  50, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.727 [0.000, 5.000],  loss: 0.015049, mae: 2.476866, mean_q: 2.985585, mean_eps: 0.100000
 5649692/6000000: episode: 6967, duration: 16.791s, episode steps: 822, steps per second:  49, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.664 [0.000, 5.000],  loss: 0.014029, mae: 2.489760, mean_q: 3.001939, mean_eps: 0.100000
 5650620/6000000: episode: 6968, duration: 19.692s, episode steps: 928, steps per second:  47, episode reward: 28.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.357 [0.000, 5.000],  loss: 0.013747, mae: 2.494123, mean_q: 3.005946, mean_eps: 0.100000
 5651311/6000000: episode: 6969, duration: 15.084s, episode steps: 691, steps per second:  46, episode reward: 21.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 3.013 [0.000, 5.000],  loss: 0.012134, mae: 2.504850, mean_q: 3.018219, mean_eps: 0.100000
 5652150/6000000: episode: 6970, duration: 17.549s, episode steps: 839, steps per second:  48, episode reward: 26.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.013747, mae: 2.484446, mean_q: 2.992673, mean_eps: 0.100000
 5653197/6000000: episode: 6971, duration: 22.426s, episode steps: 1047, steps per second:  47, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.759 [0.000, 5.000],  loss: 0.015817, mae: 2.487804, mean_q: 2.997482, mean_eps: 0.100000
 5654298/6000000: episode: 6972, duration: 24.239s, episode steps: 1101, steps per second:  45, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.891 [0.000, 5.000],  loss: 0.014648, mae: 2.510569, mean_q: 3.022793, mean_eps: 0.100000
 5655288/6000000: episode: 6973, duration: 22.676s, episode steps: 990, steps per second:  44, episode reward: 29.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.544 [0.000, 5.000],  loss: 0.014012, mae: 2.494671, mean_q: 3.006410, mean_eps: 0.100000
 5656116/6000000: episode: 6974, duration: 19.271s, episode steps: 828, steps per second:  43, episode reward: 26.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.676 [0.000, 5.000],  loss: 0.014324, mae: 2.518524, mean_q: 3.034493, mean_eps: 0.100000
 5657024/6000000: episode: 6975, duration: 19.294s, episode steps: 908, steps per second:  47, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.789 [0.000, 5.000],  loss: 0.014544, mae: 2.514760, mean_q: 3.029970, mean_eps: 0.100000
 5657888/6000000: episode: 6976, duration: 18.614s, episode steps: 864, steps per second:  46, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.745 [0.000, 5.000],  loss: 0.013308, mae: 2.502742, mean_q: 3.016104, mean_eps: 0.100000
 5658675/6000000: episode: 6977, duration: 15.956s, episode steps: 787, steps per second:  49, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.066 [0.000, 5.000],  loss: 0.013147, mae: 2.521081, mean_q: 3.038615, mean_eps: 0.100000
 5659676/6000000: episode: 6978, duration: 21.412s, episode steps: 1001, steps per second:  47, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.015230, mae: 2.519958, mean_q: 3.035248, mean_eps: 0.100000
 5660889/6000000: episode: 6979, duration: 27.012s, episode steps: 1213, steps per second:  45, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.056 [0.000, 5.000],  loss: 0.014679, mae: 2.514428, mean_q: 3.031247, mean_eps: 0.100000
 5661763/6000000: episode: 6980, duration: 20.980s, episode steps: 874, steps per second:  42, episode reward: 28.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.047 [0.000, 5.000],  loss: 0.014230, mae: 2.499962, mean_q: 3.013265, mean_eps: 0.100000
 5662248/6000000: episode: 6981, duration: 10.708s, episode steps: 485, steps per second:  45, episode reward: 12.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.948 [0.000, 5.000],  loss: 0.014617, mae: 2.517705, mean_q: 3.034590, mean_eps: 0.100000
 5663301/6000000: episode: 6982, duration: 23.056s, episode steps: 1053, steps per second:  46, episode reward: 29.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.288 [0.000, 5.000],  loss: 0.014358, mae: 2.508776, mean_q: 3.022347, mean_eps: 0.100000
 5663945/6000000: episode: 6983, duration: 14.237s, episode steps: 644, steps per second:  45, episode reward: 19.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.014833, mae: 2.516144, mean_q: 3.031530, mean_eps: 0.100000
 5665208/6000000: episode: 6984, duration: 26.566s, episode steps: 1263, steps per second:  48, episode reward: 30.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.014014, mae: 2.510612, mean_q: 3.024772, mean_eps: 0.100000
 5666309/6000000: episode: 6985, duration: 22.691s, episode steps: 1101, steps per second:  49, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.013511, mae: 2.519283, mean_q: 3.035243, mean_eps: 0.100000
 5667498/6000000: episode: 6986, duration: 24.380s, episode steps: 1189, steps per second:  49, episode reward: 32.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.833 [0.000, 5.000],  loss: 0.013269, mae: 2.509605, mean_q: 3.023198, mean_eps: 0.100000
 5668243/6000000: episode: 6987, duration: 15.659s, episode steps: 745, steps per second:  48, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.862 [0.000, 5.000],  loss: 0.013984, mae: 2.535358, mean_q: 3.054037, mean_eps: 0.100000
 5669248/6000000: episode: 6988, duration: 20.264s, episode steps: 1005, steps per second:  50, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.334 [0.000, 5.000],  loss: 0.013385, mae: 2.514400, mean_q: 3.028464, mean_eps: 0.100000
 5669957/6000000: episode: 6989, duration: 15.210s, episode steps: 709, steps per second:  47, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.247 [0.000, 5.000],  loss: 0.014500, mae: 2.546766, mean_q: 3.066648, mean_eps: 0.100000
 5670864/6000000: episode: 6990, duration: 19.363s, episode steps: 907, steps per second:  47, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.053 [0.000, 5.000],  loss: 0.013613, mae: 2.522681, mean_q: 3.038479, mean_eps: 0.100000
 5671396/6000000: episode: 6991, duration: 11.583s, episode steps: 532, steps per second:  46, episode reward: 14.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.211 [0.000, 5.000],  loss: 0.013837, mae: 2.519647, mean_q: 3.035260, mean_eps: 0.100000
 5672001/6000000: episode: 6992, duration: 13.039s, episode steps: 605, steps per second:  46, episode reward: 19.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.014081, mae: 2.520263, mean_q: 3.036279, mean_eps: 0.100000
 5672649/6000000: episode: 6993, duration: 16.067s, episode steps: 648, steps per second:  40, episode reward: 19.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.981 [0.000, 5.000],  loss: 0.014979, mae: 2.534262, mean_q: 3.051426, mean_eps: 0.100000
 5673878/6000000: episode: 6994, duration: 27.550s, episode steps: 1229, steps per second:  45, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.932 [0.000, 5.000],  loss: 0.013021, mae: 2.517000, mean_q: 3.035634, mean_eps: 0.100000
 5674614/6000000: episode: 6995, duration: 16.154s, episode steps: 736, steps per second:  46, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.011893, mae: 2.499231, mean_q: 3.012200, mean_eps: 0.100000
 5676018/6000000: episode: 6996, duration: 30.943s, episode steps: 1404, steps per second:  45, episode reward: 35.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 0.013596, mae: 2.514295, mean_q: 3.029602, mean_eps: 0.100000
 5676875/6000000: episode: 6997, duration: 17.533s, episode steps: 857, steps per second:  49, episode reward: 26.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.823 [0.000, 5.000],  loss: 0.014651, mae: 2.508122, mean_q: 3.021934, mean_eps: 0.100000
 5677803/6000000: episode: 6998, duration: 20.656s, episode steps: 928, steps per second:  45, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.248 [0.000, 5.000],  loss: 0.013719, mae: 2.494792, mean_q: 3.005386, mean_eps: 0.100000
 5679068/6000000: episode: 6999, duration: 29.448s, episode steps: 1265, steps per second:  43, episode reward: 35.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.036 [0.000, 5.000],  loss: 0.012881, mae: 2.506594, mean_q: 3.020401, mean_eps: 0.100000
 5680183/6000000: episode: 7000, duration: 26.426s, episode steps: 1115, steps per second:  42, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.014899, mae: 2.516259, mean_q: 3.031136, mean_eps: 0.100000
 5681558/6000000: episode: 7001, duration: 30.891s, episode steps: 1375, steps per second:  45, episode reward: 36.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.852 [0.000, 5.000],  loss: 0.013223, mae: 2.466193, mean_q: 2.972458, mean_eps: 0.100000
 5682723/6000000: episode: 7002, duration: 23.812s, episode steps: 1165, steps per second:  49, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.013260, mae: 2.509048, mean_q: 3.023888, mean_eps: 0.100000
 5683636/6000000: episode: 7003, duration: 19.813s, episode steps: 913, steps per second:  46, episode reward: 32.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.014961, mae: 2.527426, mean_q: 3.045181, mean_eps: 0.100000
 5684441/6000000: episode: 7004, duration: 17.685s, episode steps: 805, steps per second:  46, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.829 [0.000, 5.000],  loss: 0.013726, mae: 2.512373, mean_q: 3.025518, mean_eps: 0.100000
 5685272/6000000: episode: 7005, duration: 17.979s, episode steps: 831, steps per second:  46, episode reward: 24.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.899 [0.000, 5.000],  loss: 0.013651, mae: 2.494777, mean_q: 3.005550, mean_eps: 0.100000
 5686080/6000000: episode: 7006, duration: 21.142s, episode steps: 808, steps per second:  38, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.257 [0.000, 5.000],  loss: 0.016559, mae: 2.487076, mean_q: 2.996908, mean_eps: 0.100000
 5687023/6000000: episode: 7007, duration: 20.591s, episode steps: 943, steps per second:  46, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.013498, mae: 2.495774, mean_q: 3.007974, mean_eps: 0.100000
 5687951/6000000: episode: 7008, duration: 20.270s, episode steps: 928, steps per second:  46, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.025 [0.000, 5.000],  loss: 0.014406, mae: 2.492235, mean_q: 3.002850, mean_eps: 0.100000
 5688866/6000000: episode: 7009, duration: 20.385s, episode steps: 915, steps per second:  45, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.978 [0.000, 5.000],  loss: 0.014622, mae: 2.486526, mean_q: 2.995474, mean_eps: 0.100000
 5689477/6000000: episode: 7010, duration: 13.344s, episode steps: 611, steps per second:  46, episode reward: 15.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.707 [0.000, 5.000],  loss: 0.014071, mae: 2.511310, mean_q: 3.024020, mean_eps: 0.100000
 5690316/6000000: episode: 7011, duration: 17.425s, episode steps: 839, steps per second:  48, episode reward: 25.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.013490, mae: 2.512666, mean_q: 3.027474, mean_eps: 0.100000
 5691434/6000000: episode: 7012, duration: 24.523s, episode steps: 1118, steps per second:  46, episode reward: 31.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.250 [0.000, 5.000],  loss: 0.014158, mae: 2.506984, mean_q: 3.019825, mean_eps: 0.100000
 5692316/6000000: episode: 7013, duration: 18.658s, episode steps: 882, steps per second:  47, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.175 [0.000, 5.000],  loss: 0.012603, mae: 2.525294, mean_q: 3.040981, mean_eps: 0.100000
 5693690/6000000: episode: 7014, duration: 28.422s, episode steps: 1374, steps per second:  48, episode reward: 35.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.212 [0.000, 5.000],  loss: 0.013787, mae: 2.495563, mean_q: 3.006595, mean_eps: 0.100000
 5694666/6000000: episode: 7015, duration: 20.787s, episode steps: 976, steps per second:  47, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.025 [0.000, 5.000],  loss: 0.014868, mae: 2.529323, mean_q: 3.046880, mean_eps: 0.100000
 5695712/6000000: episode: 7016, duration: 22.491s, episode steps: 1046, steps per second:  47, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.013909, mae: 2.509699, mean_q: 3.022025, mean_eps: 0.100000
 5696630/6000000: episode: 7017, duration: 19.719s, episode steps: 918, steps per second:  47, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 0.013597, mae: 2.483780, mean_q: 2.992622, mean_eps: 0.100000
 5697715/6000000: episode: 7018, duration: 23.261s, episode steps: 1085, steps per second:  47, episode reward: 33.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.014021, mae: 2.482921, mean_q: 2.989716, mean_eps: 0.100000
 5698333/6000000: episode: 7019, duration: 12.988s, episode steps: 618, steps per second:  48, episode reward: 20.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 1.811 [0.000, 5.000],  loss: 0.014680, mae: 2.483008, mean_q: 2.990436, mean_eps: 0.100000
 5699624/6000000: episode: 7020, duration: 26.714s, episode steps: 1291, steps per second:  48, episode reward: 32.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.080 [0.000, 5.000],  loss: 0.013695, mae: 2.501769, mean_q: 3.012945, mean_eps: 0.100000
 5701308/6000000: episode: 7021, duration: 36.312s, episode steps: 1684, steps per second:  46, episode reward: 41.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.014120, mae: 2.478825, mean_q: 2.985216, mean_eps: 0.100000
 5702090/6000000: episode: 7022, duration: 16.984s, episode steps: 782, steps per second:  46, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.884 [0.000, 5.000],  loss: 0.015347, mae: 2.470845, mean_q: 2.976649, mean_eps: 0.100000
 5703131/6000000: episode: 7023, duration: 22.467s, episode steps: 1041, steps per second:  46, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.009 [0.000, 5.000],  loss: 0.013985, mae: 2.456712, mean_q: 2.960143, mean_eps: 0.100000
 5704141/6000000: episode: 7024, duration: 22.877s, episode steps: 1010, steps per second:  44, episode reward: 30.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.822 [0.000, 5.000],  loss: 0.013974, mae: 2.439054, mean_q: 2.938470, mean_eps: 0.100000
 5704978/6000000: episode: 7025, duration: 18.934s, episode steps: 837, steps per second:  44, episode reward: 24.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.748 [0.000, 5.000],  loss: 0.014167, mae: 2.445933, mean_q: 2.946443, mean_eps: 0.100000
 5705937/6000000: episode: 7026, duration: 23.067s, episode steps: 959, steps per second:  42, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.082 [0.000, 5.000],  loss: 0.013451, mae: 2.449319, mean_q: 2.951635, mean_eps: 0.100000
 5707122/6000000: episode: 7027, duration: 26.455s, episode steps: 1185, steps per second:  45, episode reward: 32.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.824 [0.000, 5.000],  loss: 0.013863, mae: 2.451968, mean_q: 2.955221, mean_eps: 0.100000
 5708092/6000000: episode: 7028, duration: 21.314s, episode steps: 970, steps per second:  46, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.014566, mae: 2.455905, mean_q: 2.959812, mean_eps: 0.100000
 5708977/6000000: episode: 7029, duration: 18.912s, episode steps: 885, steps per second:  47, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.014959, mae: 2.450439, mean_q: 2.951092, mean_eps: 0.100000
 5709808/6000000: episode: 7030, duration: 17.438s, episode steps: 831, steps per second:  48, episode reward: 26.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.013402, mae: 2.469382, mean_q: 2.976618, mean_eps: 0.100000
 5711142/6000000: episode: 7031, duration: 29.989s, episode steps: 1334, steps per second:  44, episode reward: 33.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.012424, mae: 2.448190, mean_q: 2.951511, mean_eps: 0.100000
 5711911/6000000: episode: 7032, duration: 17.263s, episode steps: 769, steps per second:  45, episode reward: 22.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.796 [0.000, 5.000],  loss: 0.016036, mae: 2.446859, mean_q: 2.947686, mean_eps: 0.100000
 5712945/6000000: episode: 7033, duration: 23.040s, episode steps: 1034, steps per second:  45, episode reward: 32.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.024 [0.000, 5.000],  loss: 0.014298, mae: 2.433717, mean_q: 2.934137, mean_eps: 0.100000
 5714035/6000000: episode: 7034, duration: 24.685s, episode steps: 1090, steps per second:  44, episode reward: 31.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.207 [0.000, 5.000],  loss: 0.014806, mae: 2.454495, mean_q: 2.956216, mean_eps: 0.100000
 5714821/6000000: episode: 7035, duration: 16.381s, episode steps: 786, steps per second:  48, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.648 [0.000, 5.000],  loss: 0.013795, mae: 2.463515, mean_q: 2.968055, mean_eps: 0.100000
 5715672/6000000: episode: 7036, duration: 17.238s, episode steps: 851, steps per second:  49, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.012914, mae: 2.451063, mean_q: 2.953677, mean_eps: 0.100000
 5716783/6000000: episode: 7037, duration: 23.007s, episode steps: 1111, steps per second:  48, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.014528, mae: 2.443709, mean_q: 2.945685, mean_eps: 0.100000
 5718091/6000000: episode: 7038, duration: 27.586s, episode steps: 1308, steps per second:  47, episode reward: 28.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.014509, mae: 2.433630, mean_q: 2.932325, mean_eps: 0.100000
 5718862/6000000: episode: 7039, duration: 16.639s, episode steps: 771, steps per second:  46, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.261 [0.000, 5.000],  loss: 0.013605, mae: 2.428686, mean_q: 2.926855, mean_eps: 0.100000
 5719831/6000000: episode: 7040, duration: 20.282s, episode steps: 969, steps per second:  48, episode reward: 32.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.013656, mae: 2.429021, mean_q: 2.927543, mean_eps: 0.100000
 5720933/6000000: episode: 7041, duration: 25.268s, episode steps: 1102, steps per second:  44, episode reward: 33.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.173 [0.000, 5.000],  loss: 0.013556, mae: 2.456631, mean_q: 2.959234, mean_eps: 0.100000
 5721823/6000000: episode: 7042, duration: 20.612s, episode steps: 890, steps per second:  43, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.013975, mae: 2.443143, mean_q: 2.941313, mean_eps: 0.100000
 5723034/6000000: episode: 7043, duration: 26.515s, episode steps: 1211, steps per second:  46, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.015178, mae: 2.431503, mean_q: 2.928004, mean_eps: 0.100000
 5724075/6000000: episode: 7044, duration: 22.360s, episode steps: 1041, steps per second:  47, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.014475, mae: 2.435581, mean_q: 2.933273, mean_eps: 0.100000
 5725463/6000000: episode: 7045, duration: 29.380s, episode steps: 1388, steps per second:  47, episode reward: 33.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.089 [0.000, 5.000],  loss: 0.014659, mae: 2.457812, mean_q: 2.960881, mean_eps: 0.100000
 5726241/6000000: episode: 7046, duration: 15.967s, episode steps: 778, steps per second:  49, episode reward: 24.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.769 [0.000, 5.000],  loss: 0.014098, mae: 2.449347, mean_q: 2.951101, mean_eps: 0.100000
 5727100/6000000: episode: 7047, duration: 19.538s, episode steps: 859, steps per second:  44, episode reward: 26.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.014878, mae: 2.436829, mean_q: 2.935470, mean_eps: 0.100000
 5727914/6000000: episode: 7048, duration: 38.743s, episode steps: 814, steps per second:  21, episode reward: 24.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.885 [0.000, 5.000],  loss: 0.013511, mae: 2.464952, mean_q: 2.969706, mean_eps: 0.100000
 5728487/6000000: episode: 7049, duration: 31.175s, episode steps: 573, steps per second:  18, episode reward: 16.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.862 [0.000, 5.000],  loss: 0.014868, mae: 2.448557, mean_q: 2.949744, mean_eps: 0.100000
 5729506/6000000: episode: 7050, duration: 53.167s, episode steps: 1019, steps per second:  19, episode reward: 31.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.751 [0.000, 5.000],  loss: 0.014719, mae: 2.445467, mean_q: 2.945573, mean_eps: 0.100000
 5730293/6000000: episode: 7051, duration: 41.825s, episode steps: 787, steps per second:  19, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.014229, mae: 2.457432, mean_q: 2.960095, mean_eps: 0.100000
 5731247/6000000: episode: 7052, duration: 60.380s, episode steps: 954, steps per second:  16, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.014112, mae: 2.456529, mean_q: 2.958984, mean_eps: 0.100000
 5732825/6000000: episode: 7053, duration: 85.274s, episode steps: 1578, steps per second:  19, episode reward: 33.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.255 [0.000, 5.000],  loss: 0.013745, mae: 2.463762, mean_q: 2.968455, mean_eps: 0.100000
 5733637/6000000: episode: 7054, duration: 44.540s, episode steps: 812, steps per second:  18, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.076 [0.000, 5.000],  loss: 0.014458, mae: 2.486081, mean_q: 2.993898, mean_eps: 0.100000
 5734369/6000000: episode: 7055, duration: 41.377s, episode steps: 732, steps per second:  18, episode reward: 22.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.194 [0.000, 5.000],  loss: 0.014695, mae: 2.437271, mean_q: 2.935394, mean_eps: 0.100000
 5735096/6000000: episode: 7056, duration: 40.855s, episode steps: 727, steps per second:  18, episode reward: 21.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.015050, mae: 2.447223, mean_q: 2.946404, mean_eps: 0.100000
 5735646/6000000: episode: 7057, duration: 30.227s, episode steps: 550, steps per second:  18, episode reward: 15.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.118 [0.000, 5.000],  loss: 0.013094, mae: 2.478104, mean_q: 2.985623, mean_eps: 0.100000
 5736309/6000000: episode: 7058, duration: 35.503s, episode steps: 663, steps per second:  19, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.013365, mae: 2.481430, mean_q: 2.989676, mean_eps: 0.100000
 5737563/6000000: episode: 7059, duration: 69.085s, episode steps: 1254, steps per second:  18, episode reward: 33.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.266 [0.000, 5.000],  loss: 0.013270, mae: 2.459990, mean_q: 2.964510, mean_eps: 0.100000
 5738453/6000000: episode: 7060, duration: 50.411s, episode steps: 890, steps per second:  18, episode reward: 29.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.107 [0.000, 5.000],  loss: 0.014859, mae: 2.472602, mean_q: 2.979326, mean_eps: 0.100000
 5739676/6000000: episode: 7061, duration: 65.406s, episode steps: 1223, steps per second:  19, episode reward: 35.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.972 [0.000, 5.000],  loss: 0.013669, mae: 2.466320, mean_q: 2.972441, mean_eps: 0.100000
 5740498/6000000: episode: 7062, duration: 43.248s, episode steps: 822, steps per second:  19, episode reward: 24.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.855 [0.000, 5.000],  loss: 0.014478, mae: 2.454178, mean_q: 2.956877, mean_eps: 0.100000
 5741303/6000000: episode: 7063, duration: 42.321s, episode steps: 805, steps per second:  19, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.678 [0.000, 5.000],  loss: 0.016026, mae: 2.447917, mean_q: 2.948921, mean_eps: 0.100000
 5742765/6000000: episode: 7064, duration: 83.414s, episode steps: 1462, steps per second:  18, episode reward: 34.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.992 [0.000, 5.000],  loss: 0.014014, mae: 2.447452, mean_q: 2.949374, mean_eps: 0.100000
 5743616/6000000: episode: 7065, duration: 30.129s, episode steps: 851, steps per second:  28, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.927 [0.000, 5.000],  loss: 0.015640, mae: 2.452110, mean_q: 2.955001, mean_eps: 0.100000
 5744766/6000000: episode: 7066, duration: 24.736s, episode steps: 1150, steps per second:  46, episode reward: 33.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.137 [0.000, 5.000],  loss: 0.013959, mae: 2.442695, mean_q: 2.942417, mean_eps: 0.100000
 5745919/6000000: episode: 7067, duration: 24.547s, episode steps: 1153, steps per second:  47, episode reward: 32.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.677 [0.000, 5.000],  loss: 0.013696, mae: 2.443843, mean_q: 2.945543, mean_eps: 0.100000
 5746805/6000000: episode: 7068, duration: 19.462s, episode steps: 886, steps per second:  46, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.013474, mae: 2.452943, mean_q: 2.954729, mean_eps: 0.100000
 5747952/6000000: episode: 7069, duration: 26.121s, episode steps: 1147, steps per second:  44, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.018 [0.000, 5.000],  loss: 0.015197, mae: 2.448689, mean_q: 2.950747, mean_eps: 0.100000
 5748877/6000000: episode: 7070, duration: 19.828s, episode steps: 925, steps per second:  47, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.936 [0.000, 5.000],  loss: 0.013518, mae: 2.432040, mean_q: 2.929766, mean_eps: 0.100000
 5749673/6000000: episode: 7071, duration: 17.252s, episode steps: 796, steps per second:  46, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.905 [0.000, 5.000],  loss: 0.012687, mae: 2.434769, mean_q: 2.933825, mean_eps: 0.100000
 5750807/6000000: episode: 7072, duration: 23.749s, episode steps: 1134, steps per second:  48, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.012792, mae: 2.455569, mean_q: 2.959016, mean_eps: 0.100000
 5751990/6000000: episode: 7073, duration: 36.641s, episode steps: 1183, steps per second:  32, episode reward: 33.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.012539, mae: 2.474410, mean_q: 2.982177, mean_eps: 0.100000
 5752784/6000000: episode: 7074, duration: 21.476s, episode steps: 794, steps per second:  37, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.015127, mae: 2.492695, mean_q: 3.001292, mean_eps: 0.100000
 5753661/6000000: episode: 7075, duration: 22.811s, episode steps: 877, steps per second:  38, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.990 [0.000, 5.000],  loss: 0.013585, mae: 2.474831, mean_q: 2.980886, mean_eps: 0.100000
 5754452/6000000: episode: 7076, duration: 23.052s, episode steps: 791, steps per second:  34, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.965 [0.000, 5.000],  loss: 0.014465, mae: 2.461202, mean_q: 2.965355, mean_eps: 0.100000
 5755210/6000000: episode: 7077, duration: 20.651s, episode steps: 758, steps per second:  37, episode reward: 24.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.715 [0.000, 5.000],  loss: 0.015436, mae: 2.465519, mean_q: 2.968746, mean_eps: 0.100000
 5756254/6000000: episode: 7078, duration: 24.609s, episode steps: 1044, steps per second:  42, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.014626, mae: 2.504946, mean_q: 3.017226, mean_eps: 0.100000
 5757212/6000000: episode: 7079, duration: 26.194s, episode steps: 958, steps per second:  37, episode reward: 30.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.609 [0.000, 5.000],  loss: 0.013718, mae: 2.483844, mean_q: 2.992499, mean_eps: 0.100000
 5757709/6000000: episode: 7080, duration: 11.664s, episode steps: 497, steps per second:  43, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.014362, mae: 2.484356, mean_q: 2.993882, mean_eps: 0.100000
 5758700/6000000: episode: 7081, duration: 23.789s, episode steps: 991, steps per second:  42, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.878 [0.000, 5.000],  loss: 0.014210, mae: 2.476754, mean_q: 2.984967, mean_eps: 0.100000
 5759267/6000000: episode: 7082, duration: 13.735s, episode steps: 567, steps per second:  41, episode reward: 16.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.194 [0.000, 5.000],  loss: 0.014491, mae: 2.476224, mean_q: 2.983277, mean_eps: 0.100000
 5760494/6000000: episode: 7083, duration: 35.609s, episode steps: 1227, steps per second:  34, episode reward: 33.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.015169, mae: 2.483797, mean_q: 2.991439, mean_eps: 0.100000
 5761346/6000000: episode: 7084, duration: 26.422s, episode steps: 852, steps per second:  32, episode reward: 26.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.028 [0.000, 5.000],  loss: 0.014927, mae: 2.509482, mean_q: 3.023228, mean_eps: 0.100000
 5762256/6000000: episode: 7085, duration: 26.912s, episode steps: 910, steps per second:  34, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.654 [0.000, 5.000],  loss: 0.014784, mae: 2.498118, mean_q: 3.009934, mean_eps: 0.100000
 5763485/6000000: episode: 7086, duration: 35.082s, episode steps: 1229, steps per second:  35, episode reward: 33.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.958 [0.000, 5.000],  loss: 0.014022, mae: 2.483885, mean_q: 2.992596, mean_eps: 0.100000
 5764661/6000000: episode: 7087, duration: 33.971s, episode steps: 1176, steps per second:  35, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.945 [0.000, 5.000],  loss: 0.014214, mae: 2.500322, mean_q: 3.013159, mean_eps: 0.100000
 5765499/6000000: episode: 7088, duration: 21.923s, episode steps: 838, steps per second:  38, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.013906, mae: 2.478336, mean_q: 2.988047, mean_eps: 0.100000
 5766520/6000000: episode: 7089, duration: 28.482s, episode steps: 1021, steps per second:  36, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.014643, mae: 2.488938, mean_q: 2.999128, mean_eps: 0.100000
 5767758/6000000: episode: 7090, duration: 36.193s, episode steps: 1238, steps per second:  34, episode reward: 32.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.015364, mae: 2.494916, mean_q: 3.004550, mean_eps: 0.100000
 5769125/6000000: episode: 7091, duration: 37.655s, episode steps: 1367, steps per second:  36, episode reward: 35.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.637 [0.000, 5.000],  loss: 0.013915, mae: 2.483453, mean_q: 2.992873, mean_eps: 0.100000
 5769982/6000000: episode: 7092, duration: 22.706s, episode steps: 857, steps per second:  38, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.187 [0.000, 5.000],  loss: 0.014016, mae: 2.477779, mean_q: 2.986646, mean_eps: 0.100000
 5771175/6000000: episode: 7093, duration: 35.444s, episode steps: 1193, steps per second:  34, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.691 [0.000, 5.000],  loss: 0.015477, mae: 2.487790, mean_q: 2.997113, mean_eps: 0.100000
 5772004/6000000: episode: 7094, duration: 20.882s, episode steps: 829, steps per second:  40, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.288 [0.000, 5.000],  loss: 0.014180, mae: 2.506609, mean_q: 3.018686, mean_eps: 0.100000
 5772867/6000000: episode: 7095, duration: 29.951s, episode steps: 863, steps per second:  29, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.920 [0.000, 5.000],  loss: 0.013564, mae: 2.507458, mean_q: 3.021217, mean_eps: 0.100000
 5773610/6000000: episode: 7096, duration: 30.925s, episode steps: 743, steps per second:  24, episode reward: 21.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.013687, mae: 2.488816, mean_q: 2.999518, mean_eps: 0.100000
 5774704/6000000: episode: 7097, duration: 52.161s, episode steps: 1094, steps per second:  21, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.898 [0.000, 5.000],  loss: 0.014819, mae: 2.504785, mean_q: 3.017802, mean_eps: 0.100000
 5775399/6000000: episode: 7098, duration: 38.483s, episode steps: 695, steps per second:  18, episode reward: 22.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 1.846 [0.000, 5.000],  loss: 0.013126, mae: 2.489837, mean_q: 3.000735, mean_eps: 0.100000
 5776499/6000000: episode: 7099, duration: 56.917s, episode steps: 1100, steps per second:  19, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.821 [0.000, 5.000],  loss: 0.013141, mae: 2.503205, mean_q: 3.016101, mean_eps: 0.100000
 5777510/6000000: episode: 7100, duration: 52.869s, episode steps: 1011, steps per second:  19, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.183 [0.000, 5.000],  loss: 0.012614, mae: 2.481793, mean_q: 2.990087, mean_eps: 0.100000
 5778623/6000000: episode: 7101, duration: 51.901s, episode steps: 1113, steps per second:  21, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.227 [0.000, 5.000],  loss: 0.013510, mae: 2.487759, mean_q: 2.997230, mean_eps: 0.100000
 5779744/6000000: episode: 7102, duration: 30.186s, episode steps: 1121, steps per second:  37, episode reward: 31.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.918 [0.000, 5.000],  loss: 0.015044, mae: 2.511059, mean_q: 3.023885, mean_eps: 0.100000
 5780991/6000000: episode: 7103, duration: 26.277s, episode steps: 1247, steps per second:  47, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.275 [0.000, 5.000],  loss: 0.013889, mae: 2.512091, mean_q: 3.027524, mean_eps: 0.100000
 5782261/6000000: episode: 7104, duration: 28.201s, episode steps: 1270, steps per second:  45, episode reward: 30.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.124 [0.000, 5.000],  loss: 0.013864, mae: 2.498140, mean_q: 3.009388, mean_eps: 0.100000
 5783194/6000000: episode: 7105, duration: 21.799s, episode steps: 933, steps per second:  43, episode reward: 30.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.013410, mae: 2.510760, mean_q: 3.025387, mean_eps: 0.100000
 5784160/6000000: episode: 7106, duration: 20.982s, episode steps: 966, steps per second:  46, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.962 [0.000, 5.000],  loss: 0.016095, mae: 2.518456, mean_q: 3.034111, mean_eps: 0.100000
 5785080/6000000: episode: 7107, duration: 19.750s, episode steps: 920, steps per second:  47, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.908 [0.000, 5.000],  loss: 0.016157, mae: 2.524097, mean_q: 3.040824, mean_eps: 0.100000
 5786175/6000000: episode: 7108, duration: 23.471s, episode steps: 1095, steps per second:  47, episode reward: 33.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.995 [0.000, 5.000],  loss: 0.014269, mae: 2.533112, mean_q: 3.050294, mean_eps: 0.100000
 5787223/6000000: episode: 7109, duration: 22.289s, episode steps: 1048, steps per second:  47, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.106 [0.000, 5.000],  loss: 0.014630, mae: 2.531691, mean_q: 3.050912, mean_eps: 0.100000
 5788152/6000000: episode: 7110, duration: 19.458s, episode steps: 929, steps per second:  48, episode reward: 28.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.704 [0.000, 5.000],  loss: 0.014529, mae: 2.510137, mean_q: 3.023843, mean_eps: 0.100000
 5789003/6000000: episode: 7111, duration: 18.632s, episode steps: 851, steps per second:  46, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.814 [0.000, 5.000],  loss: 0.015474, mae: 2.502649, mean_q: 3.013775, mean_eps: 0.100000
 5790278/6000000: episode: 7112, duration: 27.587s, episode steps: 1275, steps per second:  46, episode reward: 34.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.672 [0.000, 5.000],  loss: 0.013731, mae: 2.524864, mean_q: 3.042813, mean_eps: 0.100000
 5791064/6000000: episode: 7113, duration: 16.891s, episode steps: 786, steps per second:  47, episode reward: 24.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.014059, mae: 2.516218, mean_q: 3.031024, mean_eps: 0.100000
 5792172/6000000: episode: 7114, duration: 24.842s, episode steps: 1108, steps per second:  45, episode reward: 31.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.739 [0.000, 5.000],  loss: 0.014196, mae: 2.528431, mean_q: 3.046128, mean_eps: 0.100000
 5793281/6000000: episode: 7115, duration: 22.587s, episode steps: 1109, steps per second:  49, episode reward: 30.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.824 [0.000, 5.000],  loss: 0.014077, mae: 2.537750, mean_q: 3.057540, mean_eps: 0.100000
 5793973/6000000: episode: 7116, duration: 14.562s, episode steps: 692, steps per second:  48, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.012 [0.000, 5.000],  loss: 0.015366, mae: 2.526358, mean_q: 3.042017, mean_eps: 0.100000
 5794786/6000000: episode: 7117, duration: 17.547s, episode steps: 813, steps per second:  46, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.134 [0.000, 5.000],  loss: 0.016165, mae: 2.539102, mean_q: 3.057151, mean_eps: 0.100000
 5795879/6000000: episode: 7118, duration: 22.313s, episode steps: 1093, steps per second:  49, episode reward: 30.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.728 [0.000, 5.000],  loss: 0.014065, mae: 2.526405, mean_q: 3.043916, mean_eps: 0.100000
 5796793/6000000: episode: 7119, duration: 19.028s, episode steps: 914, steps per second:  48, episode reward: 28.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.598 [0.000, 5.000],  loss: 0.014510, mae: 2.528896, mean_q: 3.046916, mean_eps: 0.100000
 5797611/6000000: episode: 7120, duration: 16.346s, episode steps: 818, steps per second:  50, episode reward: 28.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 1.980 [0.000, 5.000],  loss: 0.014435, mae: 2.551009, mean_q: 3.073281, mean_eps: 0.100000
 5798636/6000000: episode: 7121, duration: 22.661s, episode steps: 1025, steps per second:  45, episode reward: 30.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: 0.014929, mae: 2.532332, mean_q: 3.050159, mean_eps: 0.100000
 5799497/6000000: episode: 7122, duration: 19.974s, episode steps: 861, steps per second:  43, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.307 [0.000, 5.000],  loss: 0.015413, mae: 2.534124, mean_q: 3.052440, mean_eps: 0.100000
 5800348/6000000: episode: 7123, duration: 23.251s, episode steps: 851, steps per second:  37, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.013514, mae: 2.523771, mean_q: 3.040520, mean_eps: 0.100000
 5801850/6000000: episode: 7124, duration: 31.764s, episode steps: 1502, steps per second:  47, episode reward: 34.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.963 [0.000, 5.000],  loss: 0.014773, mae: 2.529423, mean_q: 3.047030, mean_eps: 0.100000
 5803099/6000000: episode: 7125, duration: 26.892s, episode steps: 1249, steps per second:  46, episode reward: 36.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.157 [0.000, 5.000],  loss: 0.014018, mae: 2.511442, mean_q: 3.025288, mean_eps: 0.100000
 5803932/6000000: episode: 7126, duration: 17.748s, episode steps: 833, steps per second:  47, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.118 [0.000, 5.000],  loss: 0.014177, mae: 2.497893, mean_q: 3.009188, mean_eps: 0.100000
 5804759/6000000: episode: 7127, duration: 18.290s, episode steps: 827, steps per second:  45, episode reward: 24.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.014077, mae: 2.515892, mean_q: 3.030387, mean_eps: 0.100000
 5805670/6000000: episode: 7128, duration: 22.523s, episode steps: 911, steps per second:  40, episode reward: 28.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.014032, mae: 2.521587, mean_q: 3.036323, mean_eps: 0.100000
 5806462/6000000: episode: 7129, duration: 16.978s, episode steps: 792, steps per second:  47, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.053 [0.000, 5.000],  loss: 0.013696, mae: 2.523092, mean_q: 3.039839, mean_eps: 0.100000
 5807466/6000000: episode: 7130, duration: 21.894s, episode steps: 1004, steps per second:  46, episode reward: 31.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.069 [0.000, 5.000],  loss: 0.014001, mae: 2.518037, mean_q: 3.033353, mean_eps: 0.100000
 5808317/6000000: episode: 7131, duration: 18.934s, episode steps: 851, steps per second:  45, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.853 [0.000, 5.000],  loss: 0.013967, mae: 2.514021, mean_q: 3.029144, mean_eps: 0.100000
 5809390/6000000: episode: 7132, duration: 22.244s, episode steps: 1073, steps per second:  48, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.183 [0.000, 5.000],  loss: 0.013548, mae: 2.506200, mean_q: 3.018732, mean_eps: 0.100000
 5810412/6000000: episode: 7133, duration: 21.470s, episode steps: 1022, steps per second:  48, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.104 [0.000, 5.000],  loss: 0.013341, mae: 2.532725, mean_q: 3.051484, mean_eps: 0.100000
 5811237/6000000: episode: 7134, duration: 17.603s, episode steps: 825, steps per second:  47, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.973 [0.000, 5.000],  loss: 0.013592, mae: 2.527173, mean_q: 3.045253, mean_eps: 0.100000
 5812069/6000000: episode: 7135, duration: 17.615s, episode steps: 832, steps per second:  47, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.749 [0.000, 5.000],  loss: 0.012670, mae: 2.519587, mean_q: 3.033679, mean_eps: 0.100000
 5812810/6000000: episode: 7136, duration: 15.511s, episode steps: 741, steps per second:  48, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.263 [0.000, 5.000],  loss: 0.014100, mae: 2.518044, mean_q: 3.032075, mean_eps: 0.100000
 5813669/6000000: episode: 7137, duration: 18.446s, episode steps: 859, steps per second:  47, episode reward: 29.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.013496, mae: 2.532754, mean_q: 3.051351, mean_eps: 0.100000
 5815037/6000000: episode: 7138, duration: 30.744s, episode steps: 1368, steps per second:  44, episode reward: 31.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.026 [0.000, 5.000],  loss: 0.014433, mae: 2.518891, mean_q: 3.033473, mean_eps: 0.100000
 5815852/6000000: episode: 7139, duration: 19.459s, episode steps: 815, steps per second:  42, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.810 [0.000, 5.000],  loss: 0.014431, mae: 2.507309, mean_q: 3.019877, mean_eps: 0.100000
 5816377/6000000: episode: 7140, duration: 12.356s, episode steps: 525, steps per second:  42, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.476 [0.000, 5.000],  loss: 0.013079, mae: 2.489234, mean_q: 2.998283, mean_eps: 0.100000
 5817336/6000000: episode: 7141, duration: 23.283s, episode steps: 959, steps per second:  41, episode reward: 31.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.058 [0.000, 5.000],  loss: 0.014144, mae: 2.534445, mean_q: 3.052994, mean_eps: 0.100000
 5818413/6000000: episode: 7142, duration: 24.417s, episode steps: 1077, steps per second:  44, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.237 [0.000, 5.000],  loss: 0.013964, mae: 2.546033, mean_q: 3.066894, mean_eps: 0.100000
 5819427/6000000: episode: 7143, duration: 21.370s, episode steps: 1014, steps per second:  47, episode reward: 29.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.013674, mae: 2.523094, mean_q: 3.039113, mean_eps: 0.100000
 5820431/6000000: episode: 7144, duration: 20.642s, episode steps: 1004, steps per second:  49, episode reward: 30.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.714 [0.000, 5.000],  loss: 0.015714, mae: 2.535482, mean_q: 3.053451, mean_eps: 0.100000
 5821247/6000000: episode: 7145, duration: 18.128s, episode steps: 816, steps per second:  45, episode reward: 24.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.014364, mae: 2.525983, mean_q: 3.043224, mean_eps: 0.100000
 5821964/6000000: episode: 7146, duration: 16.011s, episode steps: 717, steps per second:  45, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.088 [0.000, 5.000],  loss: 0.015017, mae: 2.534035, mean_q: 3.051937, mean_eps: 0.100000
 5823013/6000000: episode: 7147, duration: 23.038s, episode steps: 1049, steps per second:  46, episode reward: 31.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.206 [0.000, 5.000],  loss: 0.015775, mae: 2.519200, mean_q: 3.033571, mean_eps: 0.100000
 5824672/6000000: episode: 7148, duration: 35.984s, episode steps: 1659, steps per second:  46, episode reward: 39.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.732 [0.000, 5.000],  loss: 0.014434, mae: 2.521568, mean_q: 3.036704, mean_eps: 0.100000
 5826121/6000000: episode: 7149, duration: 30.107s, episode steps: 1449, steps per second:  48, episode reward: 33.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.014238, mae: 2.539937, mean_q: 3.058734, mean_eps: 0.100000
 5826936/6000000: episode: 7150, duration: 16.945s, episode steps: 815, steps per second:  48, episode reward: 24.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.753 [0.000, 5.000],  loss: 0.015314, mae: 2.545706, mean_q: 3.065439, mean_eps: 0.100000
 5827768/6000000: episode: 7151, duration: 17.789s, episode steps: 832, steps per second:  47, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.014619, mae: 2.551326, mean_q: 3.073665, mean_eps: 0.100000
 5828989/6000000: episode: 7152, duration: 25.883s, episode steps: 1221, steps per second:  47, episode reward: 33.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.106 [0.000, 5.000],  loss: 0.014545, mae: 2.527497, mean_q: 3.045712, mean_eps: 0.100000
 5829614/6000000: episode: 7153, duration: 13.481s, episode steps: 625, steps per second:  46, episode reward: 19.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.013362, mae: 2.533536, mean_q: 3.052053, mean_eps: 0.100000
 5830613/6000000: episode: 7154, duration: 21.549s, episode steps: 999, steps per second:  46, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.690 [0.000, 5.000],  loss: 0.013589, mae: 2.526386, mean_q: 3.045238, mean_eps: 0.100000
 5831845/6000000: episode: 7155, duration: 27.550s, episode steps: 1232, steps per second:  45, episode reward: 33.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 0.013103, mae: 2.512068, mean_q: 3.026322, mean_eps: 0.100000
 5833007/6000000: episode: 7156, duration: 29.284s, episode steps: 1162, steps per second:  40, episode reward: 32.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.554 [0.000, 5.000],  loss: 0.014399, mae: 2.518239, mean_q: 3.035651, mean_eps: 0.100000
 5833927/6000000: episode: 7157, duration: 20.258s, episode steps: 920, steps per second:  45, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.245 [0.000, 5.000],  loss: 0.015486, mae: 2.521630, mean_q: 3.037735, mean_eps: 0.100000
 5834804/6000000: episode: 7158, duration: 18.689s, episode steps: 877, steps per second:  47, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.970 [0.000, 5.000],  loss: 0.014350, mae: 2.509457, mean_q: 3.023266, mean_eps: 0.100000
 5835801/6000000: episode: 7159, duration: 20.816s, episode steps: 997, steps per second:  48, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.940 [0.000, 5.000],  loss: 0.013907, mae: 2.518548, mean_q: 3.034072, mean_eps: 0.100000
 5836331/6000000: episode: 7160, duration: 10.987s, episode steps: 530, steps per second:  48, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.013672, mae: 2.504931, mean_q: 3.020431, mean_eps: 0.100000
 5837093/6000000: episode: 7161, duration: 16.343s, episode steps: 762, steps per second:  47, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.608 [0.000, 5.000],  loss: 0.015432, mae: 2.521681, mean_q: 3.039065, mean_eps: 0.100000
 5837923/6000000: episode: 7162, duration: 18.793s, episode steps: 830, steps per second:  44, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.141 [0.000, 5.000],  loss: 0.014601, mae: 2.502199, mean_q: 3.013575, mean_eps: 0.100000
 5839226/6000000: episode: 7163, duration: 31.122s, episode steps: 1303, steps per second:  42, episode reward: 32.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.122 [0.000, 5.000],  loss: 0.014196, mae: 2.519505, mean_q: 3.035928, mean_eps: 0.100000
 5840029/6000000: episode: 7164, duration: 18.143s, episode steps: 803, steps per second:  44, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.540 [0.000, 5.000],  loss: 0.013980, mae: 2.522944, mean_q: 3.040410, mean_eps: 0.100000
 5841106/6000000: episode: 7165, duration: 24.846s, episode steps: 1077, steps per second:  43, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.758 [0.000, 5.000],  loss: 0.013865, mae: 2.532433, mean_q: 3.049502, mean_eps: 0.100000
 5842393/6000000: episode: 7166, duration: 26.580s, episode steps: 1287, steps per second:  48, episode reward: 34.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.219 [0.000, 5.000],  loss: 0.014842, mae: 2.546319, mean_q: 3.065642, mean_eps: 0.100000
 5843335/6000000: episode: 7167, duration: 19.608s, episode steps: 942, steps per second:  48, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.001 [0.000, 5.000],  loss: 0.014843, mae: 2.533076, mean_q: 3.050265, mean_eps: 0.100000
 5844417/6000000: episode: 7168, duration: 23.554s, episode steps: 1082, steps per second:  46, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.164 [0.000, 5.000],  loss: 0.015848, mae: 2.550221, mean_q: 3.071130, mean_eps: 0.100000
 5845363/6000000: episode: 7169, duration: 19.933s, episode steps: 946, steps per second:  47, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.701 [0.000, 5.000],  loss: 0.013517, mae: 2.504010, mean_q: 3.017308, mean_eps: 0.100000
 5846295/6000000: episode: 7170, duration: 19.898s, episode steps: 932, steps per second:  47, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.013854, mae: 2.533365, mean_q: 3.051812, mean_eps: 0.100000
 5847450/6000000: episode: 7171, duration: 24.748s, episode steps: 1155, steps per second:  47, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.014571, mae: 2.513569, mean_q: 3.027211, mean_eps: 0.100000
 5848408/6000000: episode: 7172, duration: 21.191s, episode steps: 958, steps per second:  45, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.183 [0.000, 5.000],  loss: 0.012680, mae: 2.509401, mean_q: 3.022456, mean_eps: 0.100000
 5849320/6000000: episode: 7173, duration: 20.843s, episode steps: 912, steps per second:  44, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.013204, mae: 2.521692, mean_q: 3.038676, mean_eps: 0.100000
 5850454/6000000: episode: 7174, duration: 25.763s, episode steps: 1134, steps per second:  44, episode reward: 31.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.236 [0.000, 5.000],  loss: 0.013409, mae: 2.524903, mean_q: 3.041790, mean_eps: 0.100000
 5851515/6000000: episode: 7175, duration: 22.874s, episode steps: 1061, steps per second:  46, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.820 [0.000, 5.000],  loss: 0.014273, mae: 2.527814, mean_q: 3.045016, mean_eps: 0.100000
 5852332/6000000: episode: 7176, duration: 17.620s, episode steps: 817, steps per second:  46, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.020 [0.000, 5.000],  loss: 0.013799, mae: 2.531087, mean_q: 3.051000, mean_eps: 0.100000
 5853364/6000000: episode: 7177, duration: 22.295s, episode steps: 1032, steps per second:  46, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.016 [0.000, 5.000],  loss: 0.013418, mae: 2.514457, mean_q: 3.029331, mean_eps: 0.100000
 5854160/6000000: episode: 7178, duration: 17.623s, episode steps: 796, steps per second:  45, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.013524, mae: 2.498306, mean_q: 3.010580, mean_eps: 0.100000
 5855161/6000000: episode: 7179, duration: 22.764s, episode steps: 1001, steps per second:  44, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.014965, mae: 2.534714, mean_q: 3.053615, mean_eps: 0.100000
 5855988/6000000: episode: 7180, duration: 18.219s, episode steps: 827, steps per second:  45, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.014450, mae: 2.539207, mean_q: 3.056907, mean_eps: 0.100000
 5857195/6000000: episode: 7181, duration: 27.073s, episode steps: 1207, steps per second:  45, episode reward: 33.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.819 [0.000, 5.000],  loss: 0.014469, mae: 2.536281, mean_q: 3.054843, mean_eps: 0.100000
 5858318/6000000: episode: 7182, duration: 23.824s, episode steps: 1123, steps per second:  47, episode reward: 31.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.257 [0.000, 5.000],  loss: 0.014333, mae: 2.524808, mean_q: 3.039321, mean_eps: 0.100000
 5859359/6000000: episode: 7183, duration: 21.397s, episode steps: 1041, steps per second:  49, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.985 [0.000, 5.000],  loss: 0.013980, mae: 2.523266, mean_q: 3.038420, mean_eps: 0.100000
 5860230/6000000: episode: 7184, duration: 18.517s, episode steps: 871, steps per second:  47, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.923 [0.000, 5.000],  loss: 0.012549, mae: 2.513171, mean_q: 3.028360, mean_eps: 0.100000
 5861108/6000000: episode: 7185, duration: 18.457s, episode steps: 878, steps per second:  48, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.967 [0.000, 5.000],  loss: 0.013289, mae: 2.517423, mean_q: 3.032381, mean_eps: 0.100000
 5861925/6000000: episode: 7186, duration: 16.524s, episode steps: 817, steps per second:  49, episode reward: 25.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.237 [0.000, 5.000],  loss: 0.013584, mae: 2.505321, mean_q: 3.018343, mean_eps: 0.100000
 5862831/6000000: episode: 7187, duration: 18.765s, episode steps: 906, steps per second:  48, episode reward: 27.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.013437, mae: 2.500442, mean_q: 3.011941, mean_eps: 0.100000
 5863723/6000000: episode: 7188, duration: 18.569s, episode steps: 892, steps per second:  48, episode reward: 27.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.880 [0.000, 5.000],  loss: 0.013489, mae: 2.519198, mean_q: 3.033925, mean_eps: 0.100000
 5864670/6000000: episode: 7189, duration: 20.819s, episode steps: 947, steps per second:  45, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.146 [0.000, 5.000],  loss: 0.013790, mae: 2.538116, mean_q: 3.056830, mean_eps: 0.100000
 5865537/6000000: episode: 7190, duration: 20.761s, episode steps: 867, steps per second:  42, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.014577, mae: 2.534727, mean_q: 3.052855, mean_eps: 0.100000
 5866618/6000000: episode: 7191, duration: 24.995s, episode steps: 1081, steps per second:  43, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.013977, mae: 2.528267, mean_q: 3.045457, mean_eps: 0.100000
 5867312/6000000: episode: 7192, duration: 15.655s, episode steps: 694, steps per second:  44, episode reward: 21.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.833 [0.000, 5.000],  loss: 0.013807, mae: 2.518172, mean_q: 3.033290, mean_eps: 0.100000
 5868221/6000000: episode: 7193, duration: 19.964s, episode steps: 909, steps per second:  46, episode reward: 30.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.014399, mae: 2.508386, mean_q: 3.020193, mean_eps: 0.100000
 5869346/6000000: episode: 7194, duration: 24.271s, episode steps: 1125, steps per second:  46, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.014570, mae: 2.540923, mean_q: 3.061119, mean_eps: 0.100000
 5870318/6000000: episode: 7195, duration: 19.785s, episode steps: 972, steps per second:  49, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.650 [0.000, 5.000],  loss: 0.013395, mae: 2.534009, mean_q: 3.053323, mean_eps: 0.100000
 5870949/6000000: episode: 7196, duration: 13.914s, episode steps: 631, steps per second:  45, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.044 [0.000, 5.000],  loss: 0.012205, mae: 2.552337, mean_q: 3.073968, mean_eps: 0.100000
 5871723/6000000: episode: 7197, duration: 17.021s, episode steps: 774, steps per second:  45, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.014100, mae: 2.556364, mean_q: 3.078587, mean_eps: 0.100000
 5872305/6000000: episode: 7198, duration: 12.986s, episode steps: 582, steps per second:  45, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.012925, mae: 2.537152, mean_q: 3.057212, mean_eps: 0.100000
 5873095/6000000: episode: 7199, duration: 16.961s, episode steps: 790, steps per second:  47, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.014596, mae: 2.557164, mean_q: 3.080608, mean_eps: 0.100000
 5874410/6000000: episode: 7200, duration: 27.672s, episode steps: 1315, steps per second:  48, episode reward: 39.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.170 [0.000, 5.000],  loss: 0.014229, mae: 2.549587, mean_q: 3.071125, mean_eps: 0.100000
 5875059/6000000: episode: 7201, duration: 13.458s, episode steps: 649, steps per second:  48, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.325 [0.000, 5.000],  loss: 0.015248, mae: 2.540019, mean_q: 3.061645, mean_eps: 0.100000
 5875852/6000000: episode: 7202, duration: 15.937s, episode steps: 793, steps per second:  50, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.672 [0.000, 5.000],  loss: 0.014743, mae: 2.530597, mean_q: 3.050440, mean_eps: 0.100000
 5876271/6000000: episode: 7203, duration: 8.839s, episode steps: 419, steps per second:  47, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.015317, mae: 2.530479, mean_q: 3.051438, mean_eps: 0.100000
 5877086/6000000: episode: 7204, duration: 17.024s, episode steps: 815, steps per second:  48, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.865 [0.000, 5.000],  loss: 0.015383, mae: 2.523023, mean_q: 3.040228, mean_eps: 0.100000
 5878036/6000000: episode: 7205, duration: 19.701s, episode steps: 950, steps per second:  48, episode reward: 29.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.898 [0.000, 5.000],  loss: 0.014380, mae: 2.553279, mean_q: 3.075246, mean_eps: 0.100000
 5879194/6000000: episode: 7206, duration: 24.199s, episode steps: 1158, steps per second:  48, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.050 [0.000, 5.000],  loss: 0.015088, mae: 2.543362, mean_q: 3.062404, mean_eps: 0.100000
 5880016/6000000: episode: 7207, duration: 17.910s, episode steps: 822, steps per second:  46, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.236 [0.000, 5.000],  loss: 0.014250, mae: 2.506029, mean_q: 3.017553, mean_eps: 0.100000
 5880819/6000000: episode: 7208, duration: 17.780s, episode steps: 803, steps per second:  45, episode reward: 25.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.682 [0.000, 5.000],  loss: 0.014820, mae: 2.532968, mean_q: 3.052380, mean_eps: 0.100000
 5881590/6000000: episode: 7209, duration: 17.148s, episode steps: 771, steps per second:  45, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.988 [0.000, 5.000],  loss: 0.013793, mae: 2.532413, mean_q: 3.051548, mean_eps: 0.100000
 5882850/6000000: episode: 7210, duration: 30.598s, episode steps: 1260, steps per second:  41, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.608 [0.000, 5.000],  loss: 0.014121, mae: 2.524340, mean_q: 3.041502, mean_eps: 0.100000
 5883911/6000000: episode: 7211, duration: 23.175s, episode steps: 1061, steps per second:  46, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.722 [0.000, 5.000],  loss: 0.014103, mae: 2.556900, mean_q: 3.081654, mean_eps: 0.100000
 5884840/6000000: episode: 7212, duration: 19.654s, episode steps: 929, steps per second:  47, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.629 [0.000, 5.000],  loss: 0.013895, mae: 2.530576, mean_q: 3.050332, mean_eps: 0.100000
 5885547/6000000: episode: 7213, duration: 14.780s, episode steps: 707, steps per second:  48, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.798 [0.000, 5.000],  loss: 0.013421, mae: 2.515847, mean_q: 3.032315, mean_eps: 0.100000
 5886034/6000000: episode: 7214, duration: 10.015s, episode steps: 487, steps per second:  49, episode reward: 12.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.013756, mae: 2.525514, mean_q: 3.043593, mean_eps: 0.100000
 5887252/6000000: episode: 7215, duration: 26.388s, episode steps: 1218, steps per second:  46, episode reward: 34.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.013688, mae: 2.524945, mean_q: 3.041997, mean_eps: 0.100000
 5888333/6000000: episode: 7216, duration: 25.268s, episode steps: 1081, steps per second:  43, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.110 [0.000, 5.000],  loss: 0.013578, mae: 2.523930, mean_q: 3.041447, mean_eps: 0.100000
 5889313/6000000: episode: 7217, duration: 22.355s, episode steps: 980, steps per second:  44, episode reward: 28.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.473 [0.000, 5.000],  loss: 0.017335, mae: 2.554345, mean_q: 3.076667, mean_eps: 0.100000
 5890461/6000000: episode: 7218, duration: 25.231s, episode steps: 1148, steps per second:  45, episode reward: 31.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.898 [0.000, 5.000],  loss: 0.013166, mae: 2.529337, mean_q: 3.047869, mean_eps: 0.100000
 5891877/6000000: episode: 7219, duration: 30.026s, episode steps: 1416, steps per second:  47, episode reward: 35.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.115 [0.000, 5.000],  loss: 0.013823, mae: 2.524948, mean_q: 3.042183, mean_eps: 0.100000
 5892792/6000000: episode: 7220, duration: 19.207s, episode steps: 915, steps per second:  48, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.014202, mae: 2.523439, mean_q: 3.039818, mean_eps: 0.100000
 5893737/6000000: episode: 7221, duration: 20.313s, episode steps: 945, steps per second:  47, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.126 [0.000, 5.000],  loss: 0.013544, mae: 2.504563, mean_q: 3.016775, mean_eps: 0.100000
 5894612/6000000: episode: 7222, duration: 18.633s, episode steps: 875, steps per second:  47, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.403 [0.000, 5.000],  loss: 0.013932, mae: 2.524547, mean_q: 3.040319, mean_eps: 0.100000
 5895651/6000000: episode: 7223, duration: 21.683s, episode steps: 1039, steps per second:  48, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.014539, mae: 2.531935, mean_q: 3.048120, mean_eps: 0.100000
 5896549/6000000: episode: 7224, duration: 19.007s, episode steps: 898, steps per second:  47, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.997 [0.000, 5.000],  loss: 0.013035, mae: 2.546857, mean_q: 3.067995, mean_eps: 0.100000
 5897431/6000000: episode: 7225, duration: 19.618s, episode steps: 882, steps per second:  45, episode reward: 27.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.467 [0.000, 5.000],  loss: 0.013113, mae: 2.522231, mean_q: 3.037689, mean_eps: 0.100000
 5898445/6000000: episode: 7226, duration: 22.452s, episode steps: 1014, steps per second:  45, episode reward: 29.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.812 [0.000, 5.000],  loss: 0.013573, mae: 2.525917, mean_q: 3.043547, mean_eps: 0.100000
 5899421/6000000: episode: 7227, duration: 23.771s, episode steps: 976, steps per second:  41, episode reward: 30.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.013580, mae: 2.521872, mean_q: 3.037860, mean_eps: 0.100000
 5900267/6000000: episode: 7228, duration: 18.719s, episode steps: 846, steps per second:  45, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.014209, mae: 2.552690, mean_q: 3.075179, mean_eps: 0.100000
 5901558/6000000: episode: 7229, duration: 27.691s, episode steps: 1291, steps per second:  47, episode reward: 36.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.013489, mae: 2.503439, mean_q: 3.015978, mean_eps: 0.100000
 5903130/6000000: episode: 7230, duration: 34.696s, episode steps: 1572, steps per second:  45, episode reward: 35.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.014248, mae: 2.522223, mean_q: 3.037230, mean_eps: 0.100000
 5904295/6000000: episode: 7231, duration: 28.576s, episode steps: 1165, steps per second:  41, episode reward: 34.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.870 [0.000, 5.000],  loss: 0.014156, mae: 2.529179, mean_q: 3.046701, mean_eps: 0.100000
 5905364/6000000: episode: 7232, duration: 24.097s, episode steps: 1069, steps per second:  44, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.145 [0.000, 5.000],  loss: 0.014122, mae: 2.519244, mean_q: 3.037242, mean_eps: 0.100000
 5905933/6000000: episode: 7233, duration: 12.898s, episode steps: 569, steps per second:  44, episode reward: 17.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.309 [0.000, 5.000],  loss: 0.012427, mae: 2.523413, mean_q: 3.040135, mean_eps: 0.100000
 5906706/6000000: episode: 7234, duration: 17.228s, episode steps: 773, steps per second:  45, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.013533, mae: 2.528504, mean_q: 3.046084, mean_eps: 0.100000
 5907879/6000000: episode: 7235, duration: 25.335s, episode steps: 1173, steps per second:  46, episode reward: 36.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.658 [0.000, 5.000],  loss: 0.015359, mae: 2.519961, mean_q: 3.037084, mean_eps: 0.100000
 5908734/6000000: episode: 7236, duration: 17.344s, episode steps: 855, steps per second:  49, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.628 [0.000, 5.000],  loss: 0.013492, mae: 2.515307, mean_q: 3.031862, mean_eps: 0.100000
 5909661/6000000: episode: 7237, duration: 20.993s, episode steps: 927, steps per second:  44, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.654 [0.000, 5.000],  loss: 0.013541, mae: 2.517594, mean_q: 3.033814, mean_eps: 0.100000
 5910884/6000000: episode: 7238, duration: 26.287s, episode steps: 1223, steps per second:  47, episode reward: 33.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.818 [0.000, 5.000],  loss: 0.014437, mae: 2.512334, mean_q: 3.027822, mean_eps: 0.100000
 5911868/6000000: episode: 7239, duration: 21.046s, episode steps: 984, steps per second:  47, episode reward: 29.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.176 [0.000, 5.000],  loss: 0.012945, mae: 2.510086, mean_q: 3.024575, mean_eps: 0.100000
 5912625/6000000: episode: 7240, duration: 16.135s, episode steps: 757, steps per second:  47, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.865 [0.000, 5.000],  loss: 0.014568, mae: 2.511252, mean_q: 3.025655, mean_eps: 0.100000
 5913821/6000000: episode: 7241, duration: 25.967s, episode steps: 1196, steps per second:  46, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.097 [0.000, 5.000],  loss: 0.014525, mae: 2.521930, mean_q: 3.038431, mean_eps: 0.100000
 5914881/6000000: episode: 7242, duration: 24.944s, episode steps: 1060, steps per second:  42, episode reward: 30.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.814 [0.000, 5.000],  loss: 0.014156, mae: 2.524440, mean_q: 3.042945, mean_eps: 0.100000
 5915885/6000000: episode: 7243, duration: 24.381s, episode steps: 1004, steps per second:  41, episode reward: 29.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.048 [0.000, 5.000],  loss: 0.012876, mae: 2.525549, mean_q: 3.043637, mean_eps: 0.100000
 5916850/6000000: episode: 7244, duration: 20.237s, episode steps: 965, steps per second:  48, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.059 [0.000, 5.000],  loss: 0.013128, mae: 2.532480, mean_q: 3.051296, mean_eps: 0.100000
 5918051/6000000: episode: 7245, duration: 26.632s, episode steps: 1201, steps per second:  45, episode reward: 33.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.014212, mae: 2.527704, mean_q: 3.045873, mean_eps: 0.100000
 5918870/6000000: episode: 7246, duration: 17.149s, episode steps: 819, steps per second:  48, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.015429, mae: 2.528796, mean_q: 3.047691, mean_eps: 0.100000
 5919804/6000000: episode: 7247, duration: 19.955s, episode steps: 934, steps per second:  47, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.186 [0.000, 5.000],  loss: 0.013641, mae: 2.512073, mean_q: 3.027268, mean_eps: 0.100000
 5920883/6000000: episode: 7248, duration: 26.501s, episode steps: 1079, steps per second:  41, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 0.013443, mae: 2.545855, mean_q: 3.067384, mean_eps: 0.100000
 5922111/6000000: episode: 7249, duration: 27.045s, episode steps: 1228, steps per second:  45, episode reward: 34.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.007 [0.000, 5.000],  loss: 0.013897, mae: 2.541669, mean_q: 3.060769, mean_eps: 0.100000
 5923109/6000000: episode: 7250, duration: 23.225s, episode steps: 998, steps per second:  43, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.013750, mae: 2.524038, mean_q: 3.040749, mean_eps: 0.100000
 5923855/6000000: episode: 7251, duration: 16.142s, episode steps: 746, steps per second:  46, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.977 [0.000, 5.000],  loss: 0.013855, mae: 2.534361, mean_q: 3.053708, mean_eps: 0.100000
 5924886/6000000: episode: 7252, duration: 21.201s, episode steps: 1031, steps per second:  49, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.094 [0.000, 5.000],  loss: 0.013567, mae: 2.535635, mean_q: 3.054123, mean_eps: 0.100000
 5926104/6000000: episode: 7253, duration: 26.099s, episode steps: 1218, steps per second:  47, episode reward: 32.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.013730, mae: 2.544844, mean_q: 3.066239, mean_eps: 0.100000
 5927194/6000000: episode: 7254, duration: 22.919s, episode steps: 1090, steps per second:  48, episode reward: 32.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.119 [0.000, 5.000],  loss: 0.013857, mae: 2.558764, mean_q: 3.083886, mean_eps: 0.100000
 5928193/6000000: episode: 7255, duration: 20.910s, episode steps: 999, steps per second:  48, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.012579, mae: 2.537420, mean_q: 3.055247, mean_eps: 0.100000
 5929431/6000000: episode: 7256, duration: 26.154s, episode steps: 1238, steps per second:  47, episode reward: 33.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.053 [0.000, 5.000],  loss: 0.014568, mae: 2.566723, mean_q: 3.091779, mean_eps: 0.100000
 5930099/6000000: episode: 7257, duration: 14.309s, episode steps: 668, steps per second:  47, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.789 [0.000, 5.000],  loss: 0.012816, mae: 2.545699, mean_q: 3.068346, mean_eps: 0.100000
 5930832/6000000: episode: 7258, duration: 15.874s, episode steps: 733, steps per second:  46, episode reward: 22.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 0.014857, mae: 2.565476, mean_q: 3.092720, mean_eps: 0.100000
 5931484/6000000: episode: 7259, duration: 15.065s, episode steps: 652, steps per second:  43, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.819 [0.000, 5.000],  loss: 0.013559, mae: 2.544545, mean_q: 3.066085, mean_eps: 0.100000
 5932430/6000000: episode: 7260, duration: 21.625s, episode steps: 946, steps per second:  44, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.181 [0.000, 5.000],  loss: 0.013826, mae: 2.549816, mean_q: 3.071868, mean_eps: 0.100000
 5933129/6000000: episode: 7261, duration: 14.844s, episode steps: 699, steps per second:  47, episode reward: 21.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.014830, mae: 2.569537, mean_q: 3.096376, mean_eps: 0.100000
 5934078/6000000: episode: 7262, duration: 20.375s, episode steps: 949, steps per second:  47, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.016 [0.000, 5.000],  loss: 0.013383, mae: 2.574595, mean_q: 3.101042, mean_eps: 0.100000
 5935162/6000000: episode: 7263, duration: 23.031s, episode steps: 1084, steps per second:  47, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.045 [0.000, 5.000],  loss: 0.014558, mae: 2.541997, mean_q: 3.061928, mean_eps: 0.100000
 5936580/6000000: episode: 7264, duration: 29.699s, episode steps: 1418, steps per second:  48, episode reward: 32.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.103 [0.000, 5.000],  loss: 0.014354, mae: 2.542028, mean_q: 3.062575, mean_eps: 0.100000
 5937545/6000000: episode: 7265, duration: 20.889s, episode steps: 965, steps per second:  46, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.013974, mae: 2.565450, mean_q: 3.090986, mean_eps: 0.100000
 5938170/6000000: episode: 7266, duration: 13.834s, episode steps: 625, steps per second:  45, episode reward: 18.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.013998, mae: 2.557061, mean_q: 3.080319, mean_eps: 0.100000
 5939338/6000000: episode: 7267, duration: 26.095s, episode steps: 1168, steps per second:  45, episode reward: 32.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.013594, mae: 2.555480, mean_q: 3.079427, mean_eps: 0.100000
 5940319/6000000: episode: 7268, duration: 22.055s, episode steps: 981, steps per second:  44, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.074 [0.000, 5.000],  loss: 0.014997, mae: 2.580902, mean_q: 3.108604, mean_eps: 0.100000
 5940987/6000000: episode: 7269, duration: 13.765s, episode steps: 668, steps per second:  49, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.862 [0.000, 5.000],  loss: 0.013746, mae: 2.574015, mean_q: 3.102102, mean_eps: 0.100000
 5941965/6000000: episode: 7270, duration: 20.186s, episode steps: 978, steps per second:  48, episode reward: 29.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.855 [0.000, 5.000],  loss: 0.014620, mae: 2.595823, mean_q: 3.129277, mean_eps: 0.100000
 5942857/6000000: episode: 7271, duration: 19.801s, episode steps: 892, steps per second:  45, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.187 [0.000, 5.000],  loss: 0.012953, mae: 2.576784, mean_q: 3.105503, mean_eps: 0.100000
 5944066/6000000: episode: 7272, duration: 25.139s, episode steps: 1209, steps per second:  48, episode reward: 32.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.013210, mae: 2.564326, mean_q: 3.090043, mean_eps: 0.100000
 5945421/6000000: episode: 7273, duration: 28.239s, episode steps: 1355, steps per second:  48, episode reward: 34.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.893 [0.000, 5.000],  loss: 0.014463, mae: 2.578836, mean_q: 3.106009, mean_eps: 0.100000
 5946758/6000000: episode: 7274, duration: 27.866s, episode steps: 1337, steps per second:  48, episode reward: 34.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.015250, mae: 2.595418, mean_q: 3.126145, mean_eps: 0.100000
 5947451/6000000: episode: 7275, duration: 15.622s, episode steps: 693, steps per second:  44, episode reward: 20.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.013528, mae: 2.569780, mean_q: 3.097185, mean_eps: 0.100000
 5948658/6000000: episode: 7276, duration: 28.679s, episode steps: 1207, steps per second:  42, episode reward: 30.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.012977, mae: 2.580502, mean_q: 3.108826, mean_eps: 0.100000
 5949911/6000000: episode: 7277, duration: 27.247s, episode steps: 1253, steps per second:  46, episode reward: 33.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.658 [0.000, 5.000],  loss: 0.014308, mae: 2.578158, mean_q: 3.105936, mean_eps: 0.100000
 5950750/6000000: episode: 7278, duration: 17.959s, episode steps: 839, steps per second:  47, episode reward: 26.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: 0.013949, mae: 2.571869, mean_q: 3.099001, mean_eps: 0.100000
 5951880/6000000: episode: 7279, duration: 23.280s, episode steps: 1130, steps per second:  49, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.775 [0.000, 5.000],  loss: 0.014869, mae: 2.584079, mean_q: 3.114877, mean_eps: 0.100000
 5952797/6000000: episode: 7280, duration: 19.036s, episode steps: 917, steps per second:  48, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.809 [0.000, 5.000],  loss: 0.014376, mae: 2.552712, mean_q: 3.077484, mean_eps: 0.100000
 5953781/6000000: episode: 7281, duration: 21.326s, episode steps: 984, steps per second:  46, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.007 [0.000, 5.000],  loss: 0.013604, mae: 2.561253, mean_q: 3.086646, mean_eps: 0.100000
 5955145/6000000: episode: 7282, duration: 30.426s, episode steps: 1364, steps per second:  45, episode reward: 34.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.801 [0.000, 5.000],  loss: 0.014669, mae: 2.581277, mean_q: 3.110828, mean_eps: 0.100000
 5956488/6000000: episode: 7283, duration: 30.449s, episode steps: 1343, steps per second:  44, episode reward: 34.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.048 [0.000, 5.000],  loss: 0.014001, mae: 2.581712, mean_q: 3.111241, mean_eps: 0.100000
 5957450/6000000: episode: 7284, duration: 20.269s, episode steps: 962, steps per second:  47, episode reward: 28.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.013292, mae: 2.579316, mean_q: 3.107728, mean_eps: 0.100000
 5958271/6000000: episode: 7285, duration: 16.837s, episode steps: 821, steps per second:  49, episode reward: 24.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.205 [0.000, 5.000],  loss: 0.014276, mae: 2.561958, mean_q: 3.087228, mean_eps: 0.100000
 5959497/6000000: episode: 7286, duration: 25.829s, episode steps: 1226, steps per second:  47, episode reward: 33.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.588 [0.000, 5.000],  loss: 0.014254, mae: 2.579293, mean_q: 3.107947, mean_eps: 0.100000
 5960770/6000000: episode: 7287, duration: 26.647s, episode steps: 1273, steps per second:  48, episode reward: 33.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.033 [0.000, 5.000],  loss: 0.014248, mae: 2.561117, mean_q: 3.085516, mean_eps: 0.100000
 5961858/6000000: episode: 7288, duration: 22.240s, episode steps: 1088, steps per second:  49, episode reward: 31.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.851 [0.000, 5.000],  loss: 0.013588, mae: 2.578633, mean_q: 3.107027, mean_eps: 0.100000
 5963167/6000000: episode: 7289, duration: 26.835s, episode steps: 1309, steps per second:  49, episode reward: 33.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.015658, mae: 2.589201, mean_q: 3.116895, mean_eps: 0.100000
 5964185/6000000: episode: 7290, duration: 23.522s, episode steps: 1018, steps per second:  43, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 0.013500, mae: 2.589274, mean_q: 3.120138, mean_eps: 0.100000
 5965575/6000000: episode: 7291, duration: 35.921s, episode steps: 1390, steps per second:  39, episode reward: 33.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: 0.014868, mae: 2.598261, mean_q: 3.129604, mean_eps: 0.100000
 5966846/6000000: episode: 7292, duration: 28.753s, episode steps: 1271, steps per second:  44, episode reward: 33.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.757 [0.000, 5.000],  loss: 0.014090, mae: 2.601596, mean_q: 3.134998, mean_eps: 0.100000
 5967990/6000000: episode: 7293, duration: 24.850s, episode steps: 1144, steps per second:  46, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.831 [0.000, 5.000],  loss: 0.014144, mae: 2.601704, mean_q: 3.134594, mean_eps: 0.100000
 5969389/6000000: episode: 7294, duration: 29.115s, episode steps: 1399, steps per second:  48, episode reward: 35.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.840 [0.000, 5.000],  loss: 0.013863, mae: 2.588509, mean_q: 3.119571, mean_eps: 0.100000
 5970570/6000000: episode: 7295, duration: 25.783s, episode steps: 1181, steps per second:  46, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.104 [0.000, 5.000],  loss: 0.014282, mae: 2.595661, mean_q: 3.127087, mean_eps: 0.100000
 5971501/6000000: episode: 7296, duration: 20.631s, episode steps: 931, steps per second:  45, episode reward: 31.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.012 [0.000, 5.000],  loss: 0.014491, mae: 2.595712, mean_q: 3.126995, mean_eps: 0.100000
 5972535/6000000: episode: 7297, duration: 23.019s, episode steps: 1034, steps per second:  45, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.936 [0.000, 5.000],  loss: 0.013721, mae: 2.606416, mean_q: 3.141576, mean_eps: 0.100000
 5973304/6000000: episode: 7298, duration: 17.111s, episode steps: 769, steps per second:  45, episode reward: 25.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 1.928 [0.000, 5.000],  loss: 0.013657, mae: 2.605830, mean_q: 3.139554, mean_eps: 0.100000
 5974176/6000000: episode: 7299, duration: 18.428s, episode steps: 872, steps per second:  47, episode reward: 28.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 1.720 [0.000, 5.000],  loss: 0.013912, mae: 2.608983, mean_q: 3.144932, mean_eps: 0.100000
 5975269/6000000: episode: 7300, duration: 23.346s, episode steps: 1093, steps per second:  47, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.115 [0.000, 5.000],  loss: 0.012817, mae: 2.616428, mean_q: 3.154373, mean_eps: 0.100000
 5976484/6000000: episode: 7301, duration: 25.485s, episode steps: 1215, steps per second:  48, episode reward: 33.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.197 [0.000, 5.000],  loss: 0.013545, mae: 2.636799, mean_q: 3.177391, mean_eps: 0.100000
 5977593/6000000: episode: 7302, duration: 22.626s, episode steps: 1109, steps per second:  49, episode reward: 30.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.194 [0.000, 5.000],  loss: 0.014460, mae: 2.622820, mean_q: 3.160599, mean_eps: 0.100000
 5978653/6000000: episode: 7303, duration: 22.697s, episode steps: 1060, steps per second:  47, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.014202, mae: 2.619867, mean_q: 3.155974, mean_eps: 0.100000
 5979366/6000000: episode: 7304, duration: 15.347s, episode steps: 713, steps per second:  46, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.105 [0.000, 5.000],  loss: 0.014244, mae: 2.616830, mean_q: 3.153762, mean_eps: 0.100000
 5980421/6000000: episode: 7305, duration: 22.976s, episode steps: 1055, steps per second:  46, episode reward: 30.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.692 [0.000, 5.000],  loss: 0.014899, mae: 2.618969, mean_q: 3.155408, mean_eps: 0.100000
 5981506/6000000: episode: 7306, duration: 24.707s, episode steps: 1085, steps per second:  44, episode reward: 31.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.015413, mae: 2.636937, mean_q: 3.175524, mean_eps: 0.100000
 5982412/6000000: episode: 7307, duration: 20.888s, episode steps: 906, steps per second:  43, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.016341, mae: 2.608928, mean_q: 3.140995, mean_eps: 0.100000
 5983338/6000000: episode: 7308, duration: 20.439s, episode steps: 926, steps per second:  45, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.600 [0.000, 5.000],  loss: 0.013637, mae: 2.610272, mean_q: 3.144401, mean_eps: 0.100000
 5984413/6000000: episode: 7309, duration: 22.923s, episode steps: 1075, steps per second:  47, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.723 [0.000, 5.000],  loss: 0.014388, mae: 2.600108, mean_q: 3.132320, mean_eps: 0.100000
 5985286/6000000: episode: 7310, duration: 18.353s, episode steps: 873, steps per second:  48, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.895 [0.000, 5.000],  loss: 0.013776, mae: 2.605258, mean_q: 3.139535, mean_eps: 0.100000
 5985948/6000000: episode: 7311, duration: 13.974s, episode steps: 662, steps per second:  47, episode reward: 20.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.013097, mae: 2.622205, mean_q: 3.159049, mean_eps: 0.100000
 5987108/6000000: episode: 7312, duration: 25.928s, episode steps: 1160, steps per second:  45, episode reward: 31.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.868 [0.000, 5.000],  loss: 0.013018, mae: 2.584992, mean_q: 3.114808, mean_eps: 0.100000
 5988068/6000000: episode: 7313, duration: 20.965s, episode steps: 960, steps per second:  46, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.885 [0.000, 5.000],  loss: 0.014883, mae: 2.579935, mean_q: 3.106555, mean_eps: 0.100000
 5989334/6000000: episode: 7314, duration: 27.842s, episode steps: 1266, steps per second:  45, episode reward: 30.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.966 [0.000, 5.000],  loss: 0.016170, mae: 2.576220, mean_q: 3.102654, mean_eps: 0.100000
 5990443/6000000: episode: 7315, duration: 23.403s, episode steps: 1109, steps per second:  47, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.985 [0.000, 5.000],  loss: 0.015076, mae: 2.570014, mean_q: 3.096048, mean_eps: 0.100000
 5991697/6000000: episode: 7316, duration: 25.539s, episode steps: 1254, steps per second:  49, episode reward: 30.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.786 [0.000, 5.000],  loss: 0.014789, mae: 2.568053, mean_q: 3.094334, mean_eps: 0.100000
 5992482/6000000: episode: 7317, duration: 17.372s, episode steps: 785, steps per second:  45, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.424 [0.000, 5.000],  loss: 0.012282, mae: 2.555467, mean_q: 3.081048, mean_eps: 0.100000
 5993289/6000000: episode: 7318, duration: 17.398s, episode steps: 807, steps per second:  46, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.818 [0.000, 5.000],  loss: 0.014979, mae: 2.552133, mean_q: 3.074874, mean_eps: 0.100000
 5994222/6000000: episode: 7319, duration: 19.436s, episode steps: 933, steps per second:  48, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.695 [0.000, 5.000],  loss: 0.013796, mae: 2.570321, mean_q: 3.097016, mean_eps: 0.100000
 5995090/6000000: episode: 7320, duration: 18.332s, episode steps: 868, steps per second:  47, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.441 [0.000, 5.000],  loss: 0.014303, mae: 2.596705, mean_q: 3.129704, mean_eps: 0.100000
 5996098/6000000: episode: 7321, duration: 22.071s, episode steps: 1008, steps per second:  46, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.667 [0.000, 5.000],  loss: 0.014482, mae: 2.562809, mean_q: 3.087050, mean_eps: 0.100000
 5997504/6000000: episode: 7322, duration: 30.691s, episode steps: 1406, steps per second:  46, episode reward: 35.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.131 [0.000, 5.000],  loss: 0.014070, mae: 2.563751, mean_q: 3.088820, mean_eps: 0.100000
 5998208/6000000: episode: 7323, duration: 16.284s, episode steps: 704, steps per second:  43, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.111 [0.000, 5.000],  loss: 0.014535, mae: 2.548223, mean_q: 3.068478, mean_eps: 0.100000
 5999038/6000000: episode: 7324, duration: 17.989s, episode steps: 830, steps per second:  46, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.145 [0.000, 5.000],  loss: 0.015833, mae: 2.579116, mean_q: 3.106912, mean_eps: 0.100000
done, took 133850.699 seconds