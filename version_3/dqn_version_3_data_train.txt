 50435/2000000: episode: 68, duration: 15.583s, episode steps: 687, steps per second:  44, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.006000, mae: 0.026522, mean_q: 0.024521, mean_eps: 0.954804
   51173/2000000: episode: 69, duration: 23.928s, episode steps: 738, steps per second:  31, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.007968, mae: 0.025257, mean_q: 0.014828, mean_eps: 0.954276
   51671/2000000: episode: 70, duration: 13.909s, episode steps: 498, steps per second:  36, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.008242, mae: 0.029999, mean_q: 0.021858, mean_eps: 0.953720
   52091/2000000: episode: 71, duration: 10.353s, episode steps: 420, steps per second:  41, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.008818, mae: 0.026515, mean_q: 0.011670, mean_eps: 0.953308
   52560/2000000: episode: 72, duration: 9.459s, episode steps: 469, steps per second:  50, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.006763, mae: 0.022892, mean_q: 0.015155, mean_eps: 0.952908
   52942/2000000: episode: 73, duration: 7.104s, episode steps: 382, steps per second:  54, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.007622, mae: 0.018032, mean_q: 0.011596, mean_eps: 0.952525
   53759/2000000: episode: 74, duration: 13.976s, episode steps: 817, steps per second:  58, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.007590, mae: 0.031792, mean_q: 0.031925, mean_eps: 0.951985
   54375/2000000: episode: 75, duration: 10.308s, episode steps: 616, steps per second:  60, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.007153, mae: 0.023335, mean_q: 0.024001, mean_eps: 0.951341
   55023/2000000: episode: 76, duration: 11.036s, episode steps: 648, steps per second:  59, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.656 [0.000, 5.000],  loss: 0.007560, mae: 0.027453, mean_q: 0.033002, mean_eps: 0.950772
   55554/2000000: episode: 77, duration: 9.116s, episode steps: 531, steps per second:  58, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.007393, mae: 0.042411, mean_q: 0.054715, mean_eps: 0.950241
   56237/2000000: episode: 78, duration: 11.779s, episode steps: 683, steps per second:  58, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.007638, mae: 0.044074, mean_q: 0.054990, mean_eps: 0.949694
   56716/2000000: episode: 79, duration: 7.947s, episode steps: 479, steps per second:  60, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.008115, mae: 0.042678, mean_q: 0.050954, mean_eps: 0.949172
   57257/2000000: episode: 80, duration: 8.608s, episode steps: 541, steps per second:  63, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.006476, mae: 0.035669, mean_q: 0.046024, mean_eps: 0.948713
   58578/2000000: episode: 81, duration: 21.655s, episode steps: 1321, steps per second:  61, episode reward: 22.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.007590, mae: 0.041851, mean_q: 0.049287, mean_eps: 0.947874
   59091/2000000: episode: 82, duration: 8.765s, episode steps: 513, steps per second:  59, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.006931, mae: 0.038835, mean_q: 0.045064, mean_eps: 0.947049
   59606/2000000: episode: 83, duration: 9.081s, episode steps: 515, steps per second:  57, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.007477, mae: 0.040171, mean_q: 0.049854, mean_eps: 0.946587
   60193/2000000: episode: 84, duration: 9.975s, episode steps: 587, steps per second:  59, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.007329, mae: 0.044796, mean_q: 0.054478, mean_eps: 0.946090
   60694/2000000: episode: 85, duration: 8.281s, episode steps: 501, steps per second:  60, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.007680, mae: 0.057517, mean_q: 0.071171, mean_eps: 0.945600
   61802/2000000: episode: 86, duration: 18.098s, episode steps: 1108, steps per second:  61, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.006357, mae: 0.052195, mean_q: 0.061893, mean_eps: 0.944877
   62286/2000000: episode: 87, duration: 8.693s, episode steps: 484, steps per second:  56, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.008366, mae: 0.056492, mean_q: 0.066674, mean_eps: 0.944160
   62665/2000000: episode: 88, duration: 6.663s, episode steps: 379, steps per second:  57, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.007707, mae: 0.054596, mean_q: 0.065109, mean_eps: 0.943772
   63016/2000000: episode: 89, duration: 6.194s, episode steps: 351, steps per second:  57, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 0.008218, mae: 0.057052, mean_q: 0.065533, mean_eps: 0.943444
   63535/2000000: episode: 90, duration: 9.289s, episode steps: 519, steps per second:  56, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.006597, mae: 0.050601, mean_q: 0.058813, mean_eps: 0.943053
   63990/2000000: episode: 91, duration: 8.056s, episode steps: 455, steps per second:  56, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.005712, mae: 0.050185, mean_q: 0.058954, mean_eps: 0.942614
   64352/2000000: episode: 92, duration: 6.778s, episode steps: 362, steps per second:  53, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.010774, mae: 0.056406, mean_q: 0.066277, mean_eps: 0.942247
   65161/2000000: episode: 93, duration: 13.886s, episode steps: 809, steps per second:  58, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.006458, mae: 0.055666, mean_q: 0.067391, mean_eps: 0.941720
   65975/2000000: episode: 94, duration: 16.195s, episode steps: 814, steps per second:  50, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.007133, mae: 0.071168, mean_q: 0.089550, mean_eps: 0.940989
   66547/2000000: episode: 95, duration: 11.578s, episode steps: 572, steps per second:  49, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.007538, mae: 0.075206, mean_q: 0.092118, mean_eps: 0.940366
   66938/2000000: episode: 96, duration: 7.339s, episode steps: 391, steps per second:  53, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.005984, mae: 0.071985, mean_q: 0.088885, mean_eps: 0.939932
   67768/2000000: episode: 97, duration: 16.479s, episode steps: 830, steps per second:  50, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.007948, mae: 0.074118, mean_q: 0.091089, mean_eps: 0.939383
   68409/2000000: episode: 98, duration: 12.830s, episode steps: 641, steps per second:  50, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.006253, mae: 0.070324, mean_q: 0.088153, mean_eps: 0.938721
   68912/2000000: episode: 99, duration: 10.601s, episode steps: 503, steps per second:  47, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.008408, mae: 0.076214, mean_q: 0.093894, mean_eps: 0.938206
   69301/2000000: episode: 100, duration: 7.505s, episode steps: 389, steps per second:  52, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.797 [0.000, 5.000],  loss: 0.007602, mae: 0.074364, mean_q: 0.090562, mean_eps: 0.937805
   69919/2000000: episode: 101, duration: 12.519s, episode steps: 618, steps per second:  49, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.007113, mae: 0.072884, mean_q: 0.089357, mean_eps: 0.937351
   70430/2000000: episode: 102, duration: 9.656s, episode steps: 511, steps per second:  53, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.007392, mae: 0.085433, mean_q: 0.107850, mean_eps: 0.936843
   71085/2000000: episode: 103, duration: 12.390s, episode steps: 655, steps per second:  53, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.007528, mae: 0.089312, mean_q: 0.113418, mean_eps: 0.936318
   72008/2000000: episode: 104, duration: 16.819s, episode steps: 923, steps per second:  55, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.006961, mae: 0.084349, mean_q: 0.106172, mean_eps: 0.935609
   73456/2000000: episode: 105, duration: 29.259s, episode steps: 1448, steps per second:  49, episode reward: 15.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.008025, mae: 0.086599, mean_q: 0.106718, mean_eps: 0.934543
   74057/2000000: episode: 106, duration: 10.986s, episode steps: 601, steps per second:  55, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.007039, mae: 0.084405, mean_q: 0.106191, mean_eps: 0.933620
   74892/2000000: episode: 107, duration: 15.342s, episode steps: 835, steps per second:  54, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.007548, mae: 0.084011, mean_q: 0.106907, mean_eps: 0.932973
   75379/2000000: episode: 108, duration: 8.214s, episode steps: 487, steps per second:  59, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.008631, mae: 0.101138, mean_q: 0.128950, mean_eps: 0.932379
   76037/2000000: episode: 109, duration: 11.982s, episode steps: 658, steps per second:  55, episode reward:  2.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.007213, mae: 0.101803, mean_q: 0.128696, mean_eps: 0.931863
   77116/2000000: episode: 110, duration: 20.856s, episode steps: 1079, steps per second:  52, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.006463, mae: 0.097704, mean_q: 0.122302, mean_eps: 0.931082
   77670/2000000: episode: 111, duration: 10.852s, episode steps: 554, steps per second:  51, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.007088, mae: 0.098203, mean_q: 0.123667, mean_eps: 0.930347
   78841/2000000: episode: 112, duration: 22.359s, episode steps: 1171, steps per second:  52, episode reward: 14.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.007124, mae: 0.100689, mean_q: 0.126608, mean_eps: 0.929570
   80048/2000000: episode: 113, duration: 23.842s, episode steps: 1207, steps per second:  51, episode reward: 15.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.006975, mae: 0.099241, mean_q: 0.122959, mean_eps: 0.928500
   80872/2000000: episode: 114, duration: 17.128s, episode steps: 824, steps per second:  48, episode reward:  7.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.008367, mae: 0.127797, mean_q: 0.161478, mean_eps: 0.927588
   81709/2000000: episode: 115, duration: 16.193s, episode steps: 837, steps per second:  52, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.006563, mae: 0.123201, mean_q: 0.156452, mean_eps: 0.926839
   82474/2000000: episode: 116, duration: 13.110s, episode steps: 765, steps per second:  58, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.006031, mae: 0.122008, mean_q: 0.154842, mean_eps: 0.926117
   82993/2000000: episode: 117, duration: 9.414s, episode steps: 519, steps per second:  55, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.312 [0.000, 5.000],  loss: 0.007193, mae: 0.123195, mean_q: 0.156457, mean_eps: 0.925539
   83609/2000000: episode: 118, duration: 13.805s, episode steps: 616, steps per second:  45, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.007347, mae: 0.120288, mean_q: 0.153758, mean_eps: 0.925028
   84333/2000000: episode: 119, duration: 13.141s, episode steps: 724, steps per second:  55, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.006188, mae: 0.121101, mean_q: 0.154328, mean_eps: 0.924425
   84954/2000000: episode: 120, duration: 12.020s, episode steps: 621, steps per second:  52, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.007238, mae: 0.122983, mean_q: 0.156786, mean_eps: 0.923820
   85960/2000000: episode: 121, duration: 21.021s, episode steps: 1006, steps per second:  48, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.006726, mae: 0.139326, mean_q: 0.176923, mean_eps: 0.923090
   86361/2000000: episode: 122, duration: 7.447s, episode steps: 401, steps per second:  54, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.006430, mae: 0.136887, mean_q: 0.176678, mean_eps: 0.922456
   87090/2000000: episode: 123, duration: 15.140s, episode steps: 729, steps per second:  48, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.007152, mae: 0.141432, mean_q: 0.180580, mean_eps: 0.921947
   87489/2000000: episode: 124, duration: 8.264s, episode steps: 399, steps per second:  48, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.006676, mae: 0.138976, mean_q: 0.175666, mean_eps: 0.921439
   88265/2000000: episode: 125, duration: 15.747s, episode steps: 776, steps per second:  49, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.006378, mae: 0.135973, mean_q: 0.172067, mean_eps: 0.920910
   88937/2000000: episode: 126, duration: 13.944s, episode steps: 672, steps per second:  48, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.006521, mae: 0.140207, mean_q: 0.177199, mean_eps: 0.920258
   89470/2000000: episode: 127, duration: 10.664s, episode steps: 533, steps per second:  50, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.005965, mae: 0.134773, mean_q: 0.169510, mean_eps: 0.919716
   90052/2000000: episode: 128, duration: 11.565s, episode steps: 582, steps per second:  50, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.006627, mae: 0.138267, mean_q: 0.174646, mean_eps: 0.919216
   90554/2000000: episode: 129, duration: 9.378s, episode steps: 502, steps per second:  54, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.008538, mae: 0.158120, mean_q: 0.201453, mean_eps: 0.918728
   91367/2000000: episode: 130, duration: 16.723s, episode steps: 813, steps per second:  49, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.006489, mae: 0.153864, mean_q: 0.194516, mean_eps: 0.918136
   92265/2000000: episode: 131, duration: 15.650s, episode steps: 898, steps per second:  57, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.005473, mae: 0.149629, mean_q: 0.192064, mean_eps: 0.917366
   92911/2000000: episode: 132, duration: 11.219s, episode steps: 646, steps per second:  58, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.006630, mae: 0.150048, mean_q: 0.190773, mean_eps: 0.916671
   94055/2000000: episode: 133, duration: 20.143s, episode steps: 1144, steps per second:  57, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.006847, mae: 0.151898, mean_q: 0.192009, mean_eps: 0.915866
   95113/2000000: episode: 134, duration: 18.449s, episode steps: 1058, steps per second:  57, episode reward: 13.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.007224, mae: 0.154343, mean_q: 0.195492, mean_eps: 0.914874
   95851/2000000: episode: 135, duration: 12.353s, episode steps: 738, steps per second:  60, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.005340, mae: 0.160561, mean_q: 0.203720, mean_eps: 0.914066
   96451/2000000: episode: 136, duration: 9.754s, episode steps: 600, steps per second:  62, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.008019, mae: 0.167858, mean_q: 0.212808, mean_eps: 0.913465
   96980/2000000: episode: 137, duration: 9.309s, episode steps: 529, steps per second:  57, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.007244, mae: 0.164586, mean_q: 0.208624, mean_eps: 0.912957
   97689/2000000: episode: 138, duration: 12.358s, episode steps: 709, steps per second:  57, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.006344, mae: 0.163569, mean_q: 0.207965, mean_eps: 0.912399
   98674/2000000: episode: 139, duration: 17.120s, episode steps: 985, steps per second:  58, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.006440, mae: 0.160717, mean_q: 0.203540, mean_eps: 0.911636
   99335/2000000: episode: 140, duration: 11.923s, episode steps: 661, steps per second:  55, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.007025, mae: 0.163642, mean_q: 0.206870, mean_eps: 0.910896
   99966/2000000: episode: 141, duration: 10.926s, episode steps: 631, steps per second:  58, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.007300, mae: 0.158916, mean_q: 0.201265, mean_eps: 0.910315
  100605/2000000: episode: 142, duration: 11.321s, episode steps: 639, steps per second:  56, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.005947, mae: 0.174637, mean_q: 0.221067, mean_eps: 0.909743
  100993/2000000: episode: 143, duration: 7.228s, episode steps: 388, steps per second:  54, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.007075, mae: 0.174765, mean_q: 0.222164, mean_eps: 0.909280
  101601/2000000: episode: 144, duration: 10.638s, episode steps: 608, steps per second:  57, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.324 [0.000, 5.000],  loss: 0.008412, mae: 0.177855, mean_q: 0.224676, mean_eps: 0.908832
  102140/2000000: episode: 145, duration: 10.004s, episode steps: 539, steps per second:  54, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.006580, mae: 0.176071, mean_q: 0.222496, mean_eps: 0.908317
  102667/2000000: episode: 146, duration: 9.959s, episode steps: 527, steps per second:  53, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.006274, mae: 0.177770, mean_q: 0.224021, mean_eps: 0.907838
  103039/2000000: episode: 147, duration: 7.345s, episode steps: 372, steps per second:  51, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.007377, mae: 0.176141, mean_q: 0.221536, mean_eps: 0.907433
  103555/2000000: episode: 148, duration: 11.019s, episode steps: 516, steps per second:  47, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.653 [0.000, 5.000],  loss: 0.005881, mae: 0.174265, mean_q: 0.219503, mean_eps: 0.907034
  104974/2000000: episode: 149, duration: 30.249s, episode steps: 1419, steps per second:  47, episode reward: 21.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.006636, mae: 0.175835, mean_q: 0.219505, mean_eps: 0.906162
  105600/2000000: episode: 150, duration: 11.287s, episode steps: 626, steps per second:  55, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.007704, mae: 0.197726, mean_q: 0.248173, mean_eps: 0.905243
  106757/2000000: episode: 151, duration: 21.623s, episode steps: 1157, steps per second:  54, episode reward: 11.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.655 [0.000, 5.000],  loss: 0.006588, mae: 0.195702, mean_q: 0.246428, mean_eps: 0.904440
  107514/2000000: episode: 152, duration: 13.766s, episode steps: 757, steps per second:  55, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.007012, mae: 0.198470, mean_q: 0.249469, mean_eps: 0.903578
  108146/2000000: episode: 153, duration: 11.790s, episode steps: 632, steps per second:  54, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.008085, mae: 0.195889, mean_q: 0.245884, mean_eps: 0.902953
  108722/2000000: episode: 154, duration: 10.176s, episode steps: 576, steps per second:  57, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.006921, mae: 0.188990, mean_q: 0.239830, mean_eps: 0.902409
  109354/2000000: episode: 155, duration: 11.425s, episode steps: 632, steps per second:  55, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.007137, mae: 0.193156, mean_q: 0.244134, mean_eps: 0.901866
  110165/2000000: episode: 156, duration: 15.677s, episode steps: 811, steps per second:  52, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.006644, mae: 0.195546, mean_q: 0.245219, mean_eps: 0.901216
  110624/2000000: episode: 157, duration: 9.116s, episode steps: 459, steps per second:  50, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.006731, mae: 0.212954, mean_q: 0.266577, mean_eps: 0.900645
  112175/2000000: episode: 158, duration: 28.291s, episode steps: 1551, steps per second:  55, episode reward: 15.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.007594, mae: 0.211919, mean_q: 0.267016, mean_eps: 0.899742
  113252/2000000: episode: 159, duration: 19.495s, episode steps: 1077, steps per second:  55, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.006880, mae: 0.209286, mean_q: 0.263170, mean_eps: 0.898559
  113905/2000000: episode: 160, duration: 11.885s, episode steps: 653, steps per second:  55, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.007000, mae: 0.212153, mean_q: 0.265237, mean_eps: 0.897780
  114813/2000000: episode: 161, duration: 15.829s, episode steps: 908, steps per second:  57, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.007582, mae: 0.210961, mean_q: 0.264153, mean_eps: 0.897076
  115405/2000000: episode: 162, duration: 9.744s, episode steps: 592, steps per second:  61, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.007654, mae: 0.218088, mean_q: 0.275547, mean_eps: 0.896401
  116185/2000000: episode: 163, duration: 12.842s, episode steps: 780, steps per second:  61, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.006438, mae: 0.215174, mean_q: 0.270227, mean_eps: 0.895784
  116672/2000000: episode: 164, duration: 8.226s, episode steps: 487, steps per second:  59, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.690 [0.000, 5.000],  loss: 0.006868, mae: 0.218403, mean_q: 0.275730, mean_eps: 0.895215
  117380/2000000: episode: 165, duration: 13.164s, episode steps: 708, steps per second:  54, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.006332, mae: 0.218076, mean_q: 0.273615, mean_eps: 0.894678
  118454/2000000: episode: 166, duration: 19.306s, episode steps: 1074, steps per second:  56, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.007232, mae: 0.216046, mean_q: 0.270700, mean_eps: 0.893876
  119504/2000000: episode: 167, duration: 17.764s, episode steps: 1050, steps per second:  59, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.007192, mae: 0.218430, mean_q: 0.271988, mean_eps: 0.892920
  120370/2000000: episode: 168, duration: 14.718s, episode steps: 866, steps per second:  59, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.007019, mae: 0.225085, mean_q: 0.281738, mean_eps: 0.892058
  120986/2000000: episode: 169, duration: 11.141s, episode steps: 616, steps per second:  55, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.006157, mae: 0.228869, mean_q: 0.287048, mean_eps: 0.891390
  121496/2000000: episode: 170, duration: 9.409s, episode steps: 510, steps per second:  54, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.006482, mae: 0.228954, mean_q: 0.286173, mean_eps: 0.890884
  122181/2000000: episode: 171, duration: 12.038s, episode steps: 685, steps per second:  57, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.005661, mae: 0.227235, mean_q: 0.284044, mean_eps: 0.890346
  122923/2000000: episode: 172, duration: 13.563s, episode steps: 742, steps per second:  55, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.007754, mae: 0.231952, mean_q: 0.289750, mean_eps: 0.889703
  123618/2000000: episode: 173, duration: 15.437s, episode steps: 695, steps per second:  45, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.006717, mae: 0.229895, mean_q: 0.286646, mean_eps: 0.889057
  124412/2000000: episode: 174, duration: 16.557s, episode steps: 794, steps per second:  48, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.007412, mae: 0.233339, mean_q: 0.293506, mean_eps: 0.888387
  125122/2000000: episode: 175, duration: 12.442s, episode steps: 710, steps per second:  57, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.006641, mae: 0.233822, mean_q: 0.294043, mean_eps: 0.887711
  125513/2000000: episode: 176, duration: 7.185s, episode steps: 391, steps per second:  54, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.005624, mae: 0.254930, mean_q: 0.320353, mean_eps: 0.887214
  126541/2000000: episode: 177, duration: 19.166s, episode steps: 1028, steps per second:  54, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.006616, mae: 0.251748, mean_q: 0.316726, mean_eps: 0.886575
  127461/2000000: episode: 178, duration: 16.806s, episode steps: 920, steps per second:  55, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.006385, mae: 0.251367, mean_q: 0.314721, mean_eps: 0.885698
  128075/2000000: episode: 179, duration: 11.136s, episode steps: 614, steps per second:  55, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.008066, mae: 0.255735, mean_q: 0.319309, mean_eps: 0.885009
  128778/2000000: episode: 180, duration: 12.158s, episode steps: 703, steps per second:  58, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.005766, mae: 0.248213, mean_q: 0.311167, mean_eps: 0.884417
  129526/2000000: episode: 181, duration: 12.848s, episode steps: 748, steps per second:  58, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.007526, mae: 0.251017, mean_q: 0.314305, mean_eps: 0.883763
  130724/2000000: episode: 182, duration: 23.378s, episode steps: 1198, steps per second:  51, episode reward: 14.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.007180, mae: 0.271308, mean_q: 0.340045, mean_eps: 0.882888
  131293/2000000: episode: 183, duration: 10.327s, episode steps: 569, steps per second:  55, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.007270, mae: 0.281206, mean_q: 0.353733, mean_eps: 0.882093
  131961/2000000: episode: 184, duration: 11.553s, episode steps: 668, steps per second:  58, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.007415, mae: 0.276679, mean_q: 0.348191, mean_eps: 0.881535
  132934/2000000: episode: 185, duration: 16.852s, episode steps: 973, steps per second:  58, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.006767, mae: 0.279250, mean_q: 0.350854, mean_eps: 0.880797
  133849/2000000: episode: 186, duration: 16.545s, episode steps: 915, steps per second:  55, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.008236, mae: 0.282427, mean_q: 0.353302, mean_eps: 0.879947
  134639/2000000: episode: 187, duration: 13.284s, episode steps: 790, steps per second:  59, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.007789, mae: 0.279269, mean_q: 0.350382, mean_eps: 0.879180
  135250/2000000: episode: 188, duration: 9.983s, episode steps: 611, steps per second:  61, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.006466, mae: 0.284264, mean_q: 0.356501, mean_eps: 0.878550
  136110/2000000: episode: 189, duration: 14.183s, episode steps: 860, steps per second:  61, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.007437, mae: 0.300436, mean_q: 0.377300, mean_eps: 0.877888
  137069/2000000: episode: 190, duration: 17.496s, episode steps: 959, steps per second:  55, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.008111, mae: 0.298718, mean_q: 0.376672, mean_eps: 0.877069
  137912/2000000: episode: 191, duration: 15.462s, episode steps: 843, steps per second:  55, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.008016, mae: 0.299743, mean_q: 0.375320, mean_eps: 0.876259
  138783/2000000: episode: 192, duration: 14.891s, episode steps: 871, steps per second:  58, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.006929, mae: 0.296164, mean_q: 0.372030, mean_eps: 0.875489
  139553/2000000: episode: 193, duration: 13.464s, episode steps: 770, steps per second:  57, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.007459, mae: 0.296455, mean_q: 0.371799, mean_eps: 0.874749
  139962/2000000: episode: 194, duration: 7.078s, episode steps: 409, steps per second:  58, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.008444, mae: 0.296345, mean_q: 0.372767, mean_eps: 0.874218
  140608/2000000: episode: 195, duration: 11.527s, episode steps: 646, steps per second:  56, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.006597, mae: 0.324451, mean_q: 0.407187, mean_eps: 0.873744
  141002/2000000: episode: 196, duration: 15.275s, episode steps: 394, steps per second:  26, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.007146, mae: 0.320175, mean_q: 0.401779, mean_eps: 0.873276
  141511/2000000: episode: 197, duration: 9.506s, episode steps: 509, steps per second:  54, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.007505, mae: 0.326800, mean_q: 0.411451, mean_eps: 0.872870
  142199/2000000: episode: 198, duration: 12.484s, episode steps: 688, steps per second:  55, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.286 [0.000, 5.000],  loss: 0.008800, mae: 0.324783, mean_q: 0.406699, mean_eps: 0.872331
  142710/2000000: episode: 199, duration: 9.228s, episode steps: 511, steps per second:  55, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.007564, mae: 0.318729, mean_q: 0.400075, mean_eps: 0.871791
  143899/2000000: episode: 200, duration: 23.272s, episode steps: 1189, steps per second:  51, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.006480, mae: 0.320008, mean_q: 0.400416, mean_eps: 0.871026
  144466/2000000: episode: 201, duration: 10.243s, episode steps: 567, steps per second:  55, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.006088, mae: 0.315833, mean_q: 0.394569, mean_eps: 0.870236
  145539/2000000: episode: 202, duration: 19.892s, episode steps: 1073, steps per second:  54, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.008200, mae: 0.334508, mean_q: 0.415107, mean_eps: 0.869498
  146240/2000000: episode: 203, duration: 13.256s, episode steps: 701, steps per second:  53, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 0.007942, mae: 0.337199, mean_q: 0.420044, mean_eps: 0.868701
  146660/2000000: episode: 204, duration: 7.984s, episode steps: 420, steps per second:  53, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.008876, mae: 0.336473, mean_q: 0.420262, mean_eps: 0.868197
  147731/2000000: episode: 205, duration: 19.676s, episode steps: 1071, steps per second:  54, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.007119, mae: 0.337178, mean_q: 0.422271, mean_eps: 0.867525
  148308/2000000: episode: 206, duration: 10.058s, episode steps: 577, steps per second:  57, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.006454, mae: 0.331912, mean_q: 0.416134, mean_eps: 0.866784
  149220/2000000: episode: 207, duration: 15.067s, episode steps: 912, steps per second:  61, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.008106, mae: 0.342960, mean_q: 0.427434, mean_eps: 0.866114
  150187/2000000: episode: 208, duration: 17.191s, episode steps: 967, steps per second:  56, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.007816, mae: 0.343735, mean_q: 0.427464, mean_eps: 0.865268
  150883/2000000: episode: 209, duration: 12.388s, episode steps: 696, steps per second:  56, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.008406, mae: 0.374578, mean_q: 0.466997, mean_eps: 0.864519
  151551/2000000: episode: 210, duration: 29.394s, episode steps: 668, steps per second:  23, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.009112, mae: 0.370569, mean_q: 0.461711, mean_eps: 0.863906
  152146/2000000: episode: 211, duration: 31.081s, episode steps: 595, steps per second:  19, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.008500, mae: 0.365610, mean_q: 0.453083, mean_eps: 0.863337
  152560/2000000: episode: 212, duration: 21.743s, episode steps: 414, steps per second:  19, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.010029, mae: 0.373073, mean_q: 0.464246, mean_eps: 0.862883
  153316/2000000: episode: 213, duration: 12.626s, episode steps: 756, steps per second:  60, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.008942, mae: 0.370711, mean_q: 0.461651, mean_eps: 0.862358
  154002/2000000: episode: 214, duration: 12.244s, episode steps: 686, steps per second:  56, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.008136, mae: 0.367818, mean_q: 0.458865, mean_eps: 0.861708
  154705/2000000: episode: 215, duration: 13.239s, episode steps: 703, steps per second:  53, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.007815, mae: 0.364061, mean_q: 0.455245, mean_eps: 0.861081
  155097/2000000: episode: 216, duration: 6.795s, episode steps: 392, steps per second:  58, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.007972, mae: 0.377571, mean_q: 0.469390, mean_eps: 0.860588
  155595/2000000: episode: 217, duration: 9.052s, episode steps: 498, steps per second:  55, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.007878, mae: 0.397992, mean_q: 0.495050, mean_eps: 0.860189
  156012/2000000: episode: 218, duration: 7.788s, episode steps: 417, steps per second:  54, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.007626, mae: 0.388947, mean_q: 0.485298, mean_eps: 0.859778
  156399/2000000: episode: 219, duration: 6.898s, episode steps: 387, steps per second:  56, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.183 [0.000, 5.000],  loss: 0.009351, mae: 0.380585, mean_q: 0.475342, mean_eps: 0.859416
  156834/2000000: episode: 220, duration: 8.026s, episode steps: 435, steps per second:  54, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.008995, mae: 0.385395, mean_q: 0.481439, mean_eps: 0.859046
  157515/2000000: episode: 221, duration: 12.100s, episode steps: 681, steps per second:  56, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.007097, mae: 0.378708, mean_q: 0.473463, mean_eps: 0.858543
  157896/2000000: episode: 222, duration: 7.479s, episode steps: 381, steps per second:  51, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.009269, mae: 0.396353, mean_q: 0.491609, mean_eps: 0.858066
  159189/2000000: episode: 223, duration: 24.224s, episode steps: 1293, steps per second:  53, episode reward: 26.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.008416, mae: 0.388942, mean_q: 0.485955, mean_eps: 0.857312
  159901/2000000: episode: 224, duration: 14.016s, episode steps: 712, steps per second:  51, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.008155, mae: 0.383962, mean_q: 0.479148, mean_eps: 0.856409
  160643/2000000: episode: 225, duration: 15.222s, episode steps: 742, steps per second:  49, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.007365, mae: 0.406193, mean_q: 0.504197, mean_eps: 0.855755
  161035/2000000: episode: 226, duration: 7.709s, episode steps: 392, steps per second:  51, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.781 [0.000, 5.000],  loss: 0.009071, mae: 0.418427, mean_q: 0.519437, mean_eps: 0.855246
  161860/2000000: episode: 227, duration: 15.105s, episode steps: 825, steps per second:  55, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.008347, mae: 0.410217, mean_q: 0.509319, mean_eps: 0.854699
  162369/2000000: episode: 228, duration: 9.566s, episode steps: 509, steps per second:  53, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.007653, mae: 0.401505, mean_q: 0.498863, mean_eps: 0.854097
  163490/2000000: episode: 229, duration: 20.909s, episode steps: 1121, steps per second:  54, episode reward: 15.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.009133, mae: 0.412289, mean_q: 0.512161, mean_eps: 0.853363
  164152/2000000: episode: 230, duration: 12.899s, episode steps: 662, steps per second:  51, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.008872, mae: 0.412782, mean_q: 0.512891, mean_eps: 0.852562
  164577/2000000: episode: 231, duration: 7.427s, episode steps: 425, steps per second:  57, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.008438, mae: 0.407414, mean_q: 0.506337, mean_eps: 0.852072
  165095/2000000: episode: 232, duration: 9.035s, episode steps: 518, steps per second:  57, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.009043, mae: 0.404893, mean_q: 0.502310, mean_eps: 0.851648
  165468/2000000: episode: 233, duration: 6.519s, episode steps: 373, steps per second:  57, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.008624, mae: 0.428725, mean_q: 0.533784, mean_eps: 0.851248
  165967/2000000: episode: 234, duration: 8.777s, episode steps: 499, steps per second:  57, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.008032, mae: 0.415753, mean_q: 0.515769, mean_eps: 0.850856
  166460/2000000: episode: 235, duration: 9.415s, episode steps: 493, steps per second:  52, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.009247, mae: 0.429849, mean_q: 0.532888, mean_eps: 0.850409
  167144/2000000: episode: 236, duration: 12.438s, episode steps: 684, steps per second:  55, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.008358, mae: 0.420841, mean_q: 0.522368, mean_eps: 0.849880
  167910/2000000: episode: 237, duration: 13.582s, episode steps: 766, steps per second:  56, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.007488, mae: 0.425157, mean_q: 0.530012, mean_eps: 0.849227
  168517/2000000: episode: 238, duration: 10.376s, episode steps: 607, steps per second:  58, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.008700, mae: 0.427491, mean_q: 0.530116, mean_eps: 0.848607
  168909/2000000: episode: 239, duration: 6.918s, episode steps: 392, steps per second:  57, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.007594, mae: 0.422537, mean_q: 0.524425, mean_eps: 0.848157
  169405/2000000: episode: 240, duration: 8.430s, episode steps: 496, steps per second:  59, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.009142, mae: 0.421850, mean_q: 0.523104, mean_eps: 0.847758
  170062/2000000: episode: 241, duration: 11.603s, episode steps: 657, steps per second:  57, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.008284, mae: 0.423962, mean_q: 0.525562, mean_eps: 0.847239
  170708/2000000: episode: 242, duration: 11.274s, episode steps: 646, steps per second:  57, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: 0.008464, mae: 0.465799, mean_q: 0.577827, mean_eps: 0.846654
  171244/2000000: episode: 243, duration: 9.219s, episode steps: 536, steps per second:  58, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.008051, mae: 0.455820, mean_q: 0.564519, mean_eps: 0.846123
  172431/2000000: episode: 244, duration: 19.558s, episode steps: 1187, steps per second:  61, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.007881, mae: 0.458774, mean_q: 0.569271, mean_eps: 0.845348
  173040/2000000: episode: 245, duration: 10.563s, episode steps: 609, steps per second:  58, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.008715, mae: 0.454488, mean_q: 0.562559, mean_eps: 0.844539
  173654/2000000: episode: 246, duration: 10.529s, episode steps: 614, steps per second:  58, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.008902, mae: 0.460117, mean_q: 0.570775, mean_eps: 0.843989
  174358/2000000: episode: 247, duration: 12.749s, episode steps: 704, steps per second:  55, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.008418, mae: 0.463611, mean_q: 0.574870, mean_eps: 0.843395
  174845/2000000: episode: 248, duration: 9.047s, episode steps: 487, steps per second:  54, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.007902, mae: 0.461839, mean_q: 0.572191, mean_eps: 0.842858
  175517/2000000: episode: 249, duration: 12.283s, episode steps: 672, steps per second:  55, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.009673, mae: 0.497184, mean_q: 0.614642, mean_eps: 0.842336
  176186/2000000: episode: 250, duration: 12.262s, episode steps: 669, steps per second:  55, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.008286, mae: 0.498295, mean_q: 0.617164, mean_eps: 0.841733
  176981/2000000: episode: 251, duration: 14.496s, episode steps: 795, steps per second:  55, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.008238, mae: 0.507529, mean_q: 0.626407, mean_eps: 0.841074
  178093/2000000: episode: 252, duration: 20.517s, episode steps: 1112, steps per second:  54, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.008859, mae: 0.505138, mean_q: 0.623809, mean_eps: 0.840216
  178657/2000000: episode: 253, duration: 10.657s, episode steps: 564, steps per second:  53, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.686 [0.000, 5.000],  loss: 0.008422, mae: 0.512748, mean_q: 0.633842, mean_eps: 0.839462
  179290/2000000: episode: 254, duration: 11.453s, episode steps: 633, steps per second:  55, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.007898, mae: 0.504548, mean_q: 0.624483, mean_eps: 0.838923
  179942/2000000: episode: 255, duration: 12.850s, episode steps: 652, steps per second:  51, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.010142, mae: 0.515094, mean_q: 0.635755, mean_eps: 0.838346
  180627/2000000: episode: 256, duration: 13.536s, episode steps: 685, steps per second:  51, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.009669, mae: 0.545807, mean_q: 0.676286, mean_eps: 0.837744
  181171/2000000: episode: 257, duration: 10.438s, episode steps: 544, steps per second:  52, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.009546, mae: 0.552367, mean_q: 0.683250, mean_eps: 0.837192
  181729/2000000: episode: 258, duration: 10.414s, episode steps: 558, steps per second:  54, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.010119, mae: 0.551150, mean_q: 0.683984, mean_eps: 0.836695
  182441/2000000: episode: 259, duration: 13.249s, episode steps: 712, steps per second:  54, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.008990, mae: 0.549267, mean_q: 0.680189, mean_eps: 0.836123
  182953/2000000: episode: 260, duration: 10.687s, episode steps: 512, steps per second:  48, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.009812, mae: 0.546223, mean_q: 0.674079, mean_eps: 0.835572
  183910/2000000: episode: 261, duration: 18.349s, episode steps: 957, steps per second:  52, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.009752, mae: 0.551313, mean_q: 0.679141, mean_eps: 0.834911
  184485/2000000: episode: 262, duration: 11.060s, episode steps: 575, steps per second:  52, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.008353, mae: 0.551612, mean_q: 0.682161, mean_eps: 0.834222
  185426/2000000: episode: 263, duration: 17.072s, episode steps: 941, steps per second:  55, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.009457, mae: 0.559661, mean_q: 0.691306, mean_eps: 0.833540
  186092/2000000: episode: 264, duration: 12.617s, episode steps: 666, steps per second:  53, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.010290, mae: 0.560689, mean_q: 0.693832, mean_eps: 0.832818
  187125/2000000: episode: 265, duration: 19.165s, episode steps: 1033, steps per second:  54, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.008796, mae: 0.559265, mean_q: 0.692675, mean_eps: 0.832053
  187535/2000000: episode: 266, duration: 7.461s, episode steps: 410, steps per second:  55, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.008064, mae: 0.556318, mean_q: 0.688094, mean_eps: 0.831403
  188053/2000000: episode: 267, duration: 9.151s, episode steps: 518, steps per second:  57, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.172 [0.000, 5.000],  loss: 0.010132, mae: 0.554847, mean_q: 0.686614, mean_eps: 0.830985
  188688/2000000: episode: 268, duration: 11.405s, episode steps: 635, steps per second:  56, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.627 [0.000, 5.000],  loss: 0.009257, mae: 0.553551, mean_q: 0.684052, mean_eps: 0.830467
  189080/2000000: episode: 269, duration: 7.008s, episode steps: 392, steps per second:  56, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.676 [0.000, 5.000],  loss: 0.009743, mae: 0.564040, mean_q: 0.696415, mean_eps: 0.830006
  189434/2000000: episode: 270, duration: 6.719s, episode steps: 354, steps per second:  53, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.658 [0.000, 5.000],  loss: 0.009997, mae: 0.549664, mean_q: 0.679488, mean_eps: 0.829670
  190002/2000000: episode: 271, duration: 10.269s, episode steps: 568, steps per second:  55, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.008978, mae: 0.563850, mean_q: 0.700060, mean_eps: 0.829254
  190780/2000000: episode: 272, duration: 13.733s, episode steps: 778, steps per second:  57, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.008788, mae: 0.582586, mean_q: 0.721097, mean_eps: 0.828649
  191251/2000000: episode: 273, duration: 7.784s, episode steps: 471, steps per second:  61, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.008931, mae: 0.566174, mean_q: 0.700664, mean_eps: 0.828087
  191633/2000000: episode: 274, duration: 6.519s, episode steps: 382, steps per second:  59, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.009463, mae: 0.577845, mean_q: 0.713800, mean_eps: 0.827702
  192384/2000000: episode: 275, duration: 13.170s, episode steps: 751, steps per second:  57, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.010947, mae: 0.575344, mean_q: 0.710600, mean_eps: 0.827193
  193084/2000000: episode: 276, duration: 13.511s, episode steps: 700, steps per second:  52, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.009865, mae: 0.578061, mean_q: 0.715676, mean_eps: 0.826541
  194073/2000000: episode: 277, duration: 19.058s, episode steps: 989, steps per second:  52, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.010028, mae: 0.584777, mean_q: 0.722704, mean_eps: 0.825780
  194722/2000000: episode: 278, duration: 11.995s, episode steps: 649, steps per second:  54, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.009275, mae: 0.573147, mean_q: 0.706973, mean_eps: 0.825042
  195252/2000000: episode: 279, duration: 9.603s, episode steps: 530, steps per second:  55, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.008756, mae: 0.580182, mean_q: 0.718057, mean_eps: 0.824513
  195756/2000000: episode: 280, duration: 9.520s, episode steps: 504, steps per second:  53, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: 0.010702, mae: 0.611595, mean_q: 0.756135, mean_eps: 0.824048
  196603/2000000: episode: 281, duration: 15.859s, episode steps: 847, steps per second:  53, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.011011, mae: 0.601306, mean_q: 0.743760, mean_eps: 0.823440
  197189/2000000: episode: 282, duration: 11.050s, episode steps: 586, steps per second:  53, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.009623, mae: 0.593189, mean_q: 0.732583, mean_eps: 0.822794
  197853/2000000: episode: 283, duration: 12.451s, episode steps: 664, steps per second:  53, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.009354, mae: 0.598924, mean_q: 0.740406, mean_eps: 0.822230
  198529/2000000: episode: 284, duration: 13.529s, episode steps: 676, steps per second:  50, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.009989, mae: 0.590846, mean_q: 0.731660, mean_eps: 0.821627
  198916/2000000: episode: 285, duration: 8.074s, episode steps: 387, steps per second:  48, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.009494, mae: 0.590872, mean_q: 0.729726, mean_eps: 0.821150
  199674/2000000: episode: 286, duration: 16.350s, episode steps: 758, steps per second:  46, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.009063, mae: 0.597431, mean_q: 0.737086, mean_eps: 0.820635
  200607/2000000: episode: 287, duration: 17.822s, episode steps: 933, steps per second:  52, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.010838, mae: 0.622393, mean_q: 0.767887, mean_eps: 0.819874
  201326/2000000: episode: 288, duration: 13.007s, episode steps: 719, steps per second:  55, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.009324, mae: 0.638077, mean_q: 0.788523, mean_eps: 0.819131
  201854/2000000: episode: 289, duration: 10.388s, episode steps: 528, steps per second:  51, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.009221, mae: 0.638790, mean_q: 0.787157, mean_eps: 0.818569
  202570/2000000: episode: 290, duration: 14.695s, episode steps: 716, steps per second:  49, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.009636, mae: 0.631469, mean_q: 0.779275, mean_eps: 0.818009
  203297/2000000: episode: 291, duration: 14.015s, episode steps: 727, steps per second:  52, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.009056, mae: 0.625146, mean_q: 0.771804, mean_eps: 0.817359
  203667/2000000: episode: 292, duration: 6.445s, episode steps: 370, steps per second:  57, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.009020, mae: 0.650751, mean_q: 0.804099, mean_eps: 0.816866
  204495/2000000: episode: 293, duration: 14.761s, episode steps: 828, steps per second:  56, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.669 [0.000, 5.000],  loss: 0.009779, mae: 0.636050, mean_q: 0.784218, mean_eps: 0.816328
  205287/2000000: episode: 294, duration: 15.052s, episode steps: 792, steps per second:  53, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.009477, mae: 0.644870, mean_q: 0.795377, mean_eps: 0.815599
  205833/2000000: episode: 295, duration: 10.095s, episode steps: 546, steps per second:  54, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.009184, mae: 0.634796, mean_q: 0.784479, mean_eps: 0.814996
  206511/2000000: episode: 296, duration: 12.616s, episode steps: 678, steps per second:  54, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.009609, mae: 0.638443, mean_q: 0.788868, mean_eps: 0.814445
  207137/2000000: episode: 297, duration: 11.509s, episode steps: 626, steps per second:  54, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.009467, mae: 0.636233, mean_q: 0.787087, mean_eps: 0.813858
  207626/2000000: episode: 298, duration: 8.835s, episode steps: 489, steps per second:  55, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.009824, mae: 0.643931, mean_q: 0.792737, mean_eps: 0.813356
  208091/2000000: episode: 299, duration: 8.525s, episode steps: 465, steps per second:  55, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.009771, mae: 0.644021, mean_q: 0.796362, mean_eps: 0.812928
  209288/2000000: episode: 300, duration: 22.617s, episode steps: 1197, steps per second:  53, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.010374, mae: 0.641676, mean_q: 0.791036, mean_eps: 0.812181
  210231/2000000: episode: 301, duration: 16.554s, episode steps: 943, steps per second:  57, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.009026, mae: 0.647566, mean_q: 0.798331, mean_eps: 0.811218
  210978/2000000: episode: 302, duration: 13.195s, episode steps: 747, steps per second:  57, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.009821, mae: 0.689249, mean_q: 0.849777, mean_eps: 0.810456
  211393/2000000: episode: 303, duration: 7.479s, episode steps: 415, steps per second:  55, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.012146, mae: 0.687700, mean_q: 0.846850, mean_eps: 0.809933
  212355/2000000: episode: 304, duration: 19.935s, episode steps: 962, steps per second:  48, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.009616, mae: 0.681905, mean_q: 0.841678, mean_eps: 0.809313
  212911/2000000: episode: 305, duration: 11.846s, episode steps: 556, steps per second:  47, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.008036, mae: 0.682848, mean_q: 0.845261, mean_eps: 0.808631
  213322/2000000: episode: 306, duration: 7.328s, episode steps: 411, steps per second:  56, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.008392, mae: 0.679298, mean_q: 0.837821, mean_eps: 0.808196
  213799/2000000: episode: 307, duration: 9.060s, episode steps: 477, steps per second:  53, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.009917, mae: 0.683675, mean_q: 0.839486, mean_eps: 0.807796
  214199/2000000: episode: 308, duration: 7.772s, episode steps: 400, steps per second:  51, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.009650, mae: 0.683193, mean_q: 0.842826, mean_eps: 0.807402
  214890/2000000: episode: 309, duration: 13.293s, episode steps: 691, steps per second:  52, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.010551, mae: 0.694088, mean_q: 0.855085, mean_eps: 0.806910
  215954/2000000: episode: 310, duration: 19.412s, episode steps: 1064, steps per second:  55, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.010143, mae: 0.714671, mean_q: 0.880959, mean_eps: 0.806120
  216741/2000000: episode: 311, duration: 14.565s, episode steps: 787, steps per second:  54, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.009211, mae: 0.716921, mean_q: 0.885912, mean_eps: 0.805287
  217366/2000000: episode: 312, duration: 12.429s, episode steps: 625, steps per second:  50, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.009521, mae: 0.733423, mean_q: 0.904409, mean_eps: 0.804651
  217861/2000000: episode: 313, duration: 9.931s, episode steps: 495, steps per second:  50, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.009340, mae: 0.712963, mean_q: 0.880691, mean_eps: 0.804147
  218789/2000000: episode: 314, duration: 20.191s, episode steps: 928, steps per second:  46, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.009568, mae: 0.728153, mean_q: 0.897513, mean_eps: 0.803507
  219582/2000000: episode: 315, duration: 15.101s, episode steps: 793, steps per second:  53, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.010557, mae: 0.705673, mean_q: 0.868043, mean_eps: 0.802733
  220080/2000000: episode: 316, duration: 9.249s, episode steps: 498, steps per second:  54, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.011919, mae: 0.725165, mean_q: 0.891771, mean_eps: 0.802153
  220841/2000000: episode: 317, duration: 14.372s, episode steps: 761, steps per second:  53, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.010759, mae: 0.740696, mean_q: 0.912838, mean_eps: 0.801586
  221559/2000000: episode: 318, duration: 14.090s, episode steps: 718, steps per second:  51, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.286 [0.000, 5.000],  loss: 0.009845, mae: 0.739066, mean_q: 0.911624, mean_eps: 0.800920
  221938/2000000: episode: 319, duration: 7.443s, episode steps: 379, steps per second:  51, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.010787, mae: 0.752522, mean_q: 0.926861, mean_eps: 0.800427
  222451/2000000: episode: 320, duration: 9.624s, episode steps: 513, steps per second:  53, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.009781, mae: 0.749503, mean_q: 0.924335, mean_eps: 0.800025
  223096/2000000: episode: 321, duration: 12.002s, episode steps: 645, steps per second:  54, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.010312, mae: 0.741089, mean_q: 0.912151, mean_eps: 0.799505
  223672/2000000: episode: 322, duration: 10.479s, episode steps: 576, steps per second:  55, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.010707, mae: 0.748510, mean_q: 0.920170, mean_eps: 0.798956
  224319/2000000: episode: 323, duration: 12.984s, episode steps: 647, steps per second:  50, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.010401, mae: 0.751001, mean_q: 0.925424, mean_eps: 0.798405
  225196/2000000: episode: 324, duration: 16.974s, episode steps: 877, steps per second:  52, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.010050, mae: 0.743403, mean_q: 0.917318, mean_eps: 0.797720
  225855/2000000: episode: 325, duration: 11.980s, episode steps: 659, steps per second:  55, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.010169, mae: 0.778505, mean_q: 0.958415, mean_eps: 0.797028
  226740/2000000: episode: 326, duration: 16.609s, episode steps: 885, steps per second:  53, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.010325, mae: 0.771424, mean_q: 0.949816, mean_eps: 0.796334
  227468/2000000: episode: 327, duration: 13.642s, episode steps: 728, steps per second:  53, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.010977, mae: 0.787857, mean_q: 0.967685, mean_eps: 0.795608
  228009/2000000: episode: 328, duration: 10.738s, episode steps: 541, steps per second:  50, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.010869, mae: 0.778479, mean_q: 0.956202, mean_eps: 0.795036
  228522/2000000: episode: 329, duration: 9.636s, episode steps: 513, steps per second:  53, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.009691, mae: 0.778001, mean_q: 0.955889, mean_eps: 0.794561
  229149/2000000: episode: 330, duration: 10.808s, episode steps: 627, steps per second:  58, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.332 [0.000, 5.000],  loss: 0.010116, mae: 0.781834, mean_q: 0.959785, mean_eps: 0.794048
  230027/2000000: episode: 331, duration: 15.049s, episode steps: 878, steps per second:  58, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.010976, mae: 0.791155, mean_q: 0.973670, mean_eps: 0.793371
  230743/2000000: episode: 332, duration: 13.696s, episode steps: 716, steps per second:  52, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.010909, mae: 0.790844, mean_q: 0.973726, mean_eps: 0.792654
  231205/2000000: episode: 333, duration: 8.651s, episode steps: 462, steps per second:  53, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.011606, mae: 0.807630, mean_q: 0.991468, mean_eps: 0.792123
  231918/2000000: episode: 334, duration: 14.392s, episode steps: 713, steps per second:  50, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.010387, mae: 0.794512, mean_q: 0.977499, mean_eps: 0.791594
  232454/2000000: episode: 335, duration: 9.546s, episode steps: 536, steps per second:  56, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.010527, mae: 0.796245, mean_q: 0.978714, mean_eps: 0.791033
  233433/2000000: episode: 336, duration: 17.379s, episode steps: 979, steps per second:  56, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.010730, mae: 0.795933, mean_q: 0.977751, mean_eps: 0.790350
  233959/2000000: episode: 337, duration: 9.236s, episode steps: 526, steps per second:  57, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.011382, mae: 0.787770, mean_q: 0.968159, mean_eps: 0.789674
  234586/2000000: episode: 338, duration: 12.237s, episode steps: 627, steps per second:  51, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.010346, mae: 0.789996, mean_q: 0.970279, mean_eps: 0.789155
  235641/2000000: episode: 339, duration: 19.819s, episode steps: 1055, steps per second:  53, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.012526, mae: 0.792032, mean_q: 0.972970, mean_eps: 0.788397
  236018/2000000: episode: 340, duration: 7.744s, episode steps: 377, steps per second:  49, episode reward: 10.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.010812, mae: 0.818276, mean_q: 1.006543, mean_eps: 0.787753
  236493/2000000: episode: 341, duration: 9.469s, episode steps: 475, steps per second:  50, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.011886, mae: 0.786470, mean_q: 0.966289, mean_eps: 0.787370
  237019/2000000: episode: 342, duration: 11.002s, episode steps: 526, steps per second:  48, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.011510, mae: 0.805270, mean_q: 0.988339, mean_eps: 0.786920
  237838/2000000: episode: 343, duration: 19.906s, episode steps: 819, steps per second:  41, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.011453, mae: 0.810670, mean_q: 0.993532, mean_eps: 0.786315
  238457/2000000: episode: 344, duration: 12.303s, episode steps: 619, steps per second:  50, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.010549, mae: 0.805381, mean_q: 0.987838, mean_eps: 0.785667
  238958/2000000: episode: 345, duration: 9.817s, episode steps: 501, steps per second:  51, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.010935, mae: 0.811208, mean_q: 0.996146, mean_eps: 0.785163
  239712/2000000: episode: 346, duration: 14.370s, episode steps: 754, steps per second:  52, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.009706, mae: 0.801025, mean_q: 0.984398, mean_eps: 0.784599
  240248/2000000: episode: 347, duration: 10.097s, episode steps: 536, steps per second:  53, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.009486, mae: 0.822231, mean_q: 1.007983, mean_eps: 0.784020
  240877/2000000: episode: 348, duration: 12.250s, episode steps: 629, steps per second:  51, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.009750, mae: 0.836624, mean_q: 1.026710, mean_eps: 0.783494
  241371/2000000: episode: 349, duration: 9.502s, episode steps: 494, steps per second:  52, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.012161, mae: 0.846051, mean_q: 1.037337, mean_eps: 0.782988
  241922/2000000: episode: 350, duration: 11.622s, episode steps: 551, steps per second:  47, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.010856, mae: 0.840513, mean_q: 1.028883, mean_eps: 0.782519
  242472/2000000: episode: 351, duration: 10.957s, episode steps: 550, steps per second:  50, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.009821, mae: 0.836418, mean_q: 1.023768, mean_eps: 0.782024
  242892/2000000: episode: 352, duration: 9.997s, episode steps: 420, steps per second:  42, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.011313, mae: 0.850083, mean_q: 1.040606, mean_eps: 0.781588
  243665/2000000: episode: 353, duration: 18.223s, episode steps: 773, steps per second:  42, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.011023, mae: 0.833743, mean_q: 1.022082, mean_eps: 0.781050
  244499/2000000: episode: 354, duration: 16.666s, episode steps: 834, steps per second:  50, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.012096, mae: 0.844686, mean_q: 1.037120, mean_eps: 0.780326
  245433/2000000: episode: 355, duration: 20.052s, episode steps: 934, steps per second:  47, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.011789, mae: 0.860344, mean_q: 1.053371, mean_eps: 0.779531
  245817/2000000: episode: 356, duration: 8.251s, episode steps: 384, steps per second:  47, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.286 [0.000, 5.000],  loss: 0.011141, mae: 0.890213, mean_q: 1.091964, mean_eps: 0.778937
  246468/2000000: episode: 357, duration: 14.945s, episode steps: 651, steps per second:  44, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.012130, mae: 0.866716, mean_q: 1.063787, mean_eps: 0.778472
  246987/2000000: episode: 358, duration: 10.245s, episode steps: 519, steps per second:  51, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.012039, mae: 0.881276, mean_q: 1.084617, mean_eps: 0.777947
  247429/2000000: episode: 359, duration: 8.802s, episode steps: 442, steps per second:  50, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.011352, mae: 0.876233, mean_q: 1.075293, mean_eps: 0.777513
  247836/2000000: episode: 360, duration: 7.747s, episode steps: 407, steps per second:  53, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.010447, mae: 0.868926, mean_q: 1.066654, mean_eps: 0.777131
  248741/2000000: episode: 361, duration: 18.769s, episode steps: 905, steps per second:  48, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.010660, mae: 0.874080, mean_q: 1.072870, mean_eps: 0.776541
  249124/2000000: episode: 362, duration: 8.234s, episode steps: 383, steps per second:  47, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.011537, mae: 0.870480, mean_q: 1.067286, mean_eps: 0.775961
  250160/2000000: episode: 363, duration: 20.392s, episode steps: 1036, steps per second:  51, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.010635, mae: 0.881056, mean_q: 1.078343, mean_eps: 0.775324
  250632/2000000: episode: 364, duration: 9.824s, episode steps: 472, steps per second:  48, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.010117, mae: 0.910181, mean_q: 1.113836, mean_eps: 0.774645
  251596/2000000: episode: 365, duration: 18.765s, episode steps: 964, steps per second:  51, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.012149, mae: 0.904511, mean_q: 1.106652, mean_eps: 0.773999
  252242/2000000: episode: 366, duration: 13.168s, episode steps: 646, steps per second:  49, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.010642, mae: 0.898138, mean_q: 1.098297, mean_eps: 0.773274
  252695/2000000: episode: 367, duration: 10.070s, episode steps: 453, steps per second:  45, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.013557, mae: 0.908358, mean_q: 1.110443, mean_eps: 0.772779
  253232/2000000: episode: 368, duration: 12.440s, episode steps: 537, steps per second:  43, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.268 [0.000, 5.000],  loss: 0.011629, mae: 0.896452, mean_q: 1.094853, mean_eps: 0.772334
  254080/2000000: episode: 369, duration: 19.873s, episode steps: 848, steps per second:  43, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.012699, mae: 0.906198, mean_q: 1.107346, mean_eps: 0.771711
  254430/2000000: episode: 370, duration: 8.020s, episode steps: 350, steps per second:  44, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.012011, mae: 0.900029, mean_q: 1.099531, mean_eps: 0.771171
  254911/2000000: episode: 371, duration: 11.209s, episode steps: 481, steps per second:  43, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.324 [0.000, 5.000],  loss: 0.012110, mae: 0.899123, mean_q: 1.098117, mean_eps: 0.770797
  255948/2000000: episode: 372, duration: 24.189s, episode steps: 1037, steps per second:  43, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.011125, mae: 0.918197, mean_q: 1.121428, mean_eps: 0.770115
  256564/2000000: episode: 373, duration: 13.368s, episode steps: 616, steps per second:  46, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.014293, mae: 0.934332, mean_q: 1.140374, mean_eps: 0.769371
  257349/2000000: episode: 374, duration: 17.854s, episode steps: 785, steps per second:  44, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.012311, mae: 0.930970, mean_q: 1.137681, mean_eps: 0.768740
  258372/2000000: episode: 375, duration: 21.882s, episode steps: 1023, steps per second:  47, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.010520, mae: 0.929423, mean_q: 1.135934, mean_eps: 0.767926
  259139/2000000: episode: 376, duration: 15.621s, episode steps: 767, steps per second:  49, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.010182, mae: 0.925987, mean_q: 1.132559, mean_eps: 0.767121
  259747/2000000: episode: 377, duration: 13.760s, episode steps: 608, steps per second:  44, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.010694, mae: 0.926940, mean_q: 1.133507, mean_eps: 0.766502
  260687/2000000: episode: 378, duration: 20.864s, episode steps: 940, steps per second:  45, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.011228, mae: 0.957903, mean_q: 1.171274, mean_eps: 0.765806
  261961/2000000: episode: 379, duration: 26.567s, episode steps: 1274, steps per second:  48, episode reward: 22.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.011890, mae: 0.959361, mean_q: 1.171912, mean_eps: 0.764808
  262779/2000000: episode: 380, duration: 18.050s, episode steps: 818, steps per second:  45, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.011981, mae: 0.961452, mean_q: 1.175318, mean_eps: 0.763867
  263198/2000000: episode: 381, duration: 9.413s, episode steps: 419, steps per second:  45, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.220 [0.000, 5.000],  loss: 0.012253, mae: 0.969836, mean_q: 1.183799, mean_eps: 0.763311
  263709/2000000: episode: 382, duration: 9.865s, episode steps: 511, steps per second:  52, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.010618, mae: 0.954752, mean_q: 1.165196, mean_eps: 0.762891
  264262/2000000: episode: 383, duration: 10.117s, episode steps: 553, steps per second:  55, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.010403, mae: 0.967075, mean_q: 1.181972, mean_eps: 0.762413
  264938/2000000: episode: 384, duration: 12.561s, episode steps: 676, steps per second:  54, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.012363, mae: 0.958568, mean_q: 1.171435, mean_eps: 0.761860
  265483/2000000: episode: 385, duration: 11.550s, episode steps: 545, steps per second:  47, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.012816, mae: 1.017103, mean_q: 1.243228, mean_eps: 0.761311
  266048/2000000: episode: 386, duration: 11.289s, episode steps: 565, steps per second:  50, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.010139, mae: 1.015464, mean_q: 1.239973, mean_eps: 0.760812
  266542/2000000: episode: 387, duration: 10.194s, episode steps: 494, steps per second:  48, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.012159, mae: 1.021697, mean_q: 1.247795, mean_eps: 0.760335
  267271/2000000: episode: 388, duration: 14.314s, episode steps: 729, steps per second:  51, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.012025, mae: 1.003112, mean_q: 1.226657, mean_eps: 0.759785
  267832/2000000: episode: 389, duration: 11.006s, episode steps: 561, steps per second:  51, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.013159, mae: 1.019768, mean_q: 1.247359, mean_eps: 0.759205
  268575/2000000: episode: 390, duration: 16.106s, episode steps: 743, steps per second:  46, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.011837, mae: 1.029255, mean_q: 1.258083, mean_eps: 0.758618
  269109/2000000: episode: 391, duration: 11.542s, episode steps: 534, steps per second:  46, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.011987, mae: 1.020123, mean_q: 1.245560, mean_eps: 0.758042
  269504/2000000: episode: 392, duration: 8.949s, episode steps: 395, steps per second:  44, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.218 [0.000, 5.000],  loss: 0.011977, mae: 1.017370, mean_q: 1.242712, mean_eps: 0.757625
  270046/2000000: episode: 393, duration: 11.985s, episode steps: 542, steps per second:  45, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.011940, mae: 1.013921, mean_q: 1.237956, mean_eps: 0.757203
  270696/2000000: episode: 394, duration: 14.757s, episode steps: 650, steps per second:  44, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.011888, mae: 1.056606, mean_q: 1.290688, mean_eps: 0.756667
  271573/2000000: episode: 395, duration: 19.816s, episode steps: 877, steps per second:  44, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.013908, mae: 1.047047, mean_q: 1.279415, mean_eps: 0.755979
  272231/2000000: episode: 396, duration: 14.556s, episode steps: 658, steps per second:  45, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.246 [0.000, 5.000],  loss: 0.012057, mae: 1.046907, mean_q: 1.278797, mean_eps: 0.755288
  273361/2000000: episode: 397, duration: 23.596s, episode steps: 1130, steps per second:  48, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.011908, mae: 1.044366, mean_q: 1.274298, mean_eps: 0.754484
  273872/2000000: episode: 398, duration: 10.026s, episode steps: 511, steps per second:  51, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.012847, mae: 1.052339, mean_q: 1.283611, mean_eps: 0.753746
  274616/2000000: episode: 399, duration: 15.035s, episode steps: 744, steps per second:  49, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.011991, mae: 1.036267, mean_q: 1.265398, mean_eps: 0.753182
  275213/2000000: episode: 400, duration: 11.634s, episode steps: 597, steps per second:  51, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.273 [0.000, 5.000],  loss: 0.012181, mae: 1.048995, mean_q: 1.281273, mean_eps: 0.752577
  275768/2000000: episode: 401, duration: 11.179s, episode steps: 555, steps per second:  50, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.011672, mae: 1.087993, mean_q: 1.330244, mean_eps: 0.752059
  276225/2000000: episode: 402, duration: 9.702s, episode steps: 457, steps per second:  47, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.011638, mae: 1.079395, mean_q: 1.317422, mean_eps: 0.751604
  276801/2000000: episode: 403, duration: 14.175s, episode steps: 576, steps per second:  41, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.011510, mae: 1.066861, mean_q: 1.302899, mean_eps: 0.751137
  277686/2000000: episode: 404, duration: 21.302s, episode steps: 885, steps per second:  42, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.012661, mae: 1.072350, mean_q: 1.308671, mean_eps: 0.750480
  278210/2000000: episode: 405, duration: 11.134s, episode steps: 524, steps per second:  47, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.011209, mae: 1.079168, mean_q: 1.316847, mean_eps: 0.749847
  279096/2000000: episode: 406, duration: 18.977s, episode steps: 886, steps per second:  47, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.012318, mae: 1.076642, mean_q: 1.312868, mean_eps: 0.749213
  279466/2000000: episode: 407, duration: 8.201s, episode steps: 370, steps per second:  45, episode reward:  9.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.013345, mae: 1.085152, mean_q: 1.321260, mean_eps: 0.748648
  280262/2000000: episode: 408, duration: 16.915s, episode steps: 796, steps per second:  47, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.012225, mae: 1.087733, mean_q: 1.327663, mean_eps: 0.748122
  280887/2000000: episode: 409, duration: 11.567s, episode steps: 625, steps per second:  54, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.011269, mae: 1.126592, mean_q: 1.375478, mean_eps: 0.747483
  281436/2000000: episode: 410, duration: 11.043s, episode steps: 549, steps per second:  50, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.012031, mae: 1.111867, mean_q: 1.356251, mean_eps: 0.746956
  282459/2000000: episode: 411, duration: 21.357s, episode steps: 1023, steps per second:  48, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.011963, mae: 1.110125, mean_q: 1.353984, mean_eps: 0.746249
  282987/2000000: episode: 412, duration: 11.550s, episode steps: 528, steps per second:  46, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.012157, mae: 1.118270, mean_q: 1.364452, mean_eps: 0.745550
  283604/2000000: episode: 413, duration: 12.895s, episode steps: 617, steps per second:  48, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.012130, mae: 1.117689, mean_q: 1.362254, mean_eps: 0.745035
  284067/2000000: episode: 414, duration: 9.479s, episode steps: 463, steps per second:  49, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.012486, mae: 1.113805, mean_q: 1.356635, mean_eps: 0.744549
  284925/2000000: episode: 415, duration: 17.194s, episode steps: 858, steps per second:  50, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.012416, mae: 1.112396, mean_q: 1.356695, mean_eps: 0.743954
  285806/2000000: episode: 416, duration: 18.475s, episode steps: 881, steps per second:  48, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.011817, mae: 1.117548, mean_q: 1.362741, mean_eps: 0.743171
  286358/2000000: episode: 417, duration: 11.181s, episode steps: 552, steps per second:  49, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.012569, mae: 1.112505, mean_q: 1.356380, mean_eps: 0.742526
  287219/2000000: episode: 418, duration: 18.139s, episode steps: 861, steps per second:  47, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.012428, mae: 1.113250, mean_q: 1.358417, mean_eps: 0.741891
  287799/2000000: episode: 419, duration: 12.538s, episode steps: 580, steps per second:  46, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.012436, mae: 1.119725, mean_q: 1.366982, mean_eps: 0.741243
  288449/2000000: episode: 420, duration: 15.678s, episode steps: 650, steps per second:  41, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.012072, mae: 1.113760, mean_q: 1.360061, mean_eps: 0.740688
  288954/2000000: episode: 421, duration: 12.058s, episode steps: 505, steps per second:  42, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.011504, mae: 1.111385, mean_q: 1.354936, mean_eps: 0.740168
  289761/2000000: episode: 422, duration: 17.097s, episode steps: 807, steps per second:  47, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.014476, mae: 1.110072, mean_q: 1.352841, mean_eps: 0.739578
  290264/2000000: episode: 423, duration: 10.358s, episode steps: 503, steps per second:  49, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.011015, mae: 1.123526, mean_q: 1.369474, mean_eps: 0.738989
  290911/2000000: episode: 424, duration: 13.750s, episode steps: 647, steps per second:  47, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.011537, mae: 1.129708, mean_q: 1.377332, mean_eps: 0.738473
  291777/2000000: episode: 425, duration: 19.231s, episode steps: 866, steps per second:  45, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.009921, mae: 1.124142, mean_q: 1.371240, mean_eps: 0.737790
  292308/2000000: episode: 426, duration: 11.099s, episode steps: 531, steps per second:  48, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.010541, mae: 1.138435, mean_q: 1.388157, mean_eps: 0.737162
  292980/2000000: episode: 427, duration: 14.579s, episode steps: 672, steps per second:  46, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.011921, mae: 1.137342, mean_q: 1.386031, mean_eps: 0.736622
  293697/2000000: episode: 428, duration: 16.382s, episode steps: 717, steps per second:  44, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.011683, mae: 1.120937, mean_q: 1.365517, mean_eps: 0.735996
  294212/2000000: episode: 429, duration: 11.651s, episode steps: 515, steps per second:  44, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.012832, mae: 1.129537, mean_q: 1.375803, mean_eps: 0.735441
  295000/2000000: episode: 430, duration: 16.711s, episode steps: 788, steps per second:  47, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.011963, mae: 1.122938, mean_q: 1.366072, mean_eps: 0.734856
  295709/2000000: episode: 431, duration: 15.675s, episode steps: 709, steps per second:  45, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.010049, mae: 1.169415, mean_q: 1.425036, mean_eps: 0.734181
  296527/2000000: episode: 432, duration: 18.016s, episode steps: 818, steps per second:  45, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.012532, mae: 1.168527, mean_q: 1.422592, mean_eps: 0.733494
  297615/2000000: episode: 433, duration: 21.833s, episode steps: 1088, steps per second:  50, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.011327, mae: 1.170563, mean_q: 1.424407, mean_eps: 0.732637
  298089/2000000: episode: 434, duration: 9.006s, episode steps: 474, steps per second:  53, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.306 [0.000, 5.000],  loss: 0.012628, mae: 1.170123, mean_q: 1.426017, mean_eps: 0.731933
  298837/2000000: episode: 435, duration: 14.271s, episode steps: 748, steps per second:  52, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.012440, mae: 1.175148, mean_q: 1.430317, mean_eps: 0.731382
  299364/2000000: episode: 436, duration: 11.067s, episode steps: 527, steps per second:  48, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.011396, mae: 1.172392, mean_q: 1.424629, mean_eps: 0.730810
  300013/2000000: episode: 437, duration: 13.373s, episode steps: 649, steps per second:  49, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.012101, mae: 1.171580, mean_q: 1.424421, mean_eps: 0.730281
  300876/2000000: episode: 438, duration: 16.440s, episode steps: 863, steps per second:  52, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.011497, mae: 1.197954, mean_q: 1.458734, mean_eps: 0.729600
  301637/2000000: episode: 439, duration: 13.901s, episode steps: 761, steps per second:  55, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.296 [0.000, 5.000],  loss: 0.014376, mae: 1.205051, mean_q: 1.465856, mean_eps: 0.728870
  302261/2000000: episode: 440, duration: 11.549s, episode steps: 624, steps per second:  54, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.747 [0.000, 5.000],  loss: 0.013865, mae: 1.190415, mean_q: 1.448419, mean_eps: 0.728245
  303159/2000000: episode: 441, duration: 17.223s, episode steps: 898, steps per second:  52, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.013288, mae: 1.195529, mean_q: 1.452915, mean_eps: 0.727561
  303756/2000000: episode: 442, duration: 11.229s, episode steps: 597, steps per second:  53, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.012227, mae: 1.183860, mean_q: 1.440748, mean_eps: 0.726890
  304333/2000000: episode: 443, duration: 11.463s, episode steps: 577, steps per second:  50, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.011544, mae: 1.197344, mean_q: 1.458151, mean_eps: 0.726360
  304960/2000000: episode: 444, duration: 12.755s, episode steps: 627, steps per second:  49, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.012624, mae: 1.199938, mean_q: 1.459400, mean_eps: 0.725819
  305730/2000000: episode: 445, duration: 17.741s, episode steps: 770, steps per second:  43, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.013037, mae: 1.184441, mean_q: 1.442098, mean_eps: 0.725190
  306374/2000000: episode: 446, duration: 13.974s, episode steps: 644, steps per second:  46, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.011183, mae: 1.190305, mean_q: 1.448719, mean_eps: 0.724553
  306866/2000000: episode: 447, duration: 9.555s, episode steps: 492, steps per second:  51, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.012656, mae: 1.195093, mean_q: 1.453546, mean_eps: 0.724042
  307530/2000000: episode: 448, duration: 12.405s, episode steps: 664, steps per second:  54, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.013341, mae: 1.188611, mean_q: 1.444068, mean_eps: 0.723522
  308186/2000000: episode: 449, duration: 12.798s, episode steps: 656, steps per second:  51, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.323 [0.000, 5.000],  loss: 0.013753, mae: 1.185937, mean_q: 1.442402, mean_eps: 0.722928
  308855/2000000: episode: 450, duration: 13.801s, episode steps: 669, steps per second:  48, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.013563, mae: 1.191772, mean_q: 1.450902, mean_eps: 0.722332
  309897/2000000: episode: 451, duration: 20.257s, episode steps: 1042, steps per second:  51, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.011413, mae: 1.184501, mean_q: 1.441989, mean_eps: 0.721562
  310700/2000000: episode: 452, duration: 15.046s, episode steps: 803, steps per second:  53, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.010927, mae: 1.206053, mean_q: 1.469035, mean_eps: 0.720732
  311343/2000000: episode: 453, duration: 12.480s, episode steps: 643, steps per second:  52, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.012921, mae: 1.188794, mean_q: 1.448026, mean_eps: 0.720082
  312288/2000000: episode: 454, duration: 18.985s, episode steps: 945, steps per second:  50, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.012195, mae: 1.200081, mean_q: 1.460614, mean_eps: 0.719367
  312859/2000000: episode: 455, duration: 11.768s, episode steps: 571, steps per second:  49, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.012091, mae: 1.197873, mean_q: 1.458398, mean_eps: 0.718685
  313292/2000000: episode: 456, duration: 8.860s, episode steps: 433, steps per second:  49, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.681 [0.000, 5.000],  loss: 0.014022, mae: 1.215004, mean_q: 1.477916, mean_eps: 0.718233
  314259/2000000: episode: 457, duration: 19.846s, episode steps: 967, steps per second:  49, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.012275, mae: 1.193731, mean_q: 1.451200, mean_eps: 0.717603
  314739/2000000: episode: 458, duration: 9.735s, episode steps: 480, steps per second:  49, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.642 [0.000, 5.000],  loss: 0.012264, mae: 1.185555, mean_q: 1.440953, mean_eps: 0.716952
  315227/2000000: episode: 459, duration: 9.697s, episode steps: 488, steps per second:  50, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.012058, mae: 1.234354, mean_q: 1.501521, mean_eps: 0.716516
  315953/2000000: episode: 460, duration: 14.206s, episode steps: 726, steps per second:  51, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.012256, mae: 1.240007, mean_q: 1.507527, mean_eps: 0.715969
  316452/2000000: episode: 461, duration: 9.366s, episode steps: 499, steps per second:  53, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.639 [0.000, 5.000],  loss: 0.013007, mae: 1.251223, mean_q: 1.523116, mean_eps: 0.715418
  317078/2000000: episode: 462, duration: 11.989s, episode steps: 626, steps per second:  52, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.332 [0.000, 5.000],  loss: 0.012911, mae: 1.242887, mean_q: 1.512332, mean_eps: 0.714912
  317724/2000000: episode: 463, duration: 13.356s, episode steps: 646, steps per second:  48, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.013809, mae: 1.241130, mean_q: 1.508884, mean_eps: 0.714340
  318562/2000000: episode: 464, duration: 18.021s, episode steps: 838, steps per second:  47, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.012650, mae: 1.242931, mean_q: 1.510113, mean_eps: 0.713672
  319556/2000000: episode: 465, duration: 19.914s, episode steps: 994, steps per second:  50, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.012659, mae: 1.235714, mean_q: 1.502261, mean_eps: 0.712848
  319957/2000000: episode: 466, duration: 8.264s, episode steps: 401, steps per second:  49, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.013847, mae: 1.258325, mean_q: 1.530584, mean_eps: 0.712220
  320600/2000000: episode: 467, duration: 13.608s, episode steps: 643, steps per second:  47, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.012072, mae: 1.286166, mean_q: 1.564961, mean_eps: 0.711750
  321163/2000000: episode: 468, duration: 11.924s, episode steps: 563, steps per second:  47, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.831 [0.000, 5.000],  loss: 0.014427, mae: 1.302271, mean_q: 1.584544, mean_eps: 0.711208
  321777/2000000: episode: 469, duration: 13.361s, episode steps: 614, steps per second:  46, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.037 [0.000, 5.000],  loss: 0.012842, mae: 1.299635, mean_q: 1.581243, mean_eps: 0.710677
  322419/2000000: episode: 470, duration: 14.595s, episode steps: 642, steps per second:  44, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.013152, mae: 1.294313, mean_q: 1.574310, mean_eps: 0.710112
  322817/2000000: episode: 471, duration: 8.911s, episode steps: 398, steps per second:  45, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.653 [0.000, 5.000],  loss: 0.012796, mae: 1.288646, mean_q: 1.567581, mean_eps: 0.709644
  323410/2000000: episode: 472, duration: 13.412s, episode steps: 593, steps per second:  44, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.691 [0.000, 5.000],  loss: 0.012682, mae: 1.307677, mean_q: 1.592698, mean_eps: 0.709197
  324016/2000000: episode: 473, duration: 13.952s, episode steps: 606, steps per second:  43, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.823 [0.000, 5.000],  loss: 0.013324, mae: 1.311041, mean_q: 1.594628, mean_eps: 0.708659
  324659/2000000: episode: 474, duration: 14.669s, episode steps: 643, steps per second:  44, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.012383, mae: 1.290236, mean_q: 1.567986, mean_eps: 0.708098
  325380/2000000: episode: 475, duration: 16.752s, episode steps: 721, steps per second:  43, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.011927, mae: 1.303421, mean_q: 1.586007, mean_eps: 0.707484
  326467/2000000: episode: 476, duration: 23.753s, episode steps: 1087, steps per second:  46, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.012939, mae: 1.331611, mean_q: 1.619400, mean_eps: 0.706670
  326879/2000000: episode: 477, duration: 9.165s, episode steps: 412, steps per second:  45, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.012786, mae: 1.310522, mean_q: 1.593364, mean_eps: 0.705995
  327527/2000000: episode: 478, duration: 13.964s, episode steps: 648, steps per second:  46, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.011456, mae: 1.322236, mean_q: 1.607845, mean_eps: 0.705518
  328039/2000000: episode: 479, duration: 11.222s, episode steps: 512, steps per second:  46, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.012796, mae: 1.311295, mean_q: 1.593128, mean_eps: 0.704996
  328454/2000000: episode: 480, duration: 8.805s, episode steps: 415, steps per second:  47, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.014127, mae: 1.341326, mean_q: 1.629600, mean_eps: 0.704579
  329110/2000000: episode: 481, duration: 14.071s, episode steps: 656, steps per second:  47, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 0.012832, mae: 1.330229, mean_q: 1.618668, mean_eps: 0.704096
  329767/2000000: episode: 482, duration: 13.864s, episode steps: 657, steps per second:  47, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.014508, mae: 1.320165, mean_q: 1.605190, mean_eps: 0.703506
  330910/2000000: episode: 483, duration: 24.266s, episode steps: 1143, steps per second:  47, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.013870, mae: 1.348184, mean_q: 1.640310, mean_eps: 0.702696
  331877/2000000: episode: 484, duration: 21.255s, episode steps: 967, steps per second:  45, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.012226, mae: 1.346597, mean_q: 1.638284, mean_eps: 0.701745
  332598/2000000: episode: 485, duration: 14.709s, episode steps: 721, steps per second:  49, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.011951, mae: 1.335814, mean_q: 1.625585, mean_eps: 0.700986
  333368/2000000: episode: 486, duration: 15.172s, episode steps: 770, steps per second:  51, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.218 [0.000, 5.000],  loss: 0.014216, mae: 1.354942, mean_q: 1.645619, mean_eps: 0.700316
  333961/2000000: episode: 487, duration: 12.497s, episode steps: 593, steps per second:  47, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.263 [0.000, 5.000],  loss: 0.013939, mae: 1.361016, mean_q: 1.654949, mean_eps: 0.699702
  334317/2000000: episode: 488, duration: 8.150s, episode steps: 356, steps per second:  44, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.012487, mae: 1.340137, mean_q: 1.631348, mean_eps: 0.699274
  335184/2000000: episode: 489, duration: 18.795s, episode steps: 867, steps per second:  46, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.011812, mae: 1.358163, mean_q: 1.652375, mean_eps: 0.698725
  335635/2000000: episode: 490, duration: 8.853s, episode steps: 451, steps per second:  51, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.012668, mae: 1.388905, mean_q: 1.690557, mean_eps: 0.698133
  336310/2000000: episode: 491, duration: 13.811s, episode steps: 675, steps per second:  49, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.012647, mae: 1.362524, mean_q: 1.655547, mean_eps: 0.697625
  337260/2000000: episode: 492, duration: 19.501s, episode steps: 950, steps per second:  49, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.013481, mae: 1.375955, mean_q: 1.673806, mean_eps: 0.696894
  338678/2000000: episode: 493, duration: 30.005s, episode steps: 1418, steps per second:  47, episode reward: 24.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.011588, mae: 1.360994, mean_q: 1.657617, mean_eps: 0.695829
  339619/2000000: episode: 494, duration: 19.904s, episode steps: 941, steps per second:  47, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.013915, mae: 1.363571, mean_q: 1.657469, mean_eps: 0.694767
  340107/2000000: episode: 495, duration: 12.049s, episode steps: 488, steps per second:  41, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.287 [0.000, 5.000],  loss: 0.011684, mae: 1.368141, mean_q: 1.665372, mean_eps: 0.694124
  340479/2000000: episode: 496, duration: 10.028s, episode steps: 372, steps per second:  37, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.011652, mae: 1.416836, mean_q: 1.723152, mean_eps: 0.693737
  341347/2000000: episode: 497, duration: 18.825s, episode steps: 868, steps per second:  46, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.326 [0.000, 5.000],  loss: 0.013237, mae: 1.411762, mean_q: 1.717586, mean_eps: 0.693179
  342050/2000000: episode: 498, duration: 14.717s, episode steps: 703, steps per second:  48, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.014369, mae: 1.435597, mean_q: 1.746227, mean_eps: 0.692472
  342910/2000000: episode: 499, duration: 19.367s, episode steps: 860, steps per second:  44, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.287 [0.000, 5.000],  loss: 0.012672, mae: 1.419258, mean_q: 1.724817, mean_eps: 0.691768
  343567/2000000: episode: 500, duration: 14.806s, episode steps: 657, steps per second:  44, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.014760, mae: 1.420126, mean_q: 1.725659, mean_eps: 0.691086
  344572/2000000: episode: 501, duration: 21.094s, episode steps: 1005, steps per second:  48, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.014237, mae: 1.402909, mean_q: 1.703191, mean_eps: 0.690339
  345499/2000000: episode: 502, duration: 20.736s, episode steps: 927, steps per second:  45, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.351 [0.000, 5.000],  loss: 0.012434, mae: 1.441194, mean_q: 1.752064, mean_eps: 0.689469
  345876/2000000: episode: 503, duration: 8.596s, episode steps: 377, steps per second:  44, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.671 [0.000, 5.000],  loss: 0.013634, mae: 1.445558, mean_q: 1.759966, mean_eps: 0.688883
  346666/2000000: episode: 504, duration: 17.041s, episode steps: 790, steps per second:  46, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.296 [0.000, 5.000],  loss: 0.012183, mae: 1.446034, mean_q: 1.759333, mean_eps: 0.688357
  347536/2000000: episode: 505, duration: 19.069s, episode steps: 870, steps per second:  46, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.014216, mae: 1.448656, mean_q: 1.760507, mean_eps: 0.687610
  348125/2000000: episode: 506, duration: 12.787s, episode steps: 589, steps per second:  46, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.013060, mae: 1.437442, mean_q: 1.746679, mean_eps: 0.686953
  348949/2000000: episode: 507, duration: 17.227s, episode steps: 824, steps per second:  48, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.013089, mae: 1.428219, mean_q: 1.734540, mean_eps: 0.686316
  349669/2000000: episode: 508, duration: 13.565s, episode steps: 720, steps per second:  53, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.012288, mae: 1.441474, mean_q: 1.751040, mean_eps: 0.685621
  350464/2000000: episode: 509, duration: 15.671s, episode steps: 795, steps per second:  51, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.013373, mae: 1.466436, mean_q: 1.783342, mean_eps: 0.684941
  350962/2000000: episode: 510, duration: 10.518s, episode steps: 498, steps per second:  47, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.012878, mae: 1.481068, mean_q: 1.799827, mean_eps: 0.684359
  351805/2000000: episode: 511, duration: 17.538s, episode steps: 843, steps per second:  48, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.013684, mae: 1.489572, mean_q: 1.810987, mean_eps: 0.683754
  352170/2000000: episode: 512, duration: 7.844s, episode steps: 365, steps per second:  47, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.011604, mae: 1.476145, mean_q: 1.795430, mean_eps: 0.683211
  352849/2000000: episode: 513, duration: 13.623s, episode steps: 679, steps per second:  50, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.013234, mae: 1.469656, mean_q: 1.786465, mean_eps: 0.682741
  353516/2000000: episode: 514, duration: 13.750s, episode steps: 667, steps per second:  49, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.011887, mae: 1.458616, mean_q: 1.771914, mean_eps: 0.682136
  354365/2000000: episode: 515, duration: 17.489s, episode steps: 849, steps per second:  49, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.326 [0.000, 5.000],  loss: 0.014422, mae: 1.474052, mean_q: 1.791145, mean_eps: 0.681454
  355114/2000000: episode: 516, duration: 15.641s, episode steps: 749, steps per second:  48, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.015406, mae: 1.478061, mean_q: 1.795321, mean_eps: 0.680734
  355818/2000000: episode: 517, duration: 15.528s, episode steps: 704, steps per second:  45, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.012895, mae: 1.492271, mean_q: 1.813299, mean_eps: 0.680081
  356837/2000000: episode: 518, duration: 21.617s, episode steps: 1019, steps per second:  47, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.013591, mae: 1.489396, mean_q: 1.810999, mean_eps: 0.679305
  357857/2000000: episode: 519, duration: 21.298s, episode steps: 1020, steps per second:  48, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.197 [0.000, 5.000],  loss: 0.012542, mae: 1.491064, mean_q: 1.811701, mean_eps: 0.678387
  358297/2000000: episode: 520, duration: 9.435s, episode steps: 440, steps per second:  47, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.741 [0.000, 5.000],  loss: 0.012913, mae: 1.491436, mean_q: 1.812027, mean_eps: 0.677730
  358704/2000000: episode: 521, duration: 8.628s, episode steps: 407, steps per second:  47, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.732 [0.000, 5.000],  loss: 0.013096, mae: 1.495186, mean_q: 1.814980, mean_eps: 0.677350
  359398/2000000: episode: 522, duration: 15.950s, episode steps: 694, steps per second:  44, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.013439, mae: 1.496768, mean_q: 1.818443, mean_eps: 0.676855
  360233/2000000: episode: 523, duration: 20.940s, episode steps: 835, steps per second:  40, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.672 [0.000, 5.000],  loss: 0.011911, mae: 1.495779, mean_q: 1.819998, mean_eps: 0.676166
  360881/2000000: episode: 524, duration: 13.634s, episode steps: 648, steps per second:  48, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.014351, mae: 1.507547, mean_q: 1.833828, mean_eps: 0.675498
  361698/2000000: episode: 525, duration: 17.824s, episode steps: 817, steps per second:  46, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.014003, mae: 1.530538, mean_q: 1.860914, mean_eps: 0.674839
  362174/2000000: episode: 526, duration: 11.079s, episode steps: 476, steps per second:  43, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.708 [0.000, 5.000],  loss: 0.016282, mae: 1.511321, mean_q: 1.836772, mean_eps: 0.674258
  363271/2000000: episode: 527, duration: 25.122s, episode steps: 1097, steps per second:  44, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.013602, mae: 1.530206, mean_q: 1.859811, mean_eps: 0.673550
  364366/2000000: episode: 528, duration: 23.663s, episode steps: 1095, steps per second:  46, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.013684, mae: 1.516554, mean_q: 1.842139, mean_eps: 0.672564
  365081/2000000: episode: 529, duration: 16.054s, episode steps: 715, steps per second:  45, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.014194, mae: 1.532608, mean_q: 1.864168, mean_eps: 0.671748
  365987/2000000: episode: 530, duration: 19.388s, episode steps: 906, steps per second:  47, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.012901, mae: 1.551337, mean_q: 1.886623, mean_eps: 0.671019
  366740/2000000: episode: 531, duration: 15.189s, episode steps: 753, steps per second:  50, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.013670, mae: 1.533932, mean_q: 1.865428, mean_eps: 0.670274
  367550/2000000: episode: 532, duration: 17.199s, episode steps: 810, steps per second:  47, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.014362, mae: 1.542335, mean_q: 1.874684, mean_eps: 0.669570
  368128/2000000: episode: 533, duration: 13.075s, episode steps: 578, steps per second:  44, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.013927, mae: 1.546528, mean_q: 1.882636, mean_eps: 0.668946
  368743/2000000: episode: 534, duration: 13.894s, episode steps: 615, steps per second:  44, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.015384, mae: 1.554862, mean_q: 1.889731, mean_eps: 0.668409
  369820/2000000: episode: 535, duration: 23.006s, episode steps: 1077, steps per second:  47, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.013351, mae: 1.529952, mean_q: 1.859820, mean_eps: 0.667648
  370651/2000000: episode: 536, duration: 17.514s, episode steps: 831, steps per second:  47, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.013886, mae: 1.580431, mean_q: 1.920413, mean_eps: 0.666789
  371291/2000000: episode: 537, duration: 14.279s, episode steps: 640, steps per second:  45, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.017358, mae: 1.581210, mean_q: 1.924167, mean_eps: 0.666127
  372687/2000000: episode: 538, duration: 32.730s, episode steps: 1396, steps per second:  43, episode reward: 32.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.016976, mae: 1.584295, mean_q: 1.924758, mean_eps: 0.665211
  373215/2000000: episode: 539, duration: 11.494s, episode steps: 528, steps per second:  46, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.014178, mae: 1.601657, mean_q: 1.945679, mean_eps: 0.664345
  374306/2000000: episode: 540, duration: 24.481s, episode steps: 1091, steps per second:  45, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.332 [0.000, 5.000],  loss: 0.014163, mae: 1.594990, mean_q: 1.938606, mean_eps: 0.663616
  374991/2000000: episode: 541, duration: 15.371s, episode steps: 685, steps per second:  45, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.013828, mae: 1.582966, mean_q: 1.924547, mean_eps: 0.662817
  375512/2000000: episode: 542, duration: 13.171s, episode steps: 521, steps per second:  40, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.014068, mae: 1.587501, mean_q: 1.929936, mean_eps: 0.662275
  376398/2000000: episode: 543, duration: 21.126s, episode steps: 886, steps per second:  42, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.014003, mae: 1.587148, mean_q: 1.926885, mean_eps: 0.661641
  377509/2000000: episode: 544, duration: 25.150s, episode steps: 1111, steps per second:  44, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.207 [0.000, 5.000],  loss: 0.014092, mae: 1.588594, mean_q: 1.928839, mean_eps: 0.660741
  378461/2000000: episode: 545, duration: 21.703s, episode steps: 952, steps per second:  44, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.324 [0.000, 5.000],  loss: 0.015015, mae: 1.584468, mean_q: 1.924204, mean_eps: 0.659813
  379274/2000000: episode: 546, duration: 18.103s, episode steps: 813, steps per second:  45, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.013067, mae: 1.581110, mean_q: 1.920921, mean_eps: 0.659019
  379901/2000000: episode: 547, duration: 13.068s, episode steps: 627, steps per second:  48, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.014462, mae: 1.592905, mean_q: 1.932552, mean_eps: 0.658371
  380481/2000000: episode: 548, duration: 12.466s, episode steps: 580, steps per second:  47, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.205 [0.000, 5.000],  loss: 0.014948, mae: 1.615418, mean_q: 1.963954, mean_eps: 0.657827
  381250/2000000: episode: 549, duration: 17.687s, episode steps: 769, steps per second:  43, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.016840, mae: 1.629529, mean_q: 1.980944, mean_eps: 0.657221
  381975/2000000: episode: 550, duration: 15.654s, episode steps: 725, steps per second:  46, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.015000, mae: 1.625854, mean_q: 1.975921, mean_eps: 0.656549
  382751/2000000: episode: 551, duration: 15.600s, episode steps: 776, steps per second:  50, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.153 [0.000, 5.000],  loss: 0.015226, mae: 1.611316, mean_q: 1.958845, mean_eps: 0.655874
  383421/2000000: episode: 552, duration: 13.340s, episode steps: 670, steps per second:  50, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.294 [0.000, 5.000],  loss: 0.015048, mae: 1.607009, mean_q: 1.953533, mean_eps: 0.655223
  383782/2000000: episode: 553, duration: 8.843s, episode steps: 361, steps per second:  41, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.013653, mae: 1.595369, mean_q: 1.938534, mean_eps: 0.654758
  384142/2000000: episode: 554, duration: 8.136s, episode steps: 360, steps per second:  44, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.658 [0.000, 5.000],  loss: 0.014424, mae: 1.605258, mean_q: 1.951657, mean_eps: 0.654434
  384839/2000000: episode: 555, duration: 14.819s, episode steps: 697, steps per second:  47, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.798 [0.000, 5.000],  loss: 0.015789, mae: 1.606975, mean_q: 1.952137, mean_eps: 0.653959
  385527/2000000: episode: 556, duration: 13.966s, episode steps: 688, steps per second:  49, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.013699, mae: 1.654378, mean_q: 2.008177, mean_eps: 0.653336
  386145/2000000: episode: 557, duration: 12.339s, episode steps: 618, steps per second:  50, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.012076, mae: 1.663486, mean_q: 2.021859, mean_eps: 0.652748
  386654/2000000: episode: 558, duration: 10.440s, episode steps: 509, steps per second:  49, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.013246, mae: 1.669832, mean_q: 2.029918, mean_eps: 0.652240
  387422/2000000: episode: 559, duration: 15.917s, episode steps: 768, steps per second:  48, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.014605, mae: 1.658142, mean_q: 2.013595, mean_eps: 0.651666
  387972/2000000: episode: 560, duration: 12.976s, episode steps: 550, steps per second:  42, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.014390, mae: 1.645820, mean_q: 1.997403, mean_eps: 0.651074
  388875/2000000: episode: 561, duration: 20.704s, episode steps: 903, steps per second:  44, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.281 [0.000, 5.000],  loss: 0.016570, mae: 1.669425, mean_q: 2.026242, mean_eps: 0.650420
  389703/2000000: episode: 562, duration: 19.247s, episode steps: 828, steps per second:  43, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.014224, mae: 1.665701, mean_q: 2.022760, mean_eps: 0.649641
  390463/2000000: episode: 563, duration: 16.639s, episode steps: 760, steps per second:  46, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.014079, mae: 1.681100, mean_q: 2.041811, mean_eps: 0.648926
  391194/2000000: episode: 564, duration: 15.630s, episode steps: 731, steps per second:  47, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.013875, mae: 1.695448, mean_q: 2.059251, mean_eps: 0.648255
  392382/2000000: episode: 565, duration: 25.896s, episode steps: 1188, steps per second:  46, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.015494, mae: 1.682369, mean_q: 2.041545, mean_eps: 0.647391
  393027/2000000: episode: 566, duration: 13.539s, episode steps: 645, steps per second:  48, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.015929, mae: 1.711995, mean_q: 2.079315, mean_eps: 0.646566
  393699/2000000: episode: 567, duration: 14.436s, episode steps: 672, steps per second:  47, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.014801, mae: 1.676137, mean_q: 2.034602, mean_eps: 0.645974
  394224/2000000: episode: 568, duration: 11.343s, episode steps: 525, steps per second:  46, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.013954, mae: 1.707367, mean_q: 2.073815, mean_eps: 0.645436
  394725/2000000: episode: 569, duration: 11.479s, episode steps: 501, steps per second:  44, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.014404, mae: 1.685569, mean_q: 2.047783, mean_eps: 0.644973
  395521/2000000: episode: 570, duration: 18.114s, episode steps: 796, steps per second:  44, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.014273, mae: 1.752464, mean_q: 2.129752, mean_eps: 0.644388
  396499/2000000: episode: 571, duration: 20.918s, episode steps: 978, steps per second:  47, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.019333, mae: 1.740210, mean_q: 2.112113, mean_eps: 0.643591
  397286/2000000: episode: 572, duration: 17.268s, episode steps: 787, steps per second:  46, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.015192, mae: 1.735673, mean_q: 2.105718, mean_eps: 0.642797
  397788/2000000: episode: 573, duration: 11.203s, episode steps: 502, steps per second:  45, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.014425, mae: 1.731782, mean_q: 2.103061, mean_eps: 0.642218
  398569/2000000: episode: 574, duration: 17.199s, episode steps: 781, steps per second:  45, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.266 [0.000, 5.000],  loss: 0.017121, mae: 1.741864, mean_q: 2.113177, mean_eps: 0.641640
  399546/2000000: episode: 575, duration: 19.680s, episode steps: 977, steps per second:  50, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.015016, mae: 1.730904, mean_q: 2.099319, mean_eps: 0.640848
  400217/2000000: episode: 576, duration: 13.453s, episode steps: 671, steps per second:  50, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.018648, mae: 1.750467, mean_q: 2.123624, mean_eps: 0.640106
  401033/2000000: episode: 577, duration: 17.884s, episode steps: 816, steps per second:  46, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.014155, mae: 1.719001, mean_q: 2.086696, mean_eps: 0.639437
  401658/2000000: episode: 578, duration: 13.776s, episode steps: 625, steps per second:  45, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.013830, mae: 1.738644, mean_q: 2.112727, mean_eps: 0.638789
  402527/2000000: episode: 579, duration: 18.092s, episode steps: 869, steps per second:  48, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.014457, mae: 1.738110, mean_q: 2.110642, mean_eps: 0.638117
  403006/2000000: episode: 580, duration: 9.438s, episode steps: 479, steps per second:  51, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.016589, mae: 1.752631, mean_q: 2.126541, mean_eps: 0.637511
  403750/2000000: episode: 581, duration: 16.086s, episode steps: 744, steps per second:  46, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.013804, mae: 1.730946, mean_q: 2.103390, mean_eps: 0.636960
  404124/2000000: episode: 582, duration: 8.043s, episode steps: 374, steps per second:  47, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.015752, mae: 1.724441, mean_q: 2.096076, mean_eps: 0.636458
  404594/2000000: episode: 583, duration: 10.596s, episode steps: 470, steps per second:  44, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 0.015772, mae: 1.737217, mean_q: 2.109813, mean_eps: 0.636078
  405423/2000000: episode: 584, duration: 19.071s, episode steps: 829, steps per second:  43, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.201 [0.000, 5.000],  loss: 0.015261, mae: 1.744623, mean_q: 2.117703, mean_eps: 0.635493
  406276/2000000: episode: 585, duration: 20.879s, episode steps: 853, steps per second:  41, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.018370, mae: 1.752182, mean_q: 2.127490, mean_eps: 0.634737
  407208/2000000: episode: 586, duration: 22.169s, episode steps: 932, steps per second:  42, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.014774, mae: 1.748966, mean_q: 2.125554, mean_eps: 0.633934
  408076/2000000: episode: 587, duration: 19.944s, episode steps: 868, steps per second:  44, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.019523, mae: 1.754386, mean_q: 2.128319, mean_eps: 0.633124
  408749/2000000: episode: 588, duration: 14.130s, episode steps: 673, steps per second:  48, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.312 [0.000, 5.000],  loss: 0.014803, mae: 1.755951, mean_q: 2.131227, mean_eps: 0.632429
  409374/2000000: episode: 589, duration: 12.710s, episode steps: 625, steps per second:  49, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.015510, mae: 1.761972, mean_q: 2.139291, mean_eps: 0.631844
  410236/2000000: episode: 590, duration: 17.148s, episode steps: 862, steps per second:  50, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.334 [0.000, 5.000],  loss: 0.015566, mae: 1.744668, mean_q: 2.116449, mean_eps: 0.631176
  410957/2000000: episode: 591, duration: 15.433s, episode steps: 721, steps per second:  47, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.160 [0.000, 5.000],  loss: 0.013038, mae: 1.755665, mean_q: 2.130443, mean_eps: 0.630464
  412838/2000000: episode: 592, duration: 41.878s, episode steps: 1881, steps per second:  45, episode reward: 36.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.015276, mae: 1.759325, mean_q: 2.133603, mean_eps: 0.629292
  413411/2000000: episode: 593, duration: 12.960s, episode steps: 573, steps per second:  44, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.015817, mae: 1.743122, mean_q: 2.116213, mean_eps: 0.628188
  414227/2000000: episode: 594, duration: 17.409s, episode steps: 816, steps per second:  47, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.984 [0.000, 5.000],  loss: 0.014138, mae: 1.746337, mean_q: 2.116295, mean_eps: 0.627564
  415073/2000000: episode: 595, duration: 18.155s, episode steps: 846, steps per second:  47, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.014420, mae: 1.760623, mean_q: 2.135272, mean_eps: 0.626815
  416013/2000000: episode: 596, duration: 18.360s, episode steps: 940, steps per second:  51, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.272 [0.000, 5.000],  loss: 0.012801, mae: 1.741577, mean_q: 2.112528, mean_eps: 0.626010
  416704/2000000: episode: 597, duration: 14.193s, episode steps: 691, steps per second:  49, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.017894, mae: 1.760402, mean_q: 2.137061, mean_eps: 0.625278
  417285/2000000: episode: 598, duration: 12.583s, episode steps: 581, steps per second:  46, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.012903, mae: 1.763223, mean_q: 2.138977, mean_eps: 0.624705
  417806/2000000: episode: 599, duration: 11.064s, episode steps: 521, steps per second:  47, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.014744, mae: 1.724100, mean_q: 2.092257, mean_eps: 0.624209
  418291/2000000: episode: 600, duration: 9.848s, episode steps: 485, steps per second:  49, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.013840, mae: 1.760640, mean_q: 2.134681, mean_eps: 0.623757
  418971/2000000: episode: 601, duration: 13.814s, episode steps: 680, steps per second:  49, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.017436, mae: 1.765506, mean_q: 2.139596, mean_eps: 0.623233
  419802/2000000: episode: 602, duration: 16.937s, episode steps: 831, steps per second:  49, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.013534, mae: 1.755441, mean_q: 2.129679, mean_eps: 0.622553
  420553/2000000: episode: 603, duration: 14.367s, episode steps: 751, steps per second:  52, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.196 [0.000, 5.000],  loss: 0.013198, mae: 1.772708, mean_q: 2.149759, mean_eps: 0.621840
  421428/2000000: episode: 604, duration: 18.284s, episode steps: 875, steps per second:  48, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.192 [0.000, 5.000],  loss: 0.014928, mae: 1.772971, mean_q: 2.149548, mean_eps: 0.621109
  422049/2000000: episode: 605, duration: 13.123s, episode steps: 621, steps per second:  47, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.016143, mae: 1.764015, mean_q: 2.139330, mean_eps: 0.620436
  422687/2000000: episode: 606, duration: 14.019s, episode steps: 638, steps per second:  46, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.015499, mae: 1.762342, mean_q: 2.137403, mean_eps: 0.619869
  423472/2000000: episode: 607, duration: 17.556s, episode steps: 785, steps per second:  45, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.014144, mae: 1.773240, mean_q: 2.149179, mean_eps: 0.619230
  424314/2000000: episode: 608, duration: 18.488s, episode steps: 842, steps per second:  46, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.171 [0.000, 5.000],  loss: 0.015771, mae: 1.763806, mean_q: 2.138027, mean_eps: 0.618497
  425113/2000000: episode: 609, duration: 16.745s, episode steps: 799, steps per second:  48, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.124 [0.000, 5.000],  loss: 0.015355, mae: 1.758989, mean_q: 2.132400, mean_eps: 0.617757
  425608/2000000: episode: 610, duration: 10.099s, episode steps: 495, steps per second:  49, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.015973, mae: 1.822238, mean_q: 2.207260, mean_eps: 0.617176
  426071/2000000: episode: 611, duration: 9.745s, episode steps: 463, steps per second:  48, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.652 [0.000, 5.000],  loss: 0.015389, mae: 1.797772, mean_q: 2.179819, mean_eps: 0.616746
  426664/2000000: episode: 612, duration: 12.536s, episode steps: 593, steps per second:  47, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.648 [0.000, 5.000],  loss: 0.015289, mae: 1.803640, mean_q: 2.185626, mean_eps: 0.616271
  427227/2000000: episode: 613, duration: 11.810s, episode steps: 563, steps per second:  48, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.016101, mae: 1.797607, mean_q: 2.177185, mean_eps: 0.615750
  428495/2000000: episode: 614, duration: 28.991s, episode steps: 1268, steps per second:  44, episode reward: 15.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.685 [0.000, 5.000],  loss: 0.014964, mae: 1.816576, mean_q: 2.201758, mean_eps: 0.614926
  429156/2000000: episode: 615, duration: 15.832s, episode steps: 661, steps per second:  42, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.016051, mae: 1.812425, mean_q: 2.196944, mean_eps: 0.614058
  429742/2000000: episode: 616, duration: 12.431s, episode steps: 586, steps per second:  47, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.016989, mae: 1.799247, mean_q: 2.181520, mean_eps: 0.613497
  430363/2000000: episode: 617, duration: 12.999s, episode steps: 621, steps per second:  48, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.161 [0.000, 5.000],  loss: 0.013941, mae: 1.830488, mean_q: 2.219216, mean_eps: 0.612953
  431101/2000000: episode: 618, duration: 16.008s, episode steps: 738, steps per second:  46, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.013579, mae: 1.845615, mean_q: 2.234969, mean_eps: 0.612341
  431533/2000000: episode: 619, duration: 9.641s, episode steps: 432, steps per second:  45, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.002 [0.000, 5.000],  loss: 0.014945, mae: 1.864196, mean_q: 2.260379, mean_eps: 0.611814
  432315/2000000: episode: 620, duration: 16.235s, episode steps: 782, steps per second:  48, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.015865, mae: 1.849409, mean_q: 2.238283, mean_eps: 0.611268
  432971/2000000: episode: 621, duration: 12.949s, episode steps: 656, steps per second:  51, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.015265, mae: 1.833961, mean_q: 2.220527, mean_eps: 0.610622
  433776/2000000: episode: 622, duration: 16.308s, episode steps: 805, steps per second:  49, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.013307, mae: 1.862860, mean_q: 2.255371, mean_eps: 0.609965
  434383/2000000: episode: 623, duration: 12.918s, episode steps: 607, steps per second:  47, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.712 [0.000, 5.000],  loss: 0.017676, mae: 1.848347, mean_q: 2.237506, mean_eps: 0.609330
  434841/2000000: episode: 624, duration: 10.029s, episode steps: 458, steps per second:  46, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.234 [0.000, 5.000],  loss: 0.016371, mae: 1.849514, mean_q: 2.242382, mean_eps: 0.608849
  435569/2000000: episode: 625, duration: 15.245s, episode steps: 728, steps per second:  48, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.016112, mae: 1.890440, mean_q: 2.291603, mean_eps: 0.608315
  436000/2000000: episode: 626, duration: 8.738s, episode steps: 431, steps per second:  49, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: 0.016269, mae: 1.873146, mean_q: 2.271883, mean_eps: 0.607794
  436477/2000000: episode: 627, duration: 10.218s, episode steps: 477, steps per second:  47, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.012841, mae: 1.842568, mean_q: 2.234011, mean_eps: 0.607386
  437128/2000000: episode: 628, duration: 13.612s, episode steps: 651, steps per second:  48, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.014996, mae: 1.853533, mean_q: 2.244557, mean_eps: 0.606878
  437921/2000000: episode: 629, duration: 16.297s, episode steps: 793, steps per second:  49, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.016420, mae: 1.859336, mean_q: 2.249389, mean_eps: 0.606228
  439024/2000000: episode: 630, duration: 24.098s, episode steps: 1103, steps per second:  46, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.212 [0.000, 5.000],  loss: 0.016933, mae: 1.881767, mean_q: 2.277498, mean_eps: 0.605375
  440122/2000000: episode: 631, duration: 25.301s, episode steps: 1098, steps per second:  43, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.016188, mae: 1.861189, mean_q: 2.253335, mean_eps: 0.604385
  440782/2000000: episode: 632, duration: 14.784s, episode steps: 660, steps per second:  45, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.015204, mae: 1.898868, mean_q: 2.299306, mean_eps: 0.603593
  441900/2000000: episode: 633, duration: 25.034s, episode steps: 1118, steps per second:  45, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.016706, mae: 1.904416, mean_q: 2.308235, mean_eps: 0.602794
  442985/2000000: episode: 634, duration: 25.181s, episode steps: 1085, steps per second:  43, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.205 [0.000, 5.000],  loss: 0.016874, mae: 1.924699, mean_q: 2.331674, mean_eps: 0.601802
  443452/2000000: episode: 635, duration: 10.072s, episode steps: 467, steps per second:  46, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.017308, mae: 1.882093, mean_q: 2.276859, mean_eps: 0.601104
  443901/2000000: episode: 636, duration: 9.391s, episode steps: 449, steps per second:  48, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.018077, mae: 1.908990, mean_q: 2.309596, mean_eps: 0.600692
  445463/2000000: episode: 637, duration: 33.446s, episode steps: 1562, steps per second:  47, episode reward: 17.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.239 [0.000, 5.000],  loss: 0.015244, mae: 1.913401, mean_q: 2.316988, mean_eps: 0.599786
  445971/2000000: episode: 638, duration: 10.868s, episode steps: 508, steps per second:  47, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.015260, mae: 1.940284, mean_q: 2.350529, mean_eps: 0.598856
  447245/2000000: episode: 639, duration: 27.253s, episode steps: 1274, steps per second:  47, episode reward: 30.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.015722, mae: 1.927739, mean_q: 2.334138, mean_eps: 0.598053
  448373/2000000: episode: 640, duration: 23.893s, episode steps: 1128, steps per second:  47, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.016871, mae: 1.945398, mean_q: 2.355786, mean_eps: 0.596971
  449063/2000000: episode: 641, duration: 13.711s, episode steps: 690, steps per second:  50, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.674 [0.000, 5.000],  loss: 0.016579, mae: 1.922588, mean_q: 2.329826, mean_eps: 0.596154
  450146/2000000: episode: 642, duration: 20.995s, episode steps: 1083, steps per second:  52, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.018928, mae: 1.934682, mean_q: 2.344383, mean_eps: 0.595356
  450919/2000000: episode: 643, duration: 16.776s, episode steps: 773, steps per second:  46, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.300 [0.000, 5.000],  loss: 0.016758, mae: 1.979730, mean_q: 2.397227, mean_eps: 0.594521
  451721/2000000: episode: 644, duration: 17.735s, episode steps: 802, steps per second:  45, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.018316, mae: 1.952712, mean_q: 2.364990, mean_eps: 0.593812
  452366/2000000: episode: 645, duration: 13.305s, episode steps: 645, steps per second:  48, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.015434, mae: 1.969682, mean_q: 2.386571, mean_eps: 0.593160
  453092/2000000: episode: 646, duration: 13.932s, episode steps: 726, steps per second:  52, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.015454, mae: 1.971091, mean_q: 2.386693, mean_eps: 0.592545
  453581/2000000: episode: 647, duration: 9.180s, episode steps: 489, steps per second:  53, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.017759, mae: 1.959035, mean_q: 2.374229, mean_eps: 0.591998
  454121/2000000: episode: 648, duration: 10.421s, episode steps: 540, steps per second:  52, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.126 [0.000, 5.000],  loss: 0.014878, mae: 1.947940, mean_q: 2.362112, mean_eps: 0.591533
  455087/2000000: episode: 649, duration: 19.203s, episode steps: 966, steps per second:  50, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.313 [0.000, 5.000],  loss: 0.017240, mae: 1.973539, mean_q: 2.389336, mean_eps: 0.590856
  456231/2000000: episode: 650, duration: 23.326s, episode steps: 1144, steps per second:  49, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.015581, mae: 1.957700, mean_q: 2.370423, mean_eps: 0.589908
  457031/2000000: episode: 651, duration: 20.357s, episode steps: 800, steps per second:  39, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.199 [0.000, 5.000],  loss: 0.016857, mae: 1.965651, mean_q: 2.377362, mean_eps: 0.589033
  457416/2000000: episode: 652, duration: 9.599s, episode steps: 385, steps per second:  40, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.014992, mae: 1.967668, mean_q: 2.386467, mean_eps: 0.588500
  457934/2000000: episode: 653, duration: 10.792s, episode steps: 518, steps per second:  48, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.737 [0.000, 5.000],  loss: 0.015318, mae: 1.962234, mean_q: 2.375747, mean_eps: 0.588093
  458962/2000000: episode: 654, duration: 21.417s, episode steps: 1028, steps per second:  48, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.016014, mae: 1.958533, mean_q: 2.370754, mean_eps: 0.587397
  459571/2000000: episode: 655, duration: 13.463s, episode steps: 609, steps per second:  45, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.016149, mae: 1.969337, mean_q: 2.383352, mean_eps: 0.586661
  460481/2000000: episode: 656, duration: 19.486s, episode steps: 910, steps per second:  47, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.016477, mae: 2.002276, mean_q: 2.424374, mean_eps: 0.585977
  460887/2000000: episode: 657, duration: 8.712s, episode steps: 406, steps per second:  47, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.653 [0.000, 5.000],  loss: 0.018745, mae: 1.960596, mean_q: 2.372959, mean_eps: 0.585384
  461518/2000000: episode: 658, duration: 13.223s, episode steps: 631, steps per second:  48, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.046 [0.000, 5.000],  loss: 0.017195, mae: 1.992239, mean_q: 2.407896, mean_eps: 0.584918
  462152/2000000: episode: 659, duration: 15.137s, episode steps: 634, steps per second:  42, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.287 [0.000, 5.000],  loss: 0.018915, mae: 2.001428, mean_q: 2.420757, mean_eps: 0.584349
  462706/2000000: episode: 660, duration: 13.075s, episode steps: 554, steps per second:  42, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.016771, mae: 1.982718, mean_q: 2.401089, mean_eps: 0.583815
  463271/2000000: episode: 661, duration: 12.280s, episode steps: 565, steps per second:  46, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.034 [0.000, 5.000],  loss: 0.016506, mae: 1.981954, mean_q: 2.397872, mean_eps: 0.583311
  464012/2000000: episode: 662, duration: 15.904s, episode steps: 741, steps per second:  47, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.016012, mae: 2.013160, mean_q: 2.435305, mean_eps: 0.582724
  464655/2000000: episode: 663, duration: 14.335s, episode steps: 643, steps per second:  45, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.165 [0.000, 5.000],  loss: 0.017188, mae: 1.975282, mean_q: 2.390849, mean_eps: 0.582101
  465647/2000000: episode: 664, duration: 21.701s, episode steps: 992, steps per second:  46, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.014474, mae: 1.982636, mean_q: 2.399139, mean_eps: 0.581365
  466507/2000000: episode: 665, duration: 30.651s, episode steps: 860, steps per second:  28, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.015728, mae: 1.984921, mean_q: 2.401709, mean_eps: 0.580532
  467123/2000000: episode: 666, duration: 12.243s, episode steps: 616, steps per second:  50, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.101 [0.000, 5.000],  loss: 0.014702, mae: 1.979989, mean_q: 2.394155, mean_eps: 0.579867
  467871/2000000: episode: 667, duration: 14.233s, episode steps: 748, steps per second:  53, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.247 [0.000, 5.000],  loss: 0.020673, mae: 2.011054, mean_q: 2.430951, mean_eps: 0.579254
  468521/2000000: episode: 668, duration: 12.453s, episode steps: 650, steps per second:  52, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.251 [0.000, 5.000],  loss: 0.016418, mae: 1.990583, mean_q: 2.408965, mean_eps: 0.578624
  469472/2000000: episode: 669, duration: 18.428s, episode steps: 951, steps per second:  52, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.319 [0.000, 5.000],  loss: 0.016337, mae: 1.976196, mean_q: 2.393306, mean_eps: 0.577904
  470333/2000000: episode: 670, duration: 16.371s, episode steps: 861, steps per second:  53, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.015364, mae: 1.988770, mean_q: 2.406286, mean_eps: 0.577088
  471382/2000000: episode: 671, duration: 20.923s, episode steps: 1049, steps per second:  50, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.239 [0.000, 5.000],  loss: 0.016457, mae: 1.993814, mean_q: 2.412854, mean_eps: 0.576228
  472028/2000000: episode: 672, duration: 12.747s, episode steps: 646, steps per second:  51, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.226 [0.000, 5.000],  loss: 0.018498, mae: 2.019217, mean_q: 2.440356, mean_eps: 0.575466
  472995/2000000: episode: 673, duration: 19.957s, episode steps: 967, steps per second:  48, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.015618, mae: 2.005782, mean_q: 2.427956, mean_eps: 0.574741
  474309/2000000: episode: 674, duration: 26.921s, episode steps: 1314, steps per second:  49, episode reward: 31.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.129 [0.000, 5.000],  loss: 0.016700, mae: 1.991875, mean_q: 2.409319, mean_eps: 0.573713
  475159/2000000: episode: 675, duration: 16.788s, episode steps: 850, steps per second:  51, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.016156, mae: 2.011057, mean_q: 2.434638, mean_eps: 0.572739
  475759/2000000: episode: 676, duration: 11.711s, episode steps: 600, steps per second:  51, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.018551, mae: 2.021562, mean_q: 2.446071, mean_eps: 0.572088
  476294/2000000: episode: 677, duration: 10.973s, episode steps: 535, steps per second:  49, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.015557, mae: 2.006834, mean_q: 2.428799, mean_eps: 0.571577
  476994/2000000: episode: 678, duration: 14.211s, episode steps: 700, steps per second:  49, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.015485, mae: 2.009051, mean_q: 2.429763, mean_eps: 0.571020
  477647/2000000: episode: 679, duration: 12.753s, episode steps: 653, steps per second:  51, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.019033, mae: 1.999250, mean_q: 2.418097, mean_eps: 0.570412
  478275/2000000: episode: 680, duration: 12.454s, episode steps: 628, steps per second:  50, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.016032, mae: 2.007867, mean_q: 2.428236, mean_eps: 0.569836
  479205/2000000: episode: 681, duration: 19.957s, episode steps: 930, steps per second:  47, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.670 [0.000, 5.000],  loss: 0.016994, mae: 2.012913, mean_q: 2.434572, mean_eps: 0.569134
  479858/2000000: episode: 682, duration: 13.520s, episode steps: 653, steps per second:  48, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.651 [0.000, 5.000],  loss: 0.014262, mae: 1.982861, mean_q: 2.397468, mean_eps: 0.568421
  481320/2000000: episode: 683, duration: 30.554s, episode steps: 1462, steps per second:  48, episode reward: 30.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.014623, mae: 2.041478, mean_q: 2.471186, mean_eps: 0.567471
  481898/2000000: episode: 684, duration: 12.591s, episode steps: 578, steps per second:  46, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.993 [0.000, 5.000],  loss: 0.017470, mae: 2.039195, mean_q: 2.466311, mean_eps: 0.566553
  482498/2000000: episode: 685, duration: 22.450s, episode steps: 600, steps per second:  27, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.927 [0.000, 5.000],  loss: 0.019103, mae: 2.039777, mean_q: 2.465375, mean_eps: 0.566022
  483418/2000000: episode: 686, duration: 16.570s, episode steps: 920, steps per second:  56, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.015608, mae: 2.055845, mean_q: 2.484943, mean_eps: 0.565338
  484081/2000000: episode: 687, duration: 12.542s, episode steps: 663, steps per second:  53, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.015359, mae: 2.018940, mean_q: 2.441954, mean_eps: 0.564625
  484478/2000000: episode: 688, duration: 7.272s, episode steps: 397, steps per second:  55, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.014296, mae: 2.028220, mean_q: 2.456350, mean_eps: 0.564148
  485683/2000000: episode: 689, duration: 22.931s, episode steps: 1205, steps per second:  53, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.255 [0.000, 5.000],  loss: 0.015473, mae: 2.055788, mean_q: 2.487254, mean_eps: 0.563428
  486276/2000000: episode: 690, duration: 11.011s, episode steps: 593, steps per second:  54, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.636 [0.000, 5.000],  loss: 0.018614, mae: 2.075001, mean_q: 2.508697, mean_eps: 0.562620
  486836/2000000: episode: 691, duration: 10.707s, episode steps: 560, steps per second:  52, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.816 [0.000, 5.000],  loss: 0.018307, mae: 2.081348, mean_q: 2.519365, mean_eps: 0.562101
  487510/2000000: episode: 692, duration: 12.636s, episode steps: 674, steps per second:  53, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.017708, mae: 2.058421, mean_q: 2.490112, mean_eps: 0.561545
  488060/2000000: episode: 693, duration: 10.366s, episode steps: 550, steps per second:  53, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.016805, mae: 2.057744, mean_q: 2.488605, mean_eps: 0.560994
  488742/2000000: episode: 694, duration: 12.970s, episode steps: 682, steps per second:  53, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.017601, mae: 2.052992, mean_q: 2.483259, mean_eps: 0.560440
  489367/2000000: episode: 695, duration: 12.863s, episode steps: 625, steps per second:  49, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.017483, mae: 2.042032, mean_q: 2.469169, mean_eps: 0.559851
  489992/2000000: episode: 696, duration: 12.193s, episode steps: 625, steps per second:  51, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.015533, mae: 2.054890, mean_q: 2.484279, mean_eps: 0.559290
  490820/2000000: episode: 697, duration: 17.472s, episode steps: 828, steps per second:  47, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.015642, mae: 2.103449, mean_q: 2.545197, mean_eps: 0.558636
  491612/2000000: episode: 698, duration: 16.512s, episode steps: 792, steps per second:  48, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.014833, mae: 2.082969, mean_q: 2.519100, mean_eps: 0.557907
  492444/2000000: episode: 699, duration: 16.951s, episode steps: 832, steps per second:  49, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.015696, mae: 2.078675, mean_q: 2.516201, mean_eps: 0.557177
  493769/2000000: episode: 700, duration: 26.028s, episode steps: 1325, steps per second:  51, episode reward: 32.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.016475, mae: 2.079356, mean_q: 2.515267, mean_eps: 0.556205
  494584/2000000: episode: 701, duration: 16.402s, episode steps: 815, steps per second:  50, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.017778, mae: 2.082900, mean_q: 2.517851, mean_eps: 0.555242
  495400/2000000: episode: 702, duration: 17.775s, episode steps: 816, steps per second:  46, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.017235, mae: 2.121225, mean_q: 2.563851, mean_eps: 0.554509
  495942/2000000: episode: 703, duration: 11.459s, episode steps: 542, steps per second:  47, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.332 [0.000, 5.000],  loss: 0.015541, mae: 2.167031, mean_q: 2.623511, mean_eps: 0.553897
  496401/2000000: episode: 704, duration: 10.385s, episode steps: 459, steps per second:  44, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.017851, mae: 2.146125, mean_q: 2.597539, mean_eps: 0.553445
  497041/2000000: episode: 705, duration: 14.009s, episode steps: 640, steps per second:  46, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.017167, mae: 2.138942, mean_q: 2.588239, mean_eps: 0.552950
  497444/2000000: episode: 706, duration: 7.454s, episode steps: 403, steps per second:  54, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.020156, mae: 2.112766, mean_q: 2.553728, mean_eps: 0.552482
  497938/2000000: episode: 707, duration: 9.525s, episode steps: 494, steps per second:  52, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.017205, mae: 2.171004, mean_q: 2.624970, mean_eps: 0.552079
  498555/2000000: episode: 708, duration: 12.269s, episode steps: 617, steps per second:  50, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.017236, mae: 2.143710, mean_q: 2.591678, mean_eps: 0.551579
  499124/2000000: episode: 709, duration: 11.649s, episode steps: 569, steps per second:  49, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.199 [0.000, 5.000],  loss: 0.014668, mae: 2.143518, mean_q: 2.591217, mean_eps: 0.551046
  499757/2000000: episode: 710, duration: 12.413s, episode steps: 633, steps per second:  51, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.016435, mae: 2.138107, mean_q: 2.584288, mean_eps: 0.550504
  500477/2000000: episode: 711, duration: 14.770s, episode steps: 720, steps per second:  49, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.769 [0.000, 5.000],  loss: 0.017104, mae: 2.175940, mean_q: 2.630571, mean_eps: 0.549894
  501184/2000000: episode: 712, duration: 13.650s, episode steps: 707, steps per second:  52, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.016086, mae: 2.156323, mean_q: 2.609901, mean_eps: 0.549253
  501893/2000000: episode: 713, duration: 13.848s, episode steps: 709, steps per second:  51, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.017009, mae: 2.139477, mean_q: 2.590852, mean_eps: 0.548616
  502996/2000000: episode: 714, duration: 23.073s, episode steps: 1103, steps per second:  48, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 0.016010, mae: 2.133787, mean_q: 2.581060, mean_eps: 0.547800
  503713/2000000: episode: 715, duration: 14.556s, episode steps: 717, steps per second:  49, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.018358, mae: 2.161782, mean_q: 2.614941, mean_eps: 0.546981
  504269/2000000: episode: 716, duration: 11.228s, episode steps: 556, steps per second:  50, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.790 [0.000, 5.000],  loss: 0.015718, mae: 2.144244, mean_q: 2.591446, mean_eps: 0.546407
  504970/2000000: episode: 717, duration: 14.112s, episode steps: 701, steps per second:  50, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.017613, mae: 2.176048, mean_q: 2.629877, mean_eps: 0.545842
  505663/2000000: episode: 718, duration: 13.918s, episode steps: 693, steps per second:  50, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.058 [0.000, 5.000],  loss: 0.018750, mae: 2.146853, mean_q: 2.593326, mean_eps: 0.545216
  506066/2000000: episode: 719, duration: 8.304s, episode steps: 403, steps per second:  49, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.015102, mae: 2.118720, mean_q: 2.563739, mean_eps: 0.544722
  506723/2000000: episode: 720, duration: 14.034s, episode steps: 657, steps per second:  47, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.026 [0.000, 5.000],  loss: 0.014360, mae: 2.123931, mean_q: 2.567598, mean_eps: 0.544245
  507814/2000000: episode: 721, duration: 22.187s, episode steps: 1091, steps per second:  49, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.106 [0.000, 5.000],  loss: 0.017699, mae: 2.157506, mean_q: 2.609163, mean_eps: 0.543459
  508526/2000000: episode: 722, duration: 15.186s, episode steps: 712, steps per second:  47, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.020213, mae: 2.130786, mean_q: 2.577482, mean_eps: 0.542647
  509108/2000000: episode: 723, duration: 12.680s, episode steps: 582, steps per second:  46, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.074 [0.000, 5.000],  loss: 0.018309, mae: 2.135095, mean_q: 2.582258, mean_eps: 0.542066
  509494/2000000: episode: 724, duration: 8.044s, episode steps: 386, steps per second:  48, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.016298, mae: 2.131172, mean_q: 2.576454, mean_eps: 0.541630
  509885/2000000: episode: 725, duration: 8.059s, episode steps: 391, steps per second:  49, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.706 [0.000, 5.000],  loss: 0.018870, mae: 2.156009, mean_q: 2.607057, mean_eps: 0.541279
  510560/2000000: episode: 726, duration: 13.962s, episode steps: 675, steps per second:  48, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.016982, mae: 2.174364, mean_q: 2.628777, mean_eps: 0.540800
  510943/2000000: episode: 727, duration: 8.313s, episode steps: 383, steps per second:  46, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.815 [0.000, 5.000],  loss: 0.017937, mae: 2.202820, mean_q: 2.661714, mean_eps: 0.540325
  511665/2000000: episode: 728, duration: 15.906s, episode steps: 722, steps per second:  45, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.017245, mae: 2.188766, mean_q: 2.647444, mean_eps: 0.539826
  512409/2000000: episode: 729, duration: 16.377s, episode steps: 744, steps per second:  45, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.016012, mae: 2.192540, mean_q: 2.651255, mean_eps: 0.539166
  512957/2000000: episode: 730, duration: 11.594s, episode steps: 548, steps per second:  47, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.019633, mae: 2.186501, mean_q: 2.641867, mean_eps: 0.538584
  513750/2000000: episode: 731, duration: 16.627s, episode steps: 793, steps per second:  48, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.016093, mae: 2.172604, mean_q: 2.626881, mean_eps: 0.537981
  514412/2000000: episode: 732, duration: 14.678s, episode steps: 662, steps per second:  45, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.017587, mae: 2.180473, mean_q: 2.636210, mean_eps: 0.537328
  515069/2000000: episode: 733, duration: 14.222s, episode steps: 657, steps per second:  46, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.245 [0.000, 5.000],  loss: 0.017375, mae: 2.176391, mean_q: 2.632742, mean_eps: 0.536734
  515498/2000000: episode: 734, duration: 9.948s, episode steps: 429, steps per second:  43, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.748 [0.000, 5.000],  loss: 0.021175, mae: 2.248222, mean_q: 2.719805, mean_eps: 0.536244
  516217/2000000: episode: 735, duration: 15.499s, episode steps: 719, steps per second:  46, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.090 [0.000, 5.000],  loss: 0.018746, mae: 2.208531, mean_q: 2.669122, mean_eps: 0.535728
  516851/2000000: episode: 736, duration: 15.007s, episode steps: 634, steps per second:  42, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.016489, mae: 2.196823, mean_q: 2.655012, mean_eps: 0.535119
  517606/2000000: episode: 737, duration: 15.802s, episode steps: 755, steps per second:  48, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.016279, mae: 2.204532, mean_q: 2.664051, mean_eps: 0.534495
  518477/2000000: episode: 738, duration: 18.905s, episode steps: 871, steps per second:  46, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.265 [0.000, 5.000],  loss: 0.017498, mae: 2.216321, mean_q: 2.681181, mean_eps: 0.533762
  519029/2000000: episode: 739, duration: 11.779s, episode steps: 552, steps per second:  47, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.100 [0.000, 5.000],  loss: 0.016156, mae: 2.216838, mean_q: 2.681314, mean_eps: 0.533121
  519706/2000000: episode: 740, duration: 14.052s, episode steps: 677, steps per second:  48, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.016602, mae: 2.205446, mean_q: 2.666406, mean_eps: 0.532569
  520341/2000000: episode: 741, duration: 13.005s, episode steps: 635, steps per second:  49, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.247 [0.000, 5.000],  loss: 0.017138, mae: 2.238717, mean_q: 2.706498, mean_eps: 0.531978
  521225/2000000: episode: 742, duration: 17.768s, episode steps: 884, steps per second:  50, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.133 [0.000, 5.000],  loss: 0.017184, mae: 2.230624, mean_q: 2.699454, mean_eps: 0.531294
  522012/2000000: episode: 743, duration: 15.453s, episode steps: 787, steps per second:  51, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.168 [0.000, 5.000],  loss: 0.017505, mae: 2.241653, mean_q: 2.713094, mean_eps: 0.530544
  522374/2000000: episode: 744, duration: 7.300s, episode steps: 362, steps per second:  50, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.017617, mae: 2.240278, mean_q: 2.710058, mean_eps: 0.530027
  523463/2000000: episode: 745, duration: 21.605s, episode steps: 1089, steps per second:  50, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.101 [0.000, 5.000],  loss: 0.018345, mae: 2.221679, mean_q: 2.686107, mean_eps: 0.529374
  524328/2000000: episode: 746, duration: 18.090s, episode steps: 865, steps per second:  48, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.017800, mae: 2.253856, mean_q: 2.724317, mean_eps: 0.528495
  524781/2000000: episode: 747, duration: 9.611s, episode steps: 453, steps per second:  47, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.247 [0.000, 5.000],  loss: 0.017158, mae: 2.234797, mean_q: 2.700630, mean_eps: 0.527901
  525237/2000000: episode: 748, duration: 10.468s, episode steps: 456, steps per second:  44, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.765 [0.000, 5.000],  loss: 0.017195, mae: 2.276690, mean_q: 2.751333, mean_eps: 0.527491
  525772/2000000: episode: 749, duration: 11.543s, episode steps: 535, steps per second:  46, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.196 [0.000, 5.000],  loss: 0.015010, mae: 2.263558, mean_q: 2.737511, mean_eps: 0.527046
  526665/2000000: episode: 750, duration: 19.146s, episode steps: 893, steps per second:  47, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.017778, mae: 2.242547, mean_q: 2.712476, mean_eps: 0.526404
  527438/2000000: episode: 751, duration: 15.719s, episode steps: 773, steps per second:  49, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.017429, mae: 2.268184, mean_q: 2.742093, mean_eps: 0.525653
  527908/2000000: episode: 752, duration: 9.424s, episode steps: 470, steps per second:  50, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.014378, mae: 2.266749, mean_q: 2.742170, mean_eps: 0.525095
  528805/2000000: episode: 753, duration: 18.196s, episode steps: 897, steps per second:  49, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.018019, mae: 2.264166, mean_q: 2.738560, mean_eps: 0.524480
  529233/2000000: episode: 754, duration: 8.806s, episode steps: 428, steps per second:  49, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.019391, mae: 2.308804, mean_q: 2.792288, mean_eps: 0.523882
  529876/2000000: episode: 755, duration: 12.288s, episode steps: 643, steps per second:  52, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.015772, mae: 2.244364, mean_q: 2.714331, mean_eps: 0.523401
  531009/2000000: episode: 756, duration: 21.798s, episode steps: 1133, steps per second:  52, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.014576, mae: 2.284008, mean_q: 2.765836, mean_eps: 0.522602
  531952/2000000: episode: 757, duration: 19.126s, episode steps: 943, steps per second:  49, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.017926, mae: 2.281011, mean_q: 2.759958, mean_eps: 0.521668
  532571/2000000: episode: 758, duration: 12.303s, episode steps: 619, steps per second:  50, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.017727, mae: 2.289404, mean_q: 2.768198, mean_eps: 0.520966
  533679/2000000: episode: 759, duration: 21.663s, episode steps: 1108, steps per second:  51, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.016290, mae: 2.302102, mean_q: 2.783962, mean_eps: 0.520188
  534280/2000000: episode: 760, duration: 11.970s, episode steps: 601, steps per second:  50, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.017675, mae: 2.271732, mean_q: 2.749992, mean_eps: 0.519420
  535339/2000000: episode: 761, duration: 20.319s, episode steps: 1059, steps per second:  52, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.019639, mae: 2.273815, mean_q: 2.750070, mean_eps: 0.518673
  535836/2000000: episode: 762, duration: 9.408s, episode steps: 497, steps per second:  53, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.017651, mae: 2.280794, mean_q: 2.757570, mean_eps: 0.517973
  536331/2000000: episode: 763, duration: 9.422s, episode steps: 495, steps per second:  53, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.017696, mae: 2.261808, mean_q: 2.733317, mean_eps: 0.517526
  537019/2000000: episode: 764, duration: 13.089s, episode steps: 688, steps per second:  53, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.243 [0.000, 5.000],  loss: 0.031694, mae: 2.252199, mean_q: 2.721857, mean_eps: 0.516993
  537914/2000000: episode: 765, duration: 18.348s, episode steps: 895, steps per second:  49, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.018671, mae: 2.271198, mean_q: 2.747432, mean_eps: 0.516281
  538949/2000000: episode: 766, duration: 21.649s, episode steps: 1035, steps per second:  48, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.144 [0.000, 5.000],  loss: 0.016299, mae: 2.277398, mean_q: 2.755460, mean_eps: 0.515411
  539905/2000000: episode: 767, duration: 19.264s, episode steps: 956, steps per second:  50, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.112 [0.000, 5.000],  loss: 0.018026, mae: 2.264865, mean_q: 2.737612, mean_eps: 0.514515
  540599/2000000: episode: 768, duration: 14.291s, episode steps: 694, steps per second:  49, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.102 [0.000, 5.000],  loss: 0.014842, mae: 2.254887, mean_q: 2.730491, mean_eps: 0.513773
  541241/2000000: episode: 769, duration: 13.617s, episode steps: 642, steps per second:  47, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.812 [0.000, 5.000],  loss: 0.015547, mae: 2.282135, mean_q: 2.760764, mean_eps: 0.513172
  541973/2000000: episode: 770, duration: 15.993s, episode steps: 732, steps per second:  46, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.324 [0.000, 5.000],  loss: 0.020240, mae: 2.288636, mean_q: 2.767203, mean_eps: 0.512553
  543030/2000000: episode: 771, duration: 23.400s, episode steps: 1057, steps per second:  45, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.164 [0.000, 5.000],  loss: 0.020019, mae: 2.282224, mean_q: 2.760863, mean_eps: 0.511748
  543694/2000000: episode: 772, duration: 14.774s, episode steps: 664, steps per second:  45, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.017685, mae: 2.270594, mean_q: 2.748017, mean_eps: 0.510974
  544470/2000000: episode: 773, duration: 16.601s, episode steps: 776, steps per second:  47, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.020433, mae: 2.291658, mean_q: 2.773694, mean_eps: 0.510326
  545025/2000000: episode: 774, duration: 11.508s, episode steps: 555, steps per second:  48, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.017620, mae: 2.266549, mean_q: 2.741827, mean_eps: 0.509727
  546420/2000000: episode: 775, duration: 30.552s, episode steps: 1395, steps per second:  46, episode reward: 20.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.017801, mae: 2.281703, mean_q: 2.761333, mean_eps: 0.508850
  547419/2000000: episode: 776, duration: 20.789s, episode steps: 999, steps per second:  48, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.016614, mae: 2.286010, mean_q: 2.767838, mean_eps: 0.507774
  548179/2000000: episode: 777, duration: 16.539s, episode steps: 760, steps per second:  46, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.254 [0.000, 5.000],  loss: 0.019717, mae: 2.327104, mean_q: 2.815978, mean_eps: 0.506982
  548837/2000000: episode: 778, duration: 15.078s, episode steps: 658, steps per second:  44, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.094 [0.000, 5.000],  loss: 0.018446, mae: 2.281052, mean_q: 2.759313, mean_eps: 0.506343
  549301/2000000: episode: 779, duration: 10.519s, episode steps: 464, steps per second:  44, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.989 [0.000, 5.000],  loss: 0.018178, mae: 2.300742, mean_q: 2.784102, mean_eps: 0.505837
  550602/2000000: episode: 780, duration: 28.965s, episode steps: 1301, steps per second:  45, episode reward: 24.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.179 [0.000, 5.000],  loss: 0.017944, mae: 2.290060, mean_q: 2.773862, mean_eps: 0.505043
  551517/2000000: episode: 781, duration: 20.445s, episode steps: 915, steps per second:  45, episode reward: 27.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.021452, mae: 2.296577, mean_q: 2.779737, mean_eps: 0.504046
  552560/2000000: episode: 782, duration: 22.318s, episode steps: 1043, steps per second:  47, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.019581, mae: 2.287719, mean_q: 2.769606, mean_eps: 0.503166
  553514/2000000: episode: 783, duration: 19.176s, episode steps: 954, steps per second:  50, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.020702, mae: 2.316937, mean_q: 2.802771, mean_eps: 0.502268
  554496/2000000: episode: 784, duration: 20.164s, episode steps: 982, steps per second:  49, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.020407, mae: 2.302555, mean_q: 2.788716, mean_eps: 0.501396
  554951/2000000: episode: 785, duration: 9.361s, episode steps: 455, steps per second:  49, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.264 [0.000, 5.000],  loss: 0.019601, mae: 2.285874, mean_q: 2.771797, mean_eps: 0.500750
  556546/2000000: episode: 786, duration: 32.779s, episode steps: 1595, steps per second:  49, episode reward: 22.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.146 [0.000, 5.000],  loss: 0.019936, mae: 2.319947, mean_q: 2.806116, mean_eps: 0.499827
  557237/2000000: episode: 787, duration: 15.447s, episode steps: 691, steps per second:  45, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.216 [0.000, 5.000],  loss: 0.022110, mae: 2.329289, mean_q: 2.818675, mean_eps: 0.498797
  557859/2000000: episode: 788, duration: 13.467s, episode steps: 622, steps per second:  46, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.019593, mae: 2.326070, mean_q: 2.816009, mean_eps: 0.498207
  558773/2000000: episode: 789, duration: 19.343s, episode steps: 914, steps per second:  47, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.109 [0.000, 5.000],  loss: 0.020705, mae: 2.314271, mean_q: 2.799243, mean_eps: 0.497516
  559203/2000000: episode: 790, duration: 8.649s, episode steps: 430, steps per second:  50, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.017888, mae: 2.318499, mean_q: 2.802597, mean_eps: 0.496911
  559606/2000000: episode: 791, duration: 8.369s, episode steps: 403, steps per second:  48, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.278 [0.000, 5.000],  loss: 0.017938, mae: 2.328642, mean_q: 2.815995, mean_eps: 0.496536
  560346/2000000: episode: 792, duration: 16.825s, episode steps: 740, steps per second:  44, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.138 [0.000, 5.000],  loss: 0.018199, mae: 2.331767, mean_q: 2.821752, mean_eps: 0.496022
  561066/2000000: episode: 793, duration: 15.093s, episode steps: 720, steps per second:  48, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.189 [0.000, 5.000],  loss: 0.016821, mae: 2.320468, mean_q: 2.807175, mean_eps: 0.495365
  562016/2000000: episode: 794, duration: 19.837s, episode steps: 950, steps per second:  48, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.074 [0.000, 5.000],  loss: 0.018593, mae: 2.322425, mean_q: 2.808996, mean_eps: 0.494614
  562543/2000000: episode: 795, duration: 11.369s, episode steps: 527, steps per second:  46, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.250 [0.000, 5.000],  loss: 0.018405, mae: 2.294229, mean_q: 2.776588, mean_eps: 0.493950
  563131/2000000: episode: 796, duration: 11.989s, episode steps: 588, steps per second:  49, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.017043, mae: 2.307114, mean_q: 2.790699, mean_eps: 0.493448
  563680/2000000: episode: 797, duration: 11.523s, episode steps: 549, steps per second:  48, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.974 [0.000, 5.000],  loss: 0.020575, mae: 2.314902, mean_q: 2.799353, mean_eps: 0.492936
  564656/2000000: episode: 798, duration: 20.356s, episode steps: 976, steps per second:  48, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.018040, mae: 2.300105, mean_q: 2.781339, mean_eps: 0.492251
  565474/2000000: episode: 799, duration: 19.118s, episode steps: 818, steps per second:  43, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.708 [0.000, 5.000],  loss: 0.018109, mae: 2.305004, mean_q: 2.787699, mean_eps: 0.491442
  566247/2000000: episode: 800, duration: 19.426s, episode steps: 773, steps per second:  40, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.020542, mae: 2.300542, mean_q: 2.781933, mean_eps: 0.490726
  566777/2000000: episode: 801, duration: 11.518s, episode steps: 530, steps per second:  46, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.017646, mae: 2.295577, mean_q: 2.776655, mean_eps: 0.490139
  567161/2000000: episode: 802, duration: 8.346s, episode steps: 384, steps per second:  46, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.019524, mae: 2.319203, mean_q: 2.802949, mean_eps: 0.489727
  568012/2000000: episode: 803, duration: 19.504s, episode steps: 851, steps per second:  44, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.018545, mae: 2.286137, mean_q: 2.763140, mean_eps: 0.489173
  568425/2000000: episode: 804, duration: 9.536s, episode steps: 413, steps per second:  43, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.021983, mae: 2.316626, mean_q: 2.799499, mean_eps: 0.488604
  569170/2000000: episode: 805, duration: 16.045s, episode steps: 745, steps per second:  46, episode reward: 21.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.021734, mae: 2.287355, mean_q: 2.763586, mean_eps: 0.488082
  569695/2000000: episode: 806, duration: 10.984s, episode steps: 525, steps per second:  48, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.020650, mae: 2.292018, mean_q: 2.770060, mean_eps: 0.487511
  571077/2000000: episode: 807, duration: 29.414s, episode steps: 1382, steps per second:  47, episode reward: 28.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.017068, mae: 2.263273, mean_q: 2.736915, mean_eps: 0.486653
  572035/2000000: episode: 808, duration: 20.821s, episode steps: 958, steps per second:  46, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.018780, mae: 2.255727, mean_q: 2.726220, mean_eps: 0.485600
  573572/2000000: episode: 809, duration: 31.425s, episode steps: 1537, steps per second:  49, episode reward: 30.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.200 [0.000, 5.000],  loss: 0.020210, mae: 2.287343, mean_q: 2.766203, mean_eps: 0.484478
  574185/2000000: episode: 810, duration: 12.848s, episode steps: 613, steps per second:  48, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.184 [0.000, 5.000],  loss: 0.017191, mae: 2.268130, mean_q: 2.741221, mean_eps: 0.483510
  574791/2000000: episode: 811, duration: 12.395s, episode steps: 606, steps per second:  49, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 0.021507, mae: 2.247283, mean_q: 2.712931, mean_eps: 0.482961
  575727/2000000: episode: 812, duration: 21.377s, episode steps: 936, steps per second:  44, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.019591, mae: 2.268955, mean_q: 2.741587, mean_eps: 0.482268
  576466/2000000: episode: 813, duration: 19.456s, episode steps: 739, steps per second:  38, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.212 [0.000, 5.000],  loss: 0.019855, mae: 2.272732, mean_q: 2.747545, mean_eps: 0.481514
  577430/2000000: episode: 814, duration: 23.794s, episode steps: 964, steps per second:  41, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.017819, mae: 2.278420, mean_q: 2.753757, mean_eps: 0.480747
  578074/2000000: episode: 815, duration: 14.106s, episode steps: 644, steps per second:  46, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.753 [0.000, 5.000],  loss: 0.021851, mae: 2.294684, mean_q: 2.775752, mean_eps: 0.480023
  578485/2000000: episode: 816, duration: 8.832s, episode steps: 411, steps per second:  47, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.017382, mae: 2.297043, mean_q: 2.775144, mean_eps: 0.479548
  579062/2000000: episode: 817, duration: 11.966s, episode steps: 577, steps per second:  48, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.016724, mae: 2.285192, mean_q: 2.763245, mean_eps: 0.479103
  579718/2000000: episode: 818, duration: 14.411s, episode steps: 656, steps per second:  46, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.018354, mae: 2.256576, mean_q: 2.727097, mean_eps: 0.478549
  580580/2000000: episode: 819, duration: 18.285s, episode steps: 862, steps per second:  47, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.176 [0.000, 5.000],  loss: 0.019028, mae: 2.277795, mean_q: 2.753028, mean_eps: 0.477867
  580996/2000000: episode: 820, duration: 8.727s, episode steps: 416, steps per second:  48, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.195 [0.000, 5.000],  loss: 0.016607, mae: 2.281601, mean_q: 2.757857, mean_eps: 0.477293
  581476/2000000: episode: 821, duration: 11.094s, episode steps: 480, steps per second:  43, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.148 [0.000, 5.000],  loss: 0.015375, mae: 2.296088, mean_q: 2.777080, mean_eps: 0.476889
  582230/2000000: episode: 822, duration: 16.984s, episode steps: 754, steps per second:  44, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.143 [0.000, 5.000],  loss: 0.018510, mae: 2.286418, mean_q: 2.766892, mean_eps: 0.476333
  583141/2000000: episode: 823, duration: 20.528s, episode steps: 911, steps per second:  44, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.997 [0.000, 5.000],  loss: 0.017452, mae: 2.279752, mean_q: 2.754422, mean_eps: 0.475583
  583800/2000000: episode: 824, duration: 14.705s, episode steps: 659, steps per second:  45, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.019641, mae: 2.293812, mean_q: 2.772264, mean_eps: 0.474877
  584777/2000000: episode: 825, duration: 23.011s, episode steps: 977, steps per second:  42, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.016795, mae: 2.309087, mean_q: 2.789676, mean_eps: 0.474141
  585346/2000000: episode: 826, duration: 12.387s, episode steps: 569, steps per second:  46, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.016372, mae: 2.286842, mean_q: 2.764167, mean_eps: 0.473444
  586166/2000000: episode: 827, duration: 17.339s, episode steps: 820, steps per second:  47, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.019063, mae: 2.334967, mean_q: 2.823519, mean_eps: 0.472820
  587058/2000000: episode: 828, duration: 19.136s, episode steps: 892, steps per second:  47, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.016972, mae: 2.322864, mean_q: 2.807368, mean_eps: 0.472049
  587723/2000000: episode: 829, duration: 14.427s, episode steps: 665, steps per second:  46, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.062 [0.000, 5.000],  loss: 0.016574, mae: 2.297482, mean_q: 2.777861, mean_eps: 0.471349
  588538/2000000: episode: 830, duration: 17.306s, episode steps: 815, steps per second:  47, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.324 [0.000, 5.000],  loss: 0.022170, mae: 2.299366, mean_q: 2.777083, mean_eps: 0.470683
  589034/2000000: episode: 831, duration: 11.035s, episode steps: 496, steps per second:  45, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.046 [0.000, 5.000],  loss: 0.020980, mae: 2.311476, mean_q: 2.793699, mean_eps: 0.470093
  589592/2000000: episode: 832, duration: 11.805s, episode steps: 558, steps per second:  47, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.018128, mae: 2.312422, mean_q: 2.796175, mean_eps: 0.469619
  590390/2000000: episode: 833, duration: 17.892s, episode steps: 798, steps per second:  45, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.019265, mae: 2.330692, mean_q: 2.815044, mean_eps: 0.469009
  591460/2000000: episode: 834, duration: 24.898s, episode steps: 1070, steps per second:  43, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.261 [0.000, 5.000],  loss: 0.019330, mae: 2.372758, mean_q: 2.865502, mean_eps: 0.468168
  591885/2000000: episode: 835, duration: 10.098s, episode steps: 425, steps per second:  42, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.654 [0.000, 5.000],  loss: 0.021292, mae: 2.389821, mean_q: 2.884727, mean_eps: 0.467495
  592507/2000000: episode: 836, duration: 15.858s, episode steps: 622, steps per second:  39, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.111 [0.000, 5.000],  loss: 0.019063, mae: 2.323079, mean_q: 2.806852, mean_eps: 0.467024
  593653/2000000: episode: 837, duration: 26.616s, episode steps: 1146, steps per second:  43, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.019378, mae: 2.356700, mean_q: 2.847663, mean_eps: 0.466228
  594636/2000000: episode: 838, duration: 20.327s, episode steps: 983, steps per second:  48, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.252 [0.000, 5.000],  loss: 0.017827, mae: 2.355711, mean_q: 2.845828, mean_eps: 0.465270
  595661/2000000: episode: 839, duration: 21.400s, episode steps: 1025, steps per second:  48, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.905 [0.000, 5.000],  loss: 0.018470, mae: 2.340792, mean_q: 2.829172, mean_eps: 0.464367
  596149/2000000: episode: 840, duration: 10.397s, episode steps: 488, steps per second:  47, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.019258, mae: 2.322338, mean_q: 2.807250, mean_eps: 0.463685
  596732/2000000: episode: 841, duration: 11.550s, episode steps: 583, steps per second:  50, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.018372, mae: 2.339972, mean_q: 2.825421, mean_eps: 0.463204
  597212/2000000: episode: 842, duration: 9.407s, episode steps: 480, steps per second:  51, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.019875, mae: 2.333889, mean_q: 2.817479, mean_eps: 0.462727
  598296/2000000: episode: 843, duration: 22.392s, episode steps: 1084, steps per second:  48, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.017903, mae: 2.330376, mean_q: 2.815486, mean_eps: 0.462023
  598932/2000000: episode: 844, duration: 13.162s, episode steps: 636, steps per second:  48, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.099 [0.000, 5.000],  loss: 0.018569, mae: 2.353758, mean_q: 2.841791, mean_eps: 0.461249
  599798/2000000: episode: 845, duration: 17.818s, episode steps: 866, steps per second:  49, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.174 [0.000, 5.000],  loss: 0.018953, mae: 2.338830, mean_q: 2.823295, mean_eps: 0.460572
  600687/2000000: episode: 846, duration: 17.577s, episode steps: 889, steps per second:  51, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.099 [0.000, 5.000],  loss: 0.018889, mae: 2.372386, mean_q: 2.868594, mean_eps: 0.459782
  601392/2000000: episode: 847, duration: 13.709s, episode steps: 705, steps per second:  51, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.017738, mae: 2.356223, mean_q: 2.846251, mean_eps: 0.459066
  602397/2000000: episode: 848, duration: 19.936s, episode steps: 1005, steps per second:  50, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.017690, mae: 2.337233, mean_q: 2.825618, mean_eps: 0.458295
  602953/2000000: episode: 849, duration: 10.919s, episode steps: 556, steps per second:  51, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.017791, mae: 2.371109, mean_q: 2.865207, mean_eps: 0.457592
  603417/2000000: episode: 850, duration: 8.957s, episode steps: 464, steps per second:  52, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.018943, mae: 2.373079, mean_q: 2.864360, mean_eps: 0.457133
  604360/2000000: episode: 851, duration: 19.829s, episode steps: 943, steps per second:  48, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.016569, mae: 2.333736, mean_q: 2.818844, mean_eps: 0.456501
  605259/2000000: episode: 852, duration: 19.108s, episode steps: 899, steps per second:  47, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.018250, mae: 2.354371, mean_q: 2.843355, mean_eps: 0.455673
  605901/2000000: episode: 853, duration: 13.415s, episode steps: 642, steps per second:  48, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.019569, mae: 2.313659, mean_q: 2.796340, mean_eps: 0.454978
  606829/2000000: episode: 854, duration: 19.826s, episode steps: 928, steps per second:  47, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.021048, mae: 2.343011, mean_q: 2.829157, mean_eps: 0.454271
  607852/2000000: episode: 855, duration: 21.765s, episode steps: 1023, steps per second:  47, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.898 [0.000, 5.000],  loss: 0.017963, mae: 2.341142, mean_q: 2.828881, mean_eps: 0.453394
  608616/2000000: episode: 856, duration: 16.580s, episode steps: 764, steps per second:  46, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.147 [0.000, 5.000],  loss: 0.018072, mae: 2.337609, mean_q: 2.822651, mean_eps: 0.452591
  609263/2000000: episode: 857, duration: 14.231s, episode steps: 647, steps per second:  45, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.852 [0.000, 5.000],  loss: 0.019091, mae: 2.343948, mean_q: 2.833465, mean_eps: 0.451956
  610057/2000000: episode: 858, duration: 16.682s, episode steps: 794, steps per second:  48, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.708 [0.000, 5.000],  loss: 0.017264, mae: 2.328046, mean_q: 2.813526, mean_eps: 0.451306
  611099/2000000: episode: 859, duration: 21.602s, episode steps: 1042, steps per second:  48, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.015761, mae: 2.337643, mean_q: 2.824218, mean_eps: 0.450480
  612748/2000000: episode: 860, duration: 33.021s, episode steps: 1649, steps per second:  50, episode reward: 35.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.019045, mae: 2.343381, mean_q: 2.829653, mean_eps: 0.449270
  613392/2000000: episode: 861, duration: 13.251s, episode steps: 644, steps per second:  49, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.018078, mae: 2.331474, mean_q: 2.817814, mean_eps: 0.448239
  613900/2000000: episode: 862, duration: 10.974s, episode steps: 508, steps per second:  46, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.256 [0.000, 5.000],  loss: 0.016767, mae: 2.343899, mean_q: 2.829173, mean_eps: 0.447720
  614786/2000000: episode: 863, duration: 18.252s, episode steps: 886, steps per second:  49, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.052 [0.000, 5.000],  loss: 0.017156, mae: 2.333157, mean_q: 2.817872, mean_eps: 0.447092
  615552/2000000: episode: 864, duration: 16.883s, episode steps: 766, steps per second:  45, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.287 [0.000, 5.000],  loss: 0.017631, mae: 2.331670, mean_q: 2.817098, mean_eps: 0.446349
  616321/2000000: episode: 865, duration: 16.951s, episode steps: 769, steps per second:  45, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.019159, mae: 2.358293, mean_q: 2.849602, mean_eps: 0.445658
  617015/2000000: episode: 866, duration: 15.845s, episode steps: 694, steps per second:  44, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.020703, mae: 2.349978, mean_q: 2.838991, mean_eps: 0.444999
  617955/2000000: episode: 867, duration: 22.078s, episode steps: 940, steps per second:  43, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.019410, mae: 2.344184, mean_q: 2.831304, mean_eps: 0.444264
  618800/2000000: episode: 868, duration: 19.978s, episode steps: 845, steps per second:  42, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.069 [0.000, 5.000],  loss: 0.018159, mae: 2.359627, mean_q: 2.850122, mean_eps: 0.443462
  619686/2000000: episode: 869, duration: 17.898s, episode steps: 886, steps per second:  50, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.827 [0.000, 5.000],  loss: 0.019634, mae: 2.363644, mean_q: 2.854757, mean_eps: 0.442682
  620414/2000000: episode: 870, duration: 15.012s, episode steps: 728, steps per second:  48, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.254 [0.000, 5.000],  loss: 0.017800, mae: 2.365099, mean_q: 2.854890, mean_eps: 0.441955
  621119/2000000: episode: 871, duration: 15.510s, episode steps: 705, steps per second:  45, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.939 [0.000, 5.000],  loss: 0.018795, mae: 2.360242, mean_q: 2.850741, mean_eps: 0.441311
  622363/2000000: episode: 872, duration: 25.760s, episode steps: 1244, steps per second:  48, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.018121, mae: 2.388999, mean_q: 2.884006, mean_eps: 0.440434
  622933/2000000: episode: 873, duration: 11.614s, episode steps: 570, steps per second:  49, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.967 [0.000, 5.000],  loss: 0.022740, mae: 2.373315, mean_q: 2.863010, mean_eps: 0.439617
  623623/2000000: episode: 874, duration: 14.668s, episode steps: 690, steps per second:  47, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.020807, mae: 2.375933, mean_q: 2.868458, mean_eps: 0.439050
  624378/2000000: episode: 875, duration: 15.327s, episode steps: 755, steps per second:  49, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.021742, mae: 2.377282, mean_q: 2.872207, mean_eps: 0.438400
  625215/2000000: episode: 876, duration: 17.470s, episode steps: 837, steps per second:  48, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.019873, mae: 2.387817, mean_q: 2.882620, mean_eps: 0.437684
  626151/2000000: episode: 877, duration: 21.661s, episode steps: 936, steps per second:  43, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.794 [0.000, 5.000],  loss: 0.023023, mae: 2.405970, mean_q: 2.908577, mean_eps: 0.436886
  627073/2000000: episode: 878, duration: 24.544s, episode steps: 922, steps per second:  38, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.249 [0.000, 5.000],  loss: 0.017159, mae: 2.383650, mean_q: 2.880652, mean_eps: 0.436049
  627702/2000000: episode: 879, duration: 14.780s, episode steps: 629, steps per second:  43, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.153 [0.000, 5.000],  loss: 0.017734, mae: 2.378914, mean_q: 2.873940, mean_eps: 0.435351
  628680/2000000: episode: 880, duration: 22.492s, episode steps: 978, steps per second:  43, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.018074, mae: 2.391740, mean_q: 2.890216, mean_eps: 0.434629
  629678/2000000: episode: 881, duration: 24.431s, episode steps: 998, steps per second:  41, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.235 [0.000, 5.000],  loss: 0.019789, mae: 2.384384, mean_q: 2.879015, mean_eps: 0.433740
  630611/2000000: episode: 882, duration: 21.673s, episode steps: 933, steps per second:  43, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.185 [0.000, 5.000],  loss: 0.018670, mae: 2.394130, mean_q: 2.891032, mean_eps: 0.432870
  631810/2000000: episode: 883, duration: 29.071s, episode steps: 1199, steps per second:  41, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.254 [0.000, 5.000],  loss: 0.023247, mae: 2.406736, mean_q: 2.908461, mean_eps: 0.431911
  632384/2000000: episode: 884, duration: 13.284s, episode steps: 574, steps per second:  43, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.021987, mae: 2.397733, mean_q: 2.895867, mean_eps: 0.431114
  633039/2000000: episode: 885, duration: 14.397s, episode steps: 655, steps per second:  45, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.017011, mae: 2.403871, mean_q: 2.905204, mean_eps: 0.430561
  633906/2000000: episode: 886, duration: 20.166s, episode steps: 867, steps per second:  43, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.856 [0.000, 5.000],  loss: 0.024256, mae: 2.430950, mean_q: 2.940002, mean_eps: 0.429875
  634951/2000000: episode: 887, duration: 23.764s, episode steps: 1045, steps per second:  44, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.215 [0.000, 5.000],  loss: 0.017331, mae: 2.400479, mean_q: 2.899697, mean_eps: 0.429015
  636035/2000000: episode: 888, duration: 22.248s, episode steps: 1084, steps per second:  49, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.650 [0.000, 5.000],  loss: 0.017297, mae: 2.432448, mean_q: 2.941242, mean_eps: 0.428057
  637012/2000000: episode: 889, duration: 22.033s, episode steps: 977, steps per second:  44, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.060 [0.000, 5.000],  loss: 0.017821, mae: 2.425585, mean_q: 2.931636, mean_eps: 0.427130
  637600/2000000: episode: 890, duration: 12.718s, episode steps: 588, steps per second:  46, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.980 [0.000, 5.000],  loss: 0.019448, mae: 2.423066, mean_q: 2.926993, mean_eps: 0.426426
  638032/2000000: episode: 891, duration: 9.038s, episode steps: 432, steps per second:  48, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.016074, mae: 2.435956, mean_q: 2.944099, mean_eps: 0.425967
  638814/2000000: episode: 892, duration: 16.181s, episode steps: 782, steps per second:  48, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.019874, mae: 2.452271, mean_q: 2.961936, mean_eps: 0.425420
  639531/2000000: episode: 893, duration: 14.881s, episode steps: 717, steps per second:  48, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.019877, mae: 2.464919, mean_q: 2.978696, mean_eps: 0.424745
  640164/2000000: episode: 894, duration: 13.565s, episode steps: 633, steps per second:  47, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.169 [0.000, 5.000],  loss: 0.016853, mae: 2.422750, mean_q: 2.928422, mean_eps: 0.424139
  640896/2000000: episode: 895, duration: 16.167s, episode steps: 732, steps per second:  45, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.045 [0.000, 5.000],  loss: 0.019088, mae: 2.460152, mean_q: 2.972625, mean_eps: 0.423525
  642184/2000000: episode: 896, duration: 30.014s, episode steps: 1288, steps per second:  43, episode reward: 16.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.018507, mae: 2.459382, mean_q: 2.972917, mean_eps: 0.422616
  643388/2000000: episode: 897, duration: 29.939s, episode steps: 1204, steps per second:  40, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.017433, mae: 2.451543, mean_q: 2.963054, mean_eps: 0.421494
  643756/2000000: episode: 898, duration: 8.487s, episode steps: 368, steps per second:  43, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.647 [0.000, 5.000],  loss: 0.026086, mae: 2.475059, mean_q: 2.988363, mean_eps: 0.420787
  644311/2000000: episode: 899, duration: 13.242s, episode steps: 555, steps per second:  42, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.991 [0.000, 5.000],  loss: 0.019326, mae: 2.447314, mean_q: 2.959842, mean_eps: 0.420371
  644948/2000000: episode: 900, duration: 14.722s, episode steps: 637, steps per second:  43, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.670 [0.000, 5.000],  loss: 0.022449, mae: 2.449214, mean_q: 2.960285, mean_eps: 0.419835
  645638/2000000: episode: 901, duration: 15.325s, episode steps: 690, steps per second:  45, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 0.019701, mae: 2.462443, mean_q: 2.980421, mean_eps: 0.419237
  647153/2000000: episode: 902, duration: 34.252s, episode steps: 1515, steps per second:  44, episode reward: 29.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.193 [0.000, 5.000],  loss: 0.020022, mae: 2.476927, mean_q: 2.993745, mean_eps: 0.418244
  648118/2000000: episode: 903, duration: 21.660s, episode steps: 965, steps per second:  45, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.019142, mae: 2.469482, mean_q: 2.983060, mean_eps: 0.417128
  649552/2000000: episode: 904, duration: 32.926s, episode steps: 1434, steps per second:  44, episode reward: 23.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.020479, mae: 2.486285, mean_q: 3.004924, mean_eps: 0.416049
  650205/2000000: episode: 905, duration: 15.633s, episode steps: 653, steps per second:  42, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.017911, mae: 2.501709, mean_q: 3.026616, mean_eps: 0.415110
  650822/2000000: episode: 906, duration: 14.731s, episode steps: 617, steps per second:  42, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.017466, mae: 2.521122, mean_q: 3.045225, mean_eps: 0.414537
  651652/2000000: episode: 907, duration: 18.457s, episode steps: 830, steps per second:  45, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.261 [0.000, 5.000],  loss: 0.021694, mae: 2.506769, mean_q: 3.029069, mean_eps: 0.413888
  652162/2000000: episode: 908, duration: 12.007s, episode steps: 510, steps per second:  42, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.202 [0.000, 5.000],  loss: 0.023957, mae: 2.505642, mean_q: 3.023290, mean_eps: 0.413285
  653109/2000000: episode: 909, duration: 22.229s, episode steps: 947, steps per second:  43, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.680 [0.000, 5.000],  loss: 0.019332, mae: 2.481286, mean_q: 2.997619, mean_eps: 0.412628
  653712/2000000: episode: 910, duration: 13.899s, episode steps: 603, steps per second:  43, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.015 [0.000, 5.000],  loss: 0.020106, mae: 2.530467, mean_q: 3.056706, mean_eps: 0.411931
  654297/2000000: episode: 911, duration: 13.505s, episode steps: 585, steps per second:  43, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.009 [0.000, 5.000],  loss: 0.016114, mae: 2.507681, mean_q: 3.029135, mean_eps: 0.411396
  655203/2000000: episode: 912, duration: 20.476s, episode steps: 906, steps per second:  44, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.022231, mae: 2.509260, mean_q: 3.032611, mean_eps: 0.410725
  656257/2000000: episode: 913, duration: 22.158s, episode steps: 1054, steps per second:  48, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.018351, mae: 2.515837, mean_q: 3.039337, mean_eps: 0.409843
  656867/2000000: episode: 914, duration: 12.877s, episode steps: 610, steps per second:  47, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.018994, mae: 2.527143, mean_q: 3.053559, mean_eps: 0.409094
  657875/2000000: episode: 915, duration: 21.660s, episode steps: 1008, steps per second:  47, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.018672, mae: 2.527548, mean_q: 3.053254, mean_eps: 0.408367
  658791/2000000: episode: 916, duration: 19.926s, episode steps: 916, steps per second:  46, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.017023, mae: 2.517926, mean_q: 3.042531, mean_eps: 0.407501
  660433/2000000: episode: 917, duration: 37.323s, episode steps: 1642, steps per second:  44, episode reward: 36.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.268 [0.000, 5.000],  loss: 0.018711, mae: 2.551019, mean_q: 3.080355, mean_eps: 0.406349
  661502/2000000: episode: 918, duration: 26.759s, episode steps: 1069, steps per second:  40, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.016342, mae: 2.549059, mean_q: 3.079316, mean_eps: 0.405129
  662228/2000000: episode: 919, duration: 16.851s, episode steps: 726, steps per second:  43, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.675 [0.000, 5.000],  loss: 0.019046, mae: 2.539058, mean_q: 3.066695, mean_eps: 0.404322
  663464/2000000: episode: 920, duration: 29.375s, episode steps: 1236, steps per second:  42, episode reward: 18.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.017034, mae: 2.543244, mean_q: 3.070167, mean_eps: 0.403440
  664445/2000000: episode: 921, duration: 22.949s, episode steps: 981, steps per second:  43, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.284 [0.000, 5.000],  loss: 0.019573, mae: 2.555079, mean_q: 3.084096, mean_eps: 0.402441
  664957/2000000: episode: 922, duration: 11.417s, episode steps: 512, steps per second:  45, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.021749, mae: 2.520469, mean_q: 3.040963, mean_eps: 0.401768
  665924/2000000: episode: 923, duration: 23.336s, episode steps: 967, steps per second:  41, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.016070, mae: 2.575862, mean_q: 3.111767, mean_eps: 0.401104
  666401/2000000: episode: 924, duration: 11.639s, episode steps: 477, steps per second:  41, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.319 [0.000, 5.000],  loss: 0.021227, mae: 2.560350, mean_q: 3.092254, mean_eps: 0.400454
  666895/2000000: episode: 925, duration: 11.885s, episode steps: 494, steps per second:  42, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.239 [0.000, 5.000],  loss: 0.020954, mae: 2.576489, mean_q: 3.113559, mean_eps: 0.400017
  667996/2000000: episode: 926, duration: 22.485s, episode steps: 1101, steps per second:  49, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.161 [0.000, 5.000],  loss: 0.019365, mae: 2.569746, mean_q: 3.105536, mean_eps: 0.399300
  669181/2000000: episode: 927, duration: 26.110s, episode steps: 1185, steps per second:  45, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.306 [0.000, 5.000],  loss: 0.020033, mae: 2.575366, mean_q: 3.111865, mean_eps: 0.398271
  669875/2000000: episode: 928, duration: 14.015s, episode steps: 694, steps per second:  50, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.021555, mae: 2.577350, mean_q: 3.112825, mean_eps: 0.397425
  670487/2000000: episode: 929, duration: 12.554s, episode steps: 612, steps per second:  49, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.250 [0.000, 5.000],  loss: 0.016103, mae: 2.599202, mean_q: 3.143375, mean_eps: 0.396838
  670929/2000000: episode: 930, duration: 9.361s, episode steps: 442, steps per second:  47, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.699 [0.000, 5.000],  loss: 0.015835, mae: 2.580510, mean_q: 3.120996, mean_eps: 0.396363
  671447/2000000: episode: 931, duration: 10.911s, episode steps: 518, steps per second:  47, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.654 [0.000, 5.000],  loss: 0.020388, mae: 2.602634, mean_q: 3.146894, mean_eps: 0.395931
  672282/2000000: episode: 932, duration: 17.593s, episode steps: 835, steps per second:  47, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.019226, mae: 2.606738, mean_q: 3.151887, mean_eps: 0.395322
  673476/2000000: episode: 933, duration: 25.757s, episode steps: 1194, steps per second:  46, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.020077, mae: 2.614206, mean_q: 3.159292, mean_eps: 0.394410
  674666/2000000: episode: 934, duration: 27.367s, episode steps: 1190, steps per second:  43, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.019320, mae: 2.597318, mean_q: 3.137639, mean_eps: 0.393337
  675066/2000000: episode: 935, duration: 9.585s, episode steps: 400, steps per second:  42, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.277 [0.000, 5.000],  loss: 0.019822, mae: 2.604305, mean_q: 3.145745, mean_eps: 0.392621
  675810/2000000: episode: 936, duration: 16.243s, episode steps: 744, steps per second:  46, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.988 [0.000, 5.000],  loss: 0.018910, mae: 2.645156, mean_q: 3.195333, mean_eps: 0.392106
  676318/2000000: episode: 937, duration: 11.370s, episode steps: 508, steps per second:  45, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.705 [0.000, 5.000],  loss: 0.017300, mae: 2.685275, mean_q: 3.245436, mean_eps: 0.391542
  677568/2000000: episode: 938, duration: 28.644s, episode steps: 1250, steps per second:  44, episode reward: 25.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.017312, mae: 2.677162, mean_q: 3.235377, mean_eps: 0.390752
  678471/2000000: episode: 939, duration: 20.786s, episode steps: 903, steps per second:  43, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.226 [0.000, 5.000],  loss: 0.021995, mae: 2.651948, mean_q: 3.203398, mean_eps: 0.389784
  679289/2000000: episode: 940, duration: 20.161s, episode steps: 818, steps per second:  41, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.019130, mae: 2.654764, mean_q: 3.209316, mean_eps: 0.389008
  679963/2000000: episode: 941, duration: 17.018s, episode steps: 674, steps per second:  40, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.197 [0.000, 5.000],  loss: 0.018821, mae: 2.683115, mean_q: 3.242077, mean_eps: 0.388337
  680905/2000000: episode: 942, duration: 22.123s, episode steps: 942, steps per second:  43, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.017511, mae: 2.629204, mean_q: 3.175767, mean_eps: 0.387609
  682024/2000000: episode: 943, duration: 26.601s, episode steps: 1119, steps per second:  42, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.208 [0.000, 5.000],  loss: 0.017811, mae: 2.658941, mean_q: 3.213155, mean_eps: 0.386682
  682846/2000000: episode: 944, duration: 19.047s, episode steps: 822, steps per second:  43, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.018211, mae: 2.631959, mean_q: 3.181063, mean_eps: 0.385809
  683697/2000000: episode: 945, duration: 18.006s, episode steps: 851, steps per second:  47, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.114 [0.000, 5.000],  loss: 0.016963, mae: 2.660142, mean_q: 3.213800, mean_eps: 0.385055
  684355/2000000: episode: 946, duration: 15.799s, episode steps: 658, steps per second:  42, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.207 [0.000, 5.000],  loss: 0.018847, mae: 2.636449, mean_q: 3.187564, mean_eps: 0.384377
  685317/2000000: episode: 947, duration: 22.545s, episode steps: 962, steps per second:  43, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.018108, mae: 2.636061, mean_q: 3.184523, mean_eps: 0.383648
  686125/2000000: episode: 948, duration: 17.824s, episode steps: 808, steps per second:  45, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.019865, mae: 2.657001, mean_q: 3.210970, mean_eps: 0.382850
  686522/2000000: episode: 949, duration: 8.556s, episode steps: 397, steps per second:  46, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.227 [0.000, 5.000],  loss: 0.022711, mae: 2.677820, mean_q: 3.234672, mean_eps: 0.382308
  687454/2000000: episode: 950, duration: 20.671s, episode steps: 932, steps per second:  45, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.019048, mae: 2.661780, mean_q: 3.213092, mean_eps: 0.381711
  688154/2000000: episode: 951, duration: 15.982s, episode steps: 700, steps per second:  44, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.214 [0.000, 5.000],  loss: 0.018070, mae: 2.639258, mean_q: 3.189496, mean_eps: 0.380976
  689090/2000000: episode: 952, duration: 21.617s, episode steps: 936, steps per second:  43, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.200 [0.000, 5.000],  loss: 0.017529, mae: 2.678194, mean_q: 3.238047, mean_eps: 0.380240
  689827/2000000: episode: 953, duration: 20.454s, episode steps: 737, steps per second:  36, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.104 [0.000, 5.000],  loss: 0.019383, mae: 2.648262, mean_q: 3.198203, mean_eps: 0.379488
  690350/2000000: episode: 954, duration: 14.458s, episode steps: 523, steps per second:  36, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.042 [0.000, 5.000],  loss: 0.018410, mae: 2.678865, mean_q: 3.237423, mean_eps: 0.378921
  691237/2000000: episode: 955, duration: 20.758s, episode steps: 887, steps per second:  43, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.245 [0.000, 5.000],  loss: 0.021989, mae: 2.678870, mean_q: 3.237974, mean_eps: 0.378285
  691794/2000000: episode: 956, duration: 12.607s, episode steps: 557, steps per second:  44, episode reward: 17.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.233 [0.000, 5.000],  loss: 0.024237, mae: 2.691678, mean_q: 3.251549, mean_eps: 0.377636
  692495/2000000: episode: 957, duration: 14.998s, episode steps: 701, steps per second:  47, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.019562, mae: 2.697327, mean_q: 3.256285, mean_eps: 0.377070
  693403/2000000: episode: 958, duration: 20.214s, episode steps: 908, steps per second:  45, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.291 [0.000, 5.000],  loss: 0.020608, mae: 2.697550, mean_q: 3.256805, mean_eps: 0.376347
  693903/2000000: episode: 959, duration: 11.177s, episode steps: 500, steps per second:  45, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.118 [0.000, 5.000],  loss: 0.021861, mae: 2.670628, mean_q: 3.224839, mean_eps: 0.375713
  694901/2000000: episode: 960, duration: 23.509s, episode steps: 998, steps per second:  42, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.873 [0.000, 5.000],  loss: 0.018348, mae: 2.658424, mean_q: 3.211549, mean_eps: 0.375038
  696002/2000000: episode: 961, duration: 26.305s, episode steps: 1101, steps per second:  42, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.187 [0.000, 5.000],  loss: 0.019311, mae: 2.705047, mean_q: 3.268678, mean_eps: 0.374093
  697309/2000000: episode: 962, duration: 31.671s, episode steps: 1307, steps per second:  41, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.161 [0.000, 5.000],  loss: 0.019383, mae: 2.706151, mean_q: 3.269060, mean_eps: 0.373010
  697812/2000000: episode: 963, duration: 11.918s, episode steps: 503, steps per second:  42, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.024852, mae: 2.678266, mean_q: 3.235183, mean_eps: 0.372196
  698604/2000000: episode: 964, duration: 17.355s, episode steps: 792, steps per second:  46, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: 0.021528, mae: 2.697683, mean_q: 3.260738, mean_eps: 0.371615
  700014/2000000: episode: 965, duration: 31.533s, episode steps: 1410, steps per second:  45, episode reward: 21.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.804 [0.000, 5.000],  loss: 0.020124, mae: 2.699226, mean_q: 3.260914, mean_eps: 0.370623
  700970/2000000: episode: 966, duration: 21.837s, episode steps: 956, steps per second:  44, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.020739, mae: 2.734356, mean_q: 3.303238, mean_eps: 0.369557
  701516/2000000: episode: 967, duration: 12.355s, episode steps: 546, steps per second:  44, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.020799, mae: 2.744537, mean_q: 3.313213, mean_eps: 0.368882
  702189/2000000: episode: 968, duration: 14.797s, episode steps: 673, steps per second:  45, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.025178, mae: 2.749790, mean_q: 3.320603, mean_eps: 0.368333
  702820/2000000: episode: 969, duration: 13.787s, episode steps: 631, steps per second:  46, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.190 [0.000, 5.000],  loss: 0.020499, mae: 2.759605, mean_q: 3.333775, mean_eps: 0.367746
  703567/2000000: episode: 970, duration: 16.970s, episode steps: 747, steps per second:  44, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.016 [0.000, 5.000],  loss: 0.020869, mae: 2.711436, mean_q: 3.271680, mean_eps: 0.367127
  704179/2000000: episode: 971, duration: 13.698s, episode steps: 612, steps per second:  45, episode reward: 18.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.758 [0.000, 5.000],  loss: 0.025137, mae: 2.763553, mean_q: 3.336785, mean_eps: 0.366515
  704936/2000000: episode: 972, duration: 18.253s, episode steps: 757, steps per second:  41, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.627 [0.000, 5.000],  loss: 0.022476, mae: 2.745861, mean_q: 3.315132, mean_eps: 0.365900
  705622/2000000: episode: 973, duration: 17.844s, episode steps: 686, steps per second:  38, episode reward: 20.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.936 [0.000, 5.000],  loss: 0.018381, mae: 2.724944, mean_q: 3.290619, mean_eps: 0.365250
  706478/2000000: episode: 974, duration: 19.780s, episode steps: 856, steps per second:  43, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.019562, mae: 2.698874, mean_q: 3.260474, mean_eps: 0.364555
  707144/2000000: episode: 975, duration: 15.170s, episode steps: 666, steps per second:  44, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.018351, mae: 2.698835, mean_q: 3.256979, mean_eps: 0.363871
  707770/2000000: episode: 976, duration: 14.904s, episode steps: 626, steps per second:  42, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.022002, mae: 2.726264, mean_q: 3.292014, mean_eps: 0.363290
  708508/2000000: episode: 977, duration: 17.056s, episode steps: 738, steps per second:  43, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.351 [0.000, 5.000],  loss: 0.020382, mae: 2.688295, mean_q: 3.252257, mean_eps: 0.362676
  708928/2000000: episode: 978, duration: 9.942s, episode steps: 420, steps per second:  42, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.664 [0.000, 5.000],  loss: 0.019284, mae: 2.682897, mean_q: 3.240451, mean_eps: 0.362156
  709336/2000000: episode: 979, duration: 9.466s, episode steps: 408, steps per second:  43, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.034 [0.000, 5.000],  loss: 0.018737, mae: 2.697442, mean_q: 3.259296, mean_eps: 0.361783
  709970/2000000: episode: 980, duration: 15.064s, episode steps: 634, steps per second:  42, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.017437, mae: 2.702824, mean_q: 3.263800, mean_eps: 0.361313
  710438/2000000: episode: 981, duration: 11.970s, episode steps: 468, steps per second:  39, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.182 [0.000, 5.000],  loss: 0.016078, mae: 2.731111, mean_q: 3.299202, mean_eps: 0.360816
  711383/2000000: episode: 982, duration: 22.787s, episode steps: 945, steps per second:  41, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.019896, mae: 2.733942, mean_q: 3.302740, mean_eps: 0.360181
  712044/2000000: episode: 983, duration: 15.499s, episode steps: 661, steps per second:  43, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.022087, mae: 2.753897, mean_q: 3.324312, mean_eps: 0.359459
  712691/2000000: episode: 984, duration: 15.920s, episode steps: 647, steps per second:  41, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.022349, mae: 2.735955, mean_q: 3.303886, mean_eps: 0.358871
  713432/2000000: episode: 985, duration: 18.044s, episode steps: 741, steps per second:  41, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.984 [0.000, 5.000],  loss: 0.020911, mae: 2.749460, mean_q: 3.320217, mean_eps: 0.358246
  714315/2000000: episode: 986, duration: 18.793s, episode steps: 883, steps per second:  47, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.216 [0.000, 5.000],  loss: 0.021556, mae: 2.729515, mean_q: 3.295407, mean_eps: 0.357515
  714705/2000000: episode: 987, duration: 8.674s, episode steps: 390, steps per second:  45, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.023351, mae: 2.781540, mean_q: 3.364266, mean_eps: 0.356941
  715209/2000000: episode: 988, duration: 12.724s, episode steps: 504, steps per second:  40, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.017450, mae: 2.724099, mean_q: 3.292028, mean_eps: 0.356538
  716420/2000000: episode: 989, duration: 29.365s, episode steps: 1211, steps per second:  41, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.149 [0.000, 5.000],  loss: 0.022503, mae: 2.730926, mean_q: 3.297290, mean_eps: 0.355767
  717186/2000000: episode: 990, duration: 17.222s, episode steps: 766, steps per second:  44, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.168 [0.000, 5.000],  loss: 0.019064, mae: 2.688203, mean_q: 3.244605, mean_eps: 0.354878
  718261/2000000: episode: 991, duration: 24.032s, episode steps: 1075, steps per second:  45, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.087 [0.000, 5.000],  loss: 0.019534, mae: 2.725493, mean_q: 3.289311, mean_eps: 0.354048
  718682/2000000: episode: 992, duration: 9.926s, episode steps: 421, steps per second:  42, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.648 [0.000, 5.000],  loss: 0.022252, mae: 2.732035, mean_q: 3.297565, mean_eps: 0.353375
  719070/2000000: episode: 993, duration: 10.646s, episode steps: 388, steps per second:  36, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.263 [0.000, 5.000],  loss: 0.017043, mae: 2.726254, mean_q: 3.291828, mean_eps: 0.353012
  719559/2000000: episode: 994, duration: 12.954s, episode steps: 489, steps per second:  38, episode reward: 13.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: 0.020090, mae: 2.724625, mean_q: 3.287289, mean_eps: 0.352617
  720223/2000000: episode: 995, duration: 16.417s, episode steps: 664, steps per second:  40, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.098 [0.000, 5.000],  loss: 0.018648, mae: 2.704748, mean_q: 3.264767, mean_eps: 0.352099
  721220/2000000: episode: 996, duration: 24.276s, episode steps: 997, steps per second:  41, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.019727, mae: 2.740280, mean_q: 3.308637, mean_eps: 0.351352
  722323/2000000: episode: 997, duration: 27.570s, episode steps: 1103, steps per second:  40, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.102 [0.000, 5.000],  loss: 0.024473, mae: 2.741907, mean_q: 3.309249, mean_eps: 0.350407
  723495/2000000: episode: 998, duration: 27.052s, episode steps: 1172, steps per second:  43, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.022576, mae: 2.741791, mean_q: 3.307811, mean_eps: 0.349383
  724099/2000000: episode: 999, duration: 13.744s, episode steps: 604, steps per second:  44, episode reward: 17.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.674 [0.000, 5.000],  loss: 0.020009, mae: 2.757281, mean_q: 3.325006, mean_eps: 0.348584
  724866/2000000: episode: 1000, duration: 17.978s, episode steps: 767, steps per second:  43, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.021006, mae: 2.760170, mean_q: 3.330507, mean_eps: 0.347966
  725778/2000000: episode: 1001, duration: 22.483s, episode steps: 912, steps per second:  41, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.711 [0.000, 5.000],  loss: 0.017950, mae: 2.735837, mean_q: 3.302923, mean_eps: 0.347210
  726568/2000000: episode: 1002, duration: 19.026s, episode steps: 790, steps per second:  42, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.138 [0.000, 5.000],  loss: 0.025762, mae: 2.738984, mean_q: 3.307546, mean_eps: 0.346445
  727334/2000000: episode: 1003, duration: 17.937s, episode steps: 766, steps per second:  43, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.905 [0.000, 5.000],  loss: 0.023188, mae: 2.744190, mean_q: 3.309588, mean_eps: 0.345745
  727955/2000000: episode: 1004, duration: 14.898s, episode steps: 621, steps per second:  42, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.018342, mae: 2.740316, mean_q: 3.306136, mean_eps: 0.345120
  728919/2000000: episode: 1005, duration: 21.303s, episode steps: 964, steps per second:  45, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.205 [0.000, 5.000],  loss: 0.023250, mae: 2.761799, mean_q: 3.333023, mean_eps: 0.344408
  729416/2000000: episode: 1006, duration: 10.629s, episode steps: 497, steps per second:  47, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.899 [0.000, 5.000],  loss: 0.021579, mae: 2.747789, mean_q: 3.314883, mean_eps: 0.343751
  730071/2000000: episode: 1007, duration: 14.151s, episode steps: 655, steps per second:  46, episode reward: 19.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.689 [0.000, 5.000],  loss: 0.022655, mae: 2.786176, mean_q: 3.362020, mean_eps: 0.343232
  730586/2000000: episode: 1008, duration: 12.151s, episode steps: 515, steps per second:  42, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.019041, mae: 2.772721, mean_q: 3.346123, mean_eps: 0.342705
  731088/2000000: episode: 1009, duration: 11.973s, episode steps: 502, steps per second:  42, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.618 [0.000, 5.000],  loss: 0.018983, mae: 2.743561, mean_q: 3.311486, mean_eps: 0.342248
  731913/2000000: episode: 1010, duration: 18.449s, episode steps: 825, steps per second:  45, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.779 [0.000, 5.000],  loss: 0.022424, mae: 2.731137, mean_q: 3.294455, mean_eps: 0.341650
  732573/2000000: episode: 1011, duration: 14.908s, episode steps: 660, steps per second:  44, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.020845, mae: 2.785627, mean_q: 3.358371, mean_eps: 0.340980
  733539/2000000: episode: 1012, duration: 21.502s, episode steps: 966, steps per second:  45, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.020293, mae: 2.775651, mean_q: 3.349000, mean_eps: 0.340250
  734617/2000000: episode: 1013, duration: 24.797s, episode steps: 1078, steps per second:  43, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.141 [0.000, 5.000],  loss: 0.020436, mae: 2.776334, mean_q: 3.350494, mean_eps: 0.339330
  735300/2000000: episode: 1014, duration: 16.741s, episode steps: 683, steps per second:  41, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.851 [0.000, 5.000],  loss: 0.021130, mae: 2.784180, mean_q: 3.361024, mean_eps: 0.338538
  735801/2000000: episode: 1015, duration: 12.995s, episode steps: 501, steps per second:  39, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.613 [0.000, 5.000],  loss: 0.021640, mae: 2.784817, mean_q: 3.359766, mean_eps: 0.338005
  736354/2000000: episode: 1016, duration: 14.255s, episode steps: 553, steps per second:  39, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.018526, mae: 2.778706, mean_q: 3.355358, mean_eps: 0.337530
  736972/2000000: episode: 1017, duration: 14.646s, episode steps: 618, steps per second:  42, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.159 [0.000, 5.000],  loss: 0.020533, mae: 2.788008, mean_q: 3.364414, mean_eps: 0.337004
  737975/2000000: episode: 1018, duration: 23.966s, episode steps: 1003, steps per second:  42, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 0.020405, mae: 2.784325, mean_q: 3.359383, mean_eps: 0.336275
  738912/2000000: episode: 1019, duration: 21.506s, episode steps: 937, steps per second:  44, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.188 [0.000, 5.000],  loss: 0.019126, mae: 2.774532, mean_q: 3.349546, mean_eps: 0.335402
  739786/2000000: episode: 1020, duration: 20.889s, episode steps: 874, steps per second:  42, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.019479, mae: 2.782723, mean_q: 3.358868, mean_eps: 0.334587
  740168/2000000: episode: 1021, duration: 9.418s, episode steps: 382, steps per second:  41, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.220 [0.000, 5.000],  loss: 0.018855, mae: 2.804689, mean_q: 3.385964, mean_eps: 0.334022
  741093/2000000: episode: 1022, duration: 23.632s, episode steps: 925, steps per second:  39, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.024145, mae: 2.807331, mean_q: 3.387668, mean_eps: 0.333433
  742021/2000000: episode: 1023, duration: 21.762s, episode steps: 928, steps per second:  43, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.021993, mae: 2.781633, mean_q: 3.359639, mean_eps: 0.332598
  742629/2000000: episode: 1024, duration: 14.446s, episode steps: 608, steps per second:  42, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.688 [0.000, 5.000],  loss: 0.022446, mae: 2.805544, mean_q: 3.385268, mean_eps: 0.331907
  743010/2000000: episode: 1025, duration: 9.009s, episode steps: 381, steps per second:  42, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.927 [0.000, 5.000],  loss: 0.023742, mae: 2.786500, mean_q: 3.367042, mean_eps: 0.331462
  743716/2000000: episode: 1026, duration: 17.365s, episode steps: 706, steps per second:  41, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.061 [0.000, 5.000],  loss: 0.019314, mae: 2.809610, mean_q: 3.392206, mean_eps: 0.330974
  744752/2000000: episode: 1027, duration: 23.147s, episode steps: 1036, steps per second:  45, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.146 [0.000, 5.000],  loss: 0.020198, mae: 2.811268, mean_q: 3.394316, mean_eps: 0.330191
  745688/2000000: episode: 1028, duration: 21.122s, episode steps: 936, steps per second:  44, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.020183, mae: 2.799303, mean_q: 3.380236, mean_eps: 0.329304
  746666/2000000: episode: 1029, duration: 22.312s, episode steps: 978, steps per second:  44, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.020605, mae: 2.793345, mean_q: 3.371079, mean_eps: 0.328442
  747374/2000000: episode: 1030, duration: 15.631s, episode steps: 708, steps per second:  45, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.232 [0.000, 5.000],  loss: 0.018713, mae: 2.784695, mean_q: 3.361570, mean_eps: 0.327682
  748290/2000000: episode: 1031, duration: 21.108s, episode steps: 916, steps per second:  43, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.004 [0.000, 5.000],  loss: 0.018765, mae: 2.800332, mean_q: 3.379705, mean_eps: 0.326951
  749077/2000000: episode: 1032, duration: 17.077s, episode steps: 787, steps per second:  46, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.222 [0.000, 5.000],  loss: 0.017868, mae: 2.771885, mean_q: 3.346500, mean_eps: 0.326184
  749819/2000000: episode: 1033, duration: 17.134s, episode steps: 742, steps per second:  43, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.024554, mae: 2.797432, mean_q: 3.373027, mean_eps: 0.325497
  750187/2000000: episode: 1034, duration: 8.699s, episode steps: 368, steps per second:  42, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.215 [0.000, 5.000],  loss: 0.024138, mae: 2.783475, mean_q: 3.357924, mean_eps: 0.324998
  751285/2000000: episode: 1035, duration: 30.231s, episode steps: 1098, steps per second:  36, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.116 [0.000, 5.000],  loss: 0.021475, mae: 2.772854, mean_q: 3.345928, mean_eps: 0.324338
  751668/2000000: episode: 1036, duration: 10.070s, episode steps: 383, steps per second:  38, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.016640, mae: 2.789045, mean_q: 3.366327, mean_eps: 0.323672
  752621/2000000: episode: 1037, duration: 24.393s, episode steps: 953, steps per second:  39, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.641 [0.000, 5.000],  loss: 0.022122, mae: 2.790304, mean_q: 3.367929, mean_eps: 0.323070
  753540/2000000: episode: 1038, duration: 21.983s, episode steps: 919, steps per second:  42, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.942 [0.000, 5.000],  loss: 0.019298, mae: 2.794027, mean_q: 3.371118, mean_eps: 0.322228
  754306/2000000: episode: 1039, duration: 19.182s, episode steps: 766, steps per second:  40, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.067 [0.000, 5.000],  loss: 0.021335, mae: 2.787660, mean_q: 3.362895, mean_eps: 0.321470
  754955/2000000: episode: 1040, duration: 16.453s, episode steps: 649, steps per second:  39, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.021503, mae: 2.798489, mean_q: 3.376573, mean_eps: 0.320833
  755500/2000000: episode: 1041, duration: 13.878s, episode steps: 545, steps per second:  39, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.837 [0.000, 5.000],  loss: 0.021102, mae: 2.831539, mean_q: 3.420194, mean_eps: 0.320297
  756029/2000000: episode: 1042, duration: 13.766s, episode steps: 529, steps per second:  38, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.021739, mae: 2.778675, mean_q: 3.354832, mean_eps: 0.319812
  756691/2000000: episode: 1043, duration: 16.575s, episode steps: 662, steps per second:  40, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.033 [0.000, 5.000],  loss: 0.020062, mae: 2.837117, mean_q: 3.424203, mean_eps: 0.319276
  757532/2000000: episode: 1044, duration: 20.680s, episode steps: 841, steps per second:  41, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.022299, mae: 2.806781, mean_q: 3.386327, mean_eps: 0.318601
  758869/2000000: episode: 1045, duration: 32.820s, episode steps: 1337, steps per second:  41, episode reward: 23.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.239 [0.000, 5.000],  loss: 0.022514, mae: 2.817064, mean_q: 3.401045, mean_eps: 0.317620
  759495/2000000: episode: 1046, duration: 13.673s, episode steps: 626, steps per second:  46, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.019895, mae: 2.810937, mean_q: 3.393204, mean_eps: 0.316736
  760560/2000000: episode: 1047, duration: 24.308s, episode steps: 1065, steps per second:  44, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.135 [0.000, 5.000],  loss: 0.020036, mae: 2.825675, mean_q: 3.412568, mean_eps: 0.315977
  761473/2000000: episode: 1048, duration: 21.727s, episode steps: 913, steps per second:  42, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.164 [0.000, 5.000],  loss: 0.020380, mae: 2.824532, mean_q: 3.411526, mean_eps: 0.315086
  761978/2000000: episode: 1049, duration: 11.691s, episode steps: 505, steps per second:  43, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.687 [0.000, 5.000],  loss: 0.018481, mae: 2.825302, mean_q: 3.411157, mean_eps: 0.314447
  762912/2000000: episode: 1050, duration: 21.129s, episode steps: 934, steps per second:  44, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.020814, mae: 2.864742, mean_q: 3.459065, mean_eps: 0.313800
  763484/2000000: episode: 1051, duration: 13.717s, episode steps: 572, steps per second:  42, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.161 [0.000, 5.000],  loss: 0.023252, mae: 2.835602, mean_q: 3.422182, mean_eps: 0.313124
  764564/2000000: episode: 1052, duration: 26.852s, episode steps: 1080, steps per second:  40, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.020644, mae: 2.861281, mean_q: 3.455197, mean_eps: 0.312380
  764951/2000000: episode: 1053, duration: 10.026s, episode steps: 387, steps per second:  39, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.021192, mae: 2.825763, mean_q: 3.413334, mean_eps: 0.311720
  765984/2000000: episode: 1054, duration: 26.792s, episode steps: 1033, steps per second:  39, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.017585, mae: 2.856211, mean_q: 3.449296, mean_eps: 0.311081
  766613/2000000: episode: 1055, duration: 15.460s, episode steps: 629, steps per second:  41, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.027403, mae: 2.876074, mean_q: 3.471659, mean_eps: 0.310332
  767226/2000000: episode: 1056, duration: 14.051s, episode steps: 613, steps per second:  44, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.917 [0.000, 5.000],  loss: 0.019522, mae: 2.842493, mean_q: 3.435993, mean_eps: 0.309772
  768027/2000000: episode: 1057, duration: 19.226s, episode steps: 801, steps per second:  42, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.978 [0.000, 5.000],  loss: 0.019159, mae: 2.884561, mean_q: 3.486547, mean_eps: 0.309137
  769275/2000000: episode: 1058, duration: 28.389s, episode steps: 1248, steps per second:  44, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.022284, mae: 2.857929, mean_q: 3.452980, mean_eps: 0.308215
  770182/2000000: episode: 1059, duration: 21.020s, episode steps: 907, steps per second:  43, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.114 [0.000, 5.000],  loss: 0.021419, mae: 2.881476, mean_q: 3.481949, mean_eps: 0.307245
  770809/2000000: episode: 1060, duration: 20.511s, episode steps: 627, steps per second:  31, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.024539, mae: 2.922900, mean_q: 3.530351, mean_eps: 0.306554
  771410/2000000: episode: 1061, duration: 13.462s, episode steps: 601, steps per second:  45, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.024167, mae: 2.895849, mean_q: 3.497347, mean_eps: 0.306001
  772066/2000000: episode: 1062, duration: 14.682s, episode steps: 656, steps per second:  45, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.020538, mae: 2.900560, mean_q: 3.501937, mean_eps: 0.305436
  772730/2000000: episode: 1063, duration: 14.221s, episode steps: 664, steps per second:  47, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.020346, mae: 2.902499, mean_q: 3.504043, mean_eps: 0.304842
  773492/2000000: episode: 1064, duration: 16.442s, episode steps: 762, steps per second:  46, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.022528, mae: 2.937491, mean_q: 3.547352, mean_eps: 0.304201
  774163/2000000: episode: 1065, duration: 14.378s, episode steps: 671, steps per second:  47, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.294 [0.000, 5.000],  loss: 0.019244, mae: 2.890897, mean_q: 3.492262, mean_eps: 0.303557
  774831/2000000: episode: 1066, duration: 13.941s, episode steps: 668, steps per second:  48, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.021459, mae: 2.905814, mean_q: 3.506570, mean_eps: 0.302954
  775490/2000000: episode: 1067, duration: 14.625s, episode steps: 659, steps per second:  45, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.149 [0.000, 5.000],  loss: 0.021498, mae: 2.897183, mean_q: 3.501221, mean_eps: 0.302356
  776722/2000000: episode: 1068, duration: 28.644s, episode steps: 1232, steps per second:  43, episode reward: 19.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.022409, mae: 2.887810, mean_q: 3.487380, mean_eps: 0.301505
  777599/2000000: episode: 1069, duration: 19.237s, episode steps: 877, steps per second:  46, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.079 [0.000, 5.000],  loss: 0.022445, mae: 2.883490, mean_q: 3.483085, mean_eps: 0.300556
  778642/2000000: episode: 1070, duration: 22.152s, episode steps: 1043, steps per second:  47, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.216 [0.000, 5.000],  loss: 0.020387, mae: 2.893807, mean_q: 3.496412, mean_eps: 0.299692
  779679/2000000: episode: 1071, duration: 23.020s, episode steps: 1037, steps per second:  45, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.022269, mae: 2.900036, mean_q: 3.502965, mean_eps: 0.298756
  780699/2000000: episode: 1072, duration: 24.186s, episode steps: 1020, steps per second:  42, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.910 [0.000, 5.000],  loss: 0.021581, mae: 2.926449, mean_q: 3.535583, mean_eps: 0.297831
  781818/2000000: episode: 1073, duration: 29.170s, episode steps: 1119, steps per second:  38, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.021282, mae: 2.919299, mean_q: 3.528350, mean_eps: 0.296868
  782200/2000000: episode: 1074, duration: 8.791s, episode steps: 382, steps per second:  43, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.024119, mae: 2.919711, mean_q: 3.529773, mean_eps: 0.296193
  783276/2000000: episode: 1075, duration: 24.362s, episode steps: 1076, steps per second:  44, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.828 [0.000, 5.000],  loss: 0.022000, mae: 2.905149, mean_q: 3.508817, mean_eps: 0.295538
  783973/2000000: episode: 1076, duration: 17.124s, episode steps: 697, steps per second:  41, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.063 [0.000, 5.000],  loss: 0.024230, mae: 2.927367, mean_q: 3.535505, mean_eps: 0.294738
  784634/2000000: episode: 1077, duration: 15.111s, episode steps: 661, steps per second:  44, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.022346, mae: 2.913943, mean_q: 3.522695, mean_eps: 0.294126
  785413/2000000: episode: 1078, duration: 16.925s, episode steps: 779, steps per second:  46, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.055 [0.000, 5.000],  loss: 0.020208, mae: 2.936036, mean_q: 3.546392, mean_eps: 0.293478
  786110/2000000: episode: 1079, duration: 15.642s, episode steps: 697, steps per second:  45, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.258 [0.000, 5.000],  loss: 0.022960, mae: 2.904037, mean_q: 3.508710, mean_eps: 0.292814
  787339/2000000: episode: 1080, duration: 28.976s, episode steps: 1229, steps per second:  42, episode reward: 19.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.194 [0.000, 5.000],  loss: 0.019743, mae: 2.907709, mean_q: 3.511636, mean_eps: 0.291948
  788288/2000000: episode: 1081, duration: 20.984s, episode steps: 949, steps per second:  45, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: 0.022115, mae: 2.903867, mean_q: 3.505387, mean_eps: 0.290969
  788900/2000000: episode: 1082, duration: 14.232s, episode steps: 612, steps per second:  43, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.894 [0.000, 5.000],  loss: 0.020288, mae: 2.896588, mean_q: 3.500253, mean_eps: 0.290267
  789506/2000000: episode: 1083, duration: 13.758s, episode steps: 606, steps per second:  44, episode reward: 15.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.021557, mae: 2.897389, mean_q: 3.499664, mean_eps: 0.289718
  790292/2000000: episode: 1084, duration: 16.917s, episode steps: 786, steps per second:  46, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.201 [0.000, 5.000],  loss: 0.028563, mae: 2.948608, mean_q: 3.561736, mean_eps: 0.289092
  791383/2000000: episode: 1085, duration: 23.608s, episode steps: 1091, steps per second:  46, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.317 [0.000, 5.000],  loss: 0.020038, mae: 2.958074, mean_q: 3.571184, mean_eps: 0.288248
  792019/2000000: episode: 1086, duration: 14.329s, episode steps: 636, steps per second:  44, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.931 [0.000, 5.000],  loss: 0.018719, mae: 2.943975, mean_q: 3.557012, mean_eps: 0.287470
  792871/2000000: episode: 1087, duration: 18.822s, episode steps: 852, steps per second:  45, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.141 [0.000, 5.000],  loss: 0.020297, mae: 2.934527, mean_q: 3.542466, mean_eps: 0.286800
  793999/2000000: episode: 1088, duration: 24.509s, episode steps: 1128, steps per second:  46, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.051 [0.000, 5.000],  loss: 0.025776, mae: 2.935764, mean_q: 3.544202, mean_eps: 0.285909
  794485/2000000: episode: 1089, duration: 10.985s, episode steps: 486, steps per second:  44, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.971 [0.000, 5.000],  loss: 0.024630, mae: 2.963741, mean_q: 3.578155, mean_eps: 0.285182
  795108/2000000: episode: 1090, duration: 13.778s, episode steps: 623, steps per second:  45, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.894 [0.000, 5.000],  loss: 0.019952, mae: 2.952345, mean_q: 3.563275, mean_eps: 0.284684
  795823/2000000: episode: 1091, duration: 16.255s, episode steps: 715, steps per second:  44, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.287 [0.000, 5.000],  loss: 0.023089, mae: 3.011360, mean_q: 3.636922, mean_eps: 0.284082
  796509/2000000: episode: 1092, duration: 16.238s, episode steps: 686, steps per second:  42, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.700 [0.000, 5.000],  loss: 0.021650, mae: 3.006353, mean_q: 3.629960, mean_eps: 0.283451
  797190/2000000: episode: 1093, duration: 16.954s, episode steps: 681, steps per second:  40, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.021679, mae: 2.983568, mean_q: 3.602361, mean_eps: 0.282835
  798023/2000000: episode: 1094, duration: 20.674s, episode steps: 833, steps per second:  40, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.978 [0.000, 5.000],  loss: 0.024030, mae: 3.020609, mean_q: 3.646562, mean_eps: 0.282155
  799140/2000000: episode: 1095, duration: 26.670s, episode steps: 1117, steps per second:  42, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.092 [0.000, 5.000],  loss: 0.021655, mae: 3.002010, mean_q: 3.626312, mean_eps: 0.281278
  799789/2000000: episode: 1096, duration: 15.431s, episode steps: 649, steps per second:  42, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.020983, mae: 3.004516, mean_q: 3.631383, mean_eps: 0.280482
  800164/2000000: episode: 1097, duration: 8.663s, episode steps: 375, steps per second:  43, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.656 [0.000, 5.000],  loss: 0.018760, mae: 3.025643, mean_q: 3.657016, mean_eps: 0.280022
  801072/2000000: episode: 1098, duration: 20.751s, episode steps: 908, steps per second:  44, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.024540, mae: 3.076445, mean_q: 3.714752, mean_eps: 0.279446
  802229/2000000: episode: 1099, duration: 27.991s, episode steps: 1157, steps per second:  41, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.048 [0.000, 5.000],  loss: 0.020397, mae: 3.046797, mean_q: 3.679801, mean_eps: 0.278515
  803109/2000000: episode: 1100, duration: 20.210s, episode steps: 880, steps per second:  44, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.149 [0.000, 5.000],  loss: 0.022190, mae: 3.072670, mean_q: 3.710776, mean_eps: 0.277597
  803632/2000000: episode: 1101, duration: 11.754s, episode steps: 523, steps per second:  44, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.704 [0.000, 5.000],  loss: 0.023444, mae: 3.076006, mean_q: 3.715687, mean_eps: 0.276967
  804493/2000000: episode: 1102, duration: 19.338s, episode steps: 861, steps per second:  45, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.031 [0.000, 5.000],  loss: 0.023348, mae: 3.073792, mean_q: 3.710865, mean_eps: 0.276344
  805303/2000000: episode: 1103, duration: 17.628s, episode steps: 810, steps per second:  46, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 0.022849, mae: 3.078850, mean_q: 3.717840, mean_eps: 0.275592
  805938/2000000: episode: 1104, duration: 13.399s, episode steps: 635, steps per second:  47, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.112 [0.000, 5.000],  loss: 0.021326, mae: 3.116093, mean_q: 3.763152, mean_eps: 0.274942
  806444/2000000: episode: 1105, duration: 10.784s, episode steps: 506, steps per second:  47, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.029107, mae: 3.098021, mean_q: 3.743363, mean_eps: 0.274429
  806984/2000000: episode: 1106, duration: 11.680s, episode steps: 540, steps per second:  46, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.020915, mae: 3.065718, mean_q: 3.704523, mean_eps: 0.273959
  807826/2000000: episode: 1107, duration: 18.507s, episode steps: 842, steps per second:  45, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.170 [0.000, 5.000],  loss: 0.023969, mae: 3.128987, mean_q: 3.779268, mean_eps: 0.273336
  808699/2000000: episode: 1108, duration: 18.953s, episode steps: 873, steps per second:  46, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.021772, mae: 3.099562, mean_q: 3.744338, mean_eps: 0.272564
  809732/2000000: episode: 1109, duration: 22.293s, episode steps: 1033, steps per second:  46, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.023042, mae: 3.108204, mean_q: 3.752789, mean_eps: 0.271707
  810754/2000000: episode: 1110, duration: 21.900s, episode steps: 1022, steps per second:  47, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.751 [0.000, 5.000],  loss: 0.021705, mae: 3.117954, mean_q: 3.767284, mean_eps: 0.270782
  811516/2000000: episode: 1111, duration: 17.669s, episode steps: 762, steps per second:  43, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.022743, mae: 3.088530, mean_q: 3.732398, mean_eps: 0.269979
  812248/2000000: episode: 1112, duration: 16.599s, episode steps: 732, steps per second:  44, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.732 [0.000, 5.000],  loss: 0.024335, mae: 3.075828, mean_q: 3.715256, mean_eps: 0.269308
  812869/2000000: episode: 1113, duration: 16.297s, episode steps: 621, steps per second:  38, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.821 [0.000, 5.000],  loss: 0.023109, mae: 3.084167, mean_q: 3.724759, mean_eps: 0.268698
  813344/2000000: episode: 1114, duration: 12.248s, episode steps: 475, steps per second:  39, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.724 [0.000, 5.000],  loss: 0.024237, mae: 3.087426, mean_q: 3.726740, mean_eps: 0.268205
  814310/2000000: episode: 1115, duration: 22.277s, episode steps: 966, steps per second:  43, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.251 [0.000, 5.000],  loss: 0.023048, mae: 3.092145, mean_q: 3.735835, mean_eps: 0.267557
  815729/2000000: episode: 1116, duration: 32.926s, episode steps: 1419, steps per second:  43, episode reward: 26.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.840 [0.000, 5.000],  loss: 0.023104, mae: 3.074877, mean_q: 3.714666, mean_eps: 0.266482
  816396/2000000: episode: 1117, duration: 15.058s, episode steps: 667, steps per second:  44, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.739 [0.000, 5.000],  loss: 0.021741, mae: 3.111522, mean_q: 3.761025, mean_eps: 0.265544
  817490/2000000: episode: 1118, duration: 23.825s, episode steps: 1094, steps per second:  46, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.770 [0.000, 5.000],  loss: 0.023963, mae: 3.089967, mean_q: 3.731596, mean_eps: 0.264752
  818475/2000000: episode: 1119, duration: 23.103s, episode steps: 985, steps per second:  43, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.024519, mae: 3.101626, mean_q: 3.751084, mean_eps: 0.263816
  819708/2000000: episode: 1120, duration: 28.072s, episode steps: 1233, steps per second:  44, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.286 [0.000, 5.000],  loss: 0.024421, mae: 3.071948, mean_q: 3.709216, mean_eps: 0.262819
  820896/2000000: episode: 1121, duration: 26.823s, episode steps: 1188, steps per second:  44, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.023388, mae: 3.135515, mean_q: 3.787335, mean_eps: 0.261730
  821713/2000000: episode: 1122, duration: 17.838s, episode steps: 817, steps per second:  46, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.246 [0.000, 5.000],  loss: 0.020939, mae: 3.157678, mean_q: 3.814309, mean_eps: 0.260826
  822200/2000000: episode: 1123, duration: 10.208s, episode steps: 487, steps per second:  48, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.862 [0.000, 5.000],  loss: 0.024248, mae: 3.121818, mean_q: 3.772807, mean_eps: 0.260240
  822964/2000000: episode: 1124, duration: 16.784s, episode steps: 764, steps per second:  46, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.024389, mae: 3.113907, mean_q: 3.761199, mean_eps: 0.259678
  824199/2000000: episode: 1125, duration: 27.768s, episode steps: 1235, steps per second:  44, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.712 [0.000, 5.000],  loss: 0.025546, mae: 3.137040, mean_q: 3.788433, mean_eps: 0.258778
  825061/2000000: episode: 1126, duration: 18.988s, episode steps: 862, steps per second:  45, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.022343, mae: 3.126869, mean_q: 3.776081, mean_eps: 0.257833
  825900/2000000: episode: 1127, duration: 18.784s, episode steps: 839, steps per second:  45, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.028657, mae: 3.112921, mean_q: 3.759958, mean_eps: 0.257068
  826690/2000000: episode: 1128, duration: 17.266s, episode steps: 790, steps per second:  46, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.134 [0.000, 5.000],  loss: 0.026260, mae: 3.167359, mean_q: 3.824365, mean_eps: 0.256335
  827906/2000000: episode: 1129, duration: 27.890s, episode steps: 1216, steps per second:  44, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.025491, mae: 3.128798, mean_q: 3.781341, mean_eps: 0.255432
  828937/2000000: episode: 1130, duration: 26.488s, episode steps: 1031, steps per second:  39, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.026562, mae: 3.106383, mean_q: 3.752111, mean_eps: 0.254420
  829752/2000000: episode: 1131, duration: 19.570s, episode steps: 815, steps per second:  42, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.030007, mae: 3.129908, mean_q: 3.778219, mean_eps: 0.253590
  830898/2000000: episode: 1132, duration: 25.977s, episode steps: 1146, steps per second:  44, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.021 [0.000, 5.000],  loss: 0.022290, mae: 3.117120, mean_q: 3.765765, mean_eps: 0.252708
  831275/2000000: episode: 1133, duration: 8.510s, episode steps: 377, steps per second:  44, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.313 [0.000, 5.000],  loss: 0.026086, mae: 3.082765, mean_q: 3.724471, mean_eps: 0.252023
  832283/2000000: episode: 1134, duration: 22.437s, episode steps: 1008, steps per second:  45, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.655 [0.000, 5.000],  loss: 0.024097, mae: 3.118791, mean_q: 3.766657, mean_eps: 0.251400
  832948/2000000: episode: 1135, duration: 14.183s, episode steps: 665, steps per second:  47, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.823 [0.000, 5.000],  loss: 0.024457, mae: 3.123249, mean_q: 3.770690, mean_eps: 0.250647
  833612/2000000: episode: 1136, duration: 14.433s, episode steps: 664, steps per second:  46, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.025118, mae: 3.116308, mean_q: 3.760435, mean_eps: 0.250050
  834232/2000000: episode: 1137, duration: 13.993s, episode steps: 620, steps per second:  44, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.021981, mae: 3.115765, mean_q: 3.760844, mean_eps: 0.249472
  835038/2000000: episode: 1138, duration: 18.240s, episode steps: 806, steps per second:  44, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.025738, mae: 3.121037, mean_q: 3.768958, mean_eps: 0.248829
  836079/2000000: episode: 1139, duration: 23.485s, episode steps: 1041, steps per second:  44, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.038 [0.000, 5.000],  loss: 0.022030, mae: 3.154843, mean_q: 3.811630, mean_eps: 0.247998
  836605/2000000: episode: 1140, duration: 11.851s, episode steps: 526, steps per second:  44, episode reward: 13.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.791 [0.000, 5.000],  loss: 0.022875, mae: 3.142482, mean_q: 3.794856, mean_eps: 0.247292
  837688/2000000: episode: 1141, duration: 23.411s, episode steps: 1083, steps per second:  46, episode reward: 12.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.158 [0.000, 5.000],  loss: 0.024378, mae: 3.167461, mean_q: 3.826123, mean_eps: 0.246569
  838347/2000000: episode: 1142, duration: 13.910s, episode steps: 659, steps per second:  47, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.024784, mae: 3.165135, mean_q: 3.820926, mean_eps: 0.245786
  838990/2000000: episode: 1143, duration: 14.051s, episode steps: 643, steps per second:  46, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.255 [0.000, 5.000],  loss: 0.023059, mae: 3.173645, mean_q: 3.834162, mean_eps: 0.245199
  839355/2000000: episode: 1144, duration: 7.775s, episode steps: 365, steps per second:  47, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 0.896 [0.000, 5.000],  loss: 0.027560, mae: 3.148563, mean_q: 3.799444, mean_eps: 0.244745
  839723/2000000: episode: 1145, duration: 8.565s, episode steps: 368, steps per second:  43, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.027716, mae: 3.147296, mean_q: 3.798355, mean_eps: 0.244416
  840502/2000000: episode: 1146, duration: 16.687s, episode steps: 779, steps per second:  47, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.023916, mae: 3.153088, mean_q: 3.805404, mean_eps: 0.243899
  841358/2000000: episode: 1147, duration: 17.814s, episode steps: 856, steps per second:  48, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.194 [0.000, 5.000],  loss: 0.023001, mae: 3.192830, mean_q: 3.856939, mean_eps: 0.243163
  842672/2000000: episode: 1148, duration: 27.416s, episode steps: 1314, steps per second:  48, episode reward: 27.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.023632, mae: 3.176200, mean_q: 3.834869, mean_eps: 0.242187
  843543/2000000: episode: 1149, duration: 18.795s, episode steps: 871, steps per second:  46, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.970 [0.000, 5.000],  loss: 0.024245, mae: 3.191685, mean_q: 3.850747, mean_eps: 0.241205
  844479/2000000: episode: 1150, duration: 21.683s, episode steps: 936, steps per second:  43, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.275 [0.000, 5.000],  loss: 0.024871, mae: 3.192296, mean_q: 3.853883, mean_eps: 0.240391
  845178/2000000: episode: 1151, duration: 16.646s, episode steps: 699, steps per second:  42, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.714 [0.000, 5.000],  loss: 0.022697, mae: 3.209603, mean_q: 3.875301, mean_eps: 0.239655
  845801/2000000: episode: 1152, duration: 14.504s, episode steps: 623, steps per second:  43, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.844 [0.000, 5.000],  loss: 0.022572, mae: 3.170116, mean_q: 3.828492, mean_eps: 0.239059
  846887/2000000: episode: 1153, duration: 24.268s, episode steps: 1086, steps per second:  45, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.018 [0.000, 5.000],  loss: 0.020662, mae: 3.186028, mean_q: 3.849331, mean_eps: 0.238290
  847509/2000000: episode: 1154, duration: 13.815s, episode steps: 622, steps per second:  45, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.206 [0.000, 5.000],  loss: 0.026498, mae: 3.149993, mean_q: 3.804900, mean_eps: 0.237522
  848562/2000000: episode: 1155, duration: 23.027s, episode steps: 1053, steps per second:  46, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.269 [0.000, 5.000],  loss: 0.023934, mae: 3.160049, mean_q: 3.816046, mean_eps: 0.236768
  849522/2000000: episode: 1156, duration: 20.806s, episode steps: 960, steps per second:  46, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.272 [0.000, 5.000],  loss: 0.026387, mae: 3.180713, mean_q: 3.839428, mean_eps: 0.235862
  850502/2000000: episode: 1157, duration: 22.179s, episode steps: 980, steps per second:  44, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.023806, mae: 3.155020, mean_q: 3.810230, mean_eps: 0.234989
  851366/2000000: episode: 1158, duration: 18.996s, episode steps: 864, steps per second:  45, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.027278, mae: 3.166543, mean_q: 3.820476, mean_eps: 0.234159
  852036/2000000: episode: 1159, duration: 14.861s, episode steps: 670, steps per second:  45, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.022575, mae: 3.133004, mean_q: 3.782769, mean_eps: 0.233470
  852800/2000000: episode: 1160, duration: 16.817s, episode steps: 764, steps per second:  45, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.025328, mae: 3.177017, mean_q: 3.834522, mean_eps: 0.232826
  854013/2000000: episode: 1161, duration: 25.868s, episode steps: 1213, steps per second:  47, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.025697, mae: 3.166712, mean_q: 3.824235, mean_eps: 0.231935
  854955/2000000: episode: 1162, duration: 19.651s, episode steps: 942, steps per second:  48, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.024009, mae: 3.149886, mean_q: 3.803072, mean_eps: 0.230964
  855858/2000000: episode: 1163, duration: 19.011s, episode steps: 903, steps per second:  47, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.249 [0.000, 5.000],  loss: 0.021599, mae: 3.195365, mean_q: 3.858890, mean_eps: 0.230135
  856485/2000000: episode: 1164, duration: 13.318s, episode steps: 627, steps per second:  47, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.002 [0.000, 5.000],  loss: 0.025681, mae: 3.178946, mean_q: 3.836952, mean_eps: 0.229445
  857399/2000000: episode: 1165, duration: 19.028s, episode steps: 914, steps per second:  48, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.021822, mae: 3.213690, mean_q: 3.881598, mean_eps: 0.228752
  858058/2000000: episode: 1166, duration: 14.043s, episode steps: 659, steps per second:  47, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.100 [0.000, 5.000],  loss: 0.022806, mae: 3.190024, mean_q: 3.849420, mean_eps: 0.228045
  859053/2000000: episode: 1167, duration: 21.155s, episode steps: 995, steps per second:  47, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.024463, mae: 3.187742, mean_q: 3.848085, mean_eps: 0.227300
  859808/2000000: episode: 1168, duration: 16.036s, episode steps: 755, steps per second:  47, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.023794, mae: 3.167418, mean_q: 3.822275, mean_eps: 0.226513
  860712/2000000: episode: 1169, duration: 19.876s, episode steps: 904, steps per second:  45, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.019 [0.000, 5.000],  loss: 0.023304, mae: 3.187787, mean_q: 3.848562, mean_eps: 0.225768
  861107/2000000: episode: 1170, duration: 9.850s, episode steps: 395, steps per second:  40, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.127 [0.000, 5.000],  loss: 0.028314, mae: 3.193123, mean_q: 3.854300, mean_eps: 0.225183
  861978/2000000: episode: 1171, duration: 20.553s, episode steps: 871, steps per second:  42, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.211 [0.000, 5.000],  loss: 0.026920, mae: 3.194176, mean_q: 3.855206, mean_eps: 0.224612
  862554/2000000: episode: 1172, duration: 14.790s, episode steps: 576, steps per second:  39, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.101 [0.000, 5.000],  loss: 0.023175, mae: 3.199966, mean_q: 3.870244, mean_eps: 0.223961
  863766/2000000: episode: 1173, duration: 28.656s, episode steps: 1212, steps per second:  42, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.267 [0.000, 5.000],  loss: 0.027116, mae: 3.200520, mean_q: 3.864683, mean_eps: 0.223156
  864907/2000000: episode: 1174, duration: 26.519s, episode steps: 1141, steps per second:  43, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.024971, mae: 3.180760, mean_q: 3.838117, mean_eps: 0.222098
  865962/2000000: episode: 1175, duration: 24.245s, episode steps: 1055, steps per second:  44, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.022428, mae: 3.192961, mean_q: 3.857196, mean_eps: 0.221109
  866940/2000000: episode: 1176, duration: 25.144s, episode steps: 978, steps per second:  39, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.202 [0.000, 5.000],  loss: 0.026204, mae: 3.170978, mean_q: 3.830484, mean_eps: 0.220195
  868046/2000000: episode: 1177, duration: 24.929s, episode steps: 1106, steps per second:  44, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.074 [0.000, 5.000],  loss: 0.023642, mae: 3.176831, mean_q: 3.835073, mean_eps: 0.219257
  868941/2000000: episode: 1178, duration: 19.481s, episode steps: 895, steps per second:  46, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.063 [0.000, 5.000],  loss: 0.023047, mae: 3.178206, mean_q: 3.835122, mean_eps: 0.218355
  869834/2000000: episode: 1179, duration: 19.656s, episode steps: 893, steps per second:  45, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.023928, mae: 3.190317, mean_q: 3.851310, mean_eps: 0.217551
  870900/2000000: episode: 1180, duration: 22.354s, episode steps: 1066, steps per second:  48, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.296 [0.000, 5.000],  loss: 0.024315, mae: 3.191028, mean_q: 3.852206, mean_eps: 0.216671
  872122/2000000: episode: 1181, duration: 26.448s, episode steps: 1222, steps per second:  46, episode reward: 30.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.023097, mae: 3.215270, mean_q: 3.881881, mean_eps: 0.215641
  873026/2000000: episode: 1182, duration: 19.955s, episode steps: 904, steps per second:  45, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.023973, mae: 3.212818, mean_q: 3.878060, mean_eps: 0.214683
  873773/2000000: episode: 1183, duration: 15.968s, episode steps: 747, steps per second:  47, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.202 [0.000, 5.000],  loss: 0.025862, mae: 3.207059, mean_q: 3.871714, mean_eps: 0.213940
  874722/2000000: episode: 1184, duration: 20.248s, episode steps: 949, steps per second:  47, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.027341, mae: 3.213533, mean_q: 3.876279, mean_eps: 0.213177
  875330/2000000: episode: 1185, duration: 12.990s, episode steps: 608, steps per second:  47, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.956 [0.000, 5.000],  loss: 0.022796, mae: 3.168609, mean_q: 3.826656, mean_eps: 0.212477
  876022/2000000: episode: 1186, duration: 15.344s, episode steps: 692, steps per second:  45, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.819 [0.000, 5.000],  loss: 0.022347, mae: 3.192871, mean_q: 3.856433, mean_eps: 0.211892
  877310/2000000: episode: 1187, duration: 30.594s, episode steps: 1288, steps per second:  42, episode reward: 22.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.955 [0.000, 5.000],  loss: 0.023710, mae: 3.182733, mean_q: 3.842468, mean_eps: 0.211001
  878175/2000000: episode: 1188, duration: 21.425s, episode steps: 865, steps per second:  40, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.239 [0.000, 5.000],  loss: 0.022740, mae: 3.185027, mean_q: 3.845560, mean_eps: 0.210032
  879376/2000000: episode: 1189, duration: 27.147s, episode steps: 1201, steps per second:  44, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.187 [0.000, 5.000],  loss: 0.024773, mae: 3.202218, mean_q: 3.866319, mean_eps: 0.209103
  879871/2000000: episode: 1190, duration: 11.055s, episode steps: 495, steps per second:  45, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.021544, mae: 3.197658, mean_q: 3.860574, mean_eps: 0.208340
  880804/2000000: episode: 1191, duration: 20.814s, episode steps: 933, steps per second:  45, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.071 [0.000, 5.000],  loss: 0.024514, mae: 3.223833, mean_q: 3.894131, mean_eps: 0.207698
  882357/2000000: episode: 1192, duration: 35.184s, episode steps: 1553, steps per second:  44, episode reward: 31.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.232 [0.000, 5.000],  loss: 0.022975, mae: 3.231547, mean_q: 3.903513, mean_eps: 0.206578
  882987/2000000: episode: 1193, duration: 14.993s, episode steps: 630, steps per second:  42, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.171 [0.000, 5.000],  loss: 0.024856, mae: 3.207569, mean_q: 3.874409, mean_eps: 0.205595
  884091/2000000: episode: 1194, duration: 25.108s, episode steps: 1104, steps per second:  44, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.025148, mae: 3.241225, mean_q: 3.913555, mean_eps: 0.204816
  884706/2000000: episode: 1195, duration: 13.779s, episode steps: 615, steps per second:  45, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.026701, mae: 3.228349, mean_q: 3.897413, mean_eps: 0.204042
  885837/2000000: episode: 1196, duration: 24.918s, episode steps: 1131, steps per second:  45, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.023820, mae: 3.250691, mean_q: 3.926338, mean_eps: 0.203255
  886500/2000000: episode: 1197, duration: 13.930s, episode steps: 663, steps per second:  48, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.026207, mae: 3.259605, mean_q: 3.936548, mean_eps: 0.202449
  887418/2000000: episode: 1198, duration: 19.953s, episode steps: 918, steps per second:  46, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.023597, mae: 3.269640, mean_q: 3.947202, mean_eps: 0.201738
  888880/2000000: episode: 1199, duration: 32.779s, episode steps: 1462, steps per second:  45, episode reward: 30.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.080 [0.000, 5.000],  loss: 0.023745, mae: 3.279510, mean_q: 3.957742, mean_eps: 0.200667
  889771/2000000: episode: 1200, duration: 18.710s, episode steps: 891, steps per second:  48, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.024695, mae: 3.296014, mean_q: 3.978636, mean_eps: 0.199608
  890663/2000000: episode: 1201, duration: 19.370s, episode steps: 892, steps per second:  46, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.656 [0.000, 5.000],  loss: 0.025567, mae: 3.252642, mean_q: 3.925517, mean_eps: 0.198806
  891492/2000000: episode: 1202, duration: 18.176s, episode steps: 829, steps per second:  46, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.031080, mae: 3.195046, mean_q: 3.853158, mean_eps: 0.198032
  892565/2000000: episode: 1203, duration: 23.616s, episode steps: 1073, steps per second:  45, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.995 [0.000, 5.000],  loss: 0.026157, mae: 3.208476, mean_q: 3.870659, mean_eps: 0.197175
  893601/2000000: episode: 1204, duration: 24.874s, episode steps: 1036, steps per second:  42, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.734 [0.000, 5.000],  loss: 0.029275, mae: 3.225688, mean_q: 3.895249, mean_eps: 0.196224
  893996/2000000: episode: 1205, duration: 9.594s, episode steps: 395, steps per second:  41, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.759 [0.000, 5.000],  loss: 0.023931, mae: 3.286315, mean_q: 3.962403, mean_eps: 0.195582
  894985/2000000: episode: 1206, duration: 21.393s, episode steps: 989, steps per second:  46, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.026688, mae: 3.229171, mean_q: 3.895333, mean_eps: 0.194959
  896031/2000000: episode: 1207, duration: 23.613s, episode steps: 1046, steps per second:  44, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.025999, mae: 3.251135, mean_q: 3.920632, mean_eps: 0.194043
  896930/2000000: episode: 1208, duration: 20.467s, episode steps: 899, steps per second:  44, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.023 [0.000, 5.000],  loss: 0.024917, mae: 3.238714, mean_q: 3.909486, mean_eps: 0.193168
  897812/2000000: episode: 1209, duration: 20.493s, episode steps: 882, steps per second:  43, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.083 [0.000, 5.000],  loss: 0.022313, mae: 3.251045, mean_q: 3.923340, mean_eps: 0.192367
  898561/2000000: episode: 1210, duration: 18.018s, episode steps: 749, steps per second:  42, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.662 [0.000, 5.000],  loss: 0.022042, mae: 3.247502, mean_q: 3.917446, mean_eps: 0.191633
  899632/2000000: episode: 1211, duration: 24.978s, episode steps: 1071, steps per second:  43, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.128 [0.000, 5.000],  loss: 0.026858, mae: 3.232583, mean_q: 3.897657, mean_eps: 0.190814
  900762/2000000: episode: 1212, duration: 25.186s, episode steps: 1130, steps per second:  45, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.119 [0.000, 5.000],  loss: 0.023153, mae: 3.259425, mean_q: 3.931802, mean_eps: 0.189824
  901814/2000000: episode: 1213, duration: 23.090s, episode steps: 1052, steps per second:  46, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.858 [0.000, 5.000],  loss: 0.027428, mae: 3.266059, mean_q: 3.940143, mean_eps: 0.188841
  902621/2000000: episode: 1214, duration: 16.917s, episode steps: 807, steps per second:  48, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.737 [0.000, 5.000],  loss: 0.024085, mae: 3.308763, mean_q: 3.988394, mean_eps: 0.188004
  903450/2000000: episode: 1215, duration: 17.971s, episode steps: 829, steps per second:  46, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.145 [0.000, 5.000],  loss: 0.023466, mae: 3.265437, mean_q: 3.940215, mean_eps: 0.187268
  904080/2000000: episode: 1216, duration: 14.538s, episode steps: 630, steps per second:  43, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.026858, mae: 3.294926, mean_q: 3.976967, mean_eps: 0.186612
  905141/2000000: episode: 1217, duration: 23.591s, episode steps: 1061, steps per second:  45, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.125 [0.000, 5.000],  loss: 0.024739, mae: 3.289728, mean_q: 3.967284, mean_eps: 0.185851
  905597/2000000: episode: 1218, duration: 9.848s, episode steps: 456, steps per second:  46, episode reward: 11.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.182 [0.000, 5.000],  loss: 0.023810, mae: 3.265188, mean_q: 3.936114, mean_eps: 0.185167
  906422/2000000: episode: 1219, duration: 18.120s, episode steps: 825, steps per second:  46, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.026692, mae: 3.298188, mean_q: 3.976528, mean_eps: 0.184591
  907111/2000000: episode: 1220, duration: 15.735s, episode steps: 689, steps per second:  44, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.080 [0.000, 5.000],  loss: 0.023564, mae: 3.247677, mean_q: 3.915601, mean_eps: 0.183911
  907775/2000000: episode: 1221, duration: 14.807s, episode steps: 664, steps per second:  45, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.022722, mae: 3.235275, mean_q: 3.902748, mean_eps: 0.183302
  908534/2000000: episode: 1222, duration: 16.636s, episode steps: 759, steps per second:  46, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.821 [0.000, 5.000],  loss: 0.022934, mae: 3.266418, mean_q: 3.943147, mean_eps: 0.182661
  909755/2000000: episode: 1223, duration: 29.364s, episode steps: 1221, steps per second:  42, episode reward: 25.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.024111, mae: 3.268144, mean_q: 3.943678, mean_eps: 0.181770
  910663/2000000: episode: 1224, duration: 20.479s, episode steps: 908, steps per second:  44, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.027408, mae: 3.243638, mean_q: 3.912414, mean_eps: 0.180813
  911460/2000000: episode: 1225, duration: 18.148s, episode steps: 797, steps per second:  44, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.025483, mae: 3.239421, mean_q: 3.907640, mean_eps: 0.180046
  912205/2000000: episode: 1226, duration: 17.490s, episode steps: 745, steps per second:  43, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.820 [0.000, 5.000],  loss: 0.024208, mae: 3.243737, mean_q: 3.914562, mean_eps: 0.179351
  913018/2000000: episode: 1227, duration: 18.224s, episode steps: 813, steps per second:  45, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.780 [0.000, 5.000],  loss: 0.024367, mae: 3.213435, mean_q: 3.877462, mean_eps: 0.178649
  913391/2000000: episode: 1228, duration: 8.197s, episode steps: 373, steps per second:  46, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.070 [0.000, 5.000],  loss: 0.028571, mae: 3.262596, mean_q: 3.938056, mean_eps: 0.178116
  914280/2000000: episode: 1229, duration: 21.242s, episode steps: 889, steps per second:  42, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.220 [0.000, 5.000],  loss: 0.023696, mae: 3.219489, mean_q: 3.883692, mean_eps: 0.177549
  915042/2000000: episode: 1230, duration: 18.805s, episode steps: 762, steps per second:  41, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.726 [0.000, 5.000],  loss: 0.025106, mae: 3.238957, mean_q: 3.906762, mean_eps: 0.176806
  915612/2000000: episode: 1231, duration: 12.938s, episode steps: 570, steps per second:  44, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.718 [0.000, 5.000],  loss: 0.029275, mae: 3.235389, mean_q: 3.904827, mean_eps: 0.176207
  916574/2000000: episode: 1232, duration: 22.124s, episode steps: 962, steps per second:  43, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.911 [0.000, 5.000],  loss: 0.024168, mae: 3.241760, mean_q: 3.914210, mean_eps: 0.175517
  917197/2000000: episode: 1233, duration: 14.011s, episode steps: 623, steps per second:  44, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.025071, mae: 3.244851, mean_q: 3.914922, mean_eps: 0.174803
  918326/2000000: episode: 1234, duration: 24.463s, episode steps: 1129, steps per second:  46, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.666 [0.000, 5.000],  loss: 0.025519, mae: 3.267583, mean_q: 3.943364, mean_eps: 0.174014
  919016/2000000: episode: 1235, duration: 14.541s, episode steps: 690, steps per second:  47, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.025311, mae: 3.254307, mean_q: 3.927315, mean_eps: 0.173197
  920032/2000000: episode: 1236, duration: 22.778s, episode steps: 1016, steps per second:  45, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.969 [0.000, 5.000],  loss: 0.024811, mae: 3.242796, mean_q: 3.912773, mean_eps: 0.172430
  920928/2000000: episode: 1237, duration: 20.200s, episode steps: 896, steps per second:  44, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.163 [0.000, 5.000],  loss: 0.021023, mae: 3.277473, mean_q: 3.956842, mean_eps: 0.171570
  921826/2000000: episode: 1238, duration: 19.783s, episode steps: 898, steps per second:  45, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.192 [0.000, 5.000],  loss: 0.025270, mae: 3.248531, mean_q: 3.919293, mean_eps: 0.170762
  922873/2000000: episode: 1239, duration: 24.055s, episode steps: 1047, steps per second:  44, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.022020, mae: 3.287920, mean_q: 3.969696, mean_eps: 0.169885
  923747/2000000: episode: 1240, duration: 19.438s, episode steps: 874, steps per second:  45, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.838 [0.000, 5.000],  loss: 0.021475, mae: 3.293478, mean_q: 3.972997, mean_eps: 0.169021
  924353/2000000: episode: 1241, duration: 13.721s, episode steps: 606, steps per second:  44, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.026747, mae: 3.286812, mean_q: 3.965902, mean_eps: 0.168355
  924864/2000000: episode: 1242, duration: 13.369s, episode steps: 511, steps per second:  38, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.026534, mae: 3.254280, mean_q: 3.925313, mean_eps: 0.167853
  925922/2000000: episode: 1243, duration: 25.718s, episode steps: 1058, steps per second:  41, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.935 [0.000, 5.000],  loss: 0.023439, mae: 3.253514, mean_q: 3.926649, mean_eps: 0.167147
  926335/2000000: episode: 1244, duration: 9.668s, episode steps: 413, steps per second:  43, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.025469, mae: 3.265312, mean_q: 3.938076, mean_eps: 0.166485
  927323/2000000: episode: 1245, duration: 22.445s, episode steps: 988, steps per second:  44, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.023084, mae: 3.254334, mean_q: 3.924856, mean_eps: 0.165855
  927871/2000000: episode: 1246, duration: 13.088s, episode steps: 548, steps per second:  42, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.648 [0.000, 5.000],  loss: 0.024768, mae: 3.259868, mean_q: 3.931537, mean_eps: 0.165164
  928345/2000000: episode: 1247, duration: 11.128s, episode steps: 474, steps per second:  43, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.023473, mae: 3.277097, mean_q: 3.952925, mean_eps: 0.164703
  928974/2000000: episode: 1248, duration: 13.572s, episode steps: 629, steps per second:  46, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.143 [0.000, 5.000],  loss: 0.025533, mae: 3.255970, mean_q: 3.930159, mean_eps: 0.164206
  929641/2000000: episode: 1249, duration: 14.178s, episode steps: 667, steps per second:  47, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.712 [0.000, 5.000],  loss: 0.027425, mae: 3.260380, mean_q: 3.931761, mean_eps: 0.163623
  930650/2000000: episode: 1250, duration: 23.304s, episode steps: 1009, steps per second:  43, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.960 [0.000, 5.000],  loss: 0.021534, mae: 3.273239, mean_q: 3.950493, mean_eps: 0.162869
  931156/2000000: episode: 1251, duration: 11.677s, episode steps: 506, steps per second:  43, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.927 [0.000, 5.000],  loss: 0.025776, mae: 3.294627, mean_q: 3.974655, mean_eps: 0.162188
  931788/2000000: episode: 1252, duration: 13.996s, episode steps: 632, steps per second:  45, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.703 [0.000, 5.000],  loss: 0.022435, mae: 3.266713, mean_q: 3.942979, mean_eps: 0.161677
  932515/2000000: episode: 1253, duration: 16.602s, episode steps: 727, steps per second:  44, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.686 [0.000, 5.000],  loss: 0.022683, mae: 3.283975, mean_q: 3.964706, mean_eps: 0.161065
  933693/2000000: episode: 1254, duration: 25.712s, episode steps: 1178, steps per second:  46, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.022507, mae: 3.278349, mean_q: 3.954460, mean_eps: 0.160206
  934615/2000000: episode: 1255, duration: 19.229s, episode steps: 922, steps per second:  48, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.066 [0.000, 5.000],  loss: 0.023132, mae: 3.290341, mean_q: 3.969531, mean_eps: 0.159261
  935638/2000000: episode: 1256, duration: 22.818s, episode steps: 1023, steps per second:  45, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.263 [0.000, 5.000],  loss: 0.025580, mae: 3.277645, mean_q: 3.955172, mean_eps: 0.158387
  936324/2000000: episode: 1257, duration: 15.794s, episode steps: 686, steps per second:  43, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.232 [0.000, 5.000],  loss: 0.022413, mae: 3.254506, mean_q: 3.928148, mean_eps: 0.157618
  937106/2000000: episode: 1258, duration: 17.886s, episode steps: 782, steps per second:  44, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.022363, mae: 3.274126, mean_q: 3.950057, mean_eps: 0.156957
  938061/2000000: episode: 1259, duration: 22.350s, episode steps: 955, steps per second:  43, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.026987, mae: 3.263676, mean_q: 3.938816, mean_eps: 0.156174
  938899/2000000: episode: 1260, duration: 18.082s, episode steps: 838, steps per second:  46, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.792 [0.000, 5.000],  loss: 0.023213, mae: 3.238843, mean_q: 3.906753, mean_eps: 0.155368
  939582/2000000: episode: 1261, duration: 16.148s, episode steps: 683, steps per second:  42, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.848 [0.000, 5.000],  loss: 0.025830, mae: 3.266435, mean_q: 3.943838, mean_eps: 0.154684
  940388/2000000: episode: 1262, duration: 17.984s, episode steps: 806, steps per second:  45, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.768 [0.000, 5.000],  loss: 0.023345, mae: 3.270876, mean_q: 3.949213, mean_eps: 0.154014
  941702/2000000: episode: 1263, duration: 33.722s, episode steps: 1314, steps per second:  39, episode reward: 29.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.148 [0.000, 5.000],  loss: 0.025353, mae: 3.278900, mean_q: 3.954364, mean_eps: 0.153060
  942630/2000000: episode: 1264, duration: 21.430s, episode steps: 928, steps per second:  43, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.137 [0.000, 5.000],  loss: 0.023537, mae: 3.268768, mean_q: 3.943018, mean_eps: 0.152051
  943658/2000000: episode: 1265, duration: 22.791s, episode steps: 1028, steps per second:  45, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.820 [0.000, 5.000],  loss: 0.024049, mae: 3.275261, mean_q: 3.955053, mean_eps: 0.151170
  944454/2000000: episode: 1266, duration: 17.920s, episode steps: 796, steps per second:  44, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.025722, mae: 3.256881, mean_q: 3.931790, mean_eps: 0.150350
  944937/2000000: episode: 1267, duration: 10.637s, episode steps: 483, steps per second:  45, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.025495, mae: 3.285488, mean_q: 3.962205, mean_eps: 0.149774
  945627/2000000: episode: 1268, duration: 15.282s, episode steps: 690, steps per second:  45, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.916 [0.000, 5.000],  loss: 0.025233, mae: 3.302807, mean_q: 3.987528, mean_eps: 0.149246
  946623/2000000: episode: 1269, duration: 23.274s, episode steps: 996, steps per second:  43, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.026358, mae: 3.301648, mean_q: 3.983602, mean_eps: 0.148488
  947320/2000000: episode: 1270, duration: 15.716s, episode steps: 697, steps per second:  44, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.159 [0.000, 5.000],  loss: 0.024561, mae: 3.319457, mean_q: 4.004527, mean_eps: 0.147727
  948686/2000000: episode: 1271, duration: 30.170s, episode steps: 1366, steps per second:  45, episode reward: 33.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.023991, mae: 3.317575, mean_q: 4.004314, mean_eps: 0.146798
  949189/2000000: episode: 1272, duration: 11.438s, episode steps: 503, steps per second:  44, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 0.021164, mae: 3.307058, mean_q: 3.989546, mean_eps: 0.145956
  949737/2000000: episode: 1273, duration: 11.545s, episode steps: 548, steps per second:  47, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.024 [0.000, 5.000],  loss: 0.024513, mae: 3.319682, mean_q: 4.005458, mean_eps: 0.145482
  950519/2000000: episode: 1274, duration: 16.588s, episode steps: 782, steps per second:  47, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.014 [0.000, 5.000],  loss: 0.029318, mae: 3.329229, mean_q: 4.015881, mean_eps: 0.144885
  951247/2000000: episode: 1275, duration: 15.642s, episode steps: 728, steps per second:  47, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.662 [0.000, 5.000],  loss: 0.022847, mae: 3.315251, mean_q: 4.000487, mean_eps: 0.144206
  951788/2000000: episode: 1276, duration: 11.577s, episode steps: 541, steps per second:  47, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.179 [0.000, 5.000],  loss: 0.026176, mae: 3.324095, mean_q: 4.012369, mean_eps: 0.143636
  952749/2000000: episode: 1277, duration: 20.783s, episode steps: 961, steps per second:  46, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.025384, mae: 3.320473, mean_q: 4.008623, mean_eps: 0.142959
  953492/2000000: episode: 1278, duration: 16.125s, episode steps: 743, steps per second:  46, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.296 [0.000, 5.000],  loss: 0.026827, mae: 3.340796, mean_q: 4.030289, mean_eps: 0.142192
  954560/2000000: episode: 1279, duration: 23.534s, episode steps: 1068, steps per second:  45, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.255 [0.000, 5.000],  loss: 0.027953, mae: 3.317585, mean_q: 4.002658, mean_eps: 0.141378
  955147/2000000: episode: 1280, duration: 13.957s, episode steps: 587, steps per second:  42, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.024976, mae: 3.324039, mean_q: 4.014987, mean_eps: 0.140633
  956236/2000000: episode: 1281, duration: 26.152s, episode steps: 1089, steps per second:  42, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.017 [0.000, 5.000],  loss: 0.024019, mae: 3.297296, mean_q: 3.977333, mean_eps: 0.139879
  956982/2000000: episode: 1282, duration: 19.664s, episode steps: 746, steps per second:  38, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.324 [0.000, 5.000],  loss: 0.025462, mae: 3.293656, mean_q: 3.974511, mean_eps: 0.139053
  957819/2000000: episode: 1283, duration: 21.156s, episode steps: 837, steps per second:  40, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.811 [0.000, 5.000],  loss: 0.025420, mae: 3.292851, mean_q: 3.977360, mean_eps: 0.138340
  958937/2000000: episode: 1284, duration: 26.153s, episode steps: 1118, steps per second:  43, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.025463, mae: 3.272145, mean_q: 3.951187, mean_eps: 0.137460
  959612/2000000: episode: 1285, duration: 15.681s, episode steps: 675, steps per second:  43, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.015 [0.000, 5.000],  loss: 0.026221, mae: 3.322229, mean_q: 4.012495, mean_eps: 0.136653
  960396/2000000: episode: 1286, duration: 17.313s, episode steps: 784, steps per second:  45, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.742 [0.000, 5.000],  loss: 0.022587, mae: 3.304395, mean_q: 3.989906, mean_eps: 0.135998
  960928/2000000: episode: 1287, duration: 12.004s, episode steps: 532, steps per second:  44, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.182 [0.000, 5.000],  loss: 0.020604, mae: 3.291199, mean_q: 3.972837, mean_eps: 0.135406
  961595/2000000: episode: 1288, duration: 15.165s, episode steps: 667, steps per second:  44, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.026262, mae: 3.292591, mean_q: 3.974548, mean_eps: 0.134866
  962985/2000000: episode: 1289, duration: 32.521s, episode steps: 1390, steps per second:  43, episode reward: 25.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.132 [0.000, 5.000],  loss: 0.024788, mae: 3.304785, mean_q: 3.989273, mean_eps: 0.133939
  963674/2000000: episode: 1290, duration: 15.282s, episode steps: 689, steps per second:  45, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.954 [0.000, 5.000],  loss: 0.023961, mae: 3.291897, mean_q: 3.974672, mean_eps: 0.133003
  964580/2000000: episode: 1291, duration: 20.407s, episode steps: 906, steps per second:  44, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.025 [0.000, 5.000],  loss: 0.022563, mae: 3.313331, mean_q: 4.000877, mean_eps: 0.132287
  965201/2000000: episode: 1292, duration: 13.787s, episode steps: 621, steps per second:  45, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.063 [0.000, 5.000],  loss: 0.022809, mae: 3.262196, mean_q: 3.936465, mean_eps: 0.131599
  966334/2000000: episode: 1293, duration: 23.803s, episode steps: 1133, steps per second:  48, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.132 [0.000, 5.000],  loss: 0.025286, mae: 3.280357, mean_q: 3.957824, mean_eps: 0.130809
  966855/2000000: episode: 1294, duration: 11.018s, episode steps: 521, steps per second:  47, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.027930, mae: 3.255260, mean_q: 3.928239, mean_eps: 0.130065
  967463/2000000: episode: 1295, duration: 13.333s, episode steps: 608, steps per second:  46, episode reward: 15.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.084 [0.000, 5.000],  loss: 0.026183, mae: 3.261583, mean_q: 3.936054, mean_eps: 0.129558
  968243/2000000: episode: 1296, duration: 16.984s, episode steps: 780, steps per second:  46, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.847 [0.000, 5.000],  loss: 0.021218, mae: 3.285967, mean_q: 3.968181, mean_eps: 0.128933
  968883/2000000: episode: 1297, duration: 13.919s, episode steps: 640, steps per second:  46, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.024445, mae: 3.267684, mean_q: 3.944874, mean_eps: 0.128294
  969489/2000000: episode: 1298, duration: 13.361s, episode steps: 606, steps per second:  45, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.790 [0.000, 5.000],  loss: 0.027773, mae: 3.283643, mean_q: 3.960374, mean_eps: 0.127733
  970575/2000000: episode: 1299, duration: 23.577s, episode steps: 1086, steps per second:  46, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.996 [0.000, 5.000],  loss: 0.026859, mae: 3.262442, mean_q: 3.935677, mean_eps: 0.126971
  971258/2000000: episode: 1300, duration: 15.190s, episode steps: 683, steps per second:  45, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.025229, mae: 3.223098, mean_q: 3.892094, mean_eps: 0.126176
  971925/2000000: episode: 1301, duration: 14.519s, episode steps: 667, steps per second:  46, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.880 [0.000, 5.000],  loss: 0.022390, mae: 3.267993, mean_q: 3.948462, mean_eps: 0.125567
  972301/2000000: episode: 1302, duration: 8.966s, episode steps: 376, steps per second:  42, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.992 [0.000, 5.000],  loss: 0.021191, mae: 3.221200, mean_q: 3.888735, mean_eps: 0.125097
  972881/2000000: episode: 1303, duration: 13.325s, episode steps: 580, steps per second:  44, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.769 [0.000, 5.000],  loss: 0.023263, mae: 3.251086, mean_q: 3.924059, mean_eps: 0.124667
  974106/2000000: episode: 1304, duration: 28.532s, episode steps: 1225, steps per second:  43, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.004 [0.000, 5.000],  loss: 0.020511, mae: 3.233164, mean_q: 3.903653, mean_eps: 0.123855
  974710/2000000: episode: 1305, duration: 15.284s, episode steps: 604, steps per second:  40, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.019315, mae: 3.240779, mean_q: 3.913479, mean_eps: 0.123033
  975221/2000000: episode: 1306, duration: 12.499s, episode steps: 511, steps per second:  41, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.022604, mae: 3.225830, mean_q: 3.897161, mean_eps: 0.122531
  975845/2000000: episode: 1307, duration: 14.321s, episode steps: 624, steps per second:  44, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.284 [0.000, 5.000],  loss: 0.025692, mae: 3.237443, mean_q: 3.910936, mean_eps: 0.122019
  976324/2000000: episode: 1308, duration: 10.349s, episode steps: 479, steps per second:  46, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.246 [0.000, 5.000],  loss: 0.022503, mae: 3.190737, mean_q: 3.852824, mean_eps: 0.121524
  977150/2000000: episode: 1309, duration: 18.031s, episode steps: 826, steps per second:  46, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.024033, mae: 3.213889, mean_q: 3.880615, mean_eps: 0.120938
  977841/2000000: episode: 1310, duration: 15.618s, episode steps: 691, steps per second:  44, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.952 [0.000, 5.000],  loss: 0.023309, mae: 3.205918, mean_q: 3.872064, mean_eps: 0.120254
  978202/2000000: episode: 1311, duration: 8.237s, episode steps: 361, steps per second:  44, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.820 [0.000, 5.000],  loss: 0.022067, mae: 3.215631, mean_q: 3.884266, mean_eps: 0.119780
  979250/2000000: episode: 1312, duration: 24.092s, episode steps: 1048, steps per second:  44, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.560 [0.000, 5.000],  loss: 0.026642, mae: 3.251394, mean_q: 3.923420, mean_eps: 0.119147
  980086/2000000: episode: 1313, duration: 18.064s, episode steps: 836, steps per second:  46, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.020 [0.000, 5.000],  loss: 0.027158, mae: 3.234200, mean_q: 3.903041, mean_eps: 0.118299
  981118/2000000: episode: 1314, duration: 23.025s, episode steps: 1032, steps per second:  45, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.114 [0.000, 5.000],  loss: 0.021430, mae: 3.268306, mean_q: 3.944846, mean_eps: 0.117458
  981597/2000000: episode: 1315, duration: 10.398s, episode steps: 479, steps per second:  46, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.026281, mae: 3.307845, mean_q: 3.990634, mean_eps: 0.116778
  982391/2000000: episode: 1316, duration: 16.496s, episode steps: 794, steps per second:  48, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.220 [0.000, 5.000],  loss: 0.023999, mae: 3.262075, mean_q: 3.938602, mean_eps: 0.116205
  983326/2000000: episode: 1317, duration: 21.128s, episode steps: 935, steps per second:  44, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.809 [0.000, 5.000],  loss: 0.024895, mae: 3.278141, mean_q: 3.956585, mean_eps: 0.115428
  983743/2000000: episode: 1318, duration: 9.538s, episode steps: 417, steps per second:  44, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.192 [0.000, 5.000],  loss: 0.024255, mae: 3.280922, mean_q: 3.957545, mean_eps: 0.114819
  984322/2000000: episode: 1319, duration: 12.542s, episode steps: 579, steps per second:  46, episode reward: 15.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.024184, mae: 3.277359, mean_q: 3.952571, mean_eps: 0.114371
  985463/2000000: episode: 1320, duration: 24.221s, episode steps: 1141, steps per second:  47, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.048 [0.000, 5.000],  loss: 0.027528, mae: 3.258951, mean_q: 3.934804, mean_eps: 0.113597
  985841/2000000: episode: 1321, duration: 8.612s, episode steps: 378, steps per second:  44, episode reward:  9.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.275 [0.000, 5.000],  loss: 0.025518, mae: 3.293464, mean_q: 3.976804, mean_eps: 0.112913
  986354/2000000: episode: 1322, duration: 11.386s, episode steps: 513, steps per second:  45, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.713 [0.000, 5.000],  loss: 0.026102, mae: 3.275554, mean_q: 3.952994, mean_eps: 0.112512
  987142/2000000: episode: 1323, duration: 18.514s, episode steps: 788, steps per second:  43, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.712 [0.000, 5.000],  loss: 0.021871, mae: 3.263447, mean_q: 3.940080, mean_eps: 0.111927
  988152/2000000: episode: 1324, duration: 24.501s, episode steps: 1010, steps per second:  41, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.028961, mae: 3.262263, mean_q: 3.937378, mean_eps: 0.111119
  989215/2000000: episode: 1325, duration: 25.636s, episode steps: 1063, steps per second:  41, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.855 [0.000, 5.000],  loss: 0.024967, mae: 3.264774, mean_q: 3.938739, mean_eps: 0.110186
  989742/2000000: episode: 1326, duration: 12.200s, episode steps: 527, steps per second:  43, episode reward: 14.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.484 [0.000, 5.000],  loss: 0.028354, mae: 3.294043, mean_q: 3.975486, mean_eps: 0.109470
  990369/2000000: episode: 1327, duration: 14.349s, episode steps: 627, steps per second:  44, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.026880, mae: 3.284476, mean_q: 3.965518, mean_eps: 0.108950
  991029/2000000: episode: 1328, duration: 14.101s, episode steps: 660, steps per second:  47, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.025782, mae: 3.298145, mean_q: 3.982062, mean_eps: 0.108370
  992140/2000000: episode: 1329, duration: 24.071s, episode steps: 1111, steps per second:  46, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.025166, mae: 3.290403, mean_q: 3.973670, mean_eps: 0.107574
  992684/2000000: episode: 1330, duration: 11.635s, episode steps: 544, steps per second:  47, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.027999, mae: 3.289920, mean_q: 3.974575, mean_eps: 0.106831
  993986/2000000: episode: 1331, duration: 29.249s, episode steps: 1302, steps per second:  45, episode reward: 28.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.247 [0.000, 5.000],  loss: 0.024316, mae: 3.306964, mean_q: 3.994529, mean_eps: 0.105999
  995026/2000000: episode: 1332, duration: 24.469s, episode steps: 1040, steps per second:  43, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.234 [0.000, 5.000],  loss: 0.024784, mae: 3.276681, mean_q: 3.957915, mean_eps: 0.104945
  995972/2000000: episode: 1333, duration: 21.276s, episode steps: 946, steps per second:  44, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.956 [0.000, 5.000],  loss: 0.024765, mae: 3.339589, mean_q: 4.034077, mean_eps: 0.104052
  997095/2000000: episode: 1334, duration: 28.102s, episode steps: 1123, steps per second:  40, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.552 [0.000, 5.000],  loss: 0.029620, mae: 3.340821, mean_q: 4.034533, mean_eps: 0.103121
  997796/2000000: episode: 1335, duration: 15.487s, episode steps: 701, steps per second:  45, episode reward: 20.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.024406, mae: 3.346229, mean_q: 4.036706, mean_eps: 0.102300
  998571/2000000: episode: 1336, duration: 17.040s, episode steps: 775, steps per second:  45, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.030084, mae: 3.341280, mean_q: 4.030128, mean_eps: 0.101636
  999636/2000000: episode: 1337, duration: 25.487s, episode steps: 1065, steps per second:  42, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.908 [0.000, 5.000],  loss: 0.028000, mae: 3.330675, mean_q: 4.020138, mean_eps: 0.100808
 1000459/2000000: episode: 1338, duration: 18.328s, episode steps: 823, steps per second:  45, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.026080, mae: 3.365651, mean_q: 4.065242, mean_eps: 0.100073
 1001505/2000000: episode: 1339, duration: 23.905s, episode steps: 1046, steps per second:  44, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.027038, mae: 3.392681, mean_q: 4.093836, mean_eps: 0.100000
 1002025/2000000: episode: 1340, duration: 11.974s, episode steps: 520, steps per second:  43, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.110 [0.000, 5.000],  loss: 0.030930, mae: 3.369541, mean_q: 4.066962, mean_eps: 0.100000
 1002605/2000000: episode: 1341, duration: 13.109s, episode steps: 580, steps per second:  44, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.016 [0.000, 5.000],  loss: 0.030737, mae: 3.367357, mean_q: 4.060766, mean_eps: 0.100000
 1003658/2000000: episode: 1342, duration: 24.825s, episode steps: 1053, steps per second:  42, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.682 [0.000, 5.000],  loss: 0.025200, mae: 3.382068, mean_q: 4.080231, mean_eps: 0.100000
 1004572/2000000: episode: 1343, duration: 22.976s, episode steps: 914, steps per second:  40, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.024174, mae: 3.364044, mean_q: 4.058695, mean_eps: 0.100000
 1005348/2000000: episode: 1344, duration: 18.715s, episode steps: 776, steps per second:  41, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.031113, mae: 3.390352, mean_q: 4.089168, mean_eps: 0.100000
 1006379/2000000: episode: 1345, duration: 24.241s, episode steps: 1031, steps per second:  43, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.621 [0.000, 5.000],  loss: 0.024592, mae: 3.408856, mean_q: 4.111559, mean_eps: 0.100000
 1007260/2000000: episode: 1346, duration: 21.080s, episode steps: 881, steps per second:  42, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.182 [0.000, 5.000],  loss: 0.025674, mae: 3.415201, mean_q: 4.125740, mean_eps: 0.100000
 1007980/2000000: episode: 1347, duration: 17.089s, episode steps: 720, steps per second:  42, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.128 [0.000, 5.000],  loss: 0.025790, mae: 3.386071, mean_q: 4.086046, mean_eps: 0.100000
 1008811/2000000: episode: 1348, duration: 19.293s, episode steps: 831, steps per second:  43, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.030475, mae: 3.399748, mean_q: 4.103314, mean_eps: 0.100000
 1010037/2000000: episode: 1349, duration: 31.826s, episode steps: 1226, steps per second:  39, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.031 [0.000, 5.000],  loss: 0.024763, mae: 3.396287, mean_q: 4.101371, mean_eps: 0.100000
 1010393/2000000: episode: 1350, duration: 8.973s, episode steps: 356, steps per second:  40, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.461 [0.000, 5.000],  loss: 0.022906, mae: 3.429939, mean_q: 4.138963, mean_eps: 0.100000
 1011306/2000000: episode: 1351, duration: 21.681s, episode steps: 913, steps per second:  42, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.676 [0.000, 5.000],  loss: 0.024232, mae: 3.411701, mean_q: 4.119034, mean_eps: 0.100000
 1012218/2000000: episode: 1352, duration: 22.062s, episode steps: 912, steps per second:  41, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.159 [0.000, 5.000],  loss: 0.028527, mae: 3.387073, mean_q: 4.089163, mean_eps: 0.100000
 1013033/2000000: episode: 1353, duration: 18.864s, episode steps: 815, steps per second:  43, episode reward: 24.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: 0.021892, mae: 3.393732, mean_q: 4.096475, mean_eps: 0.100000
 1013568/2000000: episode: 1354, duration: 12.100s, episode steps: 535, steps per second:  44, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.028260, mae: 3.405402, mean_q: 4.110603, mean_eps: 0.100000
 1014215/2000000: episode: 1355, duration: 15.911s, episode steps: 647, steps per second:  41, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.030094, mae: 3.390416, mean_q: 4.092449, mean_eps: 0.100000
 1014733/2000000: episode: 1356, duration: 12.710s, episode steps: 518, steps per second:  41, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.029929, mae: 3.396841, mean_q: 4.100748, mean_eps: 0.100000
 1015431/2000000: episode: 1357, duration: 16.025s, episode steps: 698, steps per second:  44, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.030404, mae: 3.410609, mean_q: 4.118233, mean_eps: 0.100000
 1016063/2000000: episode: 1358, duration: 14.223s, episode steps: 632, steps per second:  44, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.049 [0.000, 5.000],  loss: 0.028113, mae: 3.417312, mean_q: 4.127330, mean_eps: 0.100000
 1017008/2000000: episode: 1359, duration: 20.716s, episode steps: 945, steps per second:  46, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.025186, mae: 3.418824, mean_q: 4.127171, mean_eps: 0.100000
 1017796/2000000: episode: 1360, duration: 16.849s, episode steps: 788, steps per second:  47, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.028650, mae: 3.405757, mean_q: 4.108708, mean_eps: 0.100000
 1019251/2000000: episode: 1361, duration: 32.061s, episode steps: 1455, steps per second:  45, episode reward: 27.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.324 [0.000, 5.000],  loss: 0.026516, mae: 3.397386, mean_q: 4.100182, mean_eps: 0.100000
 1019935/2000000: episode: 1362, duration: 16.118s, episode steps: 684, steps per second:  42, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.027717, mae: 3.421893, mean_q: 4.131493, mean_eps: 0.100000
 1020853/2000000: episode: 1363, duration: 21.266s, episode steps: 918, steps per second:  43, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.024004, mae: 3.408449, mean_q: 4.114660, mean_eps: 0.100000
 1021489/2000000: episode: 1364, duration: 14.018s, episode steps: 636, steps per second:  45, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.753 [0.000, 5.000],  loss: 0.029194, mae: 3.394030, mean_q: 4.101266, mean_eps: 0.100000
 1022182/2000000: episode: 1365, duration: 15.842s, episode steps: 693, steps per second:  44, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.028063, mae: 3.384016, mean_q: 4.083832, mean_eps: 0.100000
 1022878/2000000: episode: 1366, duration: 16.175s, episode steps: 696, steps per second:  43, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.024999, mae: 3.414803, mean_q: 4.121328, mean_eps: 0.100000
 1023603/2000000: episode: 1367, duration: 15.914s, episode steps: 725, steps per second:  46, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.081 [0.000, 5.000],  loss: 0.031900, mae: 3.423268, mean_q: 4.129464, mean_eps: 0.100000
 1024161/2000000: episode: 1368, duration: 11.828s, episode steps: 558, steps per second:  47, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.027143, mae: 3.421611, mean_q: 4.134158, mean_eps: 0.100000
 1024837/2000000: episode: 1369, duration: 15.437s, episode steps: 676, steps per second:  44, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.026708, mae: 3.411184, mean_q: 4.119109, mean_eps: 0.100000
 1025712/2000000: episode: 1370, duration: 19.924s, episode steps: 875, steps per second:  44, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.103 [0.000, 5.000],  loss: 0.026221, mae: 3.406561, mean_q: 4.112732, mean_eps: 0.100000
 1026618/2000000: episode: 1371, duration: 20.247s, episode steps: 906, steps per second:  45, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.170 [0.000, 5.000],  loss: 0.025094, mae: 3.410211, mean_q: 4.114859, mean_eps: 0.100000
 1027195/2000000: episode: 1372, duration: 12.593s, episode steps: 577, steps per second:  46, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.025318, mae: 3.468286, mean_q: 4.184022, mean_eps: 0.100000
 1028392/2000000: episode: 1373, duration: 26.500s, episode steps: 1197, steps per second:  45, episode reward: 14.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.027385, mae: 3.429574, mean_q: 4.137440, mean_eps: 0.100000
 1029685/2000000: episode: 1374, duration: 26.616s, episode steps: 1293, steps per second:  49, episode reward: 29.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.030739, mae: 3.429555, mean_q: 4.135509, mean_eps: 0.100000
 1030062/2000000: episode: 1375, duration: 8.250s, episode steps: 377, steps per second:  46, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.780 [0.000, 5.000],  loss: 0.024733, mae: 3.450120, mean_q: 4.162565, mean_eps: 0.100000
 1030898/2000000: episode: 1376, duration: 17.873s, episode steps: 836, steps per second:  47, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.006 [0.000, 5.000],  loss: 0.021195, mae: 3.463971, mean_q: 4.181198, mean_eps: 0.100000
 1031601/2000000: episode: 1377, duration: 14.991s, episode steps: 703, steps per second:  47, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.021662, mae: 3.429868, mean_q: 4.138231, mean_eps: 0.100000
 1032150/2000000: episode: 1378, duration: 11.417s, episode steps: 549, steps per second:  48, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.209 [0.000, 5.000],  loss: 0.025604, mae: 3.454003, mean_q: 4.165914, mean_eps: 0.100000
 1032757/2000000: episode: 1379, duration: 12.937s, episode steps: 607, steps per second:  47, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.802 [0.000, 5.000],  loss: 0.027432, mae: 3.438522, mean_q: 4.145900, mean_eps: 0.100000
 1033409/2000000: episode: 1380, duration: 13.725s, episode steps: 652, steps per second:  48, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.184 [0.000, 5.000],  loss: 0.027673, mae: 3.458354, mean_q: 4.174510, mean_eps: 0.100000
 1033957/2000000: episode: 1381, duration: 11.538s, episode steps: 548, steps per second:  47, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.193 [0.000, 5.000],  loss: 0.030801, mae: 3.441010, mean_q: 4.150571, mean_eps: 0.100000
 1034750/2000000: episode: 1382, duration: 16.752s, episode steps: 793, steps per second:  47, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.261 [0.000, 5.000],  loss: 0.023852, mae: 3.441174, mean_q: 4.150480, mean_eps: 0.100000
 1035383/2000000: episode: 1383, duration: 14.762s, episode steps: 633, steps per second:  43, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.227 [0.000, 5.000],  loss: 0.027458, mae: 3.430683, mean_q: 4.138798, mean_eps: 0.100000
 1036423/2000000: episode: 1384, duration: 25.242s, episode steps: 1040, steps per second:  41, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.836 [0.000, 5.000],  loss: 0.025599, mae: 3.438419, mean_q: 4.146586, mean_eps: 0.100000
 1037174/2000000: episode: 1385, duration: 17.957s, episode steps: 751, steps per second:  42, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.748 [0.000, 5.000],  loss: 0.024353, mae: 3.444750, mean_q: 4.155677, mean_eps: 0.100000
 1037895/2000000: episode: 1386, duration: 16.323s, episode steps: 721, steps per second:  44, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.743 [0.000, 5.000],  loss: 0.026951, mae: 3.445670, mean_q: 4.156659, mean_eps: 0.100000
 1038627/2000000: episode: 1387, duration: 16.218s, episode steps: 732, steps per second:  45, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.041 [0.000, 5.000],  loss: 0.026776, mae: 3.419389, mean_q: 4.126589, mean_eps: 0.100000
 1039995/2000000: episode: 1388, duration: 30.301s, episode steps: 1368, steps per second:  45, episode reward: 30.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.023362, mae: 3.441548, mean_q: 4.153754, mean_eps: 0.100000
 1041221/2000000: episode: 1389, duration: 28.651s, episode steps: 1226, steps per second:  43, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.024209, mae: 3.414304, mean_q: 4.121346, mean_eps: 0.100000
 1042222/2000000: episode: 1390, duration: 23.427s, episode steps: 1001, steps per second:  43, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.906 [0.000, 5.000],  loss: 0.025082, mae: 3.418673, mean_q: 4.124385, mean_eps: 0.100000
 1042767/2000000: episode: 1391, duration: 13.395s, episode steps: 545, steps per second:  41, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.954 [0.000, 5.000],  loss: 0.025388, mae: 3.429465, mean_q: 4.138720, mean_eps: 0.100000
 1043736/2000000: episode: 1392, duration: 33.608s, episode steps: 969, steps per second:  29, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.988 [0.000, 5.000],  loss: 0.026742, mae: 3.441936, mean_q: 4.153256, mean_eps: 0.100000
 1044572/2000000: episode: 1393, duration: 18.982s, episode steps: 836, steps per second:  44, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.072 [0.000, 5.000],  loss: 0.023548, mae: 3.438091, mean_q: 4.147416, mean_eps: 0.100000
 1045265/2000000: episode: 1394, duration: 15.181s, episode steps: 693, steps per second:  46, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.841 [0.000, 5.000],  loss: 0.024451, mae: 3.432041, mean_q: 4.139804, mean_eps: 0.100000
 1046425/2000000: episode: 1395, duration: 26.505s, episode steps: 1160, steps per second:  44, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.137 [0.000, 5.000],  loss: 0.026675, mae: 3.404403, mean_q: 4.106343, mean_eps: 0.100000
 1047433/2000000: episode: 1396, duration: 22.839s, episode steps: 1008, steps per second:  44, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.010 [0.000, 5.000],  loss: 0.025919, mae: 3.401056, mean_q: 4.103555, mean_eps: 0.100000
 1048463/2000000: episode: 1397, duration: 23.260s, episode steps: 1030, steps per second:  44, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.023450, mae: 3.396646, mean_q: 4.097247, mean_eps: 0.100000
 1049282/2000000: episode: 1398, duration: 19.148s, episode steps: 819, steps per second:  43, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.974 [0.000, 5.000],  loss: 0.024590, mae: 3.412577, mean_q: 4.115486, mean_eps: 0.100000
 1049612/2000000: episode: 1399, duration: 7.247s, episode steps: 330, steps per second:  46, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.794 [0.000, 5.000],  loss: 0.027751, mae: 3.424234, mean_q: 4.126204, mean_eps: 0.100000
 1050398/2000000: episode: 1400, duration: 17.737s, episode steps: 786, steps per second:  44, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.324 [0.000, 5.000],  loss: 0.025615, mae: 3.414572, mean_q: 4.116985, mean_eps: 0.100000
 1051395/2000000: episode: 1401, duration: 23.961s, episode steps: 997, steps per second:  42, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.312 [0.000, 5.000],  loss: 0.025286, mae: 3.392113, mean_q: 4.090901, mean_eps: 0.100000
 1052234/2000000: episode: 1402, duration: 19.306s, episode steps: 839, steps per second:  43, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.025092, mae: 3.428950, mean_q: 4.135328, mean_eps: 0.100000
 1053109/2000000: episode: 1403, duration: 19.784s, episode steps: 875, steps per second:  44, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.025811, mae: 3.426934, mean_q: 4.134353, mean_eps: 0.100000
 1054208/2000000: episode: 1404, duration: 25.813s, episode steps: 1099, steps per second:  43, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.705 [0.000, 5.000],  loss: 0.027607, mae: 3.400998, mean_q: 4.101903, mean_eps: 0.100000
 1054898/2000000: episode: 1405, duration: 15.392s, episode steps: 690, steps per second:  45, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.026506, mae: 3.413175, mean_q: 4.120253, mean_eps: 0.100000
 1055459/2000000: episode: 1406, duration: 12.562s, episode steps: 561, steps per second:  45, episode reward: 17.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.900 [0.000, 5.000],  loss: 0.021799, mae: 3.419164, mean_q: 4.125125, mean_eps: 0.100000
 1057078/2000000: episode: 1407, duration: 37.406s, episode steps: 1619, steps per second:  43, episode reward: 32.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.025520, mae: 3.430195, mean_q: 4.138218, mean_eps: 0.100000
 1057870/2000000: episode: 1408, duration: 18.151s, episode steps: 792, steps per second:  44, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.133 [0.000, 5.000],  loss: 0.023562, mae: 3.440025, mean_q: 4.150779, mean_eps: 0.100000
 1058884/2000000: episode: 1409, duration: 24.729s, episode steps: 1014, steps per second:  41, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.024533, mae: 3.403795, mean_q: 4.106369, mean_eps: 0.100000
 1059358/2000000: episode: 1410, duration: 11.896s, episode steps: 474, steps per second:  40, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.627 [0.000, 5.000],  loss: 0.025570, mae: 3.407217, mean_q: 4.108109, mean_eps: 0.100000
 1059867/2000000: episode: 1411, duration: 11.961s, episode steps: 509, steps per second:  43, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.982 [0.000, 5.000],  loss: 0.022442, mae: 3.413969, mean_q: 4.116379, mean_eps: 0.100000
 1060765/2000000: episode: 1412, duration: 20.097s, episode steps: 898, steps per second:  45, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.236 [0.000, 5.000],  loss: 0.022050, mae: 3.420365, mean_q: 4.126856, mean_eps: 0.100000
 1061795/2000000: episode: 1413, duration: 23.801s, episode steps: 1030, steps per second:  43, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.138 [0.000, 5.000],  loss: 0.025854, mae: 3.425743, mean_q: 4.131585, mean_eps: 0.100000
 1062795/2000000: episode: 1414, duration: 22.730s, episode steps: 1000, steps per second:  44, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.672 [0.000, 5.000],  loss: 0.025897, mae: 3.431285, mean_q: 4.137092, mean_eps: 0.100000
 1063804/2000000: episode: 1415, duration: 23.243s, episode steps: 1009, steps per second:  43, episode reward: 31.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.949 [0.000, 5.000],  loss: 0.026417, mae: 3.441905, mean_q: 4.149756, mean_eps: 0.100000
 1064291/2000000: episode: 1416, duration: 11.259s, episode steps: 487, steps per second:  43, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.031790, mae: 3.423201, mean_q: 4.129319, mean_eps: 0.100000
 1065083/2000000: episode: 1417, duration: 18.163s, episode steps: 792, steps per second:  44, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.351 [0.000, 5.000],  loss: 0.029078, mae: 3.444935, mean_q: 4.152375, mean_eps: 0.100000
 1065968/2000000: episode: 1418, duration: 20.587s, episode steps: 885, steps per second:  43, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.049 [0.000, 5.000],  loss: 0.025641, mae: 3.415248, mean_q: 4.119981, mean_eps: 0.100000
 1067132/2000000: episode: 1419, duration: 28.203s, episode steps: 1164, steps per second:  41, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.027379, mae: 3.441780, mean_q: 4.150529, mean_eps: 0.100000
 1067773/2000000: episode: 1420, duration: 15.240s, episode steps: 641, steps per second:  42, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.910 [0.000, 5.000],  loss: 0.023403, mae: 3.433790, mean_q: 4.141746, mean_eps: 0.100000
 1068787/2000000: episode: 1421, duration: 25.176s, episode steps: 1014, steps per second:  40, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.788 [0.000, 5.000],  loss: 0.025800, mae: 3.439295, mean_q: 4.147678, mean_eps: 0.100000
 1069497/2000000: episode: 1422, duration: 17.141s, episode steps: 710, steps per second:  41, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.158 [0.000, 5.000],  loss: 0.026093, mae: 3.435960, mean_q: 4.144027, mean_eps: 0.100000
 1070625/2000000: episode: 1423, duration: 26.955s, episode steps: 1128, steps per second:  42, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.024666, mae: 3.417244, mean_q: 4.121921, mean_eps: 0.100000
 1071171/2000000: episode: 1424, duration: 13.088s, episode steps: 546, steps per second:  42, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.905 [0.000, 5.000],  loss: 0.023832, mae: 3.399024, mean_q: 4.098425, mean_eps: 0.100000
 1072684/2000000: episode: 1425, duration: 38.376s, episode steps: 1513, steps per second:  39, episode reward: 31.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.034 [0.000, 5.000],  loss: 0.024056, mae: 3.408884, mean_q: 4.110243, mean_eps: 0.100000
 1073410/2000000: episode: 1426, duration: 17.870s, episode steps: 726, steps per second:  41, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.101 [0.000, 5.000],  loss: 0.025760, mae: 3.416388, mean_q: 4.119231, mean_eps: 0.100000
 1074674/2000000: episode: 1427, duration: 32.142s, episode steps: 1264, steps per second:  39, episode reward: 33.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.687 [0.000, 5.000],  loss: 0.023904, mae: 3.409770, mean_q: 4.110697, mean_eps: 0.100000
 1075100/2000000: episode: 1428, duration: 9.614s, episode steps: 426, steps per second:  44, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.995 [0.000, 5.000],  loss: 0.025463, mae: 3.399078, mean_q: 4.100407, mean_eps: 0.100000
 1075820/2000000: episode: 1429, duration: 16.728s, episode steps: 720, steps per second:  43, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.175 [0.000, 5.000],  loss: 0.023254, mae: 3.397480, mean_q: 4.098082, mean_eps: 0.100000
 1076882/2000000: episode: 1430, duration: 26.151s, episode steps: 1062, steps per second:  41, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.021758, mae: 3.385964, mean_q: 4.082845, mean_eps: 0.100000
 1077677/2000000: episode: 1431, duration: 18.404s, episode steps: 795, steps per second:  43, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.950 [0.000, 5.000],  loss: 0.024906, mae: 3.389134, mean_q: 4.087864, mean_eps: 0.100000
 1078355/2000000: episode: 1432, duration: 15.712s, episode steps: 678, steps per second:  43, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.204 [0.000, 5.000],  loss: 0.026649, mae: 3.408030, mean_q: 4.108654, mean_eps: 0.100000
 1079366/2000000: episode: 1433, duration: 22.888s, episode steps: 1011, steps per second:  44, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.842 [0.000, 5.000],  loss: 0.024148, mae: 3.396257, mean_q: 4.092993, mean_eps: 0.100000
 1080293/2000000: episode: 1434, duration: 21.326s, episode steps: 927, steps per second:  43, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.023598, mae: 3.378218, mean_q: 4.072113, mean_eps: 0.100000
 1080639/2000000: episode: 1435, duration: 7.851s, episode steps: 346, steps per second:  44, episode reward:  7.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.994 [0.000, 5.000],  loss: 0.025364, mae: 3.388341, mean_q: 4.087817, mean_eps: 0.100000
 1081345/2000000: episode: 1436, duration: 16.582s, episode steps: 706, steps per second:  43, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.025074, mae: 3.400628, mean_q: 4.101091, mean_eps: 0.100000
 1082365/2000000: episode: 1437, duration: 25.124s, episode steps: 1020, steps per second:  41, episode reward: 31.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.023257, mae: 3.404299, mean_q: 4.104067, mean_eps: 0.100000
 1083177/2000000: episode: 1438, duration: 19.534s, episode steps: 812, steps per second:  42, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.950 [0.000, 5.000],  loss: 0.022573, mae: 3.399988, mean_q: 4.098965, mean_eps: 0.100000
 1084395/2000000: episode: 1439, duration: 30.803s, episode steps: 1218, steps per second:  40, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.026830, mae: 3.410134, mean_q: 4.110232, mean_eps: 0.100000
 1085381/2000000: episode: 1440, duration: 23.334s, episode steps: 986, steps per second:  42, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.164 [0.000, 5.000],  loss: 0.023002, mae: 3.375510, mean_q: 4.069331, mean_eps: 0.100000
 1086354/2000000: episode: 1441, duration: 23.738s, episode steps: 973, steps per second:  41, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.084 [0.000, 5.000],  loss: 0.026507, mae: 3.386688, mean_q: 4.082948, mean_eps: 0.100000
 1087659/2000000: episode: 1442, duration: 31.381s, episode steps: 1305, steps per second:  42, episode reward: 28.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.023658, mae: 3.406675, mean_q: 4.108062, mean_eps: 0.100000
 1088620/2000000: episode: 1443, duration: 23.613s, episode steps: 961, steps per second:  41, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.122 [0.000, 5.000],  loss: 0.026587, mae: 3.384322, mean_q: 4.079694, mean_eps: 0.100000
 1089094/2000000: episode: 1444, duration: 11.966s, episode steps: 474, steps per second:  40, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.019266, mae: 3.388237, mean_q: 4.086272, mean_eps: 0.100000
 1090233/2000000: episode: 1445, duration: 26.620s, episode steps: 1139, steps per second:  43, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.819 [0.000, 5.000],  loss: 0.021786, mae: 3.399780, mean_q: 4.101001, mean_eps: 0.100000
 1091976/2000000: episode: 1446, duration: 43.197s, episode steps: 1743, steps per second:  40, episode reward: 34.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.278 [0.000, 5.000],  loss: 0.024928, mae: 3.413627, mean_q: 4.115429, mean_eps: 0.100000
 1093062/2000000: episode: 1447, duration: 26.095s, episode steps: 1086, steps per second:  42, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.172 [0.000, 5.000],  loss: 0.023401, mae: 3.400795, mean_q: 4.098574, mean_eps: 0.100000
 1093803/2000000: episode: 1448, duration: 17.999s, episode steps: 741, steps per second:  41, episode reward:  7.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.080 [0.000, 5.000],  loss: 0.023404, mae: 3.397448, mean_q: 4.096586, mean_eps: 0.100000
 1094648/2000000: episode: 1449, duration: 20.710s, episode steps: 845, steps per second:  41, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.022973, mae: 3.402253, mean_q: 4.101491, mean_eps: 0.100000
 1095907/2000000: episode: 1450, duration: 29.888s, episode steps: 1259, steps per second:  42, episode reward: 25.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.021719, mae: 3.408508, mean_q: 4.109680, mean_eps: 0.100000
 1096508/2000000: episode: 1451, duration: 15.049s, episode steps: 601, steps per second:  40, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.704 [0.000, 5.000],  loss: 0.025040, mae: 3.402073, mean_q: 4.100391, mean_eps: 0.100000
 1097378/2000000: episode: 1452, duration: 21.197s, episode steps: 870, steps per second:  41, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.898 [0.000, 5.000],  loss: 0.025642, mae: 3.397528, mean_q: 4.095221, mean_eps: 0.100000
 1098339/2000000: episode: 1453, duration: 23.077s, episode steps: 961, steps per second:  42, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.022815, mae: 3.391935, mean_q: 4.087685, mean_eps: 0.100000
 1099551/2000000: episode: 1454, duration: 30.035s, episode steps: 1212, steps per second:  40, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.092 [0.000, 5.000],  loss: 0.025838, mae: 3.383055, mean_q: 4.077670, mean_eps: 0.100000
 1100210/2000000: episode: 1455, duration: 16.206s, episode steps: 659, steps per second:  41, episode reward: 18.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.648 [0.000, 5.000],  loss: 0.024949, mae: 3.380343, mean_q: 4.075390, mean_eps: 0.100000
 1100750/2000000: episode: 1456, duration: 12.833s, episode steps: 540, steps per second:  42, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.028736, mae: 3.403063, mean_q: 4.102051, mean_eps: 0.100000
 1101682/2000000: episode: 1457, duration: 23.625s, episode steps: 932, steps per second:  39, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.061 [0.000, 5.000],  loss: 0.026094, mae: 3.381290, mean_q: 4.075260, mean_eps: 0.100000
 1102076/2000000: episode: 1458, duration: 10.218s, episode steps: 394, steps per second:  39, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.987 [0.000, 5.000],  loss: 0.031768, mae: 3.389310, mean_q: 4.082640, mean_eps: 0.100000
 1102659/2000000: episode: 1459, duration: 14.691s, episode steps: 583, steps per second:  40, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.331 [0.000, 5.000],  loss: 0.025445, mae: 3.387154, mean_q: 4.084544, mean_eps: 0.100000
 1103121/2000000: episode: 1460, duration: 11.802s, episode steps: 462, steps per second:  39, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.926 [0.000, 5.000],  loss: 0.027176, mae: 3.399138, mean_q: 4.099532, mean_eps: 0.100000
 1103998/2000000: episode: 1461, duration: 21.142s, episode steps: 877, steps per second:  41, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.026642, mae: 3.399190, mean_q: 4.096572, mean_eps: 0.100000
 1105021/2000000: episode: 1462, duration: 24.050s, episode steps: 1023, steps per second:  43, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.937 [0.000, 5.000],  loss: 0.027210, mae: 3.384312, mean_q: 4.078485, mean_eps: 0.100000
 1105604/2000000: episode: 1463, duration: 12.996s, episode steps: 583, steps per second:  45, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.576 [0.000, 5.000],  loss: 0.023567, mae: 3.379219, mean_q: 4.075415, mean_eps: 0.100000
 1106471/2000000: episode: 1464, duration: 22.333s, episode steps: 867, steps per second:  39, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.096 [0.000, 5.000],  loss: 0.027536, mae: 3.365304, mean_q: 4.058628, mean_eps: 0.100000
 1107503/2000000: episode: 1465, duration: 24.612s, episode steps: 1032, steps per second:  42, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.066 [0.000, 5.000],  loss: 0.027651, mae: 3.345653, mean_q: 4.032656, mean_eps: 0.100000
 1108563/2000000: episode: 1466, duration: 24.441s, episode steps: 1060, steps per second:  43, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.026886, mae: 3.352623, mean_q: 4.043163, mean_eps: 0.100000
 1109703/2000000: episode: 1467, duration: 26.728s, episode steps: 1140, steps per second:  43, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.027078, mae: 3.362282, mean_q: 4.055337, mean_eps: 0.100000
 1110549/2000000: episode: 1468, duration: 21.561s, episode steps: 846, steps per second:  39, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.215 [0.000, 5.000],  loss: 0.025935, mae: 3.328242, mean_q: 4.015701, mean_eps: 0.100000
 1111242/2000000: episode: 1469, duration: 19.240s, episode steps: 693, steps per second:  36, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.052 [0.000, 5.000],  loss: 0.024906, mae: 3.353397, mean_q: 4.046380, mean_eps: 0.100000
 1111822/2000000: episode: 1470, duration: 15.971s, episode steps: 580, steps per second:  36, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.027927, mae: 3.337291, mean_q: 4.026349, mean_eps: 0.100000
 1112689/2000000: episode: 1471, duration: 22.466s, episode steps: 867, steps per second:  39, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.183 [0.000, 5.000],  loss: 0.029130, mae: 3.364273, mean_q: 4.056953, mean_eps: 0.100000
 1113404/2000000: episode: 1472, duration: 16.812s, episode steps: 715, steps per second:  43, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.848 [0.000, 5.000],  loss: 0.027827, mae: 3.374188, mean_q: 4.067617, mean_eps: 0.100000
 1113942/2000000: episode: 1473, duration: 12.486s, episode steps: 538, steps per second:  43, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.023083, mae: 3.357344, mean_q: 4.048842, mean_eps: 0.100000
 1114895/2000000: episode: 1474, duration: 21.842s, episode steps: 953, steps per second:  44, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.024546, mae: 3.338564, mean_q: 4.025927, mean_eps: 0.100000
 1115563/2000000: episode: 1475, duration: 15.974s, episode steps: 668, steps per second:  42, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.898 [0.000, 5.000],  loss: 0.024057, mae: 3.324050, mean_q: 4.010747, mean_eps: 0.100000
 1116271/2000000: episode: 1476, duration: 17.667s, episode steps: 708, steps per second:  40, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.918 [0.000, 5.000],  loss: 0.025156, mae: 3.302922, mean_q: 3.985188, mean_eps: 0.100000
 1117470/2000000: episode: 1477, duration: 29.260s, episode steps: 1199, steps per second:  41, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.023661, mae: 3.341268, mean_q: 4.032599, mean_eps: 0.100000
 1118103/2000000: episode: 1478, duration: 15.033s, episode steps: 633, steps per second:  42, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.027365, mae: 3.327862, mean_q: 4.014878, mean_eps: 0.100000
 1118893/2000000: episode: 1479, duration: 18.379s, episode steps: 790, steps per second:  43, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.025656, mae: 3.337876, mean_q: 4.028722, mean_eps: 0.100000
 1119595/2000000: episode: 1480, duration: 16.734s, episode steps: 702, steps per second:  42, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.085 [0.000, 5.000],  loss: 0.028555, mae: 3.336799, mean_q: 4.026222, mean_eps: 0.100000
 1120020/2000000: episode: 1481, duration: 9.697s, episode steps: 425, steps per second:  44, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.934 [0.000, 5.000],  loss: 0.027182, mae: 3.311625, mean_q: 3.992945, mean_eps: 0.100000
 1120931/2000000: episode: 1482, duration: 21.428s, episode steps: 911, steps per second:  43, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.024197, mae: 3.346093, mean_q: 4.036264, mean_eps: 0.100000
 1122147/2000000: episode: 1483, duration: 29.627s, episode steps: 1216, steps per second:  41, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.023496, mae: 3.338931, mean_q: 4.028407, mean_eps: 0.100000
 1123243/2000000: episode: 1484, duration: 27.900s, episode steps: 1096, steps per second:  39, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.940 [0.000, 5.000],  loss: 0.025471, mae: 3.356514, mean_q: 4.049149, mean_eps: 0.100000
 1123907/2000000: episode: 1485, duration: 15.852s, episode steps: 664, steps per second:  42, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.636 [0.000, 5.000],  loss: 0.022747, mae: 3.335760, mean_q: 4.022719, mean_eps: 0.100000
 1124462/2000000: episode: 1486, duration: 13.962s, episode steps: 555, steps per second:  40, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.022489, mae: 3.326604, mean_q: 4.011748, mean_eps: 0.100000
 1125029/2000000: episode: 1487, duration: 14.441s, episode steps: 567, steps per second:  39, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.022930, mae: 3.320006, mean_q: 4.002503, mean_eps: 0.100000
 1125657/2000000: episode: 1488, duration: 16.716s, episode steps: 628, steps per second:  38, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.024721, mae: 3.348058, mean_q: 4.038004, mean_eps: 0.100000
 1126490/2000000: episode: 1489, duration: 20.572s, episode steps: 833, steps per second:  40, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.878 [0.000, 5.000],  loss: 0.028048, mae: 3.330000, mean_q: 4.016554, mean_eps: 0.100000
 1127118/2000000: episode: 1490, duration: 15.574s, episode steps: 628, steps per second:  40, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.685 [0.000, 5.000],  loss: 0.024711, mae: 3.338594, mean_q: 4.025200, mean_eps: 0.100000
 1127843/2000000: episode: 1491, duration: 17.901s, episode steps: 725, steps per second:  41, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.222 [0.000, 5.000],  loss: 0.026361, mae: 3.320165, mean_q: 4.003991, mean_eps: 0.100000
 1128469/2000000: episode: 1492, duration: 16.342s, episode steps: 626, steps per second:  38, episode reward: 18.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.091 [0.000, 5.000],  loss: 0.028718, mae: 3.328267, mean_q: 4.010359, mean_eps: 0.100000
 1128994/2000000: episode: 1493, duration: 13.328s, episode steps: 525, steps per second:  39, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.107 [0.000, 5.000],  loss: 0.025517, mae: 3.324672, mean_q: 4.008330, mean_eps: 0.100000
 1129391/2000000: episode: 1494, duration: 9.654s, episode steps: 397, steps per second:  41, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.970 [0.000, 5.000],  loss: 0.021701, mae: 3.309835, mean_q: 3.992134, mean_eps: 0.100000
 1129958/2000000: episode: 1495, duration: 14.446s, episode steps: 567, steps per second:  39, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.076 [0.000, 5.000],  loss: 0.028082, mae: 3.310044, mean_q: 3.991489, mean_eps: 0.100000
 1130813/2000000: episode: 1496, duration: 22.029s, episode steps: 855, steps per second:  39, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.024026, mae: 3.306694, mean_q: 3.987552, mean_eps: 0.100000
 1131592/2000000: episode: 1497, duration: 19.678s, episode steps: 779, steps per second:  40, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.208 [0.000, 5.000],  loss: 0.029334, mae: 3.292409, mean_q: 3.966707, mean_eps: 0.100000
 1132332/2000000: episode: 1498, duration: 18.456s, episode steps: 740, steps per second:  40, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.764 [0.000, 5.000],  loss: 0.025424, mae: 3.294945, mean_q: 3.972679, mean_eps: 0.100000
 1133401/2000000: episode: 1499, duration: 25.915s, episode steps: 1069, steps per second:  41, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.911 [0.000, 5.000],  loss: 0.023768, mae: 3.297498, mean_q: 3.975519, mean_eps: 0.100000
 1134048/2000000: episode: 1500, duration: 15.382s, episode steps: 647, steps per second:  42, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.071 [0.000, 5.000],  loss: 0.024896, mae: 3.301870, mean_q: 3.980139, mean_eps: 0.100000
 1134741/2000000: episode: 1501, duration: 15.966s, episode steps: 693, steps per second:  43, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.025879, mae: 3.271573, mean_q: 3.945123, mean_eps: 0.100000
 1135744/2000000: episode: 1502, duration: 23.751s, episode steps: 1003, steps per second:  42, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.023197, mae: 3.308308, mean_q: 3.989348, mean_eps: 0.100000
 1136270/2000000: episode: 1503, duration: 12.889s, episode steps: 526, steps per second:  41, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 3.407 [0.000, 5.000],  loss: 0.024707, mae: 3.333359, mean_q: 4.019503, mean_eps: 0.100000
 1137542/2000000: episode: 1504, duration: 30.482s, episode steps: 1272, steps per second:  42, episode reward: 28.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.664 [0.000, 5.000],  loss: 0.025407, mae: 3.310043, mean_q: 3.989862, mean_eps: 0.100000
 1138067/2000000: episode: 1505, duration: 12.775s, episode steps: 525, steps per second:  41, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.022274, mae: 3.340822, mean_q: 4.028554, mean_eps: 0.100000
 1138642/2000000: episode: 1506, duration: 14.067s, episode steps: 575, steps per second:  41, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.957 [0.000, 5.000],  loss: 0.028150, mae: 3.312932, mean_q: 3.998176, mean_eps: 0.100000
 1139405/2000000: episode: 1507, duration: 19.732s, episode steps: 763, steps per second:  39, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.273 [0.000, 5.000],  loss: 0.023892, mae: 3.299424, mean_q: 3.978138, mean_eps: 0.100000
 1140198/2000000: episode: 1508, duration: 19.313s, episode steps: 793, steps per second:  41, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.063 [0.000, 5.000],  loss: 0.023980, mae: 3.337307, mean_q: 4.024896, mean_eps: 0.100000
 1141016/2000000: episode: 1509, duration: 20.949s, episode steps: 818, steps per second:  39, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.263 [0.000, 5.000],  loss: 0.026364, mae: 3.342301, mean_q: 4.031094, mean_eps: 0.100000
 1142077/2000000: episode: 1510, duration: 25.465s, episode steps: 1061, steps per second:  42, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.123 [0.000, 5.000],  loss: 0.022273, mae: 3.329519, mean_q: 4.014639, mean_eps: 0.100000
 1142719/2000000: episode: 1511, duration: 14.778s, episode steps: 642, steps per second:  43, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.829 [0.000, 5.000],  loss: 0.024973, mae: 3.338142, mean_q: 4.025665, mean_eps: 0.100000
 1143516/2000000: episode: 1512, duration: 19.155s, episode steps: 797, steps per second:  42, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.024744, mae: 3.331543, mean_q: 4.017849, mean_eps: 0.100000
 1144424/2000000: episode: 1513, duration: 22.443s, episode steps: 908, steps per second:  40, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.023683, mae: 3.329324, mean_q: 4.014331, mean_eps: 0.100000
 1145196/2000000: episode: 1514, duration: 19.787s, episode steps: 772, steps per second:  39, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.185 [0.000, 5.000],  loss: 0.024457, mae: 3.338315, mean_q: 4.025298, mean_eps: 0.100000
 1146246/2000000: episode: 1515, duration: 27.424s, episode steps: 1050, steps per second:  38, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.023116, mae: 3.341550, mean_q: 4.028744, mean_eps: 0.100000
 1146983/2000000: episode: 1516, duration: 17.652s, episode steps: 737, steps per second:  42, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.997 [0.000, 5.000],  loss: 0.025497, mae: 3.376930, mean_q: 4.071744, mean_eps: 0.100000
 1148266/2000000: episode: 1517, duration: 32.670s, episode steps: 1283, steps per second:  39, episode reward: 19.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.966 [0.000, 5.000],  loss: 0.023582, mae: 3.347234, mean_q: 4.037770, mean_eps: 0.100000
 1149456/2000000: episode: 1518, duration: 27.410s, episode steps: 1190, steps per second:  43, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.025256, mae: 3.351185, mean_q: 4.041769, mean_eps: 0.100000
 1150020/2000000: episode: 1519, duration: 14.243s, episode steps: 564, steps per second:  40, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.294 [0.000, 5.000],  loss: 0.029272, mae: 3.351495, mean_q: 4.039128, mean_eps: 0.100000
 1150673/2000000: episode: 1520, duration: 15.909s, episode steps: 653, steps per second:  41, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.185 [0.000, 5.000],  loss: 0.027582, mae: 3.358031, mean_q: 4.049315, mean_eps: 0.100000
 1151098/2000000: episode: 1521, duration: 10.016s, episode steps: 425, steps per second:  42, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.021554, mae: 3.363770, mean_q: 4.059721, mean_eps: 0.100000
 1151876/2000000: episode: 1522, duration: 18.056s, episode steps: 778, steps per second:  43, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.023111, mae: 3.364380, mean_q: 4.055500, mean_eps: 0.100000
 1153077/2000000: episode: 1523, duration: 28.113s, episode steps: 1201, steps per second:  43, episode reward: 33.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.025014, mae: 3.360246, mean_q: 4.051430, mean_eps: 0.100000
 1153997/2000000: episode: 1524, duration: 22.016s, episode steps: 920, steps per second:  42, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.202 [0.000, 5.000],  loss: 0.026095, mae: 3.373917, mean_q: 4.066883, mean_eps: 0.100000
 1155074/2000000: episode: 1525, duration: 26.818s, episode steps: 1077, steps per second:  40, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.146 [0.000, 5.000],  loss: 0.023776, mae: 3.373834, mean_q: 4.068916, mean_eps: 0.100000
 1156036/2000000: episode: 1526, duration: 25.068s, episode steps: 962, steps per second:  38, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.026987, mae: 3.381942, mean_q: 4.076819, mean_eps: 0.100000
 1157199/2000000: episode: 1527, duration: 27.024s, episode steps: 1163, steps per second:  43, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.208 [0.000, 5.000],  loss: 0.024184, mae: 3.375791, mean_q: 4.071727, mean_eps: 0.100000
 1157728/2000000: episode: 1528, duration: 12.186s, episode steps: 529, steps per second:  43, episode reward: 17.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.025916, mae: 3.391557, mean_q: 4.088560, mean_eps: 0.100000
 1158514/2000000: episode: 1529, duration: 18.315s, episode steps: 786, steps per second:  43, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.204 [0.000, 5.000],  loss: 0.023948, mae: 3.382652, mean_q: 4.080702, mean_eps: 0.100000
 1159478/2000000: episode: 1530, duration: 22.953s, episode steps: 964, steps per second:  42, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.026113, mae: 3.377637, mean_q: 4.071142, mean_eps: 0.100000
 1160512/2000000: episode: 1531, duration: 25.342s, episode steps: 1034, steps per second:  41, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.122 [0.000, 5.000],  loss: 0.025173, mae: 3.391550, mean_q: 4.089973, mean_eps: 0.100000
 1161015/2000000: episode: 1532, duration: 12.896s, episode steps: 503, steps per second:  39, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.636 [0.000, 5.000],  loss: 0.021720, mae: 3.407721, mean_q: 4.109278, mean_eps: 0.100000
 1161695/2000000: episode: 1533, duration: 16.734s, episode steps: 680, steps per second:  41, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.023934, mae: 3.419597, mean_q: 4.125634, mean_eps: 0.100000
 1162969/2000000: episode: 1534, duration: 31.607s, episode steps: 1274, steps per second:  40, episode reward: 28.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.024091, mae: 3.427514, mean_q: 4.133473, mean_eps: 0.100000
 1164002/2000000: episode: 1535, duration: 24.478s, episode steps: 1033, steps per second:  42, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.175 [0.000, 5.000],  loss: 0.023559, mae: 3.404070, mean_q: 4.106454, mean_eps: 0.100000
 1165127/2000000: episode: 1536, duration: 26.672s, episode steps: 1125, steps per second:  42, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.025783, mae: 3.400309, mean_q: 4.100299, mean_eps: 0.100000
 1166275/2000000: episode: 1537, duration: 26.998s, episode steps: 1148, steps per second:  43, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.020814, mae: 3.431503, mean_q: 4.138222, mean_eps: 0.100000
 1167432/2000000: episode: 1538, duration: 26.120s, episode steps: 1157, steps per second:  44, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.024562, mae: 3.431811, mean_q: 4.137884, mean_eps: 0.100000
 1167938/2000000: episode: 1539, duration: 11.608s, episode steps: 506, steps per second:  44, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.921 [0.000, 5.000],  loss: 0.025968, mae: 3.434151, mean_q: 4.139681, mean_eps: 0.100000
 1168702/2000000: episode: 1540, duration: 18.043s, episode steps: 764, steps per second:  42, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.758 [0.000, 5.000],  loss: 0.024888, mae: 3.436124, mean_q: 4.144162, mean_eps: 0.100000
 1169833/2000000: episode: 1541, duration: 27.151s, episode steps: 1131, steps per second:  42, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.022938, mae: 3.442793, mean_q: 4.152849, mean_eps: 0.100000
 1170505/2000000: episode: 1542, duration: 16.776s, episode steps: 672, steps per second:  40, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.908 [0.000, 5.000],  loss: 0.023778, mae: 3.424334, mean_q: 4.131830, mean_eps: 0.100000
 1171514/2000000: episode: 1543, duration: 24.493s, episode steps: 1009, steps per second:  41, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.021205, mae: 3.429290, mean_q: 4.138058, mean_eps: 0.100000
 1172037/2000000: episode: 1544, duration: 12.697s, episode steps: 523, steps per second:  41, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.744 [0.000, 5.000],  loss: 0.023643, mae: 3.440492, mean_q: 4.148619, mean_eps: 0.100000
 1172992/2000000: episode: 1545, duration: 23.346s, episode steps: 955, steps per second:  41, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.005 [0.000, 5.000],  loss: 0.028004, mae: 3.428799, mean_q: 4.134105, mean_eps: 0.100000
 1173551/2000000: episode: 1546, duration: 13.650s, episode steps: 559, steps per second:  41, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.197 [0.000, 5.000],  loss: 0.024319, mae: 3.426919, mean_q: 4.134200, mean_eps: 0.100000
 1174496/2000000: episode: 1547, duration: 22.364s, episode steps: 945, steps per second:  42, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.027022, mae: 3.433797, mean_q: 4.140878, mean_eps: 0.100000
 1175366/2000000: episode: 1548, duration: 22.646s, episode steps: 870, steps per second:  38, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.264 [0.000, 5.000],  loss: 0.026696, mae: 3.452972, mean_q: 4.162986, mean_eps: 0.100000
 1176418/2000000: episode: 1549, duration: 27.117s, episode steps: 1052, steps per second:  39, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.760 [0.000, 5.000],  loss: 0.024302, mae: 3.447561, mean_q: 4.160538, mean_eps: 0.100000
 1177484/2000000: episode: 1550, duration: 26.869s, episode steps: 1066, steps per second:  40, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.200 [0.000, 5.000],  loss: 0.024532, mae: 3.444618, mean_q: 4.154468, mean_eps: 0.100000
 1178485/2000000: episode: 1551, duration: 24.247s, episode steps: 1001, steps per second:  41, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.026604, mae: 3.452442, mean_q: 4.167397, mean_eps: 0.100000
 1179512/2000000: episode: 1552, duration: 23.805s, episode steps: 1027, steps per second:  43, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.251 [0.000, 5.000],  loss: 0.025891, mae: 3.429012, mean_q: 4.134789, mean_eps: 0.100000
 1180400/2000000: episode: 1553, duration: 22.162s, episode steps: 888, steps per second:  40, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.402 [0.000, 5.000],  loss: 0.028958, mae: 3.446003, mean_q: 4.156091, mean_eps: 0.100000
 1180831/2000000: episode: 1554, duration: 10.609s, episode steps: 431, steps per second:  41, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.227 [0.000, 5.000],  loss: 0.023614, mae: 3.456989, mean_q: 4.174456, mean_eps: 0.100000
 1181190/2000000: episode: 1555, duration: 8.015s, episode steps: 359, steps per second:  45, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.331 [0.000, 5.000],  loss: 0.028147, mae: 3.418441, mean_q: 4.120665, mean_eps: 0.100000
 1182256/2000000: episode: 1556, duration: 24.616s, episode steps: 1066, steps per second:  43, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.942 [0.000, 5.000],  loss: 0.025746, mae: 3.431074, mean_q: 4.137894, mean_eps: 0.100000
 1182592/2000000: episode: 1557, duration: 8.108s, episode steps: 336, steps per second:  41, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 3.131 [0.000, 5.000],  loss: 0.031118, mae: 3.395359, mean_q: 4.097036, mean_eps: 0.100000
 1183306/2000000: episode: 1558, duration: 17.298s, episode steps: 714, steps per second:  41, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.028297, mae: 3.457357, mean_q: 4.169456, mean_eps: 0.100000
 1184481/2000000: episode: 1559, duration: 28.449s, episode steps: 1175, steps per second:  41, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.025138, mae: 3.429013, mean_q: 4.134461, mean_eps: 0.100000
 1185853/2000000: episode: 1560, duration: 35.298s, episode steps: 1372, steps per second:  39, episode reward: 26.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.125 [0.000, 5.000],  loss: 0.023255, mae: 3.414262, mean_q: 4.117722, mean_eps: 0.100000
 1186573/2000000: episode: 1561, duration: 18.673s, episode steps: 720, steps per second:  39, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.244 [0.000, 5.000],  loss: 0.025094, mae: 3.385317, mean_q: 4.082102, mean_eps: 0.100000
 1187636/2000000: episode: 1562, duration: 25.815s, episode steps: 1063, steps per second:  41, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.056 [0.000, 5.000],  loss: 0.026132, mae: 3.384358, mean_q: 4.081026, mean_eps: 0.100000
 1188698/2000000: episode: 1563, duration: 25.411s, episode steps: 1062, steps per second:  42, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.236 [0.000, 5.000],  loss: 0.027175, mae: 3.401750, mean_q: 4.102174, mean_eps: 0.100000
 1189468/2000000: episode: 1564, duration: 19.095s, episode steps: 770, steps per second:  40, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.834 [0.000, 5.000],  loss: 0.023413, mae: 3.412516, mean_q: 4.114890, mean_eps: 0.100000
 1190199/2000000: episode: 1565, duration: 18.390s, episode steps: 731, steps per second:  40, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.023183, mae: 3.386011, mean_q: 4.085419, mean_eps: 0.100000
 1191136/2000000: episode: 1566, duration: 23.054s, episode steps: 937, steps per second:  41, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.022327, mae: 3.375868, mean_q: 4.072241, mean_eps: 0.100000
 1191632/2000000: episode: 1567, duration: 12.213s, episode steps: 496, steps per second:  41, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 3.393 [0.000, 5.000],  loss: 0.026220, mae: 3.359078, mean_q: 4.049362, mean_eps: 0.100000
 1192808/2000000: episode: 1568, duration: 28.960s, episode steps: 1176, steps per second:  41, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.025427, mae: 3.386167, mean_q: 4.082272, mean_eps: 0.100000
 1193646/2000000: episode: 1569, duration: 19.480s, episode steps: 838, steps per second:  43, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.024345, mae: 3.382599, mean_q: 4.079362, mean_eps: 0.100000
 1194614/2000000: episode: 1570, duration: 22.902s, episode steps: 968, steps per second:  42, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.779 [0.000, 5.000],  loss: 0.026013, mae: 3.374777, mean_q: 4.067562, mean_eps: 0.100000
 1195404/2000000: episode: 1571, duration: 19.944s, episode steps: 790, steps per second:  40, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.021917, mae: 3.408114, mean_q: 4.112200, mean_eps: 0.100000
 1196230/2000000: episode: 1572, duration: 18.974s, episode steps: 826, steps per second:  44, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.243 [0.000, 5.000],  loss: 0.025539, mae: 3.375486, mean_q: 4.068490, mean_eps: 0.100000
 1197146/2000000: episode: 1573, duration: 21.150s, episode steps: 916, steps per second:  43, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.802 [0.000, 5.000],  loss: 0.023557, mae: 3.392930, mean_q: 4.090058, mean_eps: 0.100000
 1198061/2000000: episode: 1574, duration: 21.241s, episode steps: 915, steps per second:  43, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.025032, mae: 3.398402, mean_q: 4.097679, mean_eps: 0.100000
 1198894/2000000: episode: 1575, duration: 20.638s, episode steps: 833, steps per second:  40, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.115 [0.000, 5.000],  loss: 0.027435, mae: 3.382369, mean_q: 4.076379, mean_eps: 0.100000
 1199750/2000000: episode: 1576, duration: 20.992s, episode steps: 856, steps per second:  41, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.025268, mae: 3.408034, mean_q: 4.109490, mean_eps: 0.100000
 1200864/2000000: episode: 1577, duration: 28.514s, episode steps: 1114, steps per second:  39, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.027139, mae: 3.431915, mean_q: 4.138723, mean_eps: 0.100000
 1201515/2000000: episode: 1578, duration: 15.921s, episode steps: 651, steps per second:  41, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.438 [0.000, 5.000],  loss: 0.026329, mae: 3.459441, mean_q: 4.171205, mean_eps: 0.100000
 1202148/2000000: episode: 1579, duration: 14.611s, episode steps: 633, steps per second:  43, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.022914, mae: 3.428523, mean_q: 4.136762, mean_eps: 0.100000
 1202700/2000000: episode: 1580, duration: 11.976s, episode steps: 552, steps per second:  46, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.196 [0.000, 5.000],  loss: 0.022730, mae: 3.444398, mean_q: 4.154634, mean_eps: 0.100000
 1203668/2000000: episode: 1581, duration: 21.246s, episode steps: 968, steps per second:  46, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.590 [0.000, 5.000],  loss: 0.022035, mae: 3.442680, mean_q: 4.151469, mean_eps: 0.100000
 1205128/2000000: episode: 1582, duration: 34.545s, episode steps: 1460, steps per second:  42, episode reward: 30.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.963 [0.000, 5.000],  loss: 0.023584, mae: 3.445630, mean_q: 4.154537, mean_eps: 0.100000
 1206204/2000000: episode: 1583, duration: 25.410s, episode steps: 1076, steps per second:  42, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.024844, mae: 3.448570, mean_q: 4.158011, mean_eps: 0.100000
 1206859/2000000: episode: 1584, duration: 14.767s, episode steps: 655, steps per second:  44, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.240 [0.000, 5.000],  loss: 0.023759, mae: 3.455477, mean_q: 4.165646, mean_eps: 0.100000
 1207789/2000000: episode: 1585, duration: 21.577s, episode steps: 930, steps per second:  43, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.220 [0.000, 5.000],  loss: 0.026902, mae: 3.457744, mean_q: 4.166801, mean_eps: 0.100000
 1208513/2000000: episode: 1586, duration: 16.052s, episode steps: 724, steps per second:  45, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.022188, mae: 3.436120, mean_q: 4.143606, mean_eps: 0.100000
 1208997/2000000: episode: 1587, duration: 10.347s, episode steps: 484, steps per second:  47, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.874 [0.000, 5.000],  loss: 0.020154, mae: 3.461088, mean_q: 4.173611, mean_eps: 0.100000
 1209619/2000000: episode: 1588, duration: 13.452s, episode steps: 622, steps per second:  46, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.378 [0.000, 5.000],  loss: 0.027720, mae: 3.437982, mean_q: 4.147098, mean_eps: 0.100000
 1210255/2000000: episode: 1589, duration: 14.055s, episode steps: 636, steps per second:  45, episode reward: 17.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.980 [0.000, 5.000],  loss: 0.022276, mae: 3.460516, mean_q: 4.174560, mean_eps: 0.100000
 1211146/2000000: episode: 1590, duration: 20.483s, episode steps: 891, steps per second:  43, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.024985, mae: 3.453386, mean_q: 4.162925, mean_eps: 0.100000
 1212047/2000000: episode: 1591, duration: 20.763s, episode steps: 901, steps per second:  43, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.117 [0.000, 5.000],  loss: 0.024955, mae: 3.466493, mean_q: 4.180393, mean_eps: 0.100000
 1212975/2000000: episode: 1592, duration: 20.690s, episode steps: 928, steps per second:  45, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.893 [0.000, 5.000],  loss: 0.026706, mae: 3.457025, mean_q: 4.168532, mean_eps: 0.100000
 1213770/2000000: episode: 1593, duration: 18.050s, episode steps: 795, steps per second:  44, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.023819, mae: 3.451578, mean_q: 4.163001, mean_eps: 0.100000
 1214568/2000000: episode: 1594, duration: 18.365s, episode steps: 798, steps per second:  43, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.962 [0.000, 5.000],  loss: 0.021927, mae: 3.456975, mean_q: 4.167256, mean_eps: 0.100000
 1215227/2000000: episode: 1595, duration: 15.765s, episode steps: 659, steps per second:  42, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.068 [0.000, 5.000],  loss: 0.025639, mae: 3.473135, mean_q: 4.190795, mean_eps: 0.100000
 1215862/2000000: episode: 1596, duration: 15.809s, episode steps: 635, steps per second:  40, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.025497, mae: 3.490021, mean_q: 4.209747, mean_eps: 0.100000
 1216574/2000000: episode: 1597, duration: 16.424s, episode steps: 712, steps per second:  43, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.044 [0.000, 5.000],  loss: 0.022868, mae: 3.485928, mean_q: 4.202874, mean_eps: 0.100000
 1217430/2000000: episode: 1598, duration: 19.375s, episode steps: 856, steps per second:  44, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.318 [0.000, 5.000],  loss: 0.024787, mae: 3.502919, mean_q: 4.224763, mean_eps: 0.100000
 1218097/2000000: episode: 1599, duration: 15.524s, episode steps: 667, steps per second:  43, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.025185, mae: 3.541155, mean_q: 4.267242, mean_eps: 0.100000
 1218736/2000000: episode: 1600, duration: 14.538s, episode steps: 639, steps per second:  44, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.872 [0.000, 5.000],  loss: 0.020833, mae: 3.496926, mean_q: 4.216530, mean_eps: 0.100000
 1219318/2000000: episode: 1601, duration: 13.111s, episode steps: 582, steps per second:  44, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.971 [0.000, 5.000],  loss: 0.023589, mae: 3.500159, mean_q: 4.216851, mean_eps: 0.100000
 1219784/2000000: episode: 1602, duration: 10.510s, episode steps: 466, steps per second:  44, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.021988, mae: 3.465173, mean_q: 4.176375, mean_eps: 0.100000
 1220723/2000000: episode: 1603, duration: 21.226s, episode steps: 939, steps per second:  44, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.649 [0.000, 5.000],  loss: 0.020758, mae: 3.500950, mean_q: 4.221336, mean_eps: 0.100000
 1221164/2000000: episode: 1604, duration: 10.419s, episode steps: 441, steps per second:  42, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.168 [0.000, 5.000],  loss: 0.023975, mae: 3.504211, mean_q: 4.224900, mean_eps: 0.100000
 1221780/2000000: episode: 1605, duration: 14.203s, episode steps: 616, steps per second:  43, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.313 [0.000, 5.000],  loss: 0.023132, mae: 3.503460, mean_q: 4.222426, mean_eps: 0.100000
 1222781/2000000: episode: 1606, duration: 22.491s, episode steps: 1001, steps per second:  45, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.022201, mae: 3.488973, mean_q: 4.205518, mean_eps: 0.100000
 1223267/2000000: episode: 1607, duration: 10.676s, episode steps: 486, steps per second:  46, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.613 [0.000, 5.000],  loss: 0.023718, mae: 3.486936, mean_q: 4.204177, mean_eps: 0.100000
 1224767/2000000: episode: 1608, duration: 32.561s, episode steps: 1500, steps per second:  46, episode reward: 30.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.953 [0.000, 5.000],  loss: 0.021596, mae: 3.488330, mean_q: 4.205413, mean_eps: 0.100000
 1225215/2000000: episode: 1609, duration: 9.629s, episode steps: 448, steps per second:  47, episode reward: 11.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.022373, mae: 3.497972, mean_q: 4.219328, mean_eps: 0.100000
 1225751/2000000: episode: 1610, duration: 11.865s, episode steps: 536, steps per second:  45, episode reward: 15.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.664 [0.000, 5.000],  loss: 0.025793, mae: 3.472512, mean_q: 4.188742, mean_eps: 0.100000
 1226545/2000000: episode: 1611, duration: 17.713s, episode steps: 794, steps per second:  45, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.025074, mae: 3.464059, mean_q: 4.179602, mean_eps: 0.100000
 1227487/2000000: episode: 1612, duration: 20.006s, episode steps: 942, steps per second:  47, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.202 [0.000, 5.000],  loss: 0.022101, mae: 3.493155, mean_q: 4.210315, mean_eps: 0.100000
 1227984/2000000: episode: 1613, duration: 11.017s, episode steps: 497, steps per second:  45, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: 0.029031, mae: 3.459001, mean_q: 4.169718, mean_eps: 0.100000
 1228552/2000000: episode: 1614, duration: 12.038s, episode steps: 568, steps per second:  47, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.023090, mae: 3.495367, mean_q: 4.214264, mean_eps: 0.100000
 1229199/2000000: episode: 1615, duration: 13.814s, episode steps: 647, steps per second:  47, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.813 [0.000, 5.000],  loss: 0.024358, mae: 3.485380, mean_q: 4.201613, mean_eps: 0.100000
 1230145/2000000: episode: 1616, duration: 20.475s, episode steps: 946, steps per second:  46, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.822 [0.000, 5.000],  loss: 0.023255, mae: 3.477322, mean_q: 4.194301, mean_eps: 0.100000
 1230978/2000000: episode: 1617, duration: 19.283s, episode steps: 833, steps per second:  43, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.848 [0.000, 5.000],  loss: 0.027305, mae: 3.489293, mean_q: 4.206175, mean_eps: 0.100000
 1232182/2000000: episode: 1618, duration: 32.454s, episode steps: 1204, steps per second:  37, episode reward: 30.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.123 [0.000, 5.000],  loss: 0.021448, mae: 3.458426, mean_q: 4.168379, mean_eps: 0.100000
 1233345/2000000: episode: 1619, duration: 28.805s, episode steps: 1163, steps per second:  40, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.023265, mae: 3.473908, mean_q: 4.187814, mean_eps: 0.100000
 1234372/2000000: episode: 1620, duration: 23.989s, episode steps: 1027, steps per second:  43, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.023384, mae: 3.466567, mean_q: 4.177861, mean_eps: 0.100000
 1235169/2000000: episode: 1621, duration: 17.908s, episode steps: 797, steps per second:  45, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.309 [0.000, 5.000],  loss: 0.024107, mae: 3.476473, mean_q: 4.190470, mean_eps: 0.100000
 1235915/2000000: episode: 1622, duration: 17.019s, episode steps: 746, steps per second:  44, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.025754, mae: 3.512958, mean_q: 4.233425, mean_eps: 0.100000
 1236594/2000000: episode: 1623, duration: 17.584s, episode steps: 679, steps per second:  39, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.024970, mae: 3.502127, mean_q: 4.219075, mean_eps: 0.100000
 1237096/2000000: episode: 1624, duration: 12.134s, episode steps: 502, steps per second:  41, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.755 [0.000, 5.000],  loss: 0.026303, mae: 3.510318, mean_q: 4.229945, mean_eps: 0.100000
 1237720/2000000: episode: 1625, duration: 14.476s, episode steps: 624, steps per second:  43, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.697 [0.000, 5.000],  loss: 0.021638, mae: 3.503726, mean_q: 4.228143, mean_eps: 0.100000
 1238666/2000000: episode: 1626, duration: 21.251s, episode steps: 946, steps per second:  45, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.931 [0.000, 5.000],  loss: 0.021491, mae: 3.493149, mean_q: 4.209576, mean_eps: 0.100000
 1239779/2000000: episode: 1627, duration: 24.661s, episode steps: 1113, steps per second:  45, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.023713, mae: 3.500396, mean_q: 4.220066, mean_eps: 0.100000
 1240230/2000000: episode: 1628, duration: 9.692s, episode steps: 451, steps per second:  47, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.022526, mae: 3.508266, mean_q: 4.231525, mean_eps: 0.100000
 1241383/2000000: episode: 1629, duration: 25.217s, episode steps: 1153, steps per second:  46, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.954 [0.000, 5.000],  loss: 0.023814, mae: 3.499582, mean_q: 4.219727, mean_eps: 0.100000
 1241760/2000000: episode: 1630, duration: 8.602s, episode steps: 377, steps per second:  44, episode reward:  9.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.806 [0.000, 5.000],  loss: 0.025344, mae: 3.502499, mean_q: 4.222235, mean_eps: 0.100000
 1242649/2000000: episode: 1631, duration: 20.028s, episode steps: 889, steps per second:  44, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.172 [0.000, 5.000],  loss: 0.025898, mae: 3.516149, mean_q: 4.239436, mean_eps: 0.100000
 1243152/2000000: episode: 1632, duration: 10.647s, episode steps: 503, steps per second:  47, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.169 [0.000, 5.000],  loss: 0.023979, mae: 3.544875, mean_q: 4.276472, mean_eps: 0.100000
 1243945/2000000: episode: 1633, duration: 17.376s, episode steps: 793, steps per second:  46, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.686 [0.000, 5.000],  loss: 0.024017, mae: 3.518971, mean_q: 4.244554, mean_eps: 0.100000
 1244666/2000000: episode: 1634, duration: 15.692s, episode steps: 721, steps per second:  46, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.021444, mae: 3.488672, mean_q: 4.209037, mean_eps: 0.100000
 1245330/2000000: episode: 1635, duration: 14.822s, episode steps: 664, steps per second:  45, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.024008, mae: 3.507848, mean_q: 4.230046, mean_eps: 0.100000
 1246189/2000000: episode: 1636, duration: 18.880s, episode steps: 859, steps per second:  45, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.045 [0.000, 5.000],  loss: 0.024035, mae: 3.493560, mean_q: 4.218019, mean_eps: 0.100000
 1247388/2000000: episode: 1637, duration: 29.902s, episode steps: 1199, steps per second:  40, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.023196, mae: 3.514265, mean_q: 4.239490, mean_eps: 0.100000
 1248343/2000000: episode: 1638, duration: 22.469s, episode steps: 955, steps per second:  43, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.097 [0.000, 5.000],  loss: 0.022440, mae: 3.492334, mean_q: 4.212318, mean_eps: 0.100000
 1249089/2000000: episode: 1639, duration: 17.358s, episode steps: 746, steps per second:  43, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.022821, mae: 3.520619, mean_q: 4.246105, mean_eps: 0.100000
 1249871/2000000: episode: 1640, duration: 19.264s, episode steps: 782, steps per second:  41, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.041 [0.000, 5.000],  loss: 0.023256, mae: 3.489645, mean_q: 4.209213, mean_eps: 0.100000
 1250872/2000000: episode: 1641, duration: 23.419s, episode steps: 1001, steps per second:  43, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.022366, mae: 3.509442, mean_q: 4.233130, mean_eps: 0.100000
 1251634/2000000: episode: 1642, duration: 18.039s, episode steps: 762, steps per second:  42, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.024923, mae: 3.515427, mean_q: 4.241487, mean_eps: 0.100000
 1252159/2000000: episode: 1643, duration: 12.534s, episode steps: 525, steps per second:  42, episode reward: 14.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.914 [0.000, 5.000],  loss: 0.021967, mae: 3.497446, mean_q: 4.218763, mean_eps: 0.100000
 1253127/2000000: episode: 1644, duration: 22.273s, episode steps: 968, steps per second:  43, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.178 [0.000, 5.000],  loss: 0.022995, mae: 3.512022, mean_q: 4.236886, mean_eps: 0.100000
 1254037/2000000: episode: 1645, duration: 20.884s, episode steps: 910, steps per second:  44, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.114 [0.000, 5.000],  loss: 0.023531, mae: 3.502929, mean_q: 4.226190, mean_eps: 0.100000
 1255402/2000000: episode: 1646, duration: 30.782s, episode steps: 1365, steps per second:  44, episode reward: 38.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.755 [0.000, 5.000],  loss: 0.025343, mae: 3.521531, mean_q: 4.248800, mean_eps: 0.100000
 1256259/2000000: episode: 1647, duration: 18.423s, episode steps: 857, steps per second:  47, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.782 [0.000, 5.000],  loss: 0.023117, mae: 3.506958, mean_q: 4.229799, mean_eps: 0.100000
 1256867/2000000: episode: 1648, duration: 13.476s, episode steps: 608, steps per second:  45, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.043 [0.000, 5.000],  loss: 0.025534, mae: 3.547401, mean_q: 4.278334, mean_eps: 0.100000
 1257413/2000000: episode: 1649, duration: 12.430s, episode steps: 546, steps per second:  44, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.700 [0.000, 5.000],  loss: 0.024837, mae: 3.510763, mean_q: 4.234602, mean_eps: 0.100000
 1258224/2000000: episode: 1650, duration: 18.661s, episode steps: 811, steps per second:  43, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.025026, mae: 3.544722, mean_q: 4.273822, mean_eps: 0.100000
 1259168/2000000: episode: 1651, duration: 20.882s, episode steps: 944, steps per second:  45, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.027247, mae: 3.516772, mean_q: 4.240200, mean_eps: 0.100000
 1260175/2000000: episode: 1652, duration: 22.823s, episode steps: 1007, steps per second:  44, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.959 [0.000, 5.000],  loss: 0.024999, mae: 3.522506, mean_q: 4.246984, mean_eps: 0.100000
 1261044/2000000: episode: 1653, duration: 20.318s, episode steps: 869, steps per second:  43, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.743 [0.000, 5.000],  loss: 0.021986, mae: 3.520182, mean_q: 4.248951, mean_eps: 0.100000
 1261597/2000000: episode: 1654, duration: 13.419s, episode steps: 553, steps per second:  41, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.023066, mae: 3.535137, mean_q: 4.263464, mean_eps: 0.100000
 1262464/2000000: episode: 1655, duration: 22.615s, episode steps: 867, steps per second:  38, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.260 [0.000, 5.000],  loss: 0.021209, mae: 3.497633, mean_q: 4.217797, mean_eps: 0.100000
 1263277/2000000: episode: 1656, duration: 21.185s, episode steps: 813, steps per second:  38, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.990 [0.000, 5.000],  loss: 0.023854, mae: 3.519476, mean_q: 4.242476, mean_eps: 0.100000
 1264395/2000000: episode: 1657, duration: 26.641s, episode steps: 1118, steps per second:  42, episode reward: 30.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.023770, mae: 3.524424, mean_q: 4.249964, mean_eps: 0.100000
 1265021/2000000: episode: 1658, duration: 16.353s, episode steps: 626, steps per second:  38, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.856 [0.000, 5.000],  loss: 0.022846, mae: 3.525569, mean_q: 4.251654, mean_eps: 0.100000
 1265888/2000000: episode: 1659, duration: 20.380s, episode steps: 867, steps per second:  43, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.326 [0.000, 5.000],  loss: 0.025889, mae: 3.518078, mean_q: 4.243349, mean_eps: 0.100000
 1266784/2000000: episode: 1660, duration: 20.118s, episode steps: 896, steps per second:  45, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.028377, mae: 3.499910, mean_q: 4.221648, mean_eps: 0.100000
 1267674/2000000: episode: 1661, duration: 22.741s, episode steps: 890, steps per second:  39, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.022442, mae: 3.521075, mean_q: 4.246399, mean_eps: 0.100000
 1268444/2000000: episode: 1662, duration: 17.809s, episode steps: 770, steps per second:  43, episode reward: 22.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.022216, mae: 3.526527, mean_q: 4.253955, mean_eps: 0.100000
 1268998/2000000: episode: 1663, duration: 12.295s, episode steps: 554, steps per second:  45, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.829 [0.000, 5.000],  loss: 0.026456, mae: 3.517784, mean_q: 4.240084, mean_eps: 0.100000
 1270153/2000000: episode: 1664, duration: 25.437s, episode steps: 1155, steps per second:  45, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.939 [0.000, 5.000],  loss: 0.021023, mae: 3.529775, mean_q: 4.256484, mean_eps: 0.100000
 1271023/2000000: episode: 1665, duration: 18.787s, episode steps: 870, steps per second:  46, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.023626, mae: 3.539147, mean_q: 4.268833, mean_eps: 0.100000
 1271454/2000000: episode: 1666, duration: 8.981s, episode steps: 431, steps per second:  48, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.988 [0.000, 5.000],  loss: 0.025414, mae: 3.548075, mean_q: 4.279487, mean_eps: 0.100000
 1272307/2000000: episode: 1667, duration: 18.534s, episode steps: 853, steps per second:  46, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.029241, mae: 3.555523, mean_q: 4.284541, mean_eps: 0.100000
 1273123/2000000: episode: 1668, duration: 17.876s, episode steps: 816, steps per second:  46, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.902 [0.000, 5.000],  loss: 0.022649, mae: 3.539915, mean_q: 4.269046, mean_eps: 0.100000
 1273595/2000000: episode: 1669, duration: 10.766s, episode steps: 472, steps per second:  44, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.023579, mae: 3.542523, mean_q: 4.269348, mean_eps: 0.100000
 1274168/2000000: episode: 1670, duration: 12.569s, episode steps: 573, steps per second:  46, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.977 [0.000, 5.000],  loss: 0.026094, mae: 3.586375, mean_q: 4.325433, mean_eps: 0.100000
 1275239/2000000: episode: 1671, duration: 23.793s, episode steps: 1071, steps per second:  45, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.024648, mae: 3.544385, mean_q: 4.276001, mean_eps: 0.100000
 1276379/2000000: episode: 1672, duration: 24.396s, episode steps: 1140, steps per second:  47, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.884 [0.000, 5.000],  loss: 0.025070, mae: 3.553244, mean_q: 4.285019, mean_eps: 0.100000
 1277434/2000000: episode: 1673, duration: 23.693s, episode steps: 1055, steps per second:  45, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.835 [0.000, 5.000],  loss: 0.024189, mae: 3.529045, mean_q: 4.256824, mean_eps: 0.100000
 1278284/2000000: episode: 1674, duration: 20.342s, episode steps: 850, steps per second:  42, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.021459, mae: 3.531725, mean_q: 4.258334, mean_eps: 0.100000
 1279076/2000000: episode: 1675, duration: 18.511s, episode steps: 792, steps per second:  43, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.170 [0.000, 5.000],  loss: 0.023089, mae: 3.532177, mean_q: 4.257302, mean_eps: 0.100000
 1280358/2000000: episode: 1676, duration: 29.158s, episode steps: 1282, steps per second:  44, episode reward: 25.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.245 [0.000, 5.000],  loss: 0.023617, mae: 3.516736, mean_q: 4.240439, mean_eps: 0.100000
 1280721/2000000: episode: 1677, duration: 8.688s, episode steps: 363, steps per second:  42, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.021038, mae: 3.533545, mean_q: 4.259818, mean_eps: 0.100000
 1281603/2000000: episode: 1678, duration: 19.516s, episode steps: 882, steps per second:  45, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.816 [0.000, 5.000],  loss: 0.022918, mae: 3.511005, mean_q: 4.233133, mean_eps: 0.100000
 1282650/2000000: episode: 1679, duration: 23.005s, episode steps: 1047, steps per second:  46, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.848 [0.000, 5.000],  loss: 0.022098, mae: 3.528575, mean_q: 4.255283, mean_eps: 0.100000
 1283265/2000000: episode: 1680, duration: 13.480s, episode steps: 615, steps per second:  46, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.018 [0.000, 5.000],  loss: 0.023959, mae: 3.520595, mean_q: 4.245081, mean_eps: 0.100000
 1284032/2000000: episode: 1681, duration: 18.042s, episode steps: 767, steps per second:  43, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.722 [0.000, 5.000],  loss: 0.021344, mae: 3.551317, mean_q: 4.282518, mean_eps: 0.100000
 1284571/2000000: episode: 1682, duration: 12.244s, episode steps: 539, steps per second:  44, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.160 [0.000, 5.000],  loss: 0.023743, mae: 3.520108, mean_q: 4.243696, mean_eps: 0.100000
 1285379/2000000: episode: 1683, duration: 18.530s, episode steps: 808, steps per second:  44, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.022228, mae: 3.522760, mean_q: 4.248852, mean_eps: 0.100000
 1286427/2000000: episode: 1684, duration: 24.753s, episode steps: 1048, steps per second:  42, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.642 [0.000, 5.000],  loss: 0.021887, mae: 3.529735, mean_q: 4.256167, mean_eps: 0.100000
 1287588/2000000: episode: 1685, duration: 25.754s, episode steps: 1161, steps per second:  45, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.059 [0.000, 5.000],  loss: 0.020862, mae: 3.533356, mean_q: 4.263146, mean_eps: 0.100000
 1288259/2000000: episode: 1686, duration: 14.784s, episode steps: 671, steps per second:  45, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.020286, mae: 3.527916, mean_q: 4.256763, mean_eps: 0.100000
 1289084/2000000: episode: 1687, duration: 19.306s, episode steps: 825, steps per second:  43, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.262 [0.000, 5.000],  loss: 0.018460, mae: 3.529195, mean_q: 4.255422, mean_eps: 0.100000
 1290022/2000000: episode: 1688, duration: 20.751s, episode steps: 938, steps per second:  45, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.023040, mae: 3.512632, mean_q: 4.234843, mean_eps: 0.100000
 1291300/2000000: episode: 1689, duration: 27.355s, episode steps: 1278, steps per second:  47, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.752 [0.000, 5.000],  loss: 0.024074, mae: 3.519345, mean_q: 4.242207, mean_eps: 0.100000
 1292187/2000000: episode: 1690, duration: 19.669s, episode steps: 887, steps per second:  45, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.024913, mae: 3.519373, mean_q: 4.242942, mean_eps: 0.100000
 1292985/2000000: episode: 1691, duration: 17.815s, episode steps: 798, steps per second:  45, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.782 [0.000, 5.000],  loss: 0.021271, mae: 3.528998, mean_q: 4.257484, mean_eps: 0.100000
 1293760/2000000: episode: 1692, duration: 18.144s, episode steps: 775, steps per second:  43, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.190 [0.000, 5.000],  loss: 0.020853, mae: 3.550139, mean_q: 4.280889, mean_eps: 0.100000
 1295231/2000000: episode: 1693, duration: 35.751s, episode steps: 1471, steps per second:  41, episode reward: 33.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.023936, mae: 3.529390, mean_q: 4.256866, mean_eps: 0.100000
 1295726/2000000: episode: 1694, duration: 11.964s, episode steps: 495, steps per second:  41, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.802 [0.000, 5.000],  loss: 0.021450, mae: 3.521151, mean_q: 4.246758, mean_eps: 0.100000
 1296427/2000000: episode: 1695, duration: 15.742s, episode steps: 701, steps per second:  45, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.128 [0.000, 5.000],  loss: 0.017723, mae: 3.520116, mean_q: 4.246243, mean_eps: 0.100000
 1297649/2000000: episode: 1696, duration: 27.978s, episode steps: 1222, steps per second:  44, episode reward: 32.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.012 [0.000, 5.000],  loss: 0.021729, mae: 3.507723, mean_q: 4.230940, mean_eps: 0.100000
 1298303/2000000: episode: 1697, duration: 15.322s, episode steps: 654, steps per second:  43, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.020672, mae: 3.504412, mean_q: 4.226721, mean_eps: 0.100000
 1299017/2000000: episode: 1698, duration: 17.116s, episode steps: 714, steps per second:  42, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.833 [0.000, 5.000],  loss: 0.026278, mae: 3.500526, mean_q: 4.220075, mean_eps: 0.100000
 1300268/2000000: episode: 1699, duration: 28.774s, episode steps: 1251, steps per second:  43, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.023163, mae: 3.513176, mean_q: 4.235928, mean_eps: 0.100000
 1300918/2000000: episode: 1700, duration: 15.091s, episode steps: 650, steps per second:  43, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.018646, mae: 3.509525, mean_q: 4.232903, mean_eps: 0.100000
 1301819/2000000: episode: 1701, duration: 20.815s, episode steps: 901, steps per second:  43, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.049 [0.000, 5.000],  loss: 0.022642, mae: 3.513853, mean_q: 4.237438, mean_eps: 0.100000
 1302923/2000000: episode: 1702, duration: 24.718s, episode steps: 1104, steps per second:  45, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.023045, mae: 3.513555, mean_q: 4.236757, mean_eps: 0.100000
 1303918/2000000: episode: 1703, duration: 21.949s, episode steps: 995, steps per second:  45, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.317 [0.000, 5.000],  loss: 0.021691, mae: 3.510023, mean_q: 4.234913, mean_eps: 0.100000
 1304865/2000000: episode: 1704, duration: 21.752s, episode steps: 947, steps per second:  44, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.021270, mae: 3.512523, mean_q: 4.236276, mean_eps: 0.100000
 1305357/2000000: episode: 1705, duration: 11.210s, episode steps: 492, steps per second:  44, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.268 [0.000, 5.000],  loss: 0.020872, mae: 3.491190, mean_q: 4.213586, mean_eps: 0.100000
 1305949/2000000: episode: 1706, duration: 12.982s, episode steps: 592, steps per second:  46, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.166 [0.000, 5.000],  loss: 0.024127, mae: 3.498352, mean_q: 4.219727, mean_eps: 0.100000
 1307064/2000000: episode: 1707, duration: 24.747s, episode steps: 1115, steps per second:  45, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.023266, mae: 3.497005, mean_q: 4.219118, mean_eps: 0.100000
 1307745/2000000: episode: 1708, duration: 15.272s, episode steps: 681, steps per second:  45, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.022960, mae: 3.486444, mean_q: 4.204066, mean_eps: 0.100000
 1308590/2000000: episode: 1709, duration: 18.992s, episode steps: 845, steps per second:  44, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.021321, mae: 3.499636, mean_q: 4.219373, mean_eps: 0.100000
 1309536/2000000: episode: 1710, duration: 21.738s, episode steps: 946, steps per second:  44, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.021938, mae: 3.501254, mean_q: 4.221304, mean_eps: 0.100000
 1310311/2000000: episode: 1711, duration: 18.674s, episode steps: 775, steps per second:  42, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.670 [0.000, 5.000],  loss: 0.021047, mae: 3.511617, mean_q: 4.235743, mean_eps: 0.100000
 1310954/2000000: episode: 1712, duration: 15.223s, episode steps: 643, steps per second:  42, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.737 [0.000, 5.000],  loss: 0.019099, mae: 3.463494, mean_q: 4.178344, mean_eps: 0.100000
 1311430/2000000: episode: 1713, duration: 10.729s, episode steps: 476, steps per second:  44, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.975 [0.000, 5.000],  loss: 0.021767, mae: 3.473979, mean_q: 4.189267, mean_eps: 0.100000
 1311919/2000000: episode: 1714, duration: 11.315s, episode steps: 489, steps per second:  43, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.930 [0.000, 5.000],  loss: 0.018227, mae: 3.481923, mean_q: 4.196895, mean_eps: 0.100000
 1312441/2000000: episode: 1715, duration: 12.433s, episode steps: 522, steps per second:  42, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.023873, mae: 3.498404, mean_q: 4.215991, mean_eps: 0.100000
 1313077/2000000: episode: 1716, duration: 14.519s, episode steps: 636, steps per second:  44, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.682 [0.000, 5.000],  loss: 0.018730, mae: 3.452738, mean_q: 4.163318, mean_eps: 0.100000
 1313648/2000000: episode: 1717, duration: 12.893s, episode steps: 571, steps per second:  44, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.024563, mae: 3.476438, mean_q: 4.193186, mean_eps: 0.100000
 1314399/2000000: episode: 1718, duration: 16.983s, episode steps: 751, steps per second:  44, episode reward: 21.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.026169, mae: 3.476484, mean_q: 4.190477, mean_eps: 0.100000
 1315621/2000000: episode: 1719, duration: 29.119s, episode steps: 1222, steps per second:  42, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.020717, mae: 3.467382, mean_q: 4.180571, mean_eps: 0.100000
 1316596/2000000: episode: 1720, duration: 22.300s, episode steps: 975, steps per second:  44, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.892 [0.000, 5.000],  loss: 0.022512, mae: 3.458273, mean_q: 4.170194, mean_eps: 0.100000
 1318082/2000000: episode: 1721, duration: 33.648s, episode steps: 1486, steps per second:  44, episode reward: 31.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.057 [0.000, 5.000],  loss: 0.022907, mae: 3.450922, mean_q: 4.160141, mean_eps: 0.100000
 1318764/2000000: episode: 1722, duration: 14.697s, episode steps: 682, steps per second:  46, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.905 [0.000, 5.000],  loss: 0.025891, mae: 3.472124, mean_q: 4.184646, mean_eps: 0.100000
 1319564/2000000: episode: 1723, duration: 17.242s, episode steps: 800, steps per second:  46, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.964 [0.000, 5.000],  loss: 0.025495, mae: 3.476005, mean_q: 4.190019, mean_eps: 0.100000
 1320104/2000000: episode: 1724, duration: 11.890s, episode steps: 540, steps per second:  45, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.794 [0.000, 5.000],  loss: 0.020115, mae: 3.464583, mean_q: 4.180587, mean_eps: 0.100000
 1320849/2000000: episode: 1725, duration: 16.667s, episode steps: 745, steps per second:  45, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.019713, mae: 3.425338, mean_q: 4.131293, mean_eps: 0.100000
 1321339/2000000: episode: 1726, duration: 10.776s, episode steps: 490, steps per second:  45, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.022565, mae: 3.429973, mean_q: 4.134873, mean_eps: 0.100000
 1322115/2000000: episode: 1727, duration: 16.810s, episode steps: 776, steps per second:  46, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.020610, mae: 3.430983, mean_q: 4.138683, mean_eps: 0.100000
 1322614/2000000: episode: 1728, duration: 11.063s, episode steps: 499, steps per second:  45, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.693 [0.000, 5.000],  loss: 0.019455, mae: 3.426654, mean_q: 4.133239, mean_eps: 0.100000
 1323500/2000000: episode: 1729, duration: 19.002s, episode steps: 886, steps per second:  47, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.296 [0.000, 5.000],  loss: 0.027809, mae: 3.432057, mean_q: 4.139891, mean_eps: 0.100000
 1324007/2000000: episode: 1730, duration: 11.311s, episode steps: 507, steps per second:  45, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.008 [0.000, 5.000],  loss: 0.023905, mae: 3.420430, mean_q: 4.123114, mean_eps: 0.100000
 1324946/2000000: episode: 1731, duration: 21.466s, episode steps: 939, steps per second:  44, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.783 [0.000, 5.000],  loss: 0.024291, mae: 3.450865, mean_q: 4.160811, mean_eps: 0.100000
 1325579/2000000: episode: 1732, duration: 15.737s, episode steps: 633, steps per second:  40, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.885 [0.000, 5.000],  loss: 0.019286, mae: 3.450680, mean_q: 4.159538, mean_eps: 0.100000
 1326380/2000000: episode: 1733, duration: 20.451s, episode steps: 801, steps per second:  39, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.020974, mae: 3.415189, mean_q: 4.117697, mean_eps: 0.100000
 1327662/2000000: episode: 1734, duration: 29.561s, episode steps: 1282, steps per second:  43, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.764 [0.000, 5.000],  loss: 0.023614, mae: 3.428281, mean_q: 4.133291, mean_eps: 0.100000
 1328450/2000000: episode: 1735, duration: 18.701s, episode steps: 788, steps per second:  42, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.251 [0.000, 5.000],  loss: 0.021051, mae: 3.413939, mean_q: 4.119638, mean_eps: 0.100000
 1329335/2000000: episode: 1736, duration: 20.638s, episode steps: 885, steps per second:  43, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.018679, mae: 3.444719, mean_q: 4.154234, mean_eps: 0.100000
 1329971/2000000: episode: 1737, duration: 14.708s, episode steps: 636, steps per second:  43, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.970 [0.000, 5.000],  loss: 0.023225, mae: 3.443204, mean_q: 4.151683, mean_eps: 0.100000
 1330743/2000000: episode: 1738, duration: 18.057s, episode steps: 772, steps per second:  43, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.024119, mae: 3.419726, mean_q: 4.124243, mean_eps: 0.100000
 1331549/2000000: episode: 1739, duration: 18.829s, episode steps: 806, steps per second:  43, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.725 [0.000, 5.000],  loss: 0.022709, mae: 3.431378, mean_q: 4.137611, mean_eps: 0.100000
 1331949/2000000: episode: 1740, duration: 8.840s, episode steps: 400, steps per second:  45, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.021273, mae: 3.415836, mean_q: 4.122472, mean_eps: 0.100000
 1332725/2000000: episode: 1741, duration: 17.593s, episode steps: 776, steps per second:  44, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.019605, mae: 3.419868, mean_q: 4.123785, mean_eps: 0.100000
 1333679/2000000: episode: 1742, duration: 21.552s, episode steps: 954, steps per second:  44, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.955 [0.000, 5.000],  loss: 0.020852, mae: 3.431900, mean_q: 4.138116, mean_eps: 0.100000
 1334427/2000000: episode: 1743, duration: 16.561s, episode steps: 748, steps per second:  45, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.022302, mae: 3.428692, mean_q: 4.134041, mean_eps: 0.100000
 1335191/2000000: episode: 1744, duration: 16.152s, episode steps: 764, steps per second:  47, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.026370, mae: 3.435622, mean_q: 4.143805, mean_eps: 0.100000
 1336118/2000000: episode: 1745, duration: 20.126s, episode steps: 927, steps per second:  46, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.020670, mae: 3.425618, mean_q: 4.132203, mean_eps: 0.100000
 1337223/2000000: episode: 1746, duration: 23.944s, episode steps: 1105, steps per second:  46, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.695 [0.000, 5.000],  loss: 0.023778, mae: 3.448766, mean_q: 4.158513, mean_eps: 0.100000
 1337996/2000000: episode: 1747, duration: 16.644s, episode steps: 773, steps per second:  46, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.023691, mae: 3.432533, mean_q: 4.138212, mean_eps: 0.100000
 1339065/2000000: episode: 1748, duration: 23.347s, episode steps: 1069, steps per second:  46, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.148 [0.000, 5.000],  loss: 0.020574, mae: 3.434522, mean_q: 4.142454, mean_eps: 0.100000
 1340090/2000000: episode: 1749, duration: 22.946s, episode steps: 1025, steps per second:  45, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.023648, mae: 3.452299, mean_q: 4.164364, mean_eps: 0.100000
 1341231/2000000: episode: 1750, duration: 26.805s, episode steps: 1141, steps per second:  43, episode reward: 31.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.023890, mae: 3.436113, mean_q: 4.143046, mean_eps: 0.100000
 1341700/2000000: episode: 1751, duration: 11.740s, episode steps: 469, steps per second:  40, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.222 [0.000, 5.000],  loss: 0.024027, mae: 3.419040, mean_q: 4.122168, mean_eps: 0.100000
 1342381/2000000: episode: 1752, duration: 16.426s, episode steps: 681, steps per second:  41, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.698 [0.000, 5.000],  loss: 0.019356, mae: 3.390655, mean_q: 4.090147, mean_eps: 0.100000
 1343251/2000000: episode: 1753, duration: 19.388s, episode steps: 870, steps per second:  45, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.899 [0.000, 5.000],  loss: 0.022830, mae: 3.415356, mean_q: 4.118239, mean_eps: 0.100000
 1343640/2000000: episode: 1754, duration: 8.765s, episode steps: 389, steps per second:  44, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.023409, mae: 3.455025, mean_q: 4.166729, mean_eps: 0.100000
 1344198/2000000: episode: 1755, duration: 12.142s, episode steps: 558, steps per second:  46, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.158 [0.000, 5.000],  loss: 0.020584, mae: 3.411674, mean_q: 4.116846, mean_eps: 0.100000
 1344718/2000000: episode: 1756, duration: 11.186s, episode steps: 520, steps per second:  46, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.313 [0.000, 5.000],  loss: 0.018757, mae: 3.393973, mean_q: 4.095074, mean_eps: 0.100000
 1345490/2000000: episode: 1757, duration: 16.963s, episode steps: 772, steps per second:  46, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.991 [0.000, 5.000],  loss: 0.024117, mae: 3.395954, mean_q: 4.097759, mean_eps: 0.100000
 1346326/2000000: episode: 1758, duration: 18.601s, episode steps: 836, steps per second:  45, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.043 [0.000, 5.000],  loss: 0.022175, mae: 3.401485, mean_q: 4.102386, mean_eps: 0.100000
 1347274/2000000: episode: 1759, duration: 22.841s, episode steps: 948, steps per second:  42, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.021995, mae: 3.404289, mean_q: 4.105490, mean_eps: 0.100000
 1347897/2000000: episode: 1760, duration: 14.288s, episode steps: 623, steps per second:  44, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.020351, mae: 3.397866, mean_q: 4.099792, mean_eps: 0.100000
 1349014/2000000: episode: 1761, duration: 25.348s, episode steps: 1117, steps per second:  44, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.983 [0.000, 5.000],  loss: 0.023211, mae: 3.404657, mean_q: 4.104244, mean_eps: 0.100000
 1350032/2000000: episode: 1762, duration: 23.482s, episode steps: 1018, steps per second:  43, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.024410, mae: 3.405554, mean_q: 4.107543, mean_eps: 0.100000
 1351037/2000000: episode: 1763, duration: 21.665s, episode steps: 1005, steps per second:  46, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.710 [0.000, 5.000],  loss: 0.023335, mae: 3.389423, mean_q: 4.089213, mean_eps: 0.100000
 1351888/2000000: episode: 1764, duration: 18.409s, episode steps: 851, steps per second:  46, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.216 [0.000, 5.000],  loss: 0.022365, mae: 3.381266, mean_q: 4.078831, mean_eps: 0.100000
 1352633/2000000: episode: 1765, duration: 16.619s, episode steps: 745, steps per second:  45, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.019597, mae: 3.375943, mean_q: 4.071967, mean_eps: 0.100000
 1353305/2000000: episode: 1766, duration: 15.064s, episode steps: 672, steps per second:  45, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.796 [0.000, 5.000],  loss: 0.021411, mae: 3.385085, mean_q: 4.085075, mean_eps: 0.100000
 1354105/2000000: episode: 1767, duration: 17.897s, episode steps: 800, steps per second:  45, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.025567, mae: 3.372448, mean_q: 4.067854, mean_eps: 0.100000
 1354723/2000000: episode: 1768, duration: 13.372s, episode steps: 618, steps per second:  46, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.990 [0.000, 5.000],  loss: 0.024386, mae: 3.363037, mean_q: 4.057040, mean_eps: 0.100000
 1355347/2000000: episode: 1769, duration: 14.283s, episode steps: 624, steps per second:  44, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.022972, mae: 3.425818, mean_q: 4.130383, mean_eps: 0.100000
 1356337/2000000: episode: 1770, duration: 22.807s, episode steps: 990, steps per second:  43, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.019811, mae: 3.405356, mean_q: 4.107242, mean_eps: 0.100000
 1356911/2000000: episode: 1771, duration: 13.087s, episode steps: 574, steps per second:  44, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.256 [0.000, 5.000],  loss: 0.025613, mae: 3.404240, mean_q: 4.103396, mean_eps: 0.100000
 1357924/2000000: episode: 1772, duration: 25.143s, episode steps: 1013, steps per second:  40, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.033 [0.000, 5.000],  loss: 0.024640, mae: 3.416323, mean_q: 4.119579, mean_eps: 0.100000
 1358971/2000000: episode: 1773, duration: 23.862s, episode steps: 1047, steps per second:  44, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.022510, mae: 3.407910, mean_q: 4.110613, mean_eps: 0.100000
 1359959/2000000: episode: 1774, duration: 22.319s, episode steps: 988, steps per second:  44, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.642 [0.000, 5.000],  loss: 0.020864, mae: 3.395058, mean_q: 4.093606, mean_eps: 0.100000
 1360730/2000000: episode: 1775, duration: 17.830s, episode steps: 771, steps per second:  43, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.139 [0.000, 5.000],  loss: 0.020423, mae: 3.420953, mean_q: 4.129646, mean_eps: 0.100000
 1361836/2000000: episode: 1776, duration: 24.580s, episode steps: 1106, steps per second:  45, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.021094, mae: 3.410715, mean_q: 4.114634, mean_eps: 0.100000
 1362939/2000000: episode: 1777, duration: 25.343s, episode steps: 1103, steps per second:  44, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.022015, mae: 3.402919, mean_q: 4.105432, mean_eps: 0.100000
 1363436/2000000: episode: 1778, duration: 11.956s, episode steps: 497, steps per second:  42, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.163 [0.000, 5.000],  loss: 0.021689, mae: 3.410999, mean_q: 4.117290, mean_eps: 0.100000
 1364787/2000000: episode: 1779, duration: 31.358s, episode steps: 1351, steps per second:  43, episode reward: 30.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.022703, mae: 3.424284, mean_q: 4.130495, mean_eps: 0.100000
 1366562/2000000: episode: 1780, duration: 40.304s, episode steps: 1775, steps per second:  44, episode reward: 34.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.040 [0.000, 5.000],  loss: 0.023733, mae: 3.409051, mean_q: 4.111586, mean_eps: 0.100000
 1367561/2000000: episode: 1781, duration: 21.417s, episode steps: 999, steps per second:  47, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.021792, mae: 3.391094, mean_q: 4.089828, mean_eps: 0.100000
 1368273/2000000: episode: 1782, duration: 16.013s, episode steps: 712, steps per second:  44, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.024681, mae: 3.394534, mean_q: 4.094582, mean_eps: 0.100000
 1368965/2000000: episode: 1783, duration: 15.068s, episode steps: 692, steps per second:  46, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.029585, mae: 3.389472, mean_q: 4.085298, mean_eps: 0.100000
 1369516/2000000: episode: 1784, duration: 11.773s, episode steps: 551, steps per second:  47, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.799 [0.000, 5.000],  loss: 0.025409, mae: 3.407058, mean_q: 4.106613, mean_eps: 0.100000
 1370062/2000000: episode: 1785, duration: 11.592s, episode steps: 546, steps per second:  47, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.974 [0.000, 5.000],  loss: 0.024320, mae: 3.379047, mean_q: 4.073220, mean_eps: 0.100000
 1370646/2000000: episode: 1786, duration: 13.013s, episode steps: 584, steps per second:  45, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.077 [0.000, 5.000],  loss: 0.022110, mae: 3.415625, mean_q: 4.123285, mean_eps: 0.100000
 1371804/2000000: episode: 1787, duration: 25.129s, episode steps: 1158, steps per second:  46, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.023134, mae: 3.397830, mean_q: 4.097993, mean_eps: 0.100000
 1373012/2000000: episode: 1788, duration: 28.251s, episode steps: 1208, steps per second:  43, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.871 [0.000, 5.000],  loss: 0.022889, mae: 3.413909, mean_q: 4.117429, mean_eps: 0.100000
 1373559/2000000: episode: 1789, duration: 13.859s, episode steps: 547, steps per second:  39, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.517 [0.000, 5.000],  loss: 0.024193, mae: 3.410522, mean_q: 4.112330, mean_eps: 0.100000
 1373931/2000000: episode: 1790, duration: 9.651s, episode steps: 372, steps per second:  39, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.145 [0.000, 5.000],  loss: 0.018526, mae: 3.402383, mean_q: 4.105449, mean_eps: 0.100000
 1374823/2000000: episode: 1791, duration: 21.872s, episode steps: 892, steps per second:  41, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.675 [0.000, 5.000],  loss: 0.021813, mae: 3.429296, mean_q: 4.136708, mean_eps: 0.100000
 1375682/2000000: episode: 1792, duration: 19.704s, episode steps: 859, steps per second:  44, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.020448, mae: 3.393036, mean_q: 4.092990, mean_eps: 0.100000
 1376322/2000000: episode: 1793, duration: 15.078s, episode steps: 640, steps per second:  42, episode reward: 17.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.022 [0.000, 5.000],  loss: 0.020228, mae: 3.401151, mean_q: 4.101305, mean_eps: 0.100000
 1377430/2000000: episode: 1794, duration: 24.697s, episode steps: 1108, steps per second:  45, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.020899, mae: 3.410267, mean_q: 4.113008, mean_eps: 0.100000
 1378463/2000000: episode: 1795, duration: 23.930s, episode steps: 1033, steps per second:  43, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.024240, mae: 3.394682, mean_q: 4.097107, mean_eps: 0.100000
 1378836/2000000: episode: 1796, duration: 9.019s, episode steps: 373, steps per second:  41, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.172 [0.000, 5.000],  loss: 0.020006, mae: 3.379255, mean_q: 4.081200, mean_eps: 0.100000
 1379344/2000000: episode: 1797, duration: 11.934s, episode steps: 508, steps per second:  43, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.075 [0.000, 5.000],  loss: 0.023581, mae: 3.379359, mean_q: 4.075732, mean_eps: 0.100000
 1380166/2000000: episode: 1798, duration: 18.820s, episode steps: 822, steps per second:  44, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.821 [0.000, 5.000],  loss: 0.021666, mae: 3.384020, mean_q: 4.081630, mean_eps: 0.100000
 1381452/2000000: episode: 1799, duration: 30.171s, episode steps: 1286, steps per second:  43, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.247 [0.000, 5.000],  loss: 0.020821, mae: 3.361467, mean_q: 4.056898, mean_eps: 0.100000
 1382009/2000000: episode: 1800, duration: 12.890s, episode steps: 557, steps per second:  43, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.959 [0.000, 5.000],  loss: 0.019873, mae: 3.388134, mean_q: 4.088324, mean_eps: 0.100000
 1383177/2000000: episode: 1801, duration: 25.652s, episode steps: 1168, steps per second:  46, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.022784, mae: 3.384735, mean_q: 4.083069, mean_eps: 0.100000
 1383701/2000000: episode: 1802, duration: 11.871s, episode steps: 524, steps per second:  44, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.784 [0.000, 5.000],  loss: 0.021322, mae: 3.380779, mean_q: 4.077133, mean_eps: 0.100000
 1384273/2000000: episode: 1803, duration: 12.751s, episode steps: 572, steps per second:  45, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.696 [0.000, 5.000],  loss: 0.022970, mae: 3.409716, mean_q: 4.113726, mean_eps: 0.100000
 1385215/2000000: episode: 1804, duration: 21.297s, episode steps: 942, steps per second:  44, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.896 [0.000, 5.000],  loss: 0.022721, mae: 3.388347, mean_q: 4.087307, mean_eps: 0.100000
 1385791/2000000: episode: 1805, duration: 12.866s, episode steps: 576, steps per second:  45, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.122 [0.000, 5.000],  loss: 0.021008, mae: 3.405384, mean_q: 4.105526, mean_eps: 0.100000
 1386841/2000000: episode: 1806, duration: 24.157s, episode steps: 1050, steps per second:  43, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.026169, mae: 3.394268, mean_q: 4.097553, mean_eps: 0.100000
 1387325/2000000: episode: 1807, duration: 11.303s, episode steps: 484, steps per second:  43, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.022251, mae: 3.382678, mean_q: 4.080582, mean_eps: 0.100000
 1388500/2000000: episode: 1808, duration: 27.124s, episode steps: 1175, steps per second:  43, episode reward: 18.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.022500, mae: 3.398139, mean_q: 4.099044, mean_eps: 0.100000
 1389390/2000000: episode: 1809, duration: 22.039s, episode steps: 890, steps per second:  40, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.070 [0.000, 5.000],  loss: 0.022661, mae: 3.392671, mean_q: 4.091595, mean_eps: 0.100000
 1390043/2000000: episode: 1810, duration: 15.636s, episode steps: 653, steps per second:  42, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.885 [0.000, 5.000],  loss: 0.022732, mae: 3.377894, mean_q: 4.073968, mean_eps: 0.100000
 1390423/2000000: episode: 1811, duration: 8.474s, episode steps: 380, steps per second:  45, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.687 [0.000, 5.000],  loss: 0.020407, mae: 3.391628, mean_q: 4.093265, mean_eps: 0.100000
 1391301/2000000: episode: 1812, duration: 19.987s, episode steps: 878, steps per second:  44, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.020888, mae: 3.376870, mean_q: 4.073206, mean_eps: 0.100000
 1392324/2000000: episode: 1813, duration: 23.577s, episode steps: 1023, steps per second:  43, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.764 [0.000, 5.000],  loss: 0.018958, mae: 3.356872, mean_q: 4.049016, mean_eps: 0.100000
 1393365/2000000: episode: 1814, duration: 23.773s, episode steps: 1041, steps per second:  44, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.022451, mae: 3.374143, mean_q: 4.068885, mean_eps: 0.100000
 1394042/2000000: episode: 1815, duration: 16.138s, episode steps: 677, steps per second:  42, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.089 [0.000, 5.000],  loss: 0.022580, mae: 3.388584, mean_q: 4.087203, mean_eps: 0.100000
 1394841/2000000: episode: 1816, duration: 20.531s, episode steps: 799, steps per second:  39, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.606 [0.000, 5.000],  loss: 0.019553, mae: 3.374340, mean_q: 4.069183, mean_eps: 0.100000
 1395695/2000000: episode: 1817, duration: 21.353s, episode steps: 854, steps per second:  40, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.760 [0.000, 5.000],  loss: 0.019124, mae: 3.379826, mean_q: 4.079913, mean_eps: 0.100000
 1396433/2000000: episode: 1818, duration: 18.892s, episode steps: 738, steps per second:  39, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.022966, mae: 3.356188, mean_q: 4.047932, mean_eps: 0.100000
 1397120/2000000: episode: 1819, duration: 16.407s, episode steps: 687, steps per second:  42, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.239 [0.000, 5.000],  loss: 0.018579, mae: 3.348984, mean_q: 4.039106, mean_eps: 0.100000
 1397996/2000000: episode: 1820, duration: 20.128s, episode steps: 876, steps per second:  44, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.841 [0.000, 5.000],  loss: 0.020318, mae: 3.357349, mean_q: 4.048474, mean_eps: 0.100000
 1398684/2000000: episode: 1821, duration: 16.044s, episode steps: 688, steps per second:  43, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.020843, mae: 3.346061, mean_q: 4.035392, mean_eps: 0.100000
 1399577/2000000: episode: 1822, duration: 21.431s, episode steps: 893, steps per second:  42, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.823 [0.000, 5.000],  loss: 0.021683, mae: 3.367940, mean_q: 4.062671, mean_eps: 0.100000
 1400669/2000000: episode: 1823, duration: 25.934s, episode steps: 1092, steps per second:  42, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.945 [0.000, 5.000],  loss: 0.022308, mae: 3.360145, mean_q: 4.053989, mean_eps: 0.100000
 1401269/2000000: episode: 1824, duration: 15.009s, episode steps: 600, steps per second:  40, episode reward: 16.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.022834, mae: 3.392505, mean_q: 4.090283, mean_eps: 0.100000
 1402426/2000000: episode: 1825, duration: 27.768s, episode steps: 1157, steps per second:  42, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.023395, mae: 3.371053, mean_q: 4.063318, mean_eps: 0.100000
 1403047/2000000: episode: 1826, duration: 16.102s, episode steps: 621, steps per second:  39, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.709 [0.000, 5.000],  loss: 0.021404, mae: 3.353233, mean_q: 4.042425, mean_eps: 0.100000
 1403550/2000000: episode: 1827, duration: 12.965s, episode steps: 503, steps per second:  39, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.020640, mae: 3.369230, mean_q: 4.062127, mean_eps: 0.100000
 1404479/2000000: episode: 1828, duration: 23.307s, episode steps: 929, steps per second:  40, episode reward: 29.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.022192, mae: 3.365838, mean_q: 4.056928, mean_eps: 0.100000
 1405388/2000000: episode: 1829, duration: 22.981s, episode steps: 909, steps per second:  40, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.023184, mae: 3.363672, mean_q: 4.055597, mean_eps: 0.100000
 1406262/2000000: episode: 1830, duration: 21.181s, episode steps: 874, steps per second:  41, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.207 [0.000, 5.000],  loss: 0.020458, mae: 3.366268, mean_q: 4.060461, mean_eps: 0.100000
 1407177/2000000: episode: 1831, duration: 22.492s, episode steps: 915, steps per second:  41, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.022029, mae: 3.362555, mean_q: 4.055837, mean_eps: 0.100000
 1408044/2000000: episode: 1832, duration: 20.412s, episode steps: 867, steps per second:  42, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.291 [0.000, 5.000],  loss: 0.021010, mae: 3.345888, mean_q: 4.033702, mean_eps: 0.100000
 1408868/2000000: episode: 1833, duration: 20.451s, episode steps: 824, steps per second:  40, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.021515, mae: 3.353247, mean_q: 4.042764, mean_eps: 0.100000
 1409767/2000000: episode: 1834, duration: 22.273s, episode steps: 899, steps per second:  40, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.020927, mae: 3.340999, mean_q: 4.029330, mean_eps: 0.100000
 1410905/2000000: episode: 1835, duration: 28.363s, episode steps: 1138, steps per second:  40, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.020876, mae: 3.356784, mean_q: 4.048967, mean_eps: 0.100000
 1411741/2000000: episode: 1836, duration: 20.606s, episode steps: 836, steps per second:  41, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.191 [0.000, 5.000],  loss: 0.021493, mae: 3.345912, mean_q: 4.033593, mean_eps: 0.100000
 1412377/2000000: episode: 1837, duration: 15.193s, episode steps: 636, steps per second:  42, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.849 [0.000, 5.000],  loss: 0.024000, mae: 3.358620, mean_q: 4.051369, mean_eps: 0.100000
 1413265/2000000: episode: 1838, duration: 20.917s, episode steps: 888, steps per second:  42, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.023509, mae: 3.367596, mean_q: 4.058897, mean_eps: 0.100000
 1414109/2000000: episode: 1839, duration: 20.680s, episode steps: 844, steps per second:  41, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.692 [0.000, 5.000],  loss: 0.020274, mae: 3.350569, mean_q: 4.039903, mean_eps: 0.100000
 1415193/2000000: episode: 1840, duration: 25.550s, episode steps: 1084, steps per second:  42, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.798 [0.000, 5.000],  loss: 0.020898, mae: 3.345605, mean_q: 4.033774, mean_eps: 0.100000
 1415857/2000000: episode: 1841, duration: 15.208s, episode steps: 664, steps per second:  44, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.022398, mae: 3.375083, mean_q: 4.070227, mean_eps: 0.100000
 1417130/2000000: episode: 1842, duration: 29.971s, episode steps: 1273, steps per second:  42, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.020703, mae: 3.370227, mean_q: 4.062695, mean_eps: 0.100000
 1417961/2000000: episode: 1843, duration: 20.591s, episode steps: 831, steps per second:  40, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.715 [0.000, 5.000],  loss: 0.023122, mae: 3.394026, mean_q: 4.092293, mean_eps: 0.100000
 1418784/2000000: episode: 1844, duration: 21.306s, episode steps: 823, steps per second:  39, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.243 [0.000, 5.000],  loss: 0.023042, mae: 3.388419, mean_q: 4.085300, mean_eps: 0.100000
 1419761/2000000: episode: 1845, duration: 25.292s, episode steps: 977, steps per second:  39, episode reward: 28.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.023036, mae: 3.360157, mean_q: 4.051736, mean_eps: 0.100000
 1420552/2000000: episode: 1846, duration: 20.603s, episode steps: 791, steps per second:  38, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.894 [0.000, 5.000],  loss: 0.019574, mae: 3.385394, mean_q: 4.084195, mean_eps: 0.100000
 1421722/2000000: episode: 1847, duration: 30.021s, episode steps: 1170, steps per second:  39, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.742 [0.000, 5.000],  loss: 0.020630, mae: 3.378124, mean_q: 4.073707, mean_eps: 0.100000
 1422412/2000000: episode: 1848, duration: 17.576s, episode steps: 690, steps per second:  39, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.021304, mae: 3.380128, mean_q: 4.075498, mean_eps: 0.100000
 1423051/2000000: episode: 1849, duration: 16.719s, episode steps: 639, steps per second:  38, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.020274, mae: 3.366465, mean_q: 4.057959, mean_eps: 0.100000
 1423561/2000000: episode: 1850, duration: 13.234s, episode steps: 510, steps per second:  39, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.312 [0.000, 5.000],  loss: 0.019704, mae: 3.405111, mean_q: 4.104928, mean_eps: 0.100000
 1424772/2000000: episode: 1851, duration: 30.407s, episode steps: 1211, steps per second:  40, episode reward: 32.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.021901, mae: 3.394271, mean_q: 4.093541, mean_eps: 0.100000
 1425172/2000000: episode: 1852, duration: 9.943s, episode steps: 400, steps per second:  40, episode reward: 10.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.825 [0.000, 5.000],  loss: 0.017807, mae: 3.367247, mean_q: 4.060990, mean_eps: 0.100000
 1425630/2000000: episode: 1853, duration: 12.001s, episode steps: 458, steps per second:  38, episode reward: 11.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.231 [0.000, 5.000],  loss: 0.019524, mae: 3.373807, mean_q: 4.068611, mean_eps: 0.100000
 1426578/2000000: episode: 1854, duration: 22.714s, episode steps: 948, steps per second:  42, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.778 [0.000, 5.000],  loss: 0.020536, mae: 3.396194, mean_q: 4.093937, mean_eps: 0.100000
 1427470/2000000: episode: 1855, duration: 20.854s, episode steps: 892, steps per second:  43, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.028 [0.000, 5.000],  loss: 0.023314, mae: 3.368605, mean_q: 4.060641, mean_eps: 0.100000
 1428367/2000000: episode: 1856, duration: 21.838s, episode steps: 897, steps per second:  41, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.024072, mae: 3.377582, mean_q: 4.072544, mean_eps: 0.100000
 1429319/2000000: episode: 1857, duration: 22.655s, episode steps: 952, steps per second:  42, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.024142, mae: 3.386861, mean_q: 4.087944, mean_eps: 0.100000
 1430872/2000000: episode: 1858, duration: 35.194s, episode steps: 1553, steps per second:  44, episode reward: 26.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.020767, mae: 3.380323, mean_q: 4.076435, mean_eps: 0.100000
 1431485/2000000: episode: 1859, duration: 13.775s, episode steps: 613, steps per second:  44, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.835 [0.000, 5.000],  loss: 0.020097, mae: 3.390247, mean_q: 4.087717, mean_eps: 0.100000
 1432400/2000000: episode: 1860, duration: 20.488s, episode steps: 915, steps per second:  45, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.022440, mae: 3.406342, mean_q: 4.108270, mean_eps: 0.100000
 1433332/2000000: episode: 1861, duration: 21.976s, episode steps: 932, steps per second:  42, episode reward: 30.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.269 [0.000, 5.000],  loss: 0.026615, mae: 3.399550, mean_q: 4.100081, mean_eps: 0.100000
 1434210/2000000: episode: 1862, duration: 21.656s, episode steps: 878, steps per second:  41, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.018 [0.000, 5.000],  loss: 0.022034, mae: 3.383811, mean_q: 4.080510, mean_eps: 0.100000
 1435409/2000000: episode: 1863, duration: 29.307s, episode steps: 1199, steps per second:  41, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.018808, mae: 3.417372, mean_q: 4.121359, mean_eps: 0.100000
 1436054/2000000: episode: 1864, duration: 15.891s, episode steps: 645, steps per second:  41, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.022067, mae: 3.408489, mean_q: 4.110281, mean_eps: 0.100000
 1437078/2000000: episode: 1865, duration: 24.630s, episode steps: 1024, steps per second:  42, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.251 [0.000, 5.000],  loss: 0.023626, mae: 3.434497, mean_q: 4.141185, mean_eps: 0.100000
 1437946/2000000: episode: 1866, duration: 20.790s, episode steps: 868, steps per second:  42, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.268 [0.000, 5.000],  loss: 0.024079, mae: 3.432877, mean_q: 4.138235, mean_eps: 0.100000
 1438424/2000000: episode: 1867, duration: 12.410s, episode steps: 478, steps per second:  39, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.287 [0.000, 5.000],  loss: 0.023965, mae: 3.408176, mean_q: 4.106754, mean_eps: 0.100000
 1438907/2000000: episode: 1868, duration: 12.745s, episode steps: 483, steps per second:  38, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.874 [0.000, 5.000],  loss: 0.020297, mae: 3.427082, mean_q: 4.130190, mean_eps: 0.100000
 1439892/2000000: episode: 1869, duration: 23.641s, episode steps: 985, steps per second:  42, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.838 [0.000, 5.000],  loss: 0.022556, mae: 3.403212, mean_q: 4.102251, mean_eps: 0.100000
 1440755/2000000: episode: 1870, duration: 21.374s, episode steps: 863, steps per second:  40, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.701 [0.000, 5.000],  loss: 0.019913, mae: 3.404128, mean_q: 4.103687, mean_eps: 0.100000
 1441639/2000000: episode: 1871, duration: 22.218s, episode steps: 884, steps per second:  40, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.657 [0.000, 5.000],  loss: 0.020295, mae: 3.411412, mean_q: 4.111508, mean_eps: 0.100000
 1442373/2000000: episode: 1872, duration: 17.048s, episode steps: 734, steps per second:  43, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.031 [0.000, 5.000],  loss: 0.021642, mae: 3.393610, mean_q: 4.088491, mean_eps: 0.100000
 1443287/2000000: episode: 1873, duration: 22.799s, episode steps: 914, steps per second:  40, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.175 [0.000, 5.000],  loss: 0.021134, mae: 3.405058, mean_q: 4.104533, mean_eps: 0.100000
 1443959/2000000: episode: 1874, duration: 16.838s, episode steps: 672, steps per second:  40, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.022984, mae: 3.387500, mean_q: 4.082500, mean_eps: 0.100000
 1444868/2000000: episode: 1875, duration: 21.643s, episode steps: 909, steps per second:  42, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.959 [0.000, 5.000],  loss: 0.020622, mae: 3.409220, mean_q: 4.109143, mean_eps: 0.100000
 1445941/2000000: episode: 1876, duration: 25.344s, episode steps: 1073, steps per second:  42, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.020170, mae: 3.393392, mean_q: 4.090815, mean_eps: 0.100000
 1446881/2000000: episode: 1877, duration: 22.956s, episode steps: 940, steps per second:  41, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.264 [0.000, 5.000],  loss: 0.020556, mae: 3.381034, mean_q: 4.074767, mean_eps: 0.100000
 1448187/2000000: episode: 1878, duration: 35.238s, episode steps: 1306, steps per second:  37, episode reward: 33.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.022153, mae: 3.398933, mean_q: 4.097846, mean_eps: 0.100000
 1449351/2000000: episode: 1879, duration: 27.865s, episode steps: 1164, steps per second:  42, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.024823, mae: 3.396950, mean_q: 4.092209, mean_eps: 0.100000
 1449847/2000000: episode: 1880, duration: 11.595s, episode steps: 496, steps per second:  43, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.021012, mae: 3.412780, mean_q: 4.114076, mean_eps: 0.100000
 1450778/2000000: episode: 1881, duration: 21.183s, episode steps: 931, steps per second:  44, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.019541, mae: 3.402620, mean_q: 4.101746, mean_eps: 0.100000
 1451798/2000000: episode: 1882, duration: 23.711s, episode steps: 1020, steps per second:  43, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.021953, mae: 3.384143, mean_q: 4.077498, mean_eps: 0.100000
 1452929/2000000: episode: 1883, duration: 26.143s, episode steps: 1131, steps per second:  43, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.704 [0.000, 5.000],  loss: 0.022195, mae: 3.381278, mean_q: 4.075055, mean_eps: 0.100000
 1453606/2000000: episode: 1884, duration: 15.695s, episode steps: 677, steps per second:  43, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.978 [0.000, 5.000],  loss: 0.020690, mae: 3.408916, mean_q: 4.107632, mean_eps: 0.100000
 1454512/2000000: episode: 1885, duration: 20.849s, episode steps: 906, steps per second:  43, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.021228, mae: 3.386315, mean_q: 4.081540, mean_eps: 0.100000
 1455423/2000000: episode: 1886, duration: 20.501s, episode steps: 911, steps per second:  44, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.020107, mae: 3.388435, mean_q: 4.084291, mean_eps: 0.100000
 1456081/2000000: episode: 1887, duration: 15.232s, episode steps: 658, steps per second:  43, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.078 [0.000, 5.000],  loss: 0.019892, mae: 3.386981, mean_q: 4.082217, mean_eps: 0.100000
 1456616/2000000: episode: 1888, duration: 11.933s, episode steps: 535, steps per second:  45, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.669 [0.000, 5.000],  loss: 0.022749, mae: 3.414219, mean_q: 4.115614, mean_eps: 0.100000
 1457632/2000000: episode: 1889, duration: 22.575s, episode steps: 1016, steps per second:  45, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.020706, mae: 3.404790, mean_q: 4.103608, mean_eps: 0.100000
 1458395/2000000: episode: 1890, duration: 18.223s, episode steps: 763, steps per second:  42, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.806 [0.000, 5.000],  loss: 0.022246, mae: 3.402973, mean_q: 4.099414, mean_eps: 0.100000
 1459164/2000000: episode: 1891, duration: 17.562s, episode steps: 769, steps per second:  44, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.023978, mae: 3.394446, mean_q: 4.091642, mean_eps: 0.100000
 1460011/2000000: episode: 1892, duration: 20.423s, episode steps: 847, steps per second:  41, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.133 [0.000, 5.000],  loss: 0.020514, mae: 3.390340, mean_q: 4.089223, mean_eps: 0.100000
 1460883/2000000: episode: 1893, duration: 20.064s, episode steps: 872, steps per second:  43, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.096 [0.000, 5.000],  loss: 0.018748, mae: 3.368309, mean_q: 4.063328, mean_eps: 0.100000
 1461630/2000000: episode: 1894, duration: 17.410s, episode steps: 747, steps per second:  43, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.236 [0.000, 5.000],  loss: 0.023980, mae: 3.397645, mean_q: 4.095998, mean_eps: 0.100000
 1462717/2000000: episode: 1895, duration: 26.046s, episode steps: 1087, steps per second:  42, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.021042, mae: 3.371821, mean_q: 4.066041, mean_eps: 0.100000
 1463521/2000000: episode: 1896, duration: 20.442s, episode steps: 804, steps per second:  39, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.780 [0.000, 5.000],  loss: 0.020575, mae: 3.390905, mean_q: 4.086854, mean_eps: 0.100000
 1464575/2000000: episode: 1897, duration: 24.938s, episode steps: 1054, steps per second:  42, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.020101, mae: 3.395878, mean_q: 4.094014, mean_eps: 0.100000
 1465706/2000000: episode: 1898, duration: 25.131s, episode steps: 1131, steps per second:  45, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.020093, mae: 3.374086, mean_q: 4.068576, mean_eps: 0.100000
 1466479/2000000: episode: 1899, duration: 17.139s, episode steps: 773, steps per second:  45, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.021211, mae: 3.364664, mean_q: 4.056072, mean_eps: 0.100000
 1467103/2000000: episode: 1900, duration: 14.419s, episode steps: 624, steps per second:  43, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.691 [0.000, 5.000],  loss: 0.023546, mae: 3.351646, mean_q: 4.042430, mean_eps: 0.100000
 1467813/2000000: episode: 1901, duration: 16.432s, episode steps: 710, steps per second:  43, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.096 [0.000, 5.000],  loss: 0.023308, mae: 3.342665, mean_q: 4.029403, mean_eps: 0.100000
 1469060/2000000: episode: 1902, duration: 33.731s, episode steps: 1247, steps per second:  37, episode reward: 32.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.020534, mae: 3.342335, mean_q: 4.029532, mean_eps: 0.100000
 1469747/2000000: episode: 1903, duration: 16.407s, episode steps: 687, steps per second:  42, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.057 [0.000, 5.000],  loss: 0.023470, mae: 3.333837, mean_q: 4.019568, mean_eps: 0.100000
 1470281/2000000: episode: 1904, duration: 12.777s, episode steps: 534, steps per second:  42, episode reward: 14.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.242 [0.000, 5.000],  loss: 0.021838, mae: 3.317408, mean_q: 4.001201, mean_eps: 0.100000
 1471171/2000000: episode: 1905, duration: 21.870s, episode steps: 890, steps per second:  41, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.021436, mae: 3.327727, mean_q: 4.011719, mean_eps: 0.100000
 1471729/2000000: episode: 1906, duration: 13.563s, episode steps: 558, steps per second:  41, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.020497, mae: 3.327863, mean_q: 4.012198, mean_eps: 0.100000
 1472463/2000000: episode: 1907, duration: 16.060s, episode steps: 734, steps per second:  46, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.902 [0.000, 5.000],  loss: 0.022687, mae: 3.340061, mean_q: 4.028597, mean_eps: 0.100000
 1473097/2000000: episode: 1908, duration: 14.718s, episode steps: 634, steps per second:  43, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.219 [0.000, 5.000],  loss: 0.020880, mae: 3.322410, mean_q: 4.007360, mean_eps: 0.100000
 1474036/2000000: episode: 1909, duration: 22.538s, episode steps: 939, steps per second:  42, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.261 [0.000, 5.000],  loss: 0.022550, mae: 3.303121, mean_q: 3.982044, mean_eps: 0.100000
 1474970/2000000: episode: 1910, duration: 21.779s, episode steps: 934, steps per second:  43, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.956 [0.000, 5.000],  loss: 0.020321, mae: 3.316890, mean_q: 3.998935, mean_eps: 0.100000
 1475394/2000000: episode: 1911, duration: 9.728s, episode steps: 424, steps per second:  44, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.991 [0.000, 5.000],  loss: 0.018871, mae: 3.311638, mean_q: 3.996209, mean_eps: 0.100000
 1476272/2000000: episode: 1912, duration: 20.671s, episode steps: 878, steps per second:  42, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.827 [0.000, 5.000],  loss: 0.021044, mae: 3.334630, mean_q: 4.022383, mean_eps: 0.100000
 1477197/2000000: episode: 1913, duration: 21.878s, episode steps: 925, steps per second:  42, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.205 [0.000, 5.000],  loss: 0.020905, mae: 3.328554, mean_q: 4.012266, mean_eps: 0.100000
 1478302/2000000: episode: 1914, duration: 27.295s, episode steps: 1105, steps per second:  40, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.020325, mae: 3.317301, mean_q: 3.999356, mean_eps: 0.100000
 1479411/2000000: episode: 1915, duration: 27.807s, episode steps: 1109, steps per second:  40, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.092 [0.000, 5.000],  loss: 0.020966, mae: 3.320072, mean_q: 4.003023, mean_eps: 0.100000
 1480405/2000000: episode: 1916, duration: 24.272s, episode steps: 994, steps per second:  41, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.291 [0.000, 5.000],  loss: 0.021124, mae: 3.316328, mean_q: 4.000288, mean_eps: 0.100000
 1480878/2000000: episode: 1917, duration: 11.239s, episode steps: 473, steps per second:  42, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.044 [0.000, 5.000],  loss: 0.019986, mae: 3.326646, mean_q: 4.013205, mean_eps: 0.100000
 1481715/2000000: episode: 1918, duration: 20.055s, episode steps: 837, steps per second:  42, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.020726, mae: 3.343598, mean_q: 4.032131, mean_eps: 0.100000
 1482683/2000000: episode: 1919, duration: 23.480s, episode steps: 968, steps per second:  41, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.017783, mae: 3.321168, mean_q: 4.003759, mean_eps: 0.100000
 1483085/2000000: episode: 1920, duration: 10.310s, episode steps: 402, steps per second:  39, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.913 [0.000, 5.000],  loss: 0.022873, mae: 3.325956, mean_q: 4.009835, mean_eps: 0.100000
 1483936/2000000: episode: 1921, duration: 21.297s, episode steps: 851, steps per second:  40, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.731 [0.000, 5.000],  loss: 0.022257, mae: 3.320930, mean_q: 4.004293, mean_eps: 0.100000
 1484435/2000000: episode: 1922, duration: 12.378s, episode steps: 499, steps per second:  40, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.709 [0.000, 5.000],  loss: 0.021353, mae: 3.316979, mean_q: 3.999557, mean_eps: 0.100000
 1485646/2000000: episode: 1923, duration: 29.048s, episode steps: 1211, steps per second:  42, episode reward: 30.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.020496, mae: 3.327819, mean_q: 4.013165, mean_eps: 0.100000
 1487141/2000000: episode: 1924, duration: 34.722s, episode steps: 1495, steps per second:  43, episode reward: 32.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.016917, mae: 3.323762, mean_q: 4.009109, mean_eps: 0.100000
 1487817/2000000: episode: 1925, duration: 15.444s, episode steps: 676, steps per second:  44, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.956 [0.000, 5.000],  loss: 0.021583, mae: 3.317895, mean_q: 3.999948, mean_eps: 0.100000
 1488784/2000000: episode: 1926, duration: 24.009s, episode steps: 967, steps per second:  40, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.944 [0.000, 5.000],  loss: 0.018085, mae: 3.321585, mean_q: 4.006746, mean_eps: 0.100000
 1489818/2000000: episode: 1927, duration: 23.818s, episode steps: 1034, steps per second:  43, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.018357, mae: 3.319532, mean_q: 4.002537, mean_eps: 0.100000
 1490839/2000000: episode: 1928, duration: 23.179s, episode steps: 1021, steps per second:  44, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.018812, mae: 3.287668, mean_q: 3.966223, mean_eps: 0.100000
 1491463/2000000: episode: 1929, duration: 14.865s, episode steps: 624, steps per second:  42, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.800 [0.000, 5.000],  loss: 0.018257, mae: 3.295942, mean_q: 3.975503, mean_eps: 0.100000
 1492634/2000000: episode: 1930, duration: 27.747s, episode steps: 1171, steps per second:  42, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: 0.020216, mae: 3.292088, mean_q: 3.969410, mean_eps: 0.100000
 1493510/2000000: episode: 1931, duration: 22.931s, episode steps: 876, steps per second:  38, episode reward: 29.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.024009, mae: 3.287943, mean_q: 3.962965, mean_eps: 0.100000
 1494566/2000000: episode: 1932, duration: 25.474s, episode steps: 1056, steps per second:  41, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.122 [0.000, 5.000],  loss: 0.021344, mae: 3.307484, mean_q: 3.988303, mean_eps: 0.100000
 1495182/2000000: episode: 1933, duration: 14.623s, episode steps: 616, steps per second:  42, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.758 [0.000, 5.000],  loss: 0.022433, mae: 3.274027, mean_q: 3.946612, mean_eps: 0.100000
 1496165/2000000: episode: 1934, duration: 24.037s, episode steps: 983, steps per second:  41, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.997 [0.000, 5.000],  loss: 0.017951, mae: 3.290618, mean_q: 3.968439, mean_eps: 0.100000
 1497196/2000000: episode: 1935, duration: 25.387s, episode steps: 1031, steps per second:  41, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.021459, mae: 3.274787, mean_q: 3.947256, mean_eps: 0.100000
 1498031/2000000: episode: 1936, duration: 20.666s, episode steps: 835, steps per second:  40, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.021316, mae: 3.311939, mean_q: 3.992388, mean_eps: 0.100000
 1498789/2000000: episode: 1937, duration: 20.444s, episode steps: 758, steps per second:  37, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.020075, mae: 3.307217, mean_q: 3.986690, mean_eps: 0.100000
 1499391/2000000: episode: 1938, duration: 14.731s, episode steps: 602, steps per second:  41, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.020548, mae: 3.291105, mean_q: 3.968722, mean_eps: 0.100000
 1500557/2000000: episode: 1939, duration: 28.858s, episode steps: 1166, steps per second:  40, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.961 [0.000, 5.000],  loss: 0.018722, mae: 3.291461, mean_q: 3.972806, mean_eps: 0.100000
 1501638/2000000: episode: 1940, duration: 27.097s, episode steps: 1081, steps per second:  40, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.019620, mae: 3.287881, mean_q: 3.964325, mean_eps: 0.100000
 1502808/2000000: episode: 1941, duration: 27.369s, episode steps: 1170, steps per second:  43, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.019830, mae: 3.257234, mean_q: 3.926913, mean_eps: 0.100000
 1503442/2000000: episode: 1942, duration: 15.926s, episode steps: 634, steps per second:  40, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.715 [0.000, 5.000],  loss: 0.020301, mae: 3.265941, mean_q: 3.938775, mean_eps: 0.100000
 1504116/2000000: episode: 1943, duration: 16.304s, episode steps: 674, steps per second:  41, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.020067, mae: 3.267901, mean_q: 3.940282, mean_eps: 0.100000
 1504742/2000000: episode: 1944, duration: 15.397s, episode steps: 626, steps per second:  41, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.022970, mae: 3.250201, mean_q: 3.922898, mean_eps: 0.100000
 1505862/2000000: episode: 1945, duration: 26.407s, episode steps: 1120, steps per second:  42, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.318 [0.000, 5.000],  loss: 0.020202, mae: 3.280973, mean_q: 3.956833, mean_eps: 0.100000
 1506681/2000000: episode: 1946, duration: 20.335s, episode steps: 819, steps per second:  40, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.018151, mae: 3.269045, mean_q: 3.943330, mean_eps: 0.100000
 1507506/2000000: episode: 1947, duration: 22.281s, episode steps: 825, steps per second:  37, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.017429, mae: 3.295099, mean_q: 3.973373, mean_eps: 0.100000
 1508338/2000000: episode: 1948, duration: 20.996s, episode steps: 832, steps per second:  40, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.073 [0.000, 5.000],  loss: 0.019007, mae: 3.279614, mean_q: 3.956394, mean_eps: 0.100000
 1509019/2000000: episode: 1949, duration: 17.635s, episode steps: 681, steps per second:  39, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.019877, mae: 3.282188, mean_q: 3.955668, mean_eps: 0.100000
 1510043/2000000: episode: 1950, duration: 26.004s, episode steps: 1024, steps per second:  39, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.021277, mae: 3.289549, mean_q: 3.965851, mean_eps: 0.100000
 1510913/2000000: episode: 1951, duration: 19.696s, episode steps: 870, steps per second:  44, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.743 [0.000, 5.000],  loss: 0.021163, mae: 3.269181, mean_q: 3.946515, mean_eps: 0.100000
 1511710/2000000: episode: 1952, duration: 18.211s, episode steps: 797, steps per second:  44, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.088 [0.000, 5.000],  loss: 0.021402, mae: 3.278167, mean_q: 3.953548, mean_eps: 0.100000
 1512653/2000000: episode: 1953, duration: 22.007s, episode steps: 943, steps per second:  43, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.020655, mae: 3.286688, mean_q: 3.965012, mean_eps: 0.100000
 1513370/2000000: episode: 1954, duration: 18.495s, episode steps: 717, steps per second:  39, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.123 [0.000, 5.000],  loss: 0.018665, mae: 3.267819, mean_q: 3.939622, mean_eps: 0.100000
 1514110/2000000: episode: 1955, duration: 18.499s, episode steps: 740, steps per second:  40, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.127 [0.000, 5.000],  loss: 0.021468, mae: 3.297831, mean_q: 3.977057, mean_eps: 0.100000
 1514746/2000000: episode: 1956, duration: 15.736s, episode steps: 636, steps per second:  40, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.928 [0.000, 5.000],  loss: 0.017468, mae: 3.290784, mean_q: 3.968568, mean_eps: 0.100000
 1515738/2000000: episode: 1957, duration: 25.080s, episode steps: 992, steps per second:  40, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.055 [0.000, 5.000],  loss: 0.020396, mae: 3.328688, mean_q: 4.014849, mean_eps: 0.100000
 1516595/2000000: episode: 1958, duration: 20.444s, episode steps: 857, steps per second:  42, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.203 [0.000, 5.000],  loss: 0.022556, mae: 3.300826, mean_q: 3.978696, mean_eps: 0.100000
 1517160/2000000: episode: 1959, duration: 12.980s, episode steps: 565, steps per second:  44, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.935 [0.000, 5.000],  loss: 0.022172, mae: 3.318337, mean_q: 4.001583, mean_eps: 0.100000
 1517994/2000000: episode: 1960, duration: 19.792s, episode steps: 834, steps per second:  42, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.189 [0.000, 5.000],  loss: 0.020797, mae: 3.292224, mean_q: 3.968614, mean_eps: 0.100000
 1519194/2000000: episode: 1961, duration: 27.813s, episode steps: 1200, steps per second:  43, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.120 [0.000, 5.000],  loss: 0.020490, mae: 3.310404, mean_q: 3.990934, mean_eps: 0.100000
 1520320/2000000: episode: 1962, duration: 26.358s, episode steps: 1126, steps per second:  43, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.019246, mae: 3.302757, mean_q: 3.982215, mean_eps: 0.100000
 1521135/2000000: episode: 1963, duration: 19.546s, episode steps: 815, steps per second:  42, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.785 [0.000, 5.000],  loss: 0.018443, mae: 3.287000, mean_q: 3.965341, mean_eps: 0.100000
 1522618/2000000: episode: 1964, duration: 37.447s, episode steps: 1483, steps per second:  40, episode reward: 35.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.020586, mae: 3.287871, mean_q: 3.964482, mean_eps: 0.100000
 1523522/2000000: episode: 1965, duration: 23.037s, episode steps: 904, steps per second:  39, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.299 [0.000, 5.000],  loss: 0.023499, mae: 3.274529, mean_q: 3.948774, mean_eps: 0.100000
 1525043/2000000: episode: 1966, duration: 38.011s, episode steps: 1521, steps per second:  40, episode reward: 35.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.021553, mae: 3.277984, mean_q: 3.951656, mean_eps: 0.100000
 1525926/2000000: episode: 1967, duration: 22.194s, episode steps: 883, steps per second:  40, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.018372, mae: 3.316429, mean_q: 3.999998, mean_eps: 0.100000
 1526621/2000000: episode: 1968, duration: 17.327s, episode steps: 695, steps per second:  40, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.157 [0.000, 5.000],  loss: 0.020442, mae: 3.322425, mean_q: 4.006309, mean_eps: 0.100000
 1527502/2000000: episode: 1969, duration: 21.817s, episode steps: 881, steps per second:  40, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.020406, mae: 3.299234, mean_q: 3.975877, mean_eps: 0.100000
 1528252/2000000: episode: 1970, duration: 19.512s, episode steps: 750, steps per second:  38, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.018590, mae: 3.299984, mean_q: 3.977677, mean_eps: 0.100000
 1529536/2000000: episode: 1971, duration: 32.309s, episode steps: 1284, steps per second:  40, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.020073, mae: 3.308558, mean_q: 3.988521, mean_eps: 0.100000
 1530210/2000000: episode: 1972, duration: 17.185s, episode steps: 674, steps per second:  39, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.872 [0.000, 5.000],  loss: 0.020056, mae: 3.316168, mean_q: 3.999383, mean_eps: 0.100000
 1531118/2000000: episode: 1973, duration: 21.988s, episode steps: 908, steps per second:  41, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.017293, mae: 3.322869, mean_q: 4.007322, mean_eps: 0.100000
 1531782/2000000: episode: 1974, duration: 15.402s, episode steps: 664, steps per second:  43, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.018309, mae: 3.329531, mean_q: 4.015498, mean_eps: 0.100000
 1532646/2000000: episode: 1975, duration: 21.456s, episode steps: 864, steps per second:  40, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.696 [0.000, 5.000],  loss: 0.019500, mae: 3.323328, mean_q: 4.008378, mean_eps: 0.100000
 1533562/2000000: episode: 1976, duration: 22.824s, episode steps: 916, steps per second:  40, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.222 [0.000, 5.000],  loss: 0.021018, mae: 3.336119, mean_q: 4.024105, mean_eps: 0.100000
 1534577/2000000: episode: 1977, duration: 24.953s, episode steps: 1015, steps per second:  41, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.974 [0.000, 5.000],  loss: 0.020678, mae: 3.309174, mean_q: 3.991365, mean_eps: 0.100000
 1535440/2000000: episode: 1978, duration: 19.975s, episode steps: 863, steps per second:  43, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.019489, mae: 3.305137, mean_q: 3.984411, mean_eps: 0.100000
 1536548/2000000: episode: 1979, duration: 28.310s, episode steps: 1108, steps per second:  39, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.269 [0.000, 5.000],  loss: 0.020439, mae: 3.323943, mean_q: 4.007209, mean_eps: 0.100000
 1537310/2000000: episode: 1980, duration: 20.446s, episode steps: 762, steps per second:  37, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.689 [0.000, 5.000],  loss: 0.020009, mae: 3.300097, mean_q: 3.981189, mean_eps: 0.100000
 1537949/2000000: episode: 1981, duration: 16.473s, episode steps: 639, steps per second:  39, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.875 [0.000, 5.000],  loss: 0.022685, mae: 3.314431, mean_q: 3.996124, mean_eps: 0.100000
 1538561/2000000: episode: 1982, duration: 15.443s, episode steps: 612, steps per second:  40, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.466 [0.000, 5.000],  loss: 0.021249, mae: 3.306904, mean_q: 3.987162, mean_eps: 0.100000
 1539099/2000000: episode: 1983, duration: 12.773s, episode steps: 538, steps per second:  42, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.023976, mae: 3.327158, mean_q: 4.008638, mean_eps: 0.100000
 1540881/2000000: episode: 1984, duration: 42.431s, episode steps: 1782, steps per second:  42, episode reward: 37.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.018825, mae: 3.332236, mean_q: 4.018285, mean_eps: 0.100000
 1541873/2000000: episode: 1985, duration: 23.466s, episode steps: 992, steps per second:  42, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: 0.020532, mae: 3.350113, mean_q: 4.038452, mean_eps: 0.100000
 1542768/2000000: episode: 1986, duration: 21.818s, episode steps: 895, steps per second:  41, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.955 [0.000, 5.000],  loss: 0.021046, mae: 3.325053, mean_q: 4.010584, mean_eps: 0.100000
 1543691/2000000: episode: 1987, duration: 22.285s, episode steps: 923, steps per second:  41, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.020934, mae: 3.331280, mean_q: 4.015533, mean_eps: 0.100000
 1544685/2000000: episode: 1988, duration: 25.170s, episode steps: 994, steps per second:  39, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.192 [0.000, 5.000],  loss: 0.021658, mae: 3.334899, mean_q: 4.020411, mean_eps: 0.100000
 1545708/2000000: episode: 1989, duration: 24.702s, episode steps: 1023, steps per second:  41, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.019520, mae: 3.355487, mean_q: 4.044907, mean_eps: 0.100000
 1546582/2000000: episode: 1990, duration: 20.338s, episode steps: 874, steps per second:  43, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.015 [0.000, 5.000],  loss: 0.021155, mae: 3.354060, mean_q: 4.046800, mean_eps: 0.100000
 1547760/2000000: episode: 1991, duration: 28.520s, episode steps: 1178, steps per second:  41, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.021290, mae: 3.330691, mean_q: 4.014970, mean_eps: 0.100000
 1548632/2000000: episode: 1992, duration: 20.170s, episode steps: 872, steps per second:  43, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.068 [0.000, 5.000],  loss: 0.018069, mae: 3.356615, mean_q: 4.046136, mean_eps: 0.100000
 1549450/2000000: episode: 1993, duration: 18.888s, episode steps: 818, steps per second:  43, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.022855, mae: 3.331690, mean_q: 4.016209, mean_eps: 0.100000
 1550414/2000000: episode: 1994, duration: 22.520s, episode steps: 964, steps per second:  43, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.020287, mae: 3.331440, mean_q: 4.017606, mean_eps: 0.100000
 1551349/2000000: episode: 1995, duration: 23.274s, episode steps: 935, steps per second:  40, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 0.019301, mae: 3.295593, mean_q: 3.973786, mean_eps: 0.100000
 1552058/2000000: episode: 1996, duration: 18.060s, episode steps: 709, steps per second:  39, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.556 [0.000, 5.000],  loss: 0.021506, mae: 3.296807, mean_q: 3.973865, mean_eps: 0.100000
 1552668/2000000: episode: 1997, duration: 15.567s, episode steps: 610, steps per second:  39, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.833 [0.000, 5.000],  loss: 0.021344, mae: 3.313604, mean_q: 3.996598, mean_eps: 0.100000
 1553474/2000000: episode: 1998, duration: 20.503s, episode steps: 806, steps per second:  39, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.021099, mae: 3.277485, mean_q: 3.951795, mean_eps: 0.100000
 1554169/2000000: episode: 1999, duration: 16.772s, episode steps: 695, steps per second:  41, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.089 [0.000, 5.000],  loss: 0.019678, mae: 3.301232, mean_q: 3.979372, mean_eps: 0.100000
 1554840/2000000: episode: 2000, duration: 16.512s, episode steps: 671, steps per second:  41, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.082 [0.000, 5.000],  loss: 0.021520, mae: 3.279171, mean_q: 3.951963, mean_eps: 0.100000
 1555563/2000000: episode: 2001, duration: 18.105s, episode steps: 723, steps per second:  40, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.812 [0.000, 5.000],  loss: 0.018120, mae: 3.273195, mean_q: 3.948370, mean_eps: 0.100000
 1556229/2000000: episode: 2002, duration: 16.051s, episode steps: 666, steps per second:  41, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.020627, mae: 3.302044, mean_q: 3.981339, mean_eps: 0.100000
 1557058/2000000: episode: 2003, duration: 21.052s, episode steps: 829, steps per second:  39, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.252 [0.000, 5.000],  loss: 0.017599, mae: 3.315438, mean_q: 3.998313, mean_eps: 0.100000
 1557565/2000000: episode: 2004, duration: 13.112s, episode steps: 507, steps per second:  39, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.300 [0.000, 5.000],  loss: 0.021092, mae: 3.299850, mean_q: 3.978533, mean_eps: 0.100000
 1558376/2000000: episode: 2005, duration: 20.527s, episode steps: 811, steps per second:  40, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.185 [0.000, 5.000],  loss: 0.018369, mae: 3.306900, mean_q: 3.987997, mean_eps: 0.100000
 1559150/2000000: episode: 2006, duration: 19.227s, episode steps: 774, steps per second:  40, episode reward: 24.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.823 [0.000, 5.000],  loss: 0.019669, mae: 3.285334, mean_q: 3.959489, mean_eps: 0.100000
 1559975/2000000: episode: 2007, duration: 20.940s, episode steps: 825, steps per second:  39, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.766 [0.000, 5.000],  loss: 0.023269, mae: 3.275272, mean_q: 3.944954, mean_eps: 0.100000
 1560789/2000000: episode: 2008, duration: 18.398s, episode steps: 814, steps per second:  44, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.256 [0.000, 5.000],  loss: 0.021909, mae: 3.295813, mean_q: 3.971621, mean_eps: 0.100000
 1561508/2000000: episode: 2009, duration: 16.605s, episode steps: 719, steps per second:  43, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.195 [0.000, 5.000],  loss: 0.020745, mae: 3.301309, mean_q: 3.978692, mean_eps: 0.100000
 1562749/2000000: episode: 2010, duration: 31.124s, episode steps: 1241, steps per second:  40, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.288 [0.000, 5.000],  loss: 0.019934, mae: 3.278819, mean_q: 3.952999, mean_eps: 0.100000
 1563490/2000000: episode: 2011, duration: 18.430s, episode steps: 741, steps per second:  40, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.020673, mae: 3.272557, mean_q: 3.945451, mean_eps: 0.100000
 1564141/2000000: episode: 2012, duration: 15.412s, episode steps: 651, steps per second:  42, episode reward: 20.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 3.052 [0.000, 5.000],  loss: 0.019441, mae: 3.280013, mean_q: 3.955472, mean_eps: 0.100000
 1564813/2000000: episode: 2013, duration: 15.853s, episode steps: 672, steps per second:  42, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.365 [0.000, 5.000],  loss: 0.019650, mae: 3.256617, mean_q: 3.927066, mean_eps: 0.100000
 1566238/2000000: episode: 2014, duration: 35.577s, episode steps: 1425, steps per second:  40, episode reward: 32.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.834 [0.000, 5.000],  loss: 0.019576, mae: 3.291070, mean_q: 3.967816, mean_eps: 0.100000
 1566916/2000000: episode: 2015, duration: 17.450s, episode steps: 678, steps per second:  39, episode reward: 18.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.740 [0.000, 5.000],  loss: 0.018308, mae: 3.305134, mean_q: 3.983511, mean_eps: 0.100000
 1567861/2000000: episode: 2016, duration: 24.010s, episode steps: 945, steps per second:  39, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.080 [0.000, 5.000],  loss: 0.018950, mae: 3.296854, mean_q: 3.974336, mean_eps: 0.100000
 1568545/2000000: episode: 2017, duration: 16.833s, episode steps: 684, steps per second:  41, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.133 [0.000, 5.000],  loss: 0.021418, mae: 3.309984, mean_q: 3.987574, mean_eps: 0.100000
 1569460/2000000: episode: 2018, duration: 22.380s, episode steps: 915, steps per second:  41, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.349 [0.000, 5.000],  loss: 0.022733, mae: 3.309198, mean_q: 3.988544, mean_eps: 0.100000
 1570744/2000000: episode: 2019, duration: 31.268s, episode steps: 1284, steps per second:  41, episode reward: 33.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.022069, mae: 3.286023, mean_q: 3.961251, mean_eps: 0.100000
 1571670/2000000: episode: 2020, duration: 22.546s, episode steps: 926, steps per second:  41, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.772 [0.000, 5.000],  loss: 0.018819, mae: 3.311406, mean_q: 3.991784, mean_eps: 0.100000
 1572844/2000000: episode: 2021, duration: 28.348s, episode steps: 1174, steps per second:  41, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.149 [0.000, 5.000],  loss: 0.022317, mae: 3.308193, mean_q: 3.986755, mean_eps: 0.100000
 1573822/2000000: episode: 2022, duration: 23.230s, episode steps: 978, steps per second:  42, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.018393, mae: 3.283689, mean_q: 3.957718, mean_eps: 0.100000
 1574891/2000000: episode: 2023, duration: 25.681s, episode steps: 1069, steps per second:  42, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.020902, mae: 3.290916, mean_q: 3.966257, mean_eps: 0.100000
 1575428/2000000: episode: 2024, duration: 12.155s, episode steps: 537, steps per second:  44, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.898 [0.000, 5.000],  loss: 0.017362, mae: 3.284071, mean_q: 3.962121, mean_eps: 0.100000
 1576477/2000000: episode: 2025, duration: 23.900s, episode steps: 1049, steps per second:  44, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.020391, mae: 3.272420, mean_q: 3.945001, mean_eps: 0.100000
 1576945/2000000: episode: 2026, duration: 10.961s, episode steps: 468, steps per second:  43, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.197 [0.000, 5.000],  loss: 0.023189, mae: 3.269132, mean_q: 3.939906, mean_eps: 0.100000
 1578109/2000000: episode: 2027, duration: 26.525s, episode steps: 1164, steps per second:  44, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.051 [0.000, 5.000],  loss: 0.019771, mae: 3.266488, mean_q: 3.937862, mean_eps: 0.100000
 1578990/2000000: episode: 2028, duration: 20.199s, episode steps: 881, steps per second:  44, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.724 [0.000, 5.000],  loss: 0.019200, mae: 3.257057, mean_q: 3.925911, mean_eps: 0.100000
 1579849/2000000: episode: 2029, duration: 20.543s, episode steps: 859, steps per second:  42, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.022323, mae: 3.275061, mean_q: 3.945710, mean_eps: 0.100000
 1580670/2000000: episode: 2030, duration: 20.886s, episode steps: 821, steps per second:  39, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.130 [0.000, 5.000],  loss: 0.020872, mae: 3.269944, mean_q: 3.943626, mean_eps: 0.100000
 1582041/2000000: episode: 2031, duration: 37.006s, episode steps: 1371, steps per second:  37, episode reward: 35.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.019093, mae: 3.282605, mean_q: 3.956995, mean_eps: 0.100000
 1582719/2000000: episode: 2032, duration: 17.861s, episode steps: 678, steps per second:  38, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.018799, mae: 3.278994, mean_q: 3.950922, mean_eps: 0.100000
 1583943/2000000: episode: 2033, duration: 30.291s, episode steps: 1224, steps per second:  40, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.952 [0.000, 5.000],  loss: 0.022703, mae: 3.267752, mean_q: 3.936615, mean_eps: 0.100000
 1584717/2000000: episode: 2034, duration: 19.137s, episode steps: 774, steps per second:  40, episode reward: 23.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.039 [0.000, 5.000],  loss: 0.024131, mae: 3.260951, mean_q: 3.930744, mean_eps: 0.100000
 1585214/2000000: episode: 2035, duration: 12.113s, episode steps: 497, steps per second:  41, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 0.019660, mae: 3.312204, mean_q: 3.991703, mean_eps: 0.100000
 1586259/2000000: episode: 2036, duration: 27.050s, episode steps: 1045, steps per second:  39, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.018 [0.000, 5.000],  loss: 0.018416, mae: 3.304963, mean_q: 3.981248, mean_eps: 0.100000
 1587057/2000000: episode: 2037, duration: 20.777s, episode steps: 798, steps per second:  38, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.754 [0.000, 5.000],  loss: 0.017107, mae: 3.296178, mean_q: 3.971223, mean_eps: 0.100000
 1588181/2000000: episode: 2038, duration: 28.148s, episode steps: 1124, steps per second:  40, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.881 [0.000, 5.000],  loss: 0.018129, mae: 3.289679, mean_q: 3.963309, mean_eps: 0.100000
 1589168/2000000: episode: 2039, duration: 25.230s, episode steps: 987, steps per second:  39, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.021514, mae: 3.297975, mean_q: 3.973856, mean_eps: 0.100000
 1589890/2000000: episode: 2040, duration: 17.470s, episode steps: 722, steps per second:  41, episode reward: 20.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.771 [0.000, 5.000],  loss: 0.017991, mae: 3.288810, mean_q: 3.966722, mean_eps: 0.100000
 1590378/2000000: episode: 2041, duration: 11.764s, episode steps: 488, steps per second:  41, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.018924, mae: 3.296693, mean_q: 3.971655, mean_eps: 0.100000
 1590884/2000000: episode: 2042, duration: 12.908s, episode steps: 506, steps per second:  39, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.022104, mae: 3.327049, mean_q: 4.008986, mean_eps: 0.100000
 1592085/2000000: episode: 2043, duration: 29.695s, episode steps: 1201, steps per second:  40, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.020269, mae: 3.283727, mean_q: 3.955966, mean_eps: 0.100000
 1592933/2000000: episode: 2044, duration: 21.230s, episode steps: 848, steps per second:  40, episode reward: 27.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.902 [0.000, 5.000],  loss: 0.021589, mae: 3.308556, mean_q: 3.987936, mean_eps: 0.100000
 1593790/2000000: episode: 2045, duration: 19.624s, episode steps: 857, steps per second:  44, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.286 [0.000, 5.000],  loss: 0.022547, mae: 3.299357, mean_q: 3.974760, mean_eps: 0.100000
 1594495/2000000: episode: 2046, duration: 17.064s, episode steps: 705, steps per second:  41, episode reward: 22.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.577 [0.000, 5.000],  loss: 0.022275, mae: 3.275946, mean_q: 3.948066, mean_eps: 0.100000
 1595409/2000000: episode: 2047, duration: 23.338s, episode steps: 914, steps per second:  39, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.717 [0.000, 5.000],  loss: 0.020001, mae: 3.293080, mean_q: 3.968429, mean_eps: 0.100000
 1596685/2000000: episode: 2048, duration: 36.488s, episode steps: 1276, steps per second:  35, episode reward: 26.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.020716, mae: 3.297728, mean_q: 3.975915, mean_eps: 0.100000
 1597957/2000000: episode: 2049, duration: 30.824s, episode steps: 1272, steps per second:  41, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.032 [0.000, 5.000],  loss: 0.019860, mae: 3.283590, mean_q: 3.958392, mean_eps: 0.100000
 1598700/2000000: episode: 2050, duration: 18.145s, episode steps: 743, steps per second:  41, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.410 [0.000, 5.000],  loss: 0.020337, mae: 3.277839, mean_q: 3.949684, mean_eps: 0.100000
 1599283/2000000: episode: 2051, duration: 13.997s, episode steps: 583, steps per second:  42, episode reward: 15.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.018049, mae: 3.293654, mean_q: 3.968797, mean_eps: 0.100000
 1600371/2000000: episode: 2052, duration: 26.367s, episode steps: 1088, steps per second:  41, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: 0.020386, mae: 3.283881, mean_q: 3.956870, mean_eps: 0.100000
 1601058/2000000: episode: 2053, duration: 16.676s, episode steps: 687, steps per second:  41, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.016965, mae: 3.311941, mean_q: 3.993336, mean_eps: 0.100000
 1602005/2000000: episode: 2054, duration: 23.948s, episode steps: 947, steps per second:  40, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.015 [0.000, 5.000],  loss: 0.020980, mae: 3.307519, mean_q: 3.986898, mean_eps: 0.100000
 1602935/2000000: episode: 2055, duration: 23.180s, episode steps: 930, steps per second:  40, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.845 [0.000, 5.000],  loss: 0.022171, mae: 3.280911, mean_q: 3.955321, mean_eps: 0.100000
 1603552/2000000: episode: 2056, duration: 15.641s, episode steps: 617, steps per second:  39, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.878 [0.000, 5.000],  loss: 0.020895, mae: 3.284398, mean_q: 3.960544, mean_eps: 0.100000
 1604325/2000000: episode: 2057, duration: 18.034s, episode steps: 773, steps per second:  43, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.021745, mae: 3.295639, mean_q: 3.972740, mean_eps: 0.100000
 1605210/2000000: episode: 2058, duration: 20.084s, episode steps: 885, steps per second:  44, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.955 [0.000, 5.000],  loss: 0.019815, mae: 3.304813, mean_q: 3.984683, mean_eps: 0.100000
 1606317/2000000: episode: 2059, duration: 27.329s, episode steps: 1107, steps per second:  41, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.017829, mae: 3.308744, mean_q: 3.994104, mean_eps: 0.100000
 1607394/2000000: episode: 2060, duration: 26.696s, episode steps: 1077, steps per second:  40, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.275 [0.000, 5.000],  loss: 0.017974, mae: 3.292241, mean_q: 3.971997, mean_eps: 0.100000
 1608065/2000000: episode: 2061, duration: 15.881s, episode steps: 671, steps per second:  42, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.031 [0.000, 5.000],  loss: 0.019920, mae: 3.311371, mean_q: 3.992677, mean_eps: 0.100000
 1609194/2000000: episode: 2062, duration: 28.260s, episode steps: 1129, steps per second:  40, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.704 [0.000, 5.000],  loss: 0.020740, mae: 3.325830, mean_q: 4.011935, mean_eps: 0.100000
 1610349/2000000: episode: 2063, duration: 30.157s, episode steps: 1155, steps per second:  38, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.090 [0.000, 5.000],  loss: 0.019413, mae: 3.304656, mean_q: 3.985734, mean_eps: 0.100000
 1611386/2000000: episode: 2064, duration: 26.694s, episode steps: 1037, steps per second:  39, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.855 [0.000, 5.000],  loss: 0.021513, mae: 3.292544, mean_q: 3.972382, mean_eps: 0.100000
 1611756/2000000: episode: 2065, duration: 10.131s, episode steps: 370, steps per second:  37, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.018764, mae: 3.301085, mean_q: 3.979893, mean_eps: 0.100000
 1612559/2000000: episode: 2066, duration: 20.827s, episode steps: 803, steps per second:  39, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.021885, mae: 3.301513, mean_q: 3.980327, mean_eps: 0.100000
 1613128/2000000: episode: 2067, duration: 15.462s, episode steps: 569, steps per second:  37, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.020065, mae: 3.298396, mean_q: 3.975892, mean_eps: 0.100000
 1614085/2000000: episode: 2068, duration: 23.582s, episode steps: 957, steps per second:  41, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.892 [0.000, 5.000],  loss: 0.018198, mae: 3.291761, mean_q: 3.969557, mean_eps: 0.100000
 1615418/2000000: episode: 2069, duration: 32.228s, episode steps: 1333, steps per second:  41, episode reward: 27.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.019544, mae: 3.302716, mean_q: 3.981235, mean_eps: 0.100000
 1616429/2000000: episode: 2070, duration: 25.040s, episode steps: 1011, steps per second:  40, episode reward: 29.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.196 [0.000, 5.000],  loss: 0.019774, mae: 3.349172, mean_q: 4.037389, mean_eps: 0.100000
 1617127/2000000: episode: 2071, duration: 16.823s, episode steps: 698, steps per second:  41, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.022178, mae: 3.334646, mean_q: 4.019927, mean_eps: 0.100000
 1618366/2000000: episode: 2072, duration: 30.762s, episode steps: 1239, steps per second:  40, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.020280, mae: 3.317391, mean_q: 3.998902, mean_eps: 0.100000
 1619459/2000000: episode: 2073, duration: 24.880s, episode steps: 1093, steps per second:  44, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.922 [0.000, 5.000],  loss: 0.020908, mae: 3.326166, mean_q: 4.010286, mean_eps: 0.100000
 1620364/2000000: episode: 2074, duration: 22.234s, episode steps: 905, steps per second:  41, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.260 [0.000, 5.000],  loss: 0.020535, mae: 3.319056, mean_q: 4.001126, mean_eps: 0.100000
 1621314/2000000: episode: 2075, duration: 22.869s, episode steps: 950, steps per second:  42, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.192 [0.000, 5.000],  loss: 0.019240, mae: 3.347409, mean_q: 4.036483, mean_eps: 0.100000
 1622765/2000000: episode: 2076, duration: 34.034s, episode steps: 1451, steps per second:  43, episode reward: 32.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.773 [0.000, 5.000],  loss: 0.022199, mae: 3.345387, mean_q: 4.033859, mean_eps: 0.100000
 1623659/2000000: episode: 2077, duration: 22.027s, episode steps: 894, steps per second:  41, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: 0.020479, mae: 3.317506, mean_q: 4.001714, mean_eps: 0.100000
 1624161/2000000: episode: 2078, duration: 12.440s, episode steps: 502, steps per second:  40, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.021334, mae: 3.325055, mean_q: 4.008287, mean_eps: 0.100000
 1625256/2000000: episode: 2079, duration: 28.681s, episode steps: 1095, steps per second:  38, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.019457, mae: 3.307410, mean_q: 3.988065, mean_eps: 0.100000
 1626036/2000000: episode: 2080, duration: 20.216s, episode steps: 780, steps per second:  39, episode reward: 25.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 3.199 [0.000, 5.000],  loss: 0.018988, mae: 3.311284, mean_q: 3.992805, mean_eps: 0.100000
 1627142/2000000: episode: 2081, duration: 27.387s, episode steps: 1106, steps per second:  40, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.021953, mae: 3.304244, mean_q: 3.984945, mean_eps: 0.100000
 1628254/2000000: episode: 2082, duration: 26.136s, episode steps: 1112, steps per second:  43, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.217 [0.000, 5.000],  loss: 0.020825, mae: 3.294987, mean_q: 3.971988, mean_eps: 0.100000
 1628951/2000000: episode: 2083, duration: 17.527s, episode steps: 697, steps per second:  40, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.018406, mae: 3.291472, mean_q: 3.966562, mean_eps: 0.100000
 1630026/2000000: episode: 2084, duration: 26.411s, episode steps: 1075, steps per second:  41, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.020854, mae: 3.291367, mean_q: 3.968709, mean_eps: 0.100000
 1630913/2000000: episode: 2085, duration: 22.592s, episode steps: 887, steps per second:  39, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.019235, mae: 3.300829, mean_q: 3.979907, mean_eps: 0.100000
 1631884/2000000: episode: 2086, duration: 24.676s, episode steps: 971, steps per second:  39, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.200 [0.000, 5.000],  loss: 0.018544, mae: 3.304233, mean_q: 3.983245, mean_eps: 0.100000
 1632946/2000000: episode: 2087, duration: 27.526s, episode steps: 1062, steps per second:  39, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.779 [0.000, 5.000],  loss: 0.020643, mae: 3.290871, mean_q: 3.966792, mean_eps: 0.100000
 1633698/2000000: episode: 2088, duration: 17.407s, episode steps: 752, steps per second:  43, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.930 [0.000, 5.000],  loss: 0.020484, mae: 3.305305, mean_q: 3.983239, mean_eps: 0.100000
 1634613/2000000: episode: 2089, duration: 22.102s, episode steps: 915, steps per second:  41, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.020082, mae: 3.292213, mean_q: 3.969021, mean_eps: 0.100000
 1635450/2000000: episode: 2090, duration: 20.809s, episode steps: 837, steps per second:  40, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.041 [0.000, 5.000],  loss: 0.020262, mae: 3.267203, mean_q: 3.942131, mean_eps: 0.100000
 1636343/2000000: episode: 2091, duration: 22.147s, episode steps: 893, steps per second:  40, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.020430, mae: 3.257149, mean_q: 3.926942, mean_eps: 0.100000
 1637358/2000000: episode: 2092, duration: 24.264s, episode steps: 1015, steps per second:  42, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.783 [0.000, 5.000],  loss: 0.019565, mae: 3.245180, mean_q: 3.911657, mean_eps: 0.100000
 1638231/2000000: episode: 2093, duration: 21.532s, episode steps: 873, steps per second:  41, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.178 [0.000, 5.000],  loss: 0.021515, mae: 3.263270, mean_q: 3.932102, mean_eps: 0.100000
 1638723/2000000: episode: 2094, duration: 12.632s, episode steps: 492, steps per second:  39, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.728 [0.000, 5.000],  loss: 0.020772, mae: 3.252332, mean_q: 3.920458, mean_eps: 0.100000
 1639675/2000000: episode: 2095, duration: 25.178s, episode steps: 952, steps per second:  38, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.769 [0.000, 5.000],  loss: 0.020423, mae: 3.252564, mean_q: 3.920963, mean_eps: 0.100000
 1640402/2000000: episode: 2096, duration: 18.834s, episode steps: 727, steps per second:  39, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.272 [0.000, 5.000],  loss: 0.016868, mae: 3.265434, mean_q: 3.938923, mean_eps: 0.100000
 1641914/2000000: episode: 2097, duration: 36.663s, episode steps: 1512, steps per second:  41, episode reward: 27.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.746 [0.000, 5.000],  loss: 0.018286, mae: 3.253256, mean_q: 3.920555, mean_eps: 0.100000
 1642602/2000000: episode: 2098, duration: 17.211s, episode steps: 688, steps per second:  40, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.185 [0.000, 5.000],  loss: 0.019500, mae: 3.228609, mean_q: 3.891815, mean_eps: 0.100000
 1643564/2000000: episode: 2099, duration: 24.051s, episode steps: 962, steps per second:  40, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.846 [0.000, 5.000],  loss: 0.020000, mae: 3.249164, mean_q: 3.916249, mean_eps: 0.100000
 1644467/2000000: episode: 2100, duration: 23.219s, episode steps: 903, steps per second:  39, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.080 [0.000, 5.000],  loss: 0.020009, mae: 3.225967, mean_q: 3.889419, mean_eps: 0.100000
 1645424/2000000: episode: 2101, duration: 24.302s, episode steps: 957, steps per second:  39, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.089 [0.000, 5.000],  loss: 0.019462, mae: 3.242056, mean_q: 3.909698, mean_eps: 0.100000
 1646763/2000000: episode: 2102, duration: 32.286s, episode steps: 1339, steps per second:  41, episode reward: 34.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.017522, mae: 3.240543, mean_q: 3.908288, mean_eps: 0.100000
 1647702/2000000: episode: 2103, duration: 22.160s, episode steps: 939, steps per second:  42, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.021636, mae: 3.248840, mean_q: 3.919275, mean_eps: 0.100000
 1648173/2000000: episode: 2104, duration: 11.127s, episode steps: 471, steps per second:  42, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.023507, mae: 3.255976, mean_q: 3.925059, mean_eps: 0.100000
 1649211/2000000: episode: 2105, duration: 24.546s, episode steps: 1038, steps per second:  42, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.019862, mae: 3.241228, mean_q: 3.907759, mean_eps: 0.100000
 1649818/2000000: episode: 2106, duration: 14.788s, episode steps: 607, steps per second:  41, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.061 [0.000, 5.000],  loss: 0.021372, mae: 3.236286, mean_q: 3.900648, mean_eps: 0.100000
 1650451/2000000: episode: 2107, duration: 15.691s, episode steps: 633, steps per second:  40, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.018679, mae: 3.245146, mean_q: 3.913675, mean_eps: 0.100000
 1651254/2000000: episode: 2108, duration: 18.950s, episode steps: 803, steps per second:  42, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.023266, mae: 3.256612, mean_q: 3.925012, mean_eps: 0.100000
 1651970/2000000: episode: 2109, duration: 17.353s, episode steps: 716, steps per second:  41, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.050 [0.000, 5.000],  loss: 0.024779, mae: 3.235954, mean_q: 3.900533, mean_eps: 0.100000
 1652636/2000000: episode: 2110, duration: 15.717s, episode steps: 666, steps per second:  42, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.022812, mae: 3.248069, mean_q: 3.914793, mean_eps: 0.100000
 1653751/2000000: episode: 2111, duration: 27.966s, episode steps: 1115, steps per second:  40, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.021978, mae: 3.251384, mean_q: 3.920613, mean_eps: 0.100000
 1655031/2000000: episode: 2112, duration: 31.929s, episode steps: 1280, steps per second:  40, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.019857, mae: 3.240677, mean_q: 3.907356, mean_eps: 0.100000
 1656611/2000000: episode: 2113, duration: 36.513s, episode steps: 1580, steps per second:  43, episode reward: 35.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.021812, mae: 3.269124, mean_q: 3.942251, mean_eps: 0.100000
 1657357/2000000: episode: 2114, duration: 16.941s, episode steps: 746, steps per second:  44, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.060 [0.000, 5.000],  loss: 0.021008, mae: 3.266331, mean_q: 3.939595, mean_eps: 0.100000
 1658659/2000000: episode: 2115, duration: 30.077s, episode steps: 1302, steps per second:  43, episode reward: 29.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.020665, mae: 3.266412, mean_q: 3.938944, mean_eps: 0.100000
 1659460/2000000: episode: 2116, duration: 20.716s, episode steps: 801, steps per second:  39, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.040 [0.000, 5.000],  loss: 0.020376, mae: 3.262513, mean_q: 3.935739, mean_eps: 0.100000
 1660215/2000000: episode: 2117, duration: 18.985s, episode steps: 755, steps per second:  40, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.882 [0.000, 5.000],  loss: 0.023388, mae: 3.262533, mean_q: 3.932676, mean_eps: 0.100000
 1660828/2000000: episode: 2118, duration: 15.202s, episode steps: 613, steps per second:  40, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.550 [0.000, 5.000],  loss: 0.020820, mae: 3.259787, mean_q: 3.929912, mean_eps: 0.100000
 1661782/2000000: episode: 2119, duration: 23.102s, episode steps: 954, steps per second:  41, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.205 [0.000, 5.000],  loss: 0.019005, mae: 3.275007, mean_q: 3.949993, mean_eps: 0.100000
 1662753/2000000: episode: 2120, duration: 22.772s, episode steps: 971, steps per second:  43, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.227 [0.000, 5.000],  loss: 0.022037, mae: 3.279489, mean_q: 3.954038, mean_eps: 0.100000
 1663512/2000000: episode: 2121, duration: 17.247s, episode steps: 759, steps per second:  44, episode reward: 21.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.019982, mae: 3.267345, mean_q: 3.940138, mean_eps: 0.100000
 1664054/2000000: episode: 2122, duration: 13.368s, episode steps: 542, steps per second:  41, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.018589, mae: 3.261135, mean_q: 3.931892, mean_eps: 0.100000
 1665424/2000000: episode: 2123, duration: 33.388s, episode steps: 1370, steps per second:  41, episode reward: 25.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.185 [0.000, 5.000],  loss: 0.021648, mae: 3.275639, mean_q: 3.950677, mean_eps: 0.100000
 1665845/2000000: episode: 2124, duration: 9.665s, episode steps: 421, steps per second:  44, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.418 [0.000, 5.000],  loss: 0.020296, mae: 3.290033, mean_q: 3.966342, mean_eps: 0.100000
 1666777/2000000: episode: 2125, duration: 21.449s, episode steps: 932, steps per second:  43, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.709 [0.000, 5.000],  loss: 0.019959, mae: 3.264312, mean_q: 3.936131, mean_eps: 0.100000
 1667704/2000000: episode: 2126, duration: 21.811s, episode steps: 927, steps per second:  43, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.020841, mae: 3.274926, mean_q: 3.947571, mean_eps: 0.100000
 1668352/2000000: episode: 2127, duration: 15.947s, episode steps: 648, steps per second:  41, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.252 [0.000, 5.000],  loss: 0.020723, mae: 3.265697, mean_q: 3.938166, mean_eps: 0.100000
 1669381/2000000: episode: 2128, duration: 26.295s, episode steps: 1029, steps per second:  39, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.762 [0.000, 5.000],  loss: 0.021535, mae: 3.274949, mean_q: 3.948818, mean_eps: 0.100000
 1670145/2000000: episode: 2129, duration: 19.313s, episode steps: 764, steps per second:  40, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.099 [0.000, 5.000],  loss: 0.020641, mae: 3.256206, mean_q: 3.930408, mean_eps: 0.100000
 1671189/2000000: episode: 2130, duration: 25.800s, episode steps: 1044, steps per second:  40, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.021631, mae: 3.252720, mean_q: 3.922898, mean_eps: 0.100000
 1671676/2000000: episode: 2131, duration: 12.057s, episode steps: 487, steps per second:  40, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.780 [0.000, 5.000],  loss: 0.017140, mae: 3.241908, mean_q: 3.912646, mean_eps: 0.100000
 1672608/2000000: episode: 2132, duration: 23.541s, episode steps: 932, steps per second:  40, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.020546, mae: 3.217448, mean_q: 3.880521, mean_eps: 0.100000
 1673520/2000000: episode: 2133, duration: 22.769s, episode steps: 912, steps per second:  40, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.065 [0.000, 5.000],  loss: 0.018072, mae: 3.238666, mean_q: 3.907321, mean_eps: 0.100000
 1674773/2000000: episode: 2134, duration: 31.586s, episode steps: 1253, steps per second:  40, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.887 [0.000, 5.000],  loss: 0.018220, mae: 3.234886, mean_q: 3.900482, mean_eps: 0.100000
 1675808/2000000: episode: 2135, duration: 25.028s, episode steps: 1035, steps per second:  41, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.170 [0.000, 5.000],  loss: 0.019551, mae: 3.228600, mean_q: 3.895525, mean_eps: 0.100000
 1676945/2000000: episode: 2136, duration: 29.028s, episode steps: 1137, steps per second:  39, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.795 [0.000, 5.000],  loss: 0.020655, mae: 3.240296, mean_q: 3.907361, mean_eps: 0.100000
 1677608/2000000: episode: 2137, duration: 15.597s, episode steps: 663, steps per second:  43, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.101 [0.000, 5.000],  loss: 0.020230, mae: 3.231789, mean_q: 3.899125, mean_eps: 0.100000
 1678664/2000000: episode: 2138, duration: 25.064s, episode steps: 1056, steps per second:  42, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.019326, mae: 3.228084, mean_q: 3.895313, mean_eps: 0.100000
 1679138/2000000: episode: 2139, duration: 11.871s, episode steps: 474, steps per second:  40, episode reward: 12.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.793 [0.000, 5.000],  loss: 0.023880, mae: 3.220838, mean_q: 3.882695, mean_eps: 0.100000
 1679754/2000000: episode: 2140, duration: 15.271s, episode steps: 616, steps per second:  40, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.726 [0.000, 5.000],  loss: 0.020827, mae: 3.217641, mean_q: 3.881605, mean_eps: 0.100000
 1680840/2000000: episode: 2141, duration: 25.332s, episode steps: 1086, steps per second:  43, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.689 [0.000, 5.000],  loss: 0.016142, mae: 3.219831, mean_q: 3.885491, mean_eps: 0.100000
 1681787/2000000: episode: 2142, duration: 21.748s, episode steps: 947, steps per second:  44, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.843 [0.000, 5.000],  loss: 0.019591, mae: 3.202970, mean_q: 3.863388, mean_eps: 0.100000
 1682803/2000000: episode: 2143, duration: 24.777s, episode steps: 1016, steps per second:  41, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.021119, mae: 3.211278, mean_q: 3.872525, mean_eps: 0.100000
 1683643/2000000: episode: 2144, duration: 22.429s, episode steps: 840, steps per second:  37, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.982 [0.000, 5.000],  loss: 0.021549, mae: 3.216947, mean_q: 3.878917, mean_eps: 0.100000
 1684756/2000000: episode: 2145, duration: 29.333s, episode steps: 1113, steps per second:  38, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.022060, mae: 3.203147, mean_q: 3.861212, mean_eps: 0.100000
 1685566/2000000: episode: 2146, duration: 20.439s, episode steps: 810, steps per second:  40, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.351 [0.000, 5.000],  loss: 0.017121, mae: 3.217027, mean_q: 3.881597, mean_eps: 0.100000
 1686847/2000000: episode: 2147, duration: 31.507s, episode steps: 1281, steps per second:  41, episode reward: 32.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.017806, mae: 3.211245, mean_q: 3.871656, mean_eps: 0.100000
 1687485/2000000: episode: 2148, duration: 14.562s, episode steps: 638, steps per second:  44, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.801 [0.000, 5.000],  loss: 0.018635, mae: 3.165746, mean_q: 3.816000, mean_eps: 0.100000
 1688239/2000000: episode: 2149, duration: 17.271s, episode steps: 754, steps per second:  44, episode reward: 23.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.020022, mae: 3.196156, mean_q: 3.855055, mean_eps: 0.100000
 1689079/2000000: episode: 2150, duration: 21.199s, episode steps: 840, steps per second:  40, episode reward: 24.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.124 [0.000, 5.000],  loss: 0.018785, mae: 3.182158, mean_q: 3.838639, mean_eps: 0.100000
 1690765/2000000: episode: 2151, duration: 41.596s, episode steps: 1686, steps per second:  41, episode reward: 35.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.017824, mae: 3.206468, mean_q: 3.867430, mean_eps: 0.100000
 1691447/2000000: episode: 2152, duration: 16.443s, episode steps: 682, steps per second:  41, episode reward: 22.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.019148, mae: 3.174224, mean_q: 3.826812, mean_eps: 0.100000
 1692069/2000000: episode: 2153, duration: 14.766s, episode steps: 622, steps per second:  42, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.746 [0.000, 5.000],  loss: 0.019702, mae: 3.184001, mean_q: 3.839589, mean_eps: 0.100000
 1692987/2000000: episode: 2154, duration: 20.818s, episode steps: 918, steps per second:  44, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.020253, mae: 3.190916, mean_q: 3.846017, mean_eps: 0.100000
 1693505/2000000: episode: 2155, duration: 12.060s, episode steps: 518, steps per second:  43, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.019460, mae: 3.181148, mean_q: 3.834590, mean_eps: 0.100000
 1694659/2000000: episode: 2156, duration: 26.461s, episode steps: 1154, steps per second:  44, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.019785, mae: 3.173488, mean_q: 3.825769, mean_eps: 0.100000
 1695702/2000000: episode: 2157, duration: 23.912s, episode steps: 1043, steps per second:  44, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.018131, mae: 3.177371, mean_q: 3.831639, mean_eps: 0.100000
 1696380/2000000: episode: 2158, duration: 15.534s, episode steps: 678, steps per second:  44, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.830 [0.000, 5.000],  loss: 0.018949, mae: 3.184007, mean_q: 3.837946, mean_eps: 0.100000
 1696901/2000000: episode: 2159, duration: 12.290s, episode steps: 521, steps per second:  42, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.018205, mae: 3.161108, mean_q: 3.810925, mean_eps: 0.100000
 1697522/2000000: episode: 2160, duration: 14.817s, episode steps: 621, steps per second:  42, episode reward: 18.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.984 [0.000, 5.000],  loss: 0.019194, mae: 3.176573, mean_q: 3.827511, mean_eps: 0.100000
 1698670/2000000: episode: 2161, duration: 27.060s, episode steps: 1148, steps per second:  42, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.186 [0.000, 5.000],  loss: 0.022222, mae: 3.175865, mean_q: 3.828179, mean_eps: 0.100000
 1699199/2000000: episode: 2162, duration: 12.778s, episode steps: 529, steps per second:  41, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.499 [0.000, 5.000],  loss: 0.020745, mae: 3.157014, mean_q: 3.805815, mean_eps: 0.100000
 1699728/2000000: episode: 2163, duration: 12.645s, episode steps: 529, steps per second:  42, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.023190, mae: 3.169028, mean_q: 3.818558, mean_eps: 0.100000
 1700509/2000000: episode: 2164, duration: 18.881s, episode steps: 781, steps per second:  41, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.050 [0.000, 5.000],  loss: 0.018485, mae: 3.148296, mean_q: 3.796029, mean_eps: 0.100000
 1701463/2000000: episode: 2165, duration: 22.825s, episode steps: 954, steps per second:  42, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.179 [0.000, 5.000],  loss: 0.020478, mae: 3.143375, mean_q: 3.789016, mean_eps: 0.100000
 1702396/2000000: episode: 2166, duration: 22.576s, episode steps: 933, steps per second:  41, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.072 [0.000, 5.000],  loss: 0.019120, mae: 3.157645, mean_q: 3.806274, mean_eps: 0.100000
 1703088/2000000: episode: 2167, duration: 15.742s, episode steps: 692, steps per second:  44, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.010 [0.000, 5.000],  loss: 0.020406, mae: 3.147761, mean_q: 3.794418, mean_eps: 0.100000
 1704029/2000000: episode: 2168, duration: 21.818s, episode steps: 941, steps per second:  43, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.182 [0.000, 5.000],  loss: 0.018876, mae: 3.139196, mean_q: 3.783367, mean_eps: 0.100000
 1704875/2000000: episode: 2169, duration: 19.161s, episode steps: 846, steps per second:  44, episode reward: 25.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.017980, mae: 3.135842, mean_q: 3.780381, mean_eps: 0.100000
 1705685/2000000: episode: 2170, duration: 18.927s, episode steps: 810, steps per second:  43, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.736 [0.000, 5.000],  loss: 0.019097, mae: 3.111740, mean_q: 3.750685, mean_eps: 0.100000
 1706550/2000000: episode: 2171, duration: 20.242s, episode steps: 865, steps per second:  43, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.018570, mae: 3.093350, mean_q: 3.728804, mean_eps: 0.100000
 1707466/2000000: episode: 2172, duration: 20.860s, episode steps: 916, steps per second:  44, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.018380, mae: 3.110339, mean_q: 3.748986, mean_eps: 0.100000
 1708329/2000000: episode: 2173, duration: 18.896s, episode steps: 863, steps per second:  46, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.790 [0.000, 5.000],  loss: 0.022086, mae: 3.097192, mean_q: 3.733034, mean_eps: 0.100000
 1709548/2000000: episode: 2174, duration: 29.588s, episode steps: 1219, steps per second:  41, episode reward: 32.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.019670, mae: 3.097833, mean_q: 3.733720, mean_eps: 0.100000
 1710311/2000000: episode: 2175, duration: 17.749s, episode steps: 763, steps per second:  43, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.017893, mae: 3.091238, mean_q: 3.725448, mean_eps: 0.100000
 1711019/2000000: episode: 2176, duration: 14.957s, episode steps: 708, steps per second:  47, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.000 [0.000, 5.000],  loss: 0.017871, mae: 3.065380, mean_q: 3.696248, mean_eps: 0.100000
 1712007/2000000: episode: 2177, duration: 21.757s, episode steps: 988, steps per second:  45, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.738 [0.000, 5.000],  loss: 0.022601, mae: 3.083368, mean_q: 3.719865, mean_eps: 0.100000
 1712819/2000000: episode: 2178, duration: 18.309s, episode steps: 812, steps per second:  44, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.017461, mae: 3.062339, mean_q: 3.690629, mean_eps: 0.100000
 1713810/2000000: episode: 2179, duration: 23.295s, episode steps: 991, steps per second:  43, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.201 [0.000, 5.000],  loss: 0.021749, mae: 3.084629, mean_q: 3.717195, mean_eps: 0.100000
 1714609/2000000: episode: 2180, duration: 20.272s, episode steps: 799, steps per second:  39, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.038 [0.000, 5.000],  loss: 0.018649, mae: 3.075584, mean_q: 3.708446, mean_eps: 0.100000
 1715759/2000000: episode: 2181, duration: 27.821s, episode steps: 1150, steps per second:  41, episode reward: 31.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.203 [0.000, 5.000],  loss: 0.017694, mae: 3.088935, mean_q: 3.724369, mean_eps: 0.100000
 1716687/2000000: episode: 2182, duration: 21.928s, episode steps: 928, steps per second:  42, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.102 [0.000, 5.000],  loss: 0.017052, mae: 3.092791, mean_q: 3.730510, mean_eps: 0.100000
 1717781/2000000: episode: 2183, duration: 25.824s, episode steps: 1094, steps per second:  42, episode reward: 31.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.022 [0.000, 5.000],  loss: 0.018278, mae: 3.078424, mean_q: 3.711447, mean_eps: 0.100000
 1718802/2000000: episode: 2184, duration: 22.418s, episode steps: 1021, steps per second:  46, episode reward: 29.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.776 [0.000, 5.000],  loss: 0.019525, mae: 3.106582, mean_q: 3.744577, mean_eps: 0.100000
 1719907/2000000: episode: 2185, duration: 25.951s, episode steps: 1105, steps per second:  43, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.748 [0.000, 5.000],  loss: 0.018108, mae: 3.097797, mean_q: 3.734119, mean_eps: 0.100000
 1720532/2000000: episode: 2186, duration: 14.491s, episode steps: 625, steps per second:  43, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.016438, mae: 3.063316, mean_q: 3.695366, mean_eps: 0.100000
 1720902/2000000: episode: 2187, duration: 8.306s, episode steps: 370, steps per second:  45, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.018870, mae: 3.089757, mean_q: 3.722575, mean_eps: 0.100000
 1722208/2000000: episode: 2188, duration: 29.531s, episode steps: 1306, steps per second:  44, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.208 [0.000, 5.000],  loss: 0.017832, mae: 3.068251, mean_q: 3.698172, mean_eps: 0.100000
 1723208/2000000: episode: 2189, duration: 22.549s, episode steps: 1000, steps per second:  44, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.017695, mae: 3.069636, mean_q: 3.700483, mean_eps: 0.100000
 1724343/2000000: episode: 2190, duration: 24.837s, episode steps: 1135, steps per second:  46, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.919 [0.000, 5.000],  loss: 0.019042, mae: 3.079562, mean_q: 3.711706, mean_eps: 0.100000
 1725371/2000000: episode: 2191, duration: 24.616s, episode steps: 1028, steps per second:  42, episode reward: 29.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.195 [0.000, 5.000],  loss: 0.017594, mae: 3.054045, mean_q: 3.683843, mean_eps: 0.100000
 1726061/2000000: episode: 2192, duration: 15.788s, episode steps: 690, steps per second:  44, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.017343, mae: 3.067127, mean_q: 3.697138, mean_eps: 0.100000
 1727449/2000000: episode: 2193, duration: 31.518s, episode steps: 1388, steps per second:  44, episode reward: 22.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.656 [0.000, 5.000],  loss: 0.018383, mae: 3.079827, mean_q: 3.712993, mean_eps: 0.100000
 1727905/2000000: episode: 2194, duration: 10.600s, episode steps: 456, steps per second:  43, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.017359, mae: 3.066086, mean_q: 3.696597, mean_eps: 0.100000
 1728380/2000000: episode: 2195, duration: 11.027s, episode steps: 475, steps per second:  43, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.020264, mae: 3.081373, mean_q: 3.716349, mean_eps: 0.100000
 1729553/2000000: episode: 2196, duration: 28.894s, episode steps: 1173, steps per second:  41, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.673 [0.000, 5.000],  loss: 0.019235, mae: 3.071761, mean_q: 3.702343, mean_eps: 0.100000
 1730186/2000000: episode: 2197, duration: 16.370s, episode steps: 633, steps per second:  39, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.088 [0.000, 5.000],  loss: 0.020884, mae: 3.057589, mean_q: 3.684919, mean_eps: 0.100000
 1731262/2000000: episode: 2198, duration: 26.682s, episode steps: 1076, steps per second:  40, episode reward: 30.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.754 [0.000, 5.000],  loss: 0.019696, mae: 3.066599, mean_q: 3.697723, mean_eps: 0.100000
 1732016/2000000: episode: 2199, duration: 18.637s, episode steps: 754, steps per second:  40, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.018687, mae: 3.092698, mean_q: 3.727385, mean_eps: 0.100000
 1732681/2000000: episode: 2200, duration: 16.529s, episode steps: 665, steps per second:  40, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.961 [0.000, 5.000],  loss: 0.016132, mae: 3.061035, mean_q: 3.691865, mean_eps: 0.100000
 1733264/2000000: episode: 2201, duration: 13.604s, episode steps: 583, steps per second:  43, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.021585, mae: 3.070386, mean_q: 3.699927, mean_eps: 0.100000
 1734203/2000000: episode: 2202, duration: 21.450s, episode steps: 939, steps per second:  44, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.925 [0.000, 5.000],  loss: 0.017306, mae: 3.062375, mean_q: 3.692102, mean_eps: 0.100000
 1735590/2000000: episode: 2203, duration: 34.221s, episode steps: 1387, steps per second:  41, episode reward: 31.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.017516, mae: 3.066295, mean_q: 3.697271, mean_eps: 0.100000
 1736607/2000000: episode: 2204, duration: 23.050s, episode steps: 1017, steps per second:  44, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.028 [0.000, 5.000],  loss: 0.017934, mae: 3.069167, mean_q: 3.700949, mean_eps: 0.100000
 1737273/2000000: episode: 2205, duration: 14.908s, episode steps: 666, steps per second:  45, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.249 [0.000, 5.000],  loss: 0.016403, mae: 3.049668, mean_q: 3.676534, mean_eps: 0.100000
 1738086/2000000: episode: 2206, duration: 18.256s, episode steps: 813, steps per second:  45, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.019400, mae: 3.058115, mean_q: 3.687737, mean_eps: 0.100000
 1739040/2000000: episode: 2207, duration: 20.534s, episode steps: 954, steps per second:  46, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.673 [0.000, 5.000],  loss: 0.018853, mae: 3.069841, mean_q: 3.700400, mean_eps: 0.100000
 1739651/2000000: episode: 2208, duration: 13.653s, episode steps: 611, steps per second:  45, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.018675, mae: 3.084123, mean_q: 3.720446, mean_eps: 0.100000
 1740561/2000000: episode: 2209, duration: 23.587s, episode steps: 910, steps per second:  39, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.101 [0.000, 5.000],  loss: 0.017185, mae: 3.053186, mean_q: 3.684238, mean_eps: 0.100000
 1741450/2000000: episode: 2210, duration: 20.670s, episode steps: 889, steps per second:  43, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.004 [0.000, 5.000],  loss: 0.018932, mae: 3.058660, mean_q: 3.689478, mean_eps: 0.100000
 1742371/2000000: episode: 2211, duration: 21.044s, episode steps: 921, steps per second:  44, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.018616, mae: 3.066184, mean_q: 3.696850, mean_eps: 0.100000
 1743245/2000000: episode: 2212, duration: 19.711s, episode steps: 874, steps per second:  44, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.018204, mae: 3.045097, mean_q: 3.671150, mean_eps: 0.100000
 1744299/2000000: episode: 2213, duration: 25.253s, episode steps: 1054, steps per second:  42, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.020333, mae: 3.061408, mean_q: 3.691843, mean_eps: 0.100000
 1745132/2000000: episode: 2214, duration: 20.716s, episode steps: 833, steps per second:  40, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.893 [0.000, 5.000],  loss: 0.017507, mae: 3.085126, mean_q: 3.719761, mean_eps: 0.100000
 1746128/2000000: episode: 2215, duration: 25.408s, episode steps: 996, steps per second:  39, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.019610, mae: 3.054934, mean_q: 3.682373, mean_eps: 0.100000
 1747004/2000000: episode: 2216, duration: 21.676s, episode steps: 876, steps per second:  40, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.019008, mae: 3.029246, mean_q: 3.651103, mean_eps: 0.100000
 1748046/2000000: episode: 2217, duration: 25.465s, episode steps: 1042, steps per second:  41, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.018166, mae: 3.046016, mean_q: 3.672390, mean_eps: 0.100000
 1748999/2000000: episode: 2218, duration: 22.847s, episode steps: 953, steps per second:  42, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.998 [0.000, 5.000],  loss: 0.019882, mae: 3.050875, mean_q: 3.679432, mean_eps: 0.100000
 1750110/2000000: episode: 2219, duration: 26.069s, episode steps: 1111, steps per second:  43, episode reward: 30.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.020874, mae: 3.050053, mean_q: 3.676722, mean_eps: 0.100000
 1751073/2000000: episode: 2220, duration: 22.423s, episode steps: 963, steps per second:  43, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.334 [0.000, 5.000],  loss: 0.019761, mae: 3.027062, mean_q: 3.650057, mean_eps: 0.100000
 1751827/2000000: episode: 2221, duration: 17.451s, episode steps: 754, steps per second:  43, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.019165, mae: 3.053106, mean_q: 3.683948, mean_eps: 0.100000
 1752715/2000000: episode: 2222, duration: 21.434s, episode steps: 888, steps per second:  41, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.199 [0.000, 5.000],  loss: 0.019736, mae: 3.028426, mean_q: 3.651551, mean_eps: 0.100000
 1753648/2000000: episode: 2223, duration: 21.242s, episode steps: 933, steps per second:  44, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.021283, mae: 3.052066, mean_q: 3.679833, mean_eps: 0.100000
 1754332/2000000: episode: 2224, duration: 15.632s, episode steps: 684, steps per second:  44, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.782 [0.000, 5.000],  loss: 0.018416, mae: 3.058347, mean_q: 3.687900, mean_eps: 0.100000
 1755118/2000000: episode: 2225, duration: 19.006s, episode steps: 786, steps per second:  41, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.791 [0.000, 5.000],  loss: 0.018873, mae: 3.053376, mean_q: 3.680190, mean_eps: 0.100000
 1755871/2000000: episode: 2226, duration: 18.218s, episode steps: 753, steps per second:  41, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.319 [0.000, 5.000],  loss: 0.016457, mae: 3.061280, mean_q: 3.693185, mean_eps: 0.100000
 1756844/2000000: episode: 2227, duration: 23.784s, episode steps: 973, steps per second:  41, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.020798, mae: 3.036088, mean_q: 3.661183, mean_eps: 0.100000
 1758199/2000000: episode: 2228, duration: 31.792s, episode steps: 1355, steps per second:  43, episode reward: 31.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.323 [0.000, 5.000],  loss: 0.018302, mae: 3.055042, mean_q: 3.683385, mean_eps: 0.100000
 1759283/2000000: episode: 2229, duration: 25.624s, episode steps: 1084, steps per second:  42, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.019215, mae: 3.048007, mean_q: 3.673919, mean_eps: 0.100000
 1759658/2000000: episode: 2230, duration: 8.891s, episode steps: 375, steps per second:  42, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.016729, mae: 3.021652, mean_q: 3.643671, mean_eps: 0.100000
 1760525/2000000: episode: 2231, duration: 20.905s, episode steps: 867, steps per second:  41, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 3.236 [0.000, 5.000],  loss: 0.019839, mae: 3.024205, mean_q: 3.648854, mean_eps: 0.100000
 1761507/2000000: episode: 2232, duration: 22.632s, episode steps: 982, steps per second:  43, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.020809, mae: 3.022255, mean_q: 3.642726, mean_eps: 0.100000
 1762754/2000000: episode: 2233, duration: 29.927s, episode steps: 1247, steps per second:  42, episode reward: 25.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.103 [0.000, 5.000],  loss: 0.021872, mae: 3.048923, mean_q: 3.673286, mean_eps: 0.100000
 1763893/2000000: episode: 2234, duration: 26.787s, episode steps: 1139, steps per second:  43, episode reward: 32.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.002 [0.000, 5.000],  loss: 0.018316, mae: 3.045930, mean_q: 3.671733, mean_eps: 0.100000
 1765195/2000000: episode: 2235, duration: 29.172s, episode steps: 1302, steps per second:  45, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.953 [0.000, 5.000],  loss: 0.016695, mae: 3.031811, mean_q: 3.654534, mean_eps: 0.100000
 1766320/2000000: episode: 2236, duration: 25.908s, episode steps: 1125, steps per second:  43, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.017337, mae: 3.017460, mean_q: 3.639921, mean_eps: 0.100000
 1767354/2000000: episode: 2237, duration: 23.485s, episode steps: 1034, steps per second:  44, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.208 [0.000, 5.000],  loss: 0.018738, mae: 3.019531, mean_q: 3.641424, mean_eps: 0.100000
 1768093/2000000: episode: 2238, duration: 16.806s, episode steps: 739, steps per second:  44, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.936 [0.000, 5.000],  loss: 0.019670, mae: 3.014860, mean_q: 3.632811, mean_eps: 0.100000
 1768815/2000000: episode: 2239, duration: 16.619s, episode steps: 722, steps per second:  43, episode reward: 20.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.020867, mae: 3.026680, mean_q: 3.647587, mean_eps: 0.100000
 1769812/2000000: episode: 2240, duration: 22.806s, episode steps: 997, steps per second:  44, episode reward: 29.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.019694, mae: 2.990373, mean_q: 3.604486, mean_eps: 0.100000
 1770299/2000000: episode: 2241, duration: 11.764s, episode steps: 487, steps per second:  41, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.018789, mae: 3.007813, mean_q: 3.627819, mean_eps: 0.100000
 1770921/2000000: episode: 2242, duration: 15.515s, episode steps: 622, steps per second:  40, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.021131, mae: 3.010464, mean_q: 3.632657, mean_eps: 0.100000
 1771794/2000000: episode: 2243, duration: 20.987s, episode steps: 873, steps per second:  42, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.019725, mae: 3.032205, mean_q: 3.657033, mean_eps: 0.100000
 1772796/2000000: episode: 2244, duration: 24.622s, episode steps: 1002, steps per second:  41, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.117 [0.000, 5.000],  loss: 0.018195, mae: 3.027837, mean_q: 3.650821, mean_eps: 0.100000
 1773604/2000000: episode: 2245, duration: 19.568s, episode steps: 808, steps per second:  41, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.110 [0.000, 5.000],  loss: 0.018005, mae: 3.018203, mean_q: 3.642182, mean_eps: 0.100000
 1774495/2000000: episode: 2246, duration: 21.973s, episode steps: 891, steps per second:  41, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.017984, mae: 3.015423, mean_q: 3.638751, mean_eps: 0.100000
 1775866/2000000: episode: 2247, duration: 33.699s, episode steps: 1371, steps per second:  41, episode reward: 30.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.841 [0.000, 5.000],  loss: 0.019547, mae: 2.992554, mean_q: 3.609959, mean_eps: 0.100000
 1777015/2000000: episode: 2248, duration: 28.589s, episode steps: 1149, steps per second:  40, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.051 [0.000, 5.000],  loss: 0.019673, mae: 2.998453, mean_q: 3.615958, mean_eps: 0.100000
 1777987/2000000: episode: 2249, duration: 23.099s, episode steps: 972, steps per second:  42, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.724 [0.000, 5.000],  loss: 0.018978, mae: 3.001887, mean_q: 3.618540, mean_eps: 0.100000
 1778954/2000000: episode: 2250, duration: 21.957s, episode steps: 967, steps per second:  44, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.018716, mae: 2.983577, mean_q: 3.597267, mean_eps: 0.100000
 1780206/2000000: episode: 2251, duration: 28.627s, episode steps: 1252, steps per second:  44, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.019183, mae: 3.005368, mean_q: 3.623036, mean_eps: 0.100000
 1781025/2000000: episode: 2252, duration: 19.733s, episode steps: 819, steps per second:  42, episode reward: 26.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.016447, mae: 3.002888, mean_q: 3.620952, mean_eps: 0.100000
 1781844/2000000: episode: 2253, duration: 19.754s, episode steps: 819, steps per second:  41, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.018461, mae: 3.013743, mean_q: 3.633956, mean_eps: 0.100000
 1782675/2000000: episode: 2254, duration: 19.956s, episode steps: 831, steps per second:  42, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.017872, mae: 3.010540, mean_q: 3.630090, mean_eps: 0.100000
 1783514/2000000: episode: 2255, duration: 20.387s, episode steps: 839, steps per second:  41, episode reward: 28.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.195 [0.000, 5.000],  loss: 0.015742, mae: 3.024224, mean_q: 3.647564, mean_eps: 0.100000
 1785103/2000000: episode: 2256, duration: 34.861s, episode steps: 1589, steps per second:  46, episode reward: 32.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.017885, mae: 3.011931, mean_q: 3.632588, mean_eps: 0.100000
 1786157/2000000: episode: 2257, duration: 24.411s, episode steps: 1054, steps per second:  43, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.019714, mae: 2.987071, mean_q: 3.602923, mean_eps: 0.100000
 1787027/2000000: episode: 2258, duration: 19.482s, episode steps: 870, steps per second:  45, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.018171, mae: 2.986994, mean_q: 3.600455, mean_eps: 0.100000
 1788261/2000000: episode: 2259, duration: 26.975s, episode steps: 1234, steps per second:  46, episode reward: 18.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 0.020486, mae: 2.988843, mean_q: 3.602425, mean_eps: 0.100000
 1789288/2000000: episode: 2260, duration: 23.110s, episode steps: 1027, steps per second:  44, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.674 [0.000, 5.000],  loss: 0.020608, mae: 2.998258, mean_q: 3.613632, mean_eps: 0.100000
 1789954/2000000: episode: 2261, duration: 15.741s, episode steps: 666, steps per second:  42, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.174 [0.000, 5.000],  loss: 0.021576, mae: 3.005481, mean_q: 3.621580, mean_eps: 0.100000
 1790982/2000000: episode: 2262, duration: 24.204s, episode steps: 1028, steps per second:  42, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: 0.019744, mae: 3.005800, mean_q: 3.622987, mean_eps: 0.100000
 1791648/2000000: episode: 2263, duration: 16.489s, episode steps: 666, steps per second:  40, episode reward: 20.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.919 [0.000, 5.000],  loss: 0.019215, mae: 3.009251, mean_q: 3.626141, mean_eps: 0.100000
 1792783/2000000: episode: 2264, duration: 27.669s, episode steps: 1135, steps per second:  41, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.019626, mae: 3.016518, mean_q: 3.635077, mean_eps: 0.100000
 1793449/2000000: episode: 2265, duration: 17.276s, episode steps: 666, steps per second:  39, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.806 [0.000, 5.000],  loss: 0.019895, mae: 2.986744, mean_q: 3.599845, mean_eps: 0.100000
 1794799/2000000: episode: 2266, duration: 33.540s, episode steps: 1350, steps per second:  40, episode reward: 26.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.184 [0.000, 5.000],  loss: 0.018804, mae: 3.000264, mean_q: 3.617346, mean_eps: 0.100000
 1796001/2000000: episode: 2267, duration: 29.736s, episode steps: 1202, steps per second:  40, episode reward: 32.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.784 [0.000, 5.000],  loss: 0.018974, mae: 3.002385, mean_q: 3.620700, mean_eps: 0.100000
 1796926/2000000: episode: 2268, duration: 28.105s, episode steps: 925, steps per second:  33, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.704 [0.000, 5.000],  loss: 0.017191, mae: 3.027434, mean_q: 3.650572, mean_eps: 0.100000
 1797750/2000000: episode: 2269, duration: 19.754s, episode steps: 824, steps per second:  42, episode reward: 24.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.019379, mae: 3.003236, mean_q: 3.620242, mean_eps: 0.100000
 1798583/2000000: episode: 2270, duration: 20.047s, episode steps: 833, steps per second:  42, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.071 [0.000, 5.000],  loss: 0.020237, mae: 3.005306, mean_q: 3.622708, mean_eps: 0.100000
 1799432/2000000: episode: 2271, duration: 18.410s, episode steps: 849, steps per second:  46, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.994 [0.000, 5.000],  loss: 0.018162, mae: 3.005007, mean_q: 3.621685, mean_eps: 0.100000
 1800887/2000000: episode: 2272, duration: 32.429s, episode steps: 1455, steps per second:  45, episode reward: 29.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.018131, mae: 3.021528, mean_q: 3.643769, mean_eps: 0.100000
 1801506/2000000: episode: 2273, duration: 14.271s, episode steps: 619, steps per second:  43, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.233 [0.000, 5.000],  loss: 0.017304, mae: 2.998969, mean_q: 3.616364, mean_eps: 0.100000
 1802042/2000000: episode: 2274, duration: 11.866s, episode steps: 536, steps per second:  45, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.924 [0.000, 5.000],  loss: 0.020161, mae: 3.015763, mean_q: 3.634429, mean_eps: 0.100000
 1802901/2000000: episode: 2275, duration: 19.224s, episode steps: 859, steps per second:  45, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.332 [0.000, 5.000],  loss: 0.018188, mae: 3.010008, mean_q: 3.628261, mean_eps: 0.100000
 1803583/2000000: episode: 2276, duration: 15.065s, episode steps: 682, steps per second:  45, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: 0.015090, mae: 3.020827, mean_q: 3.643188, mean_eps: 0.100000
 1804855/2000000: episode: 2277, duration: 29.540s, episode steps: 1272, steps per second:  43, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.018095, mae: 3.007849, mean_q: 3.627125, mean_eps: 0.100000
 1805706/2000000: episode: 2278, duration: 20.534s, episode steps: 851, steps per second:  41, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.106 [0.000, 5.000],  loss: 0.018148, mae: 3.042864, mean_q: 3.671153, mean_eps: 0.100000
 1806520/2000000: episode: 2279, duration: 19.630s, episode steps: 814, steps per second:  41, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.019707, mae: 3.025986, mean_q: 3.650861, mean_eps: 0.100000
 1807485/2000000: episode: 2280, duration: 23.262s, episode steps: 965, steps per second:  41, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.916 [0.000, 5.000],  loss: 0.018431, mae: 3.043341, mean_q: 3.666894, mean_eps: 0.100000
 1808442/2000000: episode: 2281, duration: 24.487s, episode steps: 957, steps per second:  39, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.691 [0.000, 5.000],  loss: 0.021326, mae: 3.037579, mean_q: 3.663700, mean_eps: 0.100000
 1809174/2000000: episode: 2282, duration: 17.873s, episode steps: 732, steps per second:  41, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.018603, mae: 3.030182, mean_q: 3.653052, mean_eps: 0.100000
 1810036/2000000: episode: 2283, duration: 20.442s, episode steps: 862, steps per second:  42, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.019182, mae: 3.051626, mean_q: 3.681138, mean_eps: 0.100000
 1811121/2000000: episode: 2284, duration: 25.395s, episode steps: 1085, steps per second:  43, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.017940, mae: 3.058445, mean_q: 3.690686, mean_eps: 0.100000
 1811812/2000000: episode: 2285, duration: 16.389s, episode steps: 691, steps per second:  42, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.151 [0.000, 5.000],  loss: 0.018013, mae: 3.057625, mean_q: 3.685266, mean_eps: 0.100000
 1812727/2000000: episode: 2286, duration: 21.734s, episode steps: 915, steps per second:  42, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.005 [0.000, 5.000],  loss: 0.020772, mae: 3.057267, mean_q: 3.686898, mean_eps: 0.100000
 1813443/2000000: episode: 2287, duration: 17.429s, episode steps: 716, steps per second:  41, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.686 [0.000, 5.000],  loss: 0.022562, mae: 3.045703, mean_q: 3.670994, mean_eps: 0.100000
 1814279/2000000: episode: 2288, duration: 18.924s, episode steps: 836, steps per second:  44, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.019106, mae: 3.060751, mean_q: 3.690386, mean_eps: 0.100000
 1815283/2000000: episode: 2289, duration: 21.882s, episode steps: 1004, steps per second:  46, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.018919, mae: 3.058006, mean_q: 3.686406, mean_eps: 0.100000
 1816330/2000000: episode: 2290, duration: 23.763s, episode steps: 1047, steps per second:  44, episode reward: 29.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.004 [0.000, 5.000],  loss: 0.017577, mae: 3.034384, mean_q: 3.662003, mean_eps: 0.100000
 1817195/2000000: episode: 2291, duration: 19.998s, episode steps: 865, steps per second:  43, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.016678, mae: 3.050389, mean_q: 3.679704, mean_eps: 0.100000
 1817903/2000000: episode: 2292, duration: 15.754s, episode steps: 708, steps per second:  45, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.045 [0.000, 5.000],  loss: 0.018557, mae: 3.034925, mean_q: 3.660630, mean_eps: 0.100000
 1818734/2000000: episode: 2293, duration: 19.090s, episode steps: 831, steps per second:  44, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.072 [0.000, 5.000],  loss: 0.018528, mae: 3.038071, mean_q: 3.663677, mean_eps: 0.100000
 1819606/2000000: episode: 2294, duration: 21.329s, episode steps: 872, steps per second:  41, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.021097, mae: 3.026994, mean_q: 3.649371, mean_eps: 0.100000
 1820375/2000000: episode: 2295, duration: 19.358s, episode steps: 769, steps per second:  40, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.908 [0.000, 5.000],  loss: 0.016476, mae: 3.011596, mean_q: 3.631386, mean_eps: 0.100000
 1821345/2000000: episode: 2296, duration: 24.737s, episode steps: 970, steps per second:  39, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.778 [0.000, 5.000],  loss: 0.018418, mae: 3.026121, mean_q: 3.648929, mean_eps: 0.100000
 1822307/2000000: episode: 2297, duration: 22.269s, episode steps: 962, steps per second:  43, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.654 [0.000, 5.000],  loss: 0.016870, mae: 3.010499, mean_q: 3.631290, mean_eps: 0.100000
 1823115/2000000: episode: 2298, duration: 18.131s, episode steps: 808, steps per second:  45, episode reward: 24.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.218 [0.000, 5.000],  loss: 0.019166, mae: 3.019855, mean_q: 3.641569, mean_eps: 0.100000
 1824058/2000000: episode: 2299, duration: 22.369s, episode steps: 943, steps per second:  42, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.784 [0.000, 5.000],  loss: 0.016806, mae: 3.039133, mean_q: 3.664845, mean_eps: 0.100000
 1824930/2000000: episode: 2300, duration: 20.556s, episode steps: 872, steps per second:  42, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.916 [0.000, 5.000],  loss: 0.017205, mae: 3.045843, mean_q: 3.670836, mean_eps: 0.100000
 1825505/2000000: episode: 2301, duration: 13.000s, episode steps: 575, steps per second:  44, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.784 [0.000, 5.000],  loss: 0.019369, mae: 3.023317, mean_q: 3.645183, mean_eps: 0.100000
 1826510/2000000: episode: 2302, duration: 24.182s, episode steps: 1005, steps per second:  42, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.248 [0.000, 5.000],  loss: 0.018498, mae: 2.996365, mean_q: 3.613157, mean_eps: 0.100000
 1827035/2000000: episode: 2303, duration: 12.581s, episode steps: 525, steps per second:  42, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.998 [0.000, 5.000],  loss: 0.020463, mae: 3.021970, mean_q: 3.643567, mean_eps: 0.100000
 1827583/2000000: episode: 2304, duration: 13.177s, episode steps: 548, steps per second:  42, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.976 [0.000, 5.000],  loss: 0.016327, mae: 3.000601, mean_q: 3.617914, mean_eps: 0.100000
 1828218/2000000: episode: 2305, duration: 14.996s, episode steps: 635, steps per second:  42, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.277 [0.000, 5.000],  loss: 0.018270, mae: 3.003210, mean_q: 3.618182, mean_eps: 0.100000
 1828573/2000000: episode: 2306, duration: 8.554s, episode steps: 355, steps per second:  42, episode reward:  8.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.783 [0.000, 5.000],  loss: 0.018094, mae: 3.004864, mean_q: 3.619765, mean_eps: 0.100000
 1829590/2000000: episode: 2307, duration: 23.441s, episode steps: 1017, steps per second:  43, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.712 [0.000, 5.000],  loss: 0.018456, mae: 3.004700, mean_q: 3.620509, mean_eps: 0.100000
 1830489/2000000: episode: 2308, duration: 19.420s, episode steps: 899, steps per second:  46, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.017569, mae: 2.987760, mean_q: 3.601448, mean_eps: 0.100000
 1831040/2000000: episode: 2309, duration: 13.169s, episode steps: 551, steps per second:  42, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.272 [0.000, 5.000],  loss: 0.017629, mae: 2.958212, mean_q: 3.567504, mean_eps: 0.100000
 1832406/2000000: episode: 2310, duration: 32.212s, episode steps: 1366, steps per second:  42, episode reward: 31.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.017150, mae: 2.972398, mean_q: 3.583067, mean_eps: 0.100000
 1833369/2000000: episode: 2311, duration: 21.238s, episode steps: 963, steps per second:  45, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.747 [0.000, 5.000],  loss: 0.017489, mae: 2.970769, mean_q: 3.580680, mean_eps: 0.100000
 1834175/2000000: episode: 2312, duration: 19.278s, episode steps: 806, steps per second:  42, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.876 [0.000, 5.000],  loss: 0.018897, mae: 2.960440, mean_q: 3.568178, mean_eps: 0.100000
 1835382/2000000: episode: 2313, duration: 28.890s, episode steps: 1207, steps per second:  42, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.016358, mae: 2.961608, mean_q: 3.569062, mean_eps: 0.100000
 1836200/2000000: episode: 2314, duration: 22.559s, episode steps: 818, steps per second:  36, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.698 [0.000, 5.000],  loss: 0.017684, mae: 2.951570, mean_q: 3.558108, mean_eps: 0.100000
 1837276/2000000: episode: 2315, duration: 29.530s, episode steps: 1076, steps per second:  36, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.192 [0.000, 5.000],  loss: 0.017406, mae: 2.968172, mean_q: 3.579606, mean_eps: 0.100000
 1837945/2000000: episode: 2316, duration: 16.899s, episode steps: 669, steps per second:  40, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.807 [0.000, 5.000],  loss: 0.016387, mae: 2.952593, mean_q: 3.557594, mean_eps: 0.100000
 1838689/2000000: episode: 2317, duration: 17.618s, episode steps: 744, steps per second:  42, episode reward: 21.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.726 [0.000, 5.000],  loss: 0.019289, mae: 2.941554, mean_q: 3.546570, mean_eps: 0.100000
 1839335/2000000: episode: 2318, duration: 15.472s, episode steps: 646, steps per second:  42, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.087 [0.000, 5.000],  loss: 0.018292, mae: 2.953482, mean_q: 3.561815, mean_eps: 0.100000
 1839879/2000000: episode: 2319, duration: 12.996s, episode steps: 544, steps per second:  42, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.256 [0.000, 5.000],  loss: 0.017076, mae: 2.956925, mean_q: 3.566351, mean_eps: 0.100000
 1840788/2000000: episode: 2320, duration: 21.545s, episode steps: 909, steps per second:  42, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.017206, mae: 2.942777, mean_q: 3.548884, mean_eps: 0.100000
 1841601/2000000: episode: 2321, duration: 20.696s, episode steps: 813, steps per second:  39, episode reward: 26.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.866 [0.000, 5.000],  loss: 0.018563, mae: 2.943461, mean_q: 3.548537, mean_eps: 0.100000
 1842341/2000000: episode: 2322, duration: 18.403s, episode steps: 740, steps per second:  40, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.486 [0.000, 5.000],  loss: 0.018171, mae: 2.934833, mean_q: 3.537433, mean_eps: 0.100000
 1843303/2000000: episode: 2323, duration: 23.747s, episode steps: 962, steps per second:  41, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.017874, mae: 2.933992, mean_q: 3.536425, mean_eps: 0.100000
 1844089/2000000: episode: 2324, duration: 18.794s, episode steps: 786, steps per second:  42, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.085 [0.000, 5.000],  loss: 0.018421, mae: 2.954165, mean_q: 3.561597, mean_eps: 0.100000
 1845166/2000000: episode: 2325, duration: 24.181s, episode steps: 1077, steps per second:  45, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.933 [0.000, 5.000],  loss: 0.018350, mae: 2.944688, mean_q: 3.550871, mean_eps: 0.100000
 1846325/2000000: episode: 2326, duration: 29.017s, episode steps: 1159, steps per second:  40, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.815 [0.000, 5.000],  loss: 0.016969, mae: 2.931230, mean_q: 3.535423, mean_eps: 0.100000
 1847187/2000000: episode: 2327, duration: 20.286s, episode steps: 862, steps per second:  42, episode reward: 27.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 3.020 [0.000, 5.000],  loss: 0.018415, mae: 2.948722, mean_q: 3.555057, mean_eps: 0.100000
 1847866/2000000: episode: 2328, duration: 15.153s, episode steps: 679, steps per second:  45, episode reward: 20.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.733 [0.000, 5.000],  loss: 0.019729, mae: 2.931002, mean_q: 3.533452, mean_eps: 0.100000
 1848687/2000000: episode: 2329, duration: 19.352s, episode steps: 821, steps per second:  42, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.017587, mae: 2.936204, mean_q: 3.539817, mean_eps: 0.100000
 1849674/2000000: episode: 2330, duration: 23.030s, episode steps: 987, steps per second:  43, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.018160, mae: 2.940194, mean_q: 3.544858, mean_eps: 0.100000
 1850619/2000000: episode: 2331, duration: 23.287s, episode steps: 945, steps per second:  41, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.836 [0.000, 5.000],  loss: 0.017353, mae: 2.937357, mean_q: 3.542080, mean_eps: 0.100000
 1851376/2000000: episode: 2332, duration: 19.643s, episode steps: 757, steps per second:  39, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.806 [0.000, 5.000],  loss: 0.016499, mae: 2.928544, mean_q: 3.532418, mean_eps: 0.100000
 1852273/2000000: episode: 2333, duration: 22.406s, episode steps: 897, steps per second:  40, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.016943, mae: 2.910069, mean_q: 3.509411, mean_eps: 0.100000
 1853074/2000000: episode: 2334, duration: 19.954s, episode steps: 801, steps per second:  40, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.017443, mae: 2.917439, mean_q: 3.517513, mean_eps: 0.100000
 1853845/2000000: episode: 2335, duration: 18.690s, episode steps: 771, steps per second:  41, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.017071, mae: 2.933721, mean_q: 3.537609, mean_eps: 0.100000
 1855040/2000000: episode: 2336, duration: 27.757s, episode steps: 1195, steps per second:  43, episode reward: 33.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.003 [0.000, 5.000],  loss: 0.018060, mae: 2.924641, mean_q: 3.527359, mean_eps: 0.100000
 1855763/2000000: episode: 2337, duration: 17.609s, episode steps: 723, steps per second:  41, episode reward: 21.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.848 [0.000, 5.000],  loss: 0.017841, mae: 2.903922, mean_q: 3.501948, mean_eps: 0.100000
 1857210/2000000: episode: 2338, duration: 36.424s, episode steps: 1447, steps per second:  40, episode reward: 35.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.015902, mae: 2.914835, mean_q: 3.514793, mean_eps: 0.100000
 1857951/2000000: episode: 2339, duration: 17.623s, episode steps: 741, steps per second:  42, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.761 [0.000, 5.000],  loss: 0.017321, mae: 2.938092, mean_q: 3.542544, mean_eps: 0.100000
 1859196/2000000: episode: 2340, duration: 31.718s, episode steps: 1245, steps per second:  39, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.969 [0.000, 5.000],  loss: 0.017227, mae: 2.903618, mean_q: 3.499500, mean_eps: 0.100000
 1859828/2000000: episode: 2341, duration: 13.986s, episode steps: 632, steps per second:  45, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.691 [0.000, 5.000],  loss: 0.018099, mae: 2.908075, mean_q: 3.507106, mean_eps: 0.100000
 1860498/2000000: episode: 2342, duration: 15.617s, episode steps: 670, steps per second:  43, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.016529, mae: 2.908588, mean_q: 3.506311, mean_eps: 0.100000
 1861248/2000000: episode: 2343, duration: 17.560s, episode steps: 750, steps per second:  43, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.884 [0.000, 5.000],  loss: 0.017542, mae: 2.923174, mean_q: 3.523732, mean_eps: 0.100000
 1862026/2000000: episode: 2344, duration: 18.840s, episode steps: 778, steps per second:  41, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.024 [0.000, 5.000],  loss: 0.018029, mae: 2.916681, mean_q: 3.514488, mean_eps: 0.100000
 1862525/2000000: episode: 2345, duration: 12.306s, episode steps: 499, steps per second:  41, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.886 [0.000, 5.000],  loss: 0.016590, mae: 2.920422, mean_q: 3.520523, mean_eps: 0.100000
 1863413/2000000: episode: 2346, duration: 21.543s, episode steps: 888, steps per second:  41, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.020064, mae: 2.919311, mean_q: 3.517300, mean_eps: 0.100000
 1863899/2000000: episode: 2347, duration: 11.363s, episode steps: 486, steps per second:  43, episode reward: 12.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.263 [0.000, 5.000],  loss: 0.015983, mae: 2.923941, mean_q: 3.524544, mean_eps: 0.100000
 1864951/2000000: episode: 2348, duration: 26.544s, episode steps: 1052, steps per second:  40, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.779 [0.000, 5.000],  loss: 0.018317, mae: 2.923654, mean_q: 3.524096, mean_eps: 0.100000
 1866337/2000000: episode: 2349, duration: 36.647s, episode steps: 1386, steps per second:  38, episode reward: 32.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.001 [0.000, 5.000],  loss: 0.018226, mae: 2.934079, mean_q: 3.540269, mean_eps: 0.100000
 1867012/2000000: episode: 2350, duration: 16.095s, episode steps: 675, steps per second:  42, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.016299, mae: 2.938103, mean_q: 3.542027, mean_eps: 0.100000
 1867885/2000000: episode: 2351, duration: 20.517s, episode steps: 873, steps per second:  43, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.786 [0.000, 5.000],  loss: 0.016432, mae: 2.935455, mean_q: 3.538840, mean_eps: 0.100000
 1868514/2000000: episode: 2352, duration: 15.668s, episode steps: 629, steps per second:  40, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.019307, mae: 2.950096, mean_q: 3.555701, mean_eps: 0.100000
 1869108/2000000: episode: 2353, duration: 14.890s, episode steps: 594, steps per second:  40, episode reward: 17.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 3.079 [0.000, 5.000],  loss: 0.019170, mae: 2.947231, mean_q: 3.551248, mean_eps: 0.100000
 1869961/2000000: episode: 2354, duration: 19.783s, episode steps: 853, steps per second:  43, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.068 [0.000, 5.000],  loss: 0.018789, mae: 2.949710, mean_q: 3.554385, mean_eps: 0.100000
 1870564/2000000: episode: 2355, duration: 15.015s, episode steps: 603, steps per second:  40, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.016020, mae: 2.920370, mean_q: 3.521859, mean_eps: 0.100000
 1871699/2000000: episode: 2356, duration: 28.539s, episode steps: 1135, steps per second:  40, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.017476, mae: 2.932676, mean_q: 3.534976, mean_eps: 0.100000
 1872342/2000000: episode: 2357, duration: 15.628s, episode steps: 643, steps per second:  41, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.843 [0.000, 5.000],  loss: 0.014668, mae: 2.902409, mean_q: 3.500093, mean_eps: 0.100000
 1872691/2000000: episode: 2358, duration: 8.311s, episode steps: 349, steps per second:  42, episode reward:  7.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.957 [0.000, 5.000],  loss: 0.015998, mae: 2.934254, mean_q: 3.538966, mean_eps: 0.100000
 1873593/2000000: episode: 2359, duration: 22.360s, episode steps: 902, steps per second:  40, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.045 [0.000, 5.000],  loss: 0.019207, mae: 2.940142, mean_q: 3.544823, mean_eps: 0.100000
 1874979/2000000: episode: 2360, duration: 30.905s, episode steps: 1386, steps per second:  45, episode reward: 31.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.015965, mae: 2.906615, mean_q: 3.504072, mean_eps: 0.100000
 1875364/2000000: episode: 2361, duration: 8.857s, episode steps: 385, steps per second:  43, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.945 [0.000, 5.000],  loss: 0.016859, mae: 2.949542, mean_q: 3.555603, mean_eps: 0.100000
 1876064/2000000: episode: 2362, duration: 16.524s, episode steps: 700, steps per second:  42, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.018238, mae: 2.957926, mean_q: 3.566617, mean_eps: 0.100000
 1876855/2000000: episode: 2363, duration: 18.755s, episode steps: 791, steps per second:  42, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.219 [0.000, 5.000],  loss: 0.017344, mae: 2.964602, mean_q: 3.573125, mean_eps: 0.100000
 1877964/2000000: episode: 2364, duration: 26.367s, episode steps: 1109, steps per second:  42, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.016031, mae: 2.944722, mean_q: 3.550064, mean_eps: 0.100000
 1878639/2000000: episode: 2365, duration: 16.425s, episode steps: 675, steps per second:  41, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.861 [0.000, 5.000],  loss: 0.016372, mae: 2.979023, mean_q: 3.591203, mean_eps: 0.100000
 1879286/2000000: episode: 2366, duration: 16.871s, episode steps: 647, steps per second:  38, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.019048, mae: 2.954623, mean_q: 3.562740, mean_eps: 0.100000
 1880181/2000000: episode: 2367, duration: 22.616s, episode steps: 895, steps per second:  40, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.017598, mae: 2.961415, mean_q: 3.570344, mean_eps: 0.100000
 1881113/2000000: episode: 2368, duration: 24.793s, episode steps: 932, steps per second:  38, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.324 [0.000, 5.000],  loss: 0.017113, mae: 2.964078, mean_q: 3.574345, mean_eps: 0.100000
 1882278/2000000: episode: 2369, duration: 27.722s, episode steps: 1165, steps per second:  42, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.788 [0.000, 5.000],  loss: 0.018579, mae: 2.965281, mean_q: 3.575486, mean_eps: 0.100000
 1883150/2000000: episode: 2370, duration: 20.830s, episode steps: 872, steps per second:  42, episode reward: 26.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.916 [0.000, 5.000],  loss: 0.016321, mae: 2.958772, mean_q: 3.569182, mean_eps: 0.100000
 1884250/2000000: episode: 2371, duration: 25.100s, episode steps: 1100, steps per second:  44, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.016771, mae: 2.956977, mean_q: 3.564691, mean_eps: 0.100000
 1885366/2000000: episode: 2372, duration: 25.417s, episode steps: 1116, steps per second:  44, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.844 [0.000, 5.000],  loss: 0.018038, mae: 2.970268, mean_q: 3.582636, mean_eps: 0.100000
 1886204/2000000: episode: 2373, duration: 21.226s, episode steps: 838, steps per second:  39, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.156 [0.000, 5.000],  loss: 0.016550, mae: 2.962675, mean_q: 3.571927, mean_eps: 0.100000
 1887196/2000000: episode: 2374, duration: 24.121s, episode steps: 992, steps per second:  41, episode reward: 30.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.203 [0.000, 5.000],  loss: 0.019441, mae: 2.982346, mean_q: 3.597191, mean_eps: 0.100000
 1887881/2000000: episode: 2375, duration: 17.136s, episode steps: 685, steps per second:  40, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.117 [0.000, 5.000],  loss: 0.017910, mae: 2.989726, mean_q: 3.602527, mean_eps: 0.100000
 1888779/2000000: episode: 2376, duration: 22.637s, episode steps: 898, steps per second:  40, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.864 [0.000, 5.000],  loss: 0.018028, mae: 2.965229, mean_q: 3.573542, mean_eps: 0.100000
 1889593/2000000: episode: 2377, duration: 18.284s, episode steps: 814, steps per second:  45, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.670 [0.000, 5.000],  loss: 0.017990, mae: 2.975351, mean_q: 3.586335, mean_eps: 0.100000
 1890479/2000000: episode: 2378, duration: 19.167s, episode steps: 886, steps per second:  46, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.016491, mae: 2.976347, mean_q: 3.588621, mean_eps: 0.100000
 1890924/2000000: episode: 2379, duration: 9.907s, episode steps: 445, steps per second:  45, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.016589, mae: 3.006651, mean_q: 3.625747, mean_eps: 0.100000
 1891692/2000000: episode: 2380, duration: 16.315s, episode steps: 768, steps per second:  47, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.016203, mae: 3.004227, mean_q: 3.622855, mean_eps: 0.100000
 1892594/2000000: episode: 2381, duration: 20.366s, episode steps: 902, steps per second:  44, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.020668, mae: 2.988262, mean_q: 3.603183, mean_eps: 0.100000
 1893488/2000000: episode: 2382, duration: 21.443s, episode steps: 894, steps per second:  42, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.824 [0.000, 5.000],  loss: 0.018248, mae: 3.023448, mean_q: 3.647554, mean_eps: 0.100000
 1894201/2000000: episode: 2383, duration: 17.343s, episode steps: 713, steps per second:  41, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.129 [0.000, 5.000],  loss: 0.019001, mae: 3.024122, mean_q: 3.646268, mean_eps: 0.100000
 1895007/2000000: episode: 2384, duration: 21.174s, episode steps: 806, steps per second:  38, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.017498, mae: 3.023175, mean_q: 3.645001, mean_eps: 0.100000
 1895997/2000000: episode: 2385, duration: 26.938s, episode steps: 990, steps per second:  37, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.016847, mae: 3.001839, mean_q: 3.620537, mean_eps: 0.100000
 1896425/2000000: episode: 2386, duration: 11.685s, episode steps: 428, steps per second:  37, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.192 [0.000, 5.000],  loss: 0.018086, mae: 2.969368, mean_q: 3.581102, mean_eps: 0.100000
 1897308/2000000: episode: 2387, duration: 22.697s, episode steps: 883, steps per second:  39, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.017909, mae: 3.005754, mean_q: 3.624053, mean_eps: 0.100000
 1897705/2000000: episode: 2388, duration: 11.065s, episode steps: 397, steps per second:  36, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.723 [0.000, 5.000],  loss: 0.014801, mae: 2.987601, mean_q: 3.601967, mean_eps: 0.100000
 1898336/2000000: episode: 2389, duration: 15.932s, episode steps: 631, steps per second:  40, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.019296, mae: 3.004119, mean_q: 3.622451, mean_eps: 0.100000
 1899181/2000000: episode: 2390, duration: 19.619s, episode steps: 845, steps per second:  43, episode reward: 26.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.832 [0.000, 5.000],  loss: 0.017180, mae: 3.015168, mean_q: 3.636097, mean_eps: 0.100000
 1899990/2000000: episode: 2391, duration: 17.956s, episode steps: 809, steps per second:  45, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.016552, mae: 3.006110, mean_q: 3.623297, mean_eps: 0.100000
 1901019/2000000: episode: 2392, duration: 23.605s, episode steps: 1029, steps per second:  44, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.953 [0.000, 5.000],  loss: 0.018954, mae: 3.030653, mean_q: 3.655682, mean_eps: 0.100000
 1901699/2000000: episode: 2393, duration: 16.721s, episode steps: 680, steps per second:  41, episode reward: 21.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 0.019094, mae: 3.015941, mean_q: 3.635070, mean_eps: 0.100000
 1902335/2000000: episode: 2394, duration: 14.861s, episode steps: 636, steps per second:  43, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.015722, mae: 3.018110, mean_q: 3.641134, mean_eps: 0.100000
 1903473/2000000: episode: 2395, duration: 28.328s, episode steps: 1138, steps per second:  40, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.017206, mae: 3.038444, mean_q: 3.664421, mean_eps: 0.100000
 1904111/2000000: episode: 2396, duration: 14.430s, episode steps: 638, steps per second:  44, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.018638, mae: 3.031499, mean_q: 3.656920, mean_eps: 0.100000
 1904600/2000000: episode: 2397, duration: 10.839s, episode steps: 489, steps per second:  45, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.720 [0.000, 5.000],  loss: 0.018438, mae: 3.017034, mean_q: 3.639052, mean_eps: 0.100000
 1905689/2000000: episode: 2398, duration: 26.264s, episode steps: 1089, steps per second:  41, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.017029, mae: 3.012916, mean_q: 3.633946, mean_eps: 0.100000
 1906224/2000000: episode: 2399, duration: 13.384s, episode steps: 535, steps per second:  40, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.850 [0.000, 5.000],  loss: 0.018330, mae: 3.022150, mean_q: 3.643083, mean_eps: 0.100000
 1907156/2000000: episode: 2400, duration: 21.230s, episode steps: 932, steps per second:  44, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.195 [0.000, 5.000],  loss: 0.015633, mae: 3.023115, mean_q: 3.646545, mean_eps: 0.100000
 1908323/2000000: episode: 2401, duration: 29.016s, episode steps: 1167, steps per second:  40, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.996 [0.000, 5.000],  loss: 0.018439, mae: 3.018548, mean_q: 3.638308, mean_eps: 0.100000
 1909434/2000000: episode: 2402, duration: 28.039s, episode steps: 1111, steps per second:  40, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.016589, mae: 3.004685, mean_q: 3.623366, mean_eps: 0.100000
 1910438/2000000: episode: 2403, duration: 25.637s, episode steps: 1004, steps per second:  39, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.015531, mae: 3.037363, mean_q: 3.661939, mean_eps: 0.100000
 1911101/2000000: episode: 2404, duration: 17.277s, episode steps: 663, steps per second:  38, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.249 [0.000, 5.000],  loss: 0.017683, mae: 3.034523, mean_q: 3.661704, mean_eps: 0.100000
 1911970/2000000: episode: 2405, duration: 20.155s, episode steps: 869, steps per second:  43, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.779 [0.000, 5.000],  loss: 0.014651, mae: 3.031060, mean_q: 3.654133, mean_eps: 0.100000
 1912366/2000000: episode: 2406, duration: 9.866s, episode steps: 396, steps per second:  40, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.015046, mae: 3.036628, mean_q: 3.661069, mean_eps: 0.100000
 1913086/2000000: episode: 2407, duration: 16.598s, episode steps: 720, steps per second:  43, episode reward: 21.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.324 [0.000, 5.000],  loss: 0.017763, mae: 3.012987, mean_q: 3.631177, mean_eps: 0.100000
 1913659/2000000: episode: 2408, duration: 14.448s, episode steps: 573, steps per second:  40, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.681 [0.000, 5.000],  loss: 0.017925, mae: 3.035012, mean_q: 3.661374, mean_eps: 0.100000
 1915148/2000000: episode: 2409, duration: 34.618s, episode steps: 1489, steps per second:  43, episode reward: 34.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.018348, mae: 3.022464, mean_q: 3.643273, mean_eps: 0.100000
 1916068/2000000: episode: 2410, duration: 23.308s, episode steps: 920, steps per second:  39, episode reward: 28.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.015758, mae: 3.027787, mean_q: 3.652578, mean_eps: 0.100000
 1916964/2000000: episode: 2411, duration: 20.682s, episode steps: 896, steps per second:  43, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.748 [0.000, 5.000],  loss: 0.016355, mae: 3.019216, mean_q: 3.641971, mean_eps: 0.100000
 1917851/2000000: episode: 2412, duration: 19.972s, episode steps: 887, steps per second:  44, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.708 [0.000, 5.000],  loss: 0.019557, mae: 3.021642, mean_q: 3.644342, mean_eps: 0.100000
 1918513/2000000: episode: 2413, duration: 14.972s, episode steps: 662, steps per second:  44, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.019997, mae: 3.014524, mean_q: 3.634626, mean_eps: 0.100000
 1919695/2000000: episode: 2414, duration: 24.492s, episode steps: 1182, steps per second:  48, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.019770, mae: 3.019202, mean_q: 3.639527, mean_eps: 0.100000
 1920418/2000000: episode: 2415, duration: 16.596s, episode steps: 723, steps per second:  44, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.746 [0.000, 5.000],  loss: 0.015739, mae: 3.017029, mean_q: 3.638554, mean_eps: 0.100000
 1921258/2000000: episode: 2416, duration: 18.828s, episode steps: 840, steps per second:  45, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.857 [0.000, 5.000],  loss: 0.016749, mae: 2.983235, mean_q: 3.596638, mean_eps: 0.100000
 1921939/2000000: episode: 2417, duration: 15.472s, episode steps: 681, steps per second:  44, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.627 [0.000, 5.000],  loss: 0.017415, mae: 3.003059, mean_q: 3.620984, mean_eps: 0.100000
 1923272/2000000: episode: 2418, duration: 30.037s, episode steps: 1333, steps per second:  44, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.015800, mae: 2.997723, mean_q: 3.614687, mean_eps: 0.100000
 1924423/2000000: episode: 2419, duration: 26.885s, episode steps: 1151, steps per second:  43, episode reward: 33.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.995 [0.000, 5.000],  loss: 0.016834, mae: 2.988097, mean_q: 3.603550, mean_eps: 0.100000
 1925474/2000000: episode: 2420, duration: 23.634s, episode steps: 1051, steps per second:  44, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.787 [0.000, 5.000],  loss: 0.016327, mae: 3.001969, mean_q: 3.619286, mean_eps: 0.100000
 1926613/2000000: episode: 2421, duration: 26.598s, episode steps: 1139, steps per second:  43, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.018083, mae: 2.988212, mean_q: 3.603171, mean_eps: 0.100000
 1927595/2000000: episode: 2422, duration: 21.637s, episode steps: 982, steps per second:  45, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.171 [0.000, 5.000],  loss: 0.017093, mae: 2.989980, mean_q: 3.604615, mean_eps: 0.100000
 1928262/2000000: episode: 2423, duration: 14.265s, episode steps: 667, steps per second:  47, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.189 [0.000, 5.000],  loss: 0.019626, mae: 3.009092, mean_q: 3.626772, mean_eps: 0.100000
 1929032/2000000: episode: 2424, duration: 17.539s, episode steps: 770, steps per second:  44, episode reward: 27.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.017121, mae: 2.996639, mean_q: 3.611740, mean_eps: 0.100000
 1930061/2000000: episode: 2425, duration: 22.248s, episode steps: 1029, steps per second:  46, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.017095, mae: 2.980311, mean_q: 3.593755, mean_eps: 0.100000
 1931097/2000000: episode: 2426, duration: 23.234s, episode steps: 1036, steps per second:  45, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.255 [0.000, 5.000],  loss: 0.016678, mae: 2.967767, mean_q: 3.576145, mean_eps: 0.100000
 1931721/2000000: episode: 2427, duration: 15.237s, episode steps: 624, steps per second:  41, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.756 [0.000, 5.000],  loss: 0.018202, mae: 2.956023, mean_q: 3.562657, mean_eps: 0.100000
 1932446/2000000: episode: 2428, duration: 16.015s, episode steps: 725, steps per second:  45, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.396 [0.000, 5.000],  loss: 0.017347, mae: 2.957430, mean_q: 3.565022, mean_eps: 0.100000
 1933147/2000000: episode: 2429, duration: 15.028s, episode steps: 701, steps per second:  47, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.016962, mae: 2.988696, mean_q: 3.603469, mean_eps: 0.100000
 1934067/2000000: episode: 2430, duration: 20.041s, episode steps: 920, steps per second:  46, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.791 [0.000, 5.000],  loss: 0.016408, mae: 2.957048, mean_q: 3.563434, mean_eps: 0.100000
 1935051/2000000: episode: 2431, duration: 20.999s, episode steps: 984, steps per second:  47, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.017871, mae: 2.968992, mean_q: 3.577524, mean_eps: 0.100000
 1935854/2000000: episode: 2432, duration: 16.749s, episode steps: 803, steps per second:  48, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.016142, mae: 2.943286, mean_q: 3.549847, mean_eps: 0.100000
 1936871/2000000: episode: 2433, duration: 21.148s, episode steps: 1017, steps per second:  48, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.018156, mae: 2.952313, mean_q: 3.558168, mean_eps: 0.100000
 1937926/2000000: episode: 2434, duration: 22.085s, episode steps: 1055, steps per second:  48, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.018966, mae: 2.963038, mean_q: 3.570418, mean_eps: 0.100000
 1938805/2000000: episode: 2435, duration: 18.492s, episode steps: 879, steps per second:  48, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.175 [0.000, 5.000],  loss: 0.016881, mae: 2.957927, mean_q: 3.564851, mean_eps: 0.100000
 1939657/2000000: episode: 2436, duration: 18.196s, episode steps: 852, steps per second:  47, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.877 [0.000, 5.000],  loss: 0.019275, mae: 2.960603, mean_q: 3.569588, mean_eps: 0.100000
 1940391/2000000: episode: 2437, duration: 15.931s, episode steps: 734, steps per second:  46, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.018299, mae: 2.954175, mean_q: 3.563662, mean_eps: 0.100000
 1941394/2000000: episode: 2438, duration: 22.583s, episode steps: 1003, steps per second:  44, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.847 [0.000, 5.000],  loss: 0.018175, mae: 2.963970, mean_q: 3.574119, mean_eps: 0.100000
 1941791/2000000: episode: 2439, duration: 9.261s, episode steps: 397, steps per second:  43, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.990 [0.000, 5.000],  loss: 0.016889, mae: 2.989058, mean_q: 3.600746, mean_eps: 0.100000
 1942464/2000000: episode: 2440, duration: 16.070s, episode steps: 673, steps per second:  42, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.669 [0.000, 5.000],  loss: 0.018923, mae: 2.970247, mean_q: 3.580238, mean_eps: 0.100000
 1943256/2000000: episode: 2441, duration: 19.563s, episode steps: 792, steps per second:  40, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.126 [0.000, 5.000],  loss: 0.019253, mae: 2.964062, mean_q: 3.573200, mean_eps: 0.100000
 1943977/2000000: episode: 2442, duration: 16.747s, episode steps: 721, steps per second:  43, episode reward: 20.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.730 [0.000, 5.000],  loss: 0.017315, mae: 2.982884, mean_q: 3.596272, mean_eps: 0.100000
 1944598/2000000: episode: 2443, duration: 13.417s, episode steps: 621, steps per second:  46, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.020824, mae: 2.961649, mean_q: 3.567029, mean_eps: 0.100000
 1945061/2000000: episode: 2444, duration: 10.831s, episode steps: 463, steps per second:  43, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.015207, mae: 2.964671, mean_q: 3.577583, mean_eps: 0.100000
 1946047/2000000: episode: 2445, duration: 22.233s, episode steps: 986, steps per second:  44, episode reward: 30.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 3.253 [0.000, 5.000],  loss: 0.016529, mae: 2.980972, mean_q: 3.594290, mean_eps: 0.100000
 1946712/2000000: episode: 2446, duration: 14.582s, episode steps: 665, steps per second:  46, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.016617, mae: 2.966955, mean_q: 3.578353, mean_eps: 0.100000
 1947903/2000000: episode: 2447, duration: 27.775s, episode steps: 1191, steps per second:  43, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.710 [0.000, 5.000],  loss: 0.016696, mae: 2.987627, mean_q: 3.603792, mean_eps: 0.100000
 1948751/2000000: episode: 2448, duration: 18.943s, episode steps: 848, steps per second:  45, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.017367, mae: 2.981013, mean_q: 3.591823, mean_eps: 0.100000
 1949575/2000000: episode: 2449, duration: 17.975s, episode steps: 824, steps per second:  46, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.017602, mae: 2.978526, mean_q: 3.590209, mean_eps: 0.100000
 1950568/2000000: episode: 2450, duration: 21.738s, episode steps: 993, steps per second:  46, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.016960, mae: 2.971889, mean_q: 3.583194, mean_eps: 0.100000
 1951346/2000000: episode: 2451, duration: 16.471s, episode steps: 778, steps per second:  47, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.674 [0.000, 5.000],  loss: 0.015928, mae: 2.942175, mean_q: 3.551298, mean_eps: 0.100000
 1952113/2000000: episode: 2452, duration: 15.949s, episode steps: 767, steps per second:  48, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.850 [0.000, 5.000],  loss: 0.016988, mae: 2.934188, mean_q: 3.537751, mean_eps: 0.100000
 1953244/2000000: episode: 2453, duration: 24.049s, episode steps: 1131, steps per second:  47, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.014424, mae: 2.942163, mean_q: 3.547369, mean_eps: 0.100000
 1953967/2000000: episode: 2454, duration: 15.763s, episode steps: 723, steps per second:  46, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.686 [0.000, 5.000],  loss: 0.015651, mae: 2.943573, mean_q: 3.549543, mean_eps: 0.100000
 1954549/2000000: episode: 2455, duration: 12.444s, episode steps: 582, steps per second:  47, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.017888, mae: 2.949449, mean_q: 3.553769, mean_eps: 0.100000
 1955189/2000000: episode: 2456, duration: 13.830s, episode steps: 640, steps per second:  46, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.015932, mae: 2.943434, mean_q: 3.549439, mean_eps: 0.100000
 1955631/2000000: episode: 2457, duration: 9.893s, episode steps: 442, steps per second:  45, episode reward: 12.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.785 [0.000, 5.000],  loss: 0.016015, mae: 2.928833, mean_q: 3.533521, mean_eps: 0.100000
 1956596/2000000: episode: 2458, duration: 20.690s, episode steps: 965, steps per second:  47, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.015420, mae: 2.934624, mean_q: 3.538794, mean_eps: 0.100000
 1957278/2000000: episode: 2459, duration: 15.321s, episode steps: 682, steps per second:  45, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.113 [0.000, 5.000],  loss: 0.015515, mae: 2.936949, mean_q: 3.541225, mean_eps: 0.100000
 1959020/2000000: episode: 2460, duration: 44.490s, episode steps: 1742, steps per second:  39, episode reward: 44.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.016894, mae: 2.935538, mean_q: 3.539857, mean_eps: 0.100000
 1960221/2000000: episode: 2461, duration: 29.282s, episode steps: 1201, steps per second:  41, episode reward: 30.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.724 [0.000, 5.000],  loss: 0.016120, mae: 2.923452, mean_q: 3.524258, mean_eps: 0.100000
 1960882/2000000: episode: 2462, duration: 15.312s, episode steps: 661, steps per second:  43, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.016112, mae: 2.906548, mean_q: 3.505775, mean_eps: 0.100000
 1961585/2000000: episode: 2463, duration: 15.326s, episode steps: 703, steps per second:  46, episode reward: 22.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.017403, mae: 2.908293, mean_q: 3.505825, mean_eps: 0.100000
 1962458/2000000: episode: 2464, duration: 18.839s, episode steps: 873, steps per second:  46, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.857 [0.000, 5.000],  loss: 0.017280, mae: 2.921128, mean_q: 3.520764, mean_eps: 0.100000
 1963075/2000000: episode: 2465, duration: 13.775s, episode steps: 617, steps per second:  45, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.097 [0.000, 5.000],  loss: 0.019447, mae: 2.902929, mean_q: 3.497559, mean_eps: 0.100000
 1964372/2000000: episode: 2466, duration: 29.546s, episode steps: 1297, steps per second:  44, episode reward: 31.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.983 [0.000, 5.000],  loss: 0.017300, mae: 2.933068, mean_q: 3.535738, mean_eps: 0.100000
 1965032/2000000: episode: 2467, duration: 16.146s, episode steps: 660, steps per second:  41, episode reward: 18.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.017244, mae: 2.909750, mean_q: 3.510079, mean_eps: 0.100000
 1965804/2000000: episode: 2468, duration: 16.959s, episode steps: 772, steps per second:  46, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.858 [0.000, 5.000],  loss: 0.018077, mae: 2.906339, mean_q: 3.503271, mean_eps: 0.100000
 1966336/2000000: episode: 2469, duration: 11.905s, episode steps: 532, steps per second:  45, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.752 [0.000, 5.000],  loss: 0.016541, mae: 2.902047, mean_q: 3.496993, mean_eps: 0.100000
 1967523/2000000: episode: 2470, duration: 25.388s, episode steps: 1187, steps per second:  47, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.016307, mae: 2.904002, mean_q: 3.501552, mean_eps: 0.100000
 1968750/2000000: episode: 2471, duration: 25.221s, episode steps: 1227, steps per second:  49, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.842 [0.000, 5.000],  loss: 0.019249, mae: 2.898964, mean_q: 3.493678, mean_eps: 0.100000
 1969750/2000000: episode: 2472, duration: 21.636s, episode steps: 1000, steps per second:  46, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.018431, mae: 2.908053, mean_q: 3.503691, mean_eps: 0.100000
 1970909/2000000: episode: 2473, duration: 24.441s, episode steps: 1159, steps per second:  47, episode reward: 32.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.783 [0.000, 5.000],  loss: 0.018241, mae: 2.923269, mean_q: 3.524812, mean_eps: 0.100000
 1971781/2000000: episode: 2474, duration: 18.530s, episode steps: 872, steps per second:  47, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.015373, mae: 2.948621, mean_q: 3.556848, mean_eps: 0.100000
 1972924/2000000: episode: 2475, duration: 24.540s, episode steps: 1143, steps per second:  47, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.042 [0.000, 5.000],  loss: 0.017052, mae: 2.941431, mean_q: 3.546555, mean_eps: 0.100000
 1973782/2000000: episode: 2476, duration: 18.880s, episode steps: 858, steps per second:  45, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.089 [0.000, 5.000],  loss: 0.018343, mae: 2.938150, mean_q: 3.541471, mean_eps: 0.100000
 1974883/2000000: episode: 2477, duration: 25.957s, episode steps: 1101, steps per second:  42, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.017896, mae: 2.949423, mean_q: 3.554073, mean_eps: 0.100000
 1975761/2000000: episode: 2478, duration: 20.749s, episode steps: 878, steps per second:  42, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.016033, mae: 2.947301, mean_q: 3.553944, mean_eps: 0.100000
 1976584/2000000: episode: 2479, duration: 20.331s, episode steps: 823, steps per second:  40, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.729 [0.000, 5.000],  loss: 0.016375, mae: 2.921207, mean_q: 3.521405, mean_eps: 0.100000
 1977705/2000000: episode: 2480, duration: 25.859s, episode steps: 1121, steps per second:  43, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.017687, mae: 2.928871, mean_q: 3.531161, mean_eps: 0.100000
 1978313/2000000: episode: 2481, duration: 15.309s, episode steps: 608, steps per second:  40, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.019843, mae: 2.931068, mean_q: 3.531233, mean_eps: 0.100000
 1979757/2000000: episode: 2482, duration: 35.563s, episode steps: 1444, steps per second:  41, episode reward: 31.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.248 [0.000, 5.000],  loss: 0.018860, mae: 2.926877, mean_q: 3.526787, mean_eps: 0.100000
 1981169/2000000: episode: 2483, duration: 33.671s, episode steps: 1412, steps per second:  42, episode reward: 32.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.018359, mae: 2.934900, mean_q: 3.537658, mean_eps: 0.100000
 1982061/2000000: episode: 2484, duration: 19.945s, episode steps: 892, steps per second:  45, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.231 [0.000, 5.000],  loss: 0.016470, mae: 2.930380, mean_q: 3.533282, mean_eps: 0.100000
 1983099/2000000: episode: 2485, duration: 23.139s, episode steps: 1038, steps per second:  45, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.019139, mae: 2.936835, mean_q: 3.539054, mean_eps: 0.100000
 1984073/2000000: episode: 2486, duration: 20.588s, episode steps: 974, steps per second:  47, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.257 [0.000, 5.000],  loss: 0.019324, mae: 2.944920, mean_q: 3.551844, mean_eps: 0.100000
 1985367/2000000: episode: 2487, duration: 28.244s, episode steps: 1294, steps per second:  46, episode reward: 32.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.017494, mae: 2.933929, mean_q: 3.537598, mean_eps: 0.100000
 1986081/2000000: episode: 2488, duration: 15.678s, episode steps: 714, steps per second:  46, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.016854, mae: 2.920385, mean_q: 3.521067, mean_eps: 0.100000
 1987126/2000000: episode: 2489, duration: 22.401s, episode steps: 1045, steps per second:  47, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.679 [0.000, 5.000],  loss: 0.017261, mae: 2.944396, mean_q: 3.549747, mean_eps: 0.100000
 1988089/2000000: episode: 2490, duration: 21.872s, episode steps: 963, steps per second:  44, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.974 [0.000, 5.000],  loss: 0.017655, mae: 2.922363, mean_q: 3.524470, mean_eps: 0.100000
 1988730/2000000: episode: 2491, duration: 14.759s, episode steps: 641, steps per second:  43, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.011 [0.000, 5.000],  loss: 0.019685, mae: 2.941618, mean_q: 3.545409, mean_eps: 0.100000
 1989480/2000000: episode: 2492, duration: 17.569s, episode steps: 750, steps per second:  43, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.018749, mae: 2.944030, mean_q: 3.547582, mean_eps: 0.100000
 1990017/2000000: episode: 2493, duration: 13.430s, episode steps: 537, steps per second:  40, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.050 [0.000, 5.000],  loss: 0.017913, mae: 2.937277, mean_q: 3.538889, mean_eps: 0.100000
 1990420/2000000: episode: 2494, duration: 10.108s, episode steps: 403, steps per second:  40, episode reward: 10.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.018238, mae: 2.972899, mean_q: 3.583667, mean_eps: 0.100000
 1991390/2000000: episode: 2495, duration: 22.874s, episode steps: 970, steps per second:  42, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.696 [0.000, 5.000],  loss: 0.016212, mae: 2.951443, mean_q: 3.559948, mean_eps: 0.100000
 1992179/2000000: episode: 2496, duration: 18.275s, episode steps: 789, steps per second:  43, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.133 [0.000, 5.000],  loss: 0.016328, mae: 2.957246, mean_q: 3.565600, mean_eps: 0.100000
 1992959/2000000: episode: 2497, duration: 17.943s, episode steps: 780, steps per second:  43, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.019151, mae: 2.940988, mean_q: 3.544923, mean_eps: 0.100000
 1993462/2000000: episode: 2498, duration: 11.214s, episode steps: 503, steps per second:  45, episode reward: 14.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.018361, mae: 2.987173, mean_q: 3.602068, mean_eps: 0.100000
 1994067/2000000: episode: 2499, duration: 12.966s, episode steps: 605, steps per second:  47, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.248 [0.000, 5.000],  loss: 0.016921, mae: 2.962798, mean_q: 3.571152, mean_eps: 0.100000
 1995024/2000000: episode: 2500, duration: 20.602s, episode steps: 957, steps per second:  46, episode reward: 28.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.018542, mae: 2.966917, mean_q: 3.576870, mean_eps: 0.100000
 1995516/2000000: episode: 2501, duration: 11.504s, episode steps: 492, steps per second:  43, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.835 [0.000, 5.000],  loss: 0.015355, mae: 2.965606, mean_q: 3.579461, mean_eps: 0.100000
 1996624/2000000: episode: 2502, duration: 25.333s, episode steps: 1108, steps per second:  44, episode reward: 33.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.844 [0.000, 5.000],  loss: 0.016790, mae: 2.956364, mean_q: 3.566436, mean_eps: 0.100000
 1997411/2000000: episode: 2503, duration: 16.963s, episode steps: 787, steps per second:  46, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.018651, mae: 2.952792, mean_q: 3.562961, mean_eps: 0.100000
 1998473/2000000: episode: 2504, duration: 23.746s, episode steps: 1062, steps per second:  45, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.019260, mae: 2.951978, mean_q: 3.558797, mean_eps: 0.100000
 1999094/2000000: episode: 2505, duration: 13.295s, episode steps: 621, steps per second:  47, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.166 [0.000, 5.000],  loss: 0.018552, mae: 2.958244, mean_q: 3.565524, mean_eps: 0.100000
 1999783/2000000: episode: 2506, duration: 13.988s, episode steps: 689, steps per second:  49, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.020005, mae: 2.942560, mean_q: 3.547286, mean_eps: 0.100000
done, took 44369.325 seconds
