{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUehXgCyIRdq"
   },
   "source": [
    "# Actividad - Proyecto práctico\n",
    "\n",
    "\n",
    "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
    "*   Alumno 1: Gil Garcera, Javier \n",
    "*   Alumno 2: Palomares Mateo, Abel \n",
    "*   Alumno 3: Serrano López, Francisco Rubén \n",
    "*   Alumno 4: Vegas Romero, David \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwpYlnjWJhS9"
   },
   "source": [
    "---\n",
    "## **PARTE 1** - Instalación y requisitos previos\n",
    "\n",
    "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
    "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
    "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
    "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
    "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RU2BPrK2JkP0"
   },
   "source": [
    "---\n",
    "### 1.1. Preparar enviroment (solo local)\n",
    "\n",
    "\n",
    "\n",
    "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
    "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
    "2. Instalar Anaconda\n",
    "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
    "\n",
    "\n",
    "```\n",
    "conda create --name miar_rl python=3.8\n",
    "conda activate miar_rl\n",
    "cd \"PATH_TO_FOLDER\"\n",
    "conda install git\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "\n",
    "4. Abrir la notebook con *jupyter-notebook*.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "jupyter-notebook\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-kixNPiJqTc"
   },
   "source": [
    "---\n",
    "### 1.2. Localizar entorno de trabajo: Google colab o local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "S_YDFwZ-JscI"
   },
   "outputs": [],
   "source": [
    "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
    "mount='/content/gdrive'\n",
    "drive_root = mount + \"/My Drive/08_MIAR/actividades/proyecto practico\"\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Dp_a1iBJ0tf"
   },
   "source": [
    "---\n",
    "### 1.3. Montar carpeta de datos local (solo Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "I6n7MIefJ21i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos en el directorio: \n",
      "['dqn_SpaceInvaders-v0_weights_1150000.h5f.index', 'dqn_SpaceInvaders-v0_weights_900000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1575000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_475000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1625000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1325000.h5f.index', 'dqn_SpaceInvaders-v0_weights_250000.h5f.index', 'dqn_SpaceInvaders-v0_weights_10000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1000000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1325000.h5f.data-00000-of-00001', 'Proyecto_práctico_16_4.ipynb', 'dqn_SpaceInvaders-v0_weights_1475000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_8000.h5f.index', 'dqn_SpaceInvaders-v0_weights_825000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1650000.h5f.index', 'dqn_SpaceInvaders-v0_weights_25000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1925000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1275000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1050000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1875000.h5f.index', 'dqn_SpaceInvaders-v0_weights_550000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1825000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1425000.h5f.index', 'dqn_SpaceInvaders-v0_weights_325000.h5f.index', 'dqn_SpaceInvaders-v0_weights_150000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1775000.h5f.index', 'dqn_SpaceInvaders-v0_weights_275000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1125000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_625000.h5f.index', 'dqn_SpaceInvaders-v0_weights_600000.h5f.index', 'dqn_SpaceInvaders-v0_weights_575000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1125000.h5f.index', 'dqn_SpaceInvaders-v0_weights_800000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_6000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_175000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1450000.h5f.data-00000-of-00001', 'Proyecto_práctico_local.py', 'dqn_SpaceInvaders-v0_weights_1525000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_900000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1425000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1050000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_300000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1375000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1000.h5f.index', 'dqn_SpaceInvaders-v0_weights_7000.h5f.data-00000-of-00001', '.jupyter', 'dqn_SpaceInvaders-v0_weights_1675000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_725000.h5f.index', 'dqn_SpaceInvaders-v0_weights_8000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_400000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_4000.h5f.index', 'dqn_SpaceInvaders-v0_weights_6000.h5f.index', 'dqn_SpaceInvaders-v0_weights_425000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_700000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1750000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_log.json', 'dqn_SpaceInvaders-v0_weights_9000.h5f.index', 'alternativa_google_collab.txt', 'dqn_SpaceInvaders-v0_weights_500000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_725000.h5f.data-00000-of-00001', 'Proyecto_práctico.ipynb', 'dqn_SpaceInvaders-v0_weights_1225000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1350000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1075000.h5f.index', 'dqn_SpaceInvaders-v0_weights_3000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_750000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1200000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1350000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1175000.h5f.index', 'dqn_SpaceInvaders-v0_weights_175000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1400000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1650000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_450000.h5f.index', 'dqn_SpaceInvaders-v0_weights_675000.h5f.index', 'dqn_SpaceInvaders-v0_weights_925000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_2000000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1025000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1625000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1825000.h5f.index', 'dqn_SpaceInvaders-v0_weights_525000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_950000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1750000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1950000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1775000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_200000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_125000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1025000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1175000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_375000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1550000.h5f.index', 'dqn_SpaceInvaders-v0_weights_75000.h5f.index', 'dqn_SpaceInvaders-v0_weights.h5f.index', 'dqn_SpaceInvaders-v0_weights_1550000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1500000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_2000000.h5f.index', 'dqn_SpaceInvaders-v0_weights_225000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1250000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1150000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_800000.h5f.index', 'dqn_SpaceInvaders-v0_weights_825000.h5f.data-00000-of-00001', 'Proyecto_práctico_16_3.ipynb', 'dqn_SpaceInvaders-v0_weights_350000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_975000.h5f.index', 'dqn_SpaceInvaders-v0_weights_100000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1900000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1075000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1375000.h5f.index', 'dqn_SpaceInvaders-v0_weights_950000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_350000.h5f.index', 'dqn_SpaceInvaders-v0_weights_600000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_50000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1275000.h5f.index', 'dqn_SpaceInvaders-v0_weights_500000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1800000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_100000.h5f.data-00000-of-00001', 'pesos_16_4', 'dqn_SpaceInvaders-v0_weights_1000000.h5f.index', 'dqn_SpaceInvaders-v0_weights_300000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_375000.h5f.index', 'dqn_SpaceInvaders-v0_weights_475000.h5f.index', 'dqn_SpaceInvaders-v0_weights_10000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_20000.h5f.index', 'dqn_SpaceInvaders-v0_weights_50000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_5000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_975000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_675000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_450000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1900000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1475000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1725000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1700000.h5f.index', 'dqn_SpaceInvaders-v0_weights_3000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1300000.h5f.index', 'dqn_SpaceInvaders-v0_weights_700000.h5f.index', 'dqn_SpaceInvaders-v0_weights_275000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1950000.h5f.index', 'dqn_SpaceInvaders-v0_weights_650000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1525000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1600000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2000.h5f.index', 'dqn_SpaceInvaders-v0_weights_325000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_575000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_875000.h5f.index', 'dqn_SpaceInvaders-v0_weights_5000.h5f.index', 'dqn_SpaceInvaders-v0_weights_25000.h5f.data-00000-of-00001', 'Proyecto_práctico_local.ipynb', 'dqn_SpaceInvaders-v0_weights_1225000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1800000.h5f.index', 'dqn_SpaceInvaders-v0_weights_40000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1850000.h5f.index', 'dqn_SpaceInvaders-v0_weights_40000.h5f.index', 'dqn_SpaceInvaders-v0_weights_850000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_75000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_625000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1500000.h5f.index', 'dqn_SpaceInvaders-v0_weights_400000.h5f.index', 'dqn_SpaceInvaders-v0_weights_20000.h5f.data-00000-of-00001', 'README.md', 'dqn_SpaceInvaders-v0_weights_1575000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1300000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_225000.h5f.index', 'Proyecto_práctico_local_error.ipynb', 'dqn_SpaceInvaders-v0_weights_650000.h5f.data-00000-of-00001', '.ipynb_checkpoints', 'dqn_SpaceInvaders-v0_weights_550000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1400000.h5f.index', '.git', 'dqn_SpaceInvaders-v0_weights_125000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_775000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1675000.h5f.index', 'dqn_SpaceInvaders-v0_weights.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_775000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1975000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1725000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1875000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_7000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1250000.h5f.index', 'dqn_SpaceInvaders-v0_weights_875000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_9000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_425000.h5f.index', '.gitignore', 'dqn_SpaceInvaders-v0_weights_250000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1975000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_750000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1100000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_925000.h5f.index', 'checkpoint', 'dqn_SpaceInvaders-v0_weights_1850000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_525000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1450000.h5f.index', 'dqn_SpaceInvaders-v0_weights_150000.h5f.index', 'dqn_SpaceInvaders-v0_weights_4000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_200000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1100000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1700000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1925000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1200000.h5f.index', 'dqn_SpaceInvaders-v0_weights_850000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1600000.h5f.data-00000-of-00001']\n"
     ]
    }
   ],
   "source": [
    "# Switch to the directory on the Google Drive that you want to use\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "if IN_COLAB:\n",
    "  print(\"We're running Colab\")\n",
    "\n",
    "  if IN_COLAB:\n",
    "    # Mount the Google Drive at mount\n",
    "    print(\"Colab: mounting Google drive on \", mount)\n",
    "\n",
    "    drive.mount(mount)\n",
    "\n",
    "    # Create drive_root if it doesn't exist\n",
    "    create_drive_root = True\n",
    "    if create_drive_root:\n",
    "      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
    "      os.makedirs(drive_root, exist_ok=True)\n",
    "\n",
    "    # Change to the directory\n",
    "    print(\"\\nColab: Changing directory to \", drive_root)\n",
    "    %cd $drive_root\n",
    "# Verify we're in the correct working directory\n",
    "%pwd\n",
    "print(\"Archivos en el directorio: \")\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1ZSL5bpJ560"
   },
   "source": [
    "---\n",
    "### 1.4. Instalar librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UbVRjvHCJ8UF"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install tensorflow==2.8\n",
    "  %pip install tensorflow==2.12.1\n",
    "elif False:\n",
    "  %pip install numpy==1.23.5\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install pyglet==1.5.0\n",
    "  %pip install h5py==3.10.0\n",
    "  %pip install Pillow==9.5.0\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install Keras==2.12.0\n",
    "  %pip install tensorflow==2.12.1\n",
    "  %pip install torch==2.0.1\n",
    "  %pip install agents==1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hzP_5ZuGb2X"
   },
   "source": [
    "---\n",
    "## **PARTE 2**. Enunciado\n",
    "\n",
    "Consideraciones a tener en cuenta:\n",
    "\n",
    "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
    "\n",
    "- Para nuestro ejercicio, el requisito mínimo será alcanzado cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
    "\n",
    "Este proyecto práctico consta de tres partes:\n",
    "\n",
    "1.   Implementar la red neuronal que se usará en la solución\n",
    "2.   Implementar las distintas piezas de la solución DQN\n",
    "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
    "\n",
    "**Rúbrica**: Se valorará la originalidad en la solución aportada, así como la capacidad de discutir los resultados de forma detallada. El requisito mínimo servirá para aprobar la actividad, bajo premisa de que la discusión del resultado sera apropiada.\n",
    "\n",
    "IMPORTANTE:\n",
    "\n",
    "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
    "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
    "* Se deberá entregar unicamente el notebook y los pesos del mejor modelo en un fichero .zip, de forma organizada.\n",
    "* Cada alumno deberá de subir la solución de forma individual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_b3mzw8IzJP"
   },
   "source": [
    "---\n",
    "## **PARTE 3**. Desarrollo y preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duPmUNOVGb2a"
   },
   "source": [
    "#### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "j3eRhgI-Gb2a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 20:35:49.841696: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-03 20:35:49.992913: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-03 20:35:50.020371: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/fraserlp/anaconda3/envs/miar_rl/lib/python3.8/site-packages/cv2/../../lib64:/usr/local/cuda-11.2/lib64:/usr/local/lib:\n",
      "2025-07-03 20:35:50.020379: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-07-03 20:35:50.041775: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-03 20:35:50.488226: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/fraserlp/anaconda3/envs/miar_rl/lib/python3.8/site-packages/cv2/../../lib64:/usr/local/cuda-11.2/lib64:/usr/local/lib:\n",
      "2025-07-03 20:35:50.488301: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/fraserlp/anaconda3/envs/miar_rl/lib/python3.8/site-packages/cv2/../../lib64:/usr/local/cuda-11.2/lib64:/usr/local/lib:\n",
      "2025-07-03 20:35:50.488307: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4jgQjzoGb2a"
   },
   "source": [
    "#### Configuración base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jwOE6I_KGb2a"
   },
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n",
    "\"\"\"\n",
    "class RewardShapingWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(RewardShapingWrapper, self).__init__(env)\n",
    "        self.last_action = None\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "\n",
    "        # Penalizar NOOP (acción 0) o incentivar movimiento\n",
    "        if action == 0:  # NOOP\n",
    "            reward -= 0.01\n",
    "        elif action == 1:  # LEFT\n",
    "            reward += 0.01\n",
    "        elif action == 2:  # RIGHT\n",
    "            reward += 0.02\n",
    "        elif action == 3:  # LEFTFIRE\n",
    "            reward += 0.1\n",
    "        elif action == 4:  # RIGHTFIRE\n",
    "            reward += 0.2\n",
    "        if self.last_action == action:\n",
    "            reward -= 0.002  # pequeña penalización por repetir\n",
    "        \n",
    "        self.last_action = action\n",
    "\n",
    "        return obs, reward, done, info\n",
    "\"\"\"\n",
    "\n",
    "class BiasedEpsGreedyQPolicy(EpsGreedyQPolicy):\n",
    "    def __init__(self, bias_action=2, bias_strength=3, bias_duration=50000, **kwargs):\n",
    "        super(BiasedEpsGreedyQPolicy, self).__init__(**kwargs)\n",
    "        self.bias_action = bias_action  # Acción hacia la derecha (verifica con get_action_meanings)\n",
    "        self.bias_strength = bias_strength\n",
    "        self.bias_duration = bias_duration\n",
    "        self.step_count = 0\n",
    "\n",
    "    def select_action(self, q_values):\n",
    "        self.step_count += 1\n",
    "        if np.random.rand() < self.eps:\n",
    "            if self.step_count < self.bias_duration:\n",
    "                # Más probabilidad de elegir la acción \"bias_action\"\n",
    "                biased_choices = [self.bias_action] * self.bias_strength + list(range(len(q_values)))\n",
    "                action = np.random.choice(biased_choices)\n",
    "            else:\n",
    "                action = np.random.randint(len(q_values))\n",
    "        else:\n",
    "            action = np.argmax(q_values)\n",
    "        return action\n",
    "\n",
    "env_name = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_name)\n",
    "#env = RewardShapingWrapper(env)\n",
    "\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9jGEZUcpGb2a"
   },
   "outputs": [],
   "source": [
    "\"\"\"class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize(INPUT_SHAPE).convert('L')\n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('uint8')\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)\n",
    "\n",
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "\n",
    "        # Recorta la parte superior (por ejemplo, los primeros 30-34 píxeles)\n",
    "        cropped = observation[34:, :, :]  # Recorta arriba, conserva canales\n",
    "\n",
    "        img = Image.fromarray(cropped)\n",
    "        img = img.resize(INPUT_SHAPE).convert('L')  # Resize y escala de grises\n",
    "        processed_observation = np.array(img)\n",
    "\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('uint8')\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)\n",
    "\"\"\"\n",
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        assert observation.ndim == 3  # (H, W, C) = (210, 160, 3)\n",
    "\n",
    "        # Convertir a escala de grises con promedio (más rápido que PIL)\n",
    "        grayscale = np.mean(observation, axis=2).astype(np.uint8)  # (210, 160)\n",
    "\n",
    "        # Recortar (remueve parte superior y tal vez inferior)\n",
    "#        cropped = grayscale[34:194, :]  # Queda (160, 160)\n",
    "        cropped = grayscale[30:190, 10:150]\n",
    "\n",
    "        #cropped = grayscale[84:210-20, :]\n",
    "\n",
    "        # Redimensionar a 84x84\n",
    "        img = Image.fromarray(cropped)\n",
    "        img = img.resize((84, 84), resample=Image.BILINEAR)\n",
    "        \n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == (84, 84)\n",
    "\n",
    "        return processed_observation.astype('uint8')\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        # Normalización para Keras\n",
    "        return batch.astype('float32') / 255.\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yitXTADGb2b"
   },
   "source": [
    "1. Implementación de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "O4GKrfWSGb2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de acciones disponibles:6\n",
      "Formato de las observaciones:\n",
      "channels_last\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " permute (Permute)           (None, 84, 84, 4)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 20, 20, 32)        8224      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 20, 20, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 9, 9, 64)          32832     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 9, 9, 64)          0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1606144   \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 3078      \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 6)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,687,206\n",
      "Trainable params: 1,687,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Numero de acciones disponibles:\" + str(nb_actions))\n",
    "print(\"Formato de las observaciones:\")\n",
    "\n",
    "# Next, we build our model. We use the same model that was described by Mnih et al. (2015).\n",
    "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "model = Sequential()\n",
    "print(K.image_data_format())\n",
    "if K.image_data_format() == 'channels_last':\n",
    "    # (width, height, channels)\n",
    "    model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "elif K.image_data_format() == 'channels_first':\n",
    "    # (channels, width, height)\n",
    "    model.add(Permute((1, 2, 3), input_shape=input_shape))\n",
    "else:\n",
    "    raise RuntimeError('Unknown image_dim_ordering.')\n",
    "\n",
    "model.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, (3, 3), strides=(1, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Let's define the memory for storing the experience\\nmemory = SequentialMemory(limit=50000, window_length=1)\\n\\n# Define the policy that our agent will follow\\npolicy = BoltzmannQPolicy()\\n\\n# Define the agent\\ndqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory,\\n               nb_steps_warmup=10,\\n               target_model_update=1e-2, policy=policy)\\n\\nadam_obj = Adam(learning_rate=1e-3)\\ndqn.compile(adam_obj, metrics=['mae'])\\n\\n# Train the agent\\ndqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\\n\\n# After training is done, we save the final weights.\\ndqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\\n# Finally, evaluate our algorithm for 5 episodes.\\ndqn.load_weights('dqn_{}_weights.h5f'.format(ENV_NAME))\\ndqn.test(env, nb_episodes=10, visualize=False)\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Let's define the memory for storing the experience\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "\n",
    "# Define the policy that our agent will follow\n",
    "policy = BoltzmannQPolicy()\n",
    "\n",
    "# Define the agent\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory,\n",
    "               nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "\n",
    "adam_obj = Adam(learning_rate=1e-3)\n",
    "dqn.compile(adam_obj, metrics=['mae'])\n",
    "\n",
    "# Train the agent\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "dqn.load_weights('dqn_{}_weights.h5f'.format(ENV_NAME))\n",
    "dqn.test(env, nb_episodes=10, visualize=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=3000000, window_length=WINDOW_LENGTH)\n",
    "processor = AtariProcessor()\n",
    "\"\"\"\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n",
    "                              value_max=1., value_min=.1, value_test=.05,\n",
    "                              nb_steps=20000)\n",
    "\"\"\"\n",
    "policy = LinearAnnealedPolicy(BiasedEpsGreedyQPolicy(bias_action=2, bias_strength=3, bias_duration=50000),\n",
    "                              attr='eps',\n",
    "                              value_max=1., value_min=.1, value_test=.05,\n",
    "                              nb_steps=3000000)\n",
    "\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy,\n",
    "               memory=memory, processor=processor,\n",
    "               nb_steps_warmup=100000, gamma=0.995,\n",
    "               target_model_update=50,\n",
    "               train_interval=4)\n",
    "\n",
    "#dqn.compile(Adam(learning_rate=.00025), metrics=['mae', 'mse'])\n",
    "dqn.compile(Adam(learning_rate=1e-5), metrics=['mse'])\n",
    "\n",
    "# Training part\n",
    "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
    "checkpoint_weights_filename = 'dqn_' + env_name + '_weights_{step}.h5f'\n",
    "log_filename = 'dqn_{}_log.json'.format(env_name)\n",
    "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=25000)]\n",
    "callbacks += [FileLogger(log_filename, interval=2000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB9-_5HPGb2b"
   },
   "source": [
    "2. Implementación de la solución DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "foSlxWH1Gb2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 3000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "20000/20000 [==============================] - 59s 3ms/step - reward: 0.0155\n",
      "30 episodes - episode_reward: 10.167 [1.000, 21.000] - ale.lives: 1.888\n",
      "\n",
      "Interval 2 (20000 steps performed)\n",
      "20000/20000 [==============================] - 59s 3ms/step - reward: 0.0163\n",
      "28 episodes - episode_reward: 11.714 [4.000, 29.000] - ale.lives: 1.978\n",
      "\n",
      "Interval 3 (40000 steps performed)\n",
      "20000/20000 [==============================] - 58s 3ms/step - reward: 0.0151\n",
      "29 episodes - episode_reward: 10.276 [2.000, 21.000] - ale.lives: 1.987\n",
      "\n",
      "Interval 4 (60000 steps performed)\n",
      "20000/20000 [==============================] - 58s 3ms/step - reward: 0.0138\n",
      "28 episodes - episode_reward: 9.929 [4.000, 19.000] - ale.lives: 2.043\n",
      "\n",
      "Interval 5 (80000 steps performed)\n",
      "20000/20000 [==============================] - 58s 3ms/step - reward: 0.0127\n",
      "30 episodes - episode_reward: 8.567 [4.000, 17.000] - ale.lives: 2.115\n",
      "\n",
      "Interval 6 (100000 steps performed)\n",
      "20000/20000 [==============================] - 328s 16ms/step - reward: 0.0143\n",
      "29 episodes - episode_reward: 9.690 [4.000, 28.000] - loss: 0.034 - mse: 48.488 - mean_q: 7.522 - mean_eps: 0.967 - ale.lives: 2.124\n",
      "\n",
      "Interval 7 (120000 steps performed)\n",
      "20000/20000 [==============================] - 329s 16ms/step - reward: 0.0138\n",
      "27 episodes - episode_reward: 10.000 [4.000, 22.000] - loss: 0.033 - mse: 42.646 - mean_q: 7.044 - mean_eps: 0.961 - ale.lives: 2.122\n",
      "\n",
      "Interval 8 (140000 steps performed)\n",
      "20000/20000 [==============================] - 330s 16ms/step - reward: 0.0132\n",
      "31 episodes - episode_reward: 8.548 [4.000, 14.000] - loss: 0.034 - mse: 45.542 - mean_q: 7.284 - mean_eps: 0.955 - ale.lives: 2.084\n",
      "\n",
      "Interval 9 (160000 steps performed)\n",
      "20000/20000 [==============================] - 331s 17ms/step - reward: 0.0134\n",
      "28 episodes - episode_reward: 9.786 [3.000, 19.000] - loss: 0.038 - mse: 53.504 - mean_q: 7.897 - mean_eps: 0.949 - ale.lives: 2.160\n",
      "\n",
      "Interval 10 (180000 steps performed)\n",
      "20000/20000 [==============================] - 332s 17ms/step - reward: 0.0131\n",
      "29 episodes - episode_reward: 8.759 [4.000, 15.000] - loss: 0.035 - mse: 43.644 - mean_q: 7.119 - mean_eps: 0.943 - ale.lives: 2.074\n",
      "\n",
      "Interval 11 (200000 steps performed)\n",
      "20000/20000 [==============================] - 334s 17ms/step - reward: 0.0146\n",
      "29 episodes - episode_reward: 10.448 [4.000, 23.000] - loss: 0.033 - mse: 44.117 - mean_q: 7.170 - mean_eps: 0.937 - ale.lives: 2.188\n",
      "\n",
      "Interval 12 (220000 steps performed)\n",
      "20000/20000 [==============================] - 335s 17ms/step - reward: 0.0146\n",
      "27 episodes - episode_reward: 10.630 [2.000, 33.000] - loss: 0.034 - mse: 46.242 - mean_q: 7.332 - mean_eps: 0.931 - ale.lives: 2.107\n",
      "\n",
      "Interval 13 (240000 steps performed)\n",
      "20000/20000 [==============================] - 336s 17ms/step - reward: 0.0134\n",
      "29 episodes - episode_reward: 9.414 [2.000, 21.000] - loss: 0.033 - mse: 43.908 - mean_q: 7.138 - mean_eps: 0.925 - ale.lives: 2.120\n",
      "\n",
      "Interval 14 (260000 steps performed)\n",
      "20000/20000 [==============================] - 338s 17ms/step - reward: 0.0140\n",
      "26 episodes - episode_reward: 10.769 [4.000, 24.000] - loss: 0.037 - mse: 49.261 - mean_q: 7.550 - mean_eps: 0.919 - ale.lives: 2.119\n",
      "\n",
      "Interval 15 (280000 steps performed)\n",
      "20000/20000 [==============================] - 339s 17ms/step - reward: 0.0134\n",
      "26 episodes - episode_reward: 10.077 [3.000, 18.000] - loss: 0.031 - mse: 44.575 - mean_q: 7.176 - mean_eps: 0.913 - ale.lives: 2.172\n",
      "\n",
      "Interval 16 (300000 steps performed)\n",
      "20000/20000 [==============================] - 341s 17ms/step - reward: 0.0115\n",
      "31 episodes - episode_reward: 7.581 [2.000, 19.000] - loss: 0.029 - mse: 39.652 - mean_q: 6.793 - mean_eps: 0.907 - ale.lives: 2.049\n",
      "\n",
      "Interval 17 (320000 steps performed)\n",
      "20000/20000 [==============================] - 342s 17ms/step - reward: 0.0143\n",
      "29 episodes - episode_reward: 9.724 [2.000, 21.000] - loss: 0.036 - mse: 46.117 - mean_q: 7.323 - mean_eps: 0.901 - ale.lives: 2.179\n",
      "\n",
      "Interval 18 (340000 steps performed)\n",
      "20000/20000 [==============================] - 344s 17ms/step - reward: 0.0144\n",
      "30 episodes - episode_reward: 9.667 [3.000, 19.000] - loss: 0.033 - mse: 44.217 - mean_q: 7.163 - mean_eps: 0.895 - ale.lives: 2.076\n",
      "\n",
      "Interval 19 (360000 steps performed)\n",
      "20000/20000 [==============================] - 345s 17ms/step - reward: 0.0143\n",
      "28 episodes - episode_reward: 10.321 [4.000, 22.000] - loss: 0.030 - mse: 38.826 - mean_q: 6.711 - mean_eps: 0.889 - ale.lives: 1.975\n",
      "\n",
      "Interval 20 (380000 steps performed)\n",
      "20000/20000 [==============================] - 347s 17ms/step - reward: 0.0138\n",
      "27 episodes - episode_reward: 10.111 [4.000, 21.000] - loss: 0.032 - mse: 43.465 - mean_q: 7.097 - mean_eps: 0.883 - ale.lives: 2.093\n",
      "\n",
      "Interval 21 (400000 steps performed)\n",
      "20000/20000 [==============================] - 349s 17ms/step - reward: 0.0135\n",
      "26 episodes - episode_reward: 10.385 [2.000, 36.000] - loss: 0.033 - mse: 40.893 - mean_q: 6.873 - mean_eps: 0.877 - ale.lives: 2.074\n",
      "\n",
      "Interval 22 (420000 steps performed)\n",
      "20000/20000 [==============================] - 350s 17ms/step - reward: 0.0143\n",
      "27 episodes - episode_reward: 10.259 [4.000, 26.000] - loss: 0.032 - mse: 43.887 - mean_q: 7.120 - mean_eps: 0.871 - ale.lives: 2.097\n",
      "\n",
      "Interval 23 (440000 steps performed)\n",
      "20000/20000 [==============================] - 352s 18ms/step - reward: 0.0123\n",
      "28 episodes - episode_reward: 9.143 [3.000, 22.000] - loss: 0.032 - mse: 43.125 - mean_q: 7.063 - mean_eps: 0.865 - ale.lives: 2.158\n",
      "\n",
      "Interval 24 (460000 steps performed)\n",
      "20000/20000 [==============================] - 353s 18ms/step - reward: 0.0137\n",
      "30 episodes - episode_reward: 8.933 [2.000, 19.000] - loss: 0.029 - mse: 37.775 - mean_q: 6.645 - mean_eps: 0.859 - ale.lives: 2.060\n",
      "\n",
      "Interval 25 (480000 steps performed)\n",
      "20000/20000 [==============================] - 355s 18ms/step - reward: 0.0134\n",
      "27 episodes - episode_reward: 10.000 [3.000, 28.000] - loss: 0.031 - mse: 41.791 - mean_q: 6.991 - mean_eps: 0.853 - ale.lives: 2.111\n",
      "\n",
      "Interval 26 (500000 steps performed)\n",
      "20000/20000 [==============================] - 357s 18ms/step - reward: 0.0143\n",
      "28 episodes - episode_reward: 10.429 [4.000, 24.000] - loss: 0.031 - mse: 41.077 - mean_q: 6.915 - mean_eps: 0.847 - ale.lives: 2.067\n",
      "\n",
      "Interval 27 (520000 steps performed)\n",
      "20000/20000 [==============================] - 359s 18ms/step - reward: 0.0154\n",
      "26 episodes - episode_reward: 11.846 [4.000, 24.000] - loss: 0.032 - mse: 43.414 - mean_q: 7.115 - mean_eps: 0.841 - ale.lives: 2.015\n",
      "\n",
      "Interval 28 (540000 steps performed)\n",
      "20000/20000 [==============================] - 361s 18ms/step - reward: 0.0136\n",
      "25 episodes - episode_reward: 10.760 [2.000, 22.000] - loss: 0.028 - mse: 38.747 - mean_q: 6.714 - mean_eps: 0.835 - ale.lives: 2.051\n",
      "\n",
      "Interval 29 (560000 steps performed)\n",
      "20000/20000 [==============================] - 364s 18ms/step - reward: 0.0141\n",
      "27 episodes - episode_reward: 10.444 [2.000, 21.000] - loss: 0.032 - mse: 41.051 - mean_q: 6.915 - mean_eps: 0.829 - ale.lives: 2.051\n",
      "\n",
      "Interval 30 (580000 steps performed)\n",
      "20000/20000 [==============================] - 366s 18ms/step - reward: 0.0133\n",
      "32 episodes - episode_reward: 8.250 [1.000, 17.000] - loss: 0.034 - mse: 44.410 - mean_q: 7.194 - mean_eps: 0.823 - ale.lives: 2.109\n",
      "\n",
      "Interval 31 (600000 steps performed)\n",
      "20000/20000 [==============================] - 368s 18ms/step - reward: 0.0146\n",
      "29 episodes - episode_reward: 9.310 [3.000, 21.000] - loss: 0.032 - mse: 42.650 - mean_q: 7.047 - mean_eps: 0.817 - ale.lives: 2.087\n",
      "\n",
      "Interval 32 (620000 steps performed)\n",
      "20000/20000 [==============================] - 371s 19ms/step - reward: 0.0130\n",
      "29 episodes - episode_reward: 9.586 [3.000, 29.000] - loss: 0.029 - mse: 41.837 - mean_q: 6.986 - mean_eps: 0.811 - ale.lives: 2.080\n",
      "\n",
      "Interval 33 (640000 steps performed)\n",
      "20000/20000 [==============================] - 373s 19ms/step - reward: 0.0138\n",
      "29 episodes - episode_reward: 9.793 [4.000, 21.000] - loss: 0.031 - mse: 41.718 - mean_q: 6.984 - mean_eps: 0.805 - ale.lives: 2.121\n",
      "\n",
      "Interval 34 (660000 steps performed)\n",
      "20000/20000 [==============================] - 375s 19ms/step - reward: 0.0139\n",
      "27 episodes - episode_reward: 9.926 [3.000, 27.000] - loss: 0.030 - mse: 38.542 - mean_q: 6.719 - mean_eps: 0.799 - ale.lives: 2.100\n",
      "\n",
      "Interval 35 (680000 steps performed)\n",
      "20000/20000 [==============================] - 378s 19ms/step - reward: 0.0147\n",
      "28 episodes - episode_reward: 10.821 [3.000, 22.000] - loss: 0.030 - mse: 40.926 - mean_q: 6.916 - mean_eps: 0.793 - ale.lives: 2.001\n",
      "\n",
      "Interval 36 (700000 steps performed)\n",
      "20000/20000 [==============================] - 380s 19ms/step - reward: 0.0146\n",
      "29 episodes - episode_reward: 10.276 [3.000, 18.000] - loss: 0.029 - mse: 36.435 - mean_q: 6.519 - mean_eps: 0.787 - ale.lives: 2.027\n",
      "\n",
      "Interval 37 (720000 steps performed)\n",
      "20000/20000 [==============================] - 383s 19ms/step - reward: 0.0143\n",
      "28 episodes - episode_reward: 9.893 [5.000, 22.000] - loss: 0.033 - mse: 41.368 - mean_q: 6.950 - mean_eps: 0.781 - ale.lives: 2.077\n",
      "\n",
      "Interval 38 (740000 steps performed)\n",
      "20000/20000 [==============================] - 386s 19ms/step - reward: 0.0148\n",
      "26 episodes - episode_reward: 11.423 [5.000, 25.000] - loss: 0.030 - mse: 38.013 - mean_q: 6.657 - mean_eps: 0.775 - ale.lives: 2.047\n",
      "\n",
      "Interval 39 (760000 steps performed)\n",
      "20000/20000 [==============================] - 388s 19ms/step - reward: 0.0146\n",
      "28 episodes - episode_reward: 10.607 [4.000, 26.000] - loss: 0.029 - mse: 38.377 - mean_q: 6.701 - mean_eps: 0.769 - ale.lives: 2.061\n",
      "\n",
      "Interval 40 (780000 steps performed)\n",
      "20000/20000 [==============================] - 391s 20ms/step - reward: 0.0132\n",
      "25 episodes - episode_reward: 10.640 [4.000, 16.000] - loss: 0.030 - mse: 36.122 - mean_q: 6.496 - mean_eps: 0.763 - ale.lives: 2.042\n",
      "\n",
      "Interval 41 (800000 steps performed)\n",
      "20000/20000 [==============================] - 394s 20ms/step - reward: 0.0141\n",
      "27 episodes - episode_reward: 10.407 [3.000, 22.000] - loss: 0.031 - mse: 38.034 - mean_q: 6.666 - mean_eps: 0.757 - ale.lives: 2.075\n",
      "\n",
      "Interval 42 (820000 steps performed)\n",
      "20000/20000 [==============================] - 397s 20ms/step - reward: 0.0138\n",
      "26 episodes - episode_reward: 10.231 [5.000, 18.000] - loss: 0.028 - mse: 40.082 - mean_q: 6.822 - mean_eps: 0.751 - ale.lives: 2.125\n",
      "\n",
      "Interval 43 (840000 steps performed)\n",
      "20000/20000 [==============================] - 401s 20ms/step - reward: 0.0129\n",
      "28 episodes - episode_reward: 9.429 [3.000, 22.000] - loss: 0.029 - mse: 37.376 - mean_q: 6.597 - mean_eps: 0.745 - ale.lives: 2.050\n",
      "\n",
      "Interval 44 (860000 steps performed)\n",
      "20000/20000 [==============================] - 404s 20ms/step - reward: 0.0155\n",
      "25 episodes - episode_reward: 12.200 [5.000, 27.000] - loss: 0.031 - mse: 38.456 - mean_q: 6.708 - mean_eps: 0.739 - ale.lives: 2.105\n",
      "\n",
      "Interval 45 (880000 steps performed)\n",
      "20000/20000 [==============================] - 406s 20ms/step - reward: 0.0139\n",
      "26 episodes - episode_reward: 10.846 [4.000, 21.000] - loss: 0.031 - mse: 41.704 - mean_q: 6.978 - mean_eps: 0.733 - ale.lives: 2.075\n",
      "\n",
      "Interval 46 (900000 steps performed)\n",
      "20000/20000 [==============================] - 409s 20ms/step - reward: 0.0141\n",
      "26 episodes - episode_reward: 10.346 [4.000, 19.000] - loss: 0.029 - mse: 35.899 - mean_q: 6.467 - mean_eps: 0.727 - ale.lives: 2.141\n",
      "\n",
      "Interval 47 (920000 steps performed)\n",
      "20000/20000 [==============================] - 412s 21ms/step - reward: 0.0140\n",
      "29 episodes - episode_reward: 10.000 [5.000, 23.000] - loss: 0.031 - mse: 39.842 - mean_q: 6.823 - mean_eps: 0.721 - ale.lives: 2.115\n",
      "\n",
      "Interval 48 (940000 steps performed)\n",
      "20000/20000 [==============================] - 414s 21ms/step - reward: 0.0135\n",
      "27 episodes - episode_reward: 10.111 [5.000, 18.000] - loss: 0.028 - mse: 36.051 - mean_q: 6.477 - mean_eps: 0.715 - ale.lives: 1.999\n",
      "\n",
      "Interval 49 (960000 steps performed)\n",
      "20000/20000 [==============================] - 417s 21ms/step - reward: 0.0140\n",
      "27 episodes - episode_reward: 10.333 [4.000, 21.000] - loss: 0.029 - mse: 40.035 - mean_q: 6.823 - mean_eps: 0.709 - ale.lives: 2.034\n",
      "\n",
      "Interval 50 (980000 steps performed)\n",
      "20000/20000 [==============================] - 420s 21ms/step - reward: 0.0127\n",
      "29 episodes - episode_reward: 8.793 [3.000, 14.000] - loss: 0.029 - mse: 39.935 - mean_q: 6.814 - mean_eps: 0.703 - ale.lives: 2.095\n",
      "\n",
      "Interval 51 (1000000 steps performed)\n",
      "20000/20000 [==============================] - 423s 21ms/step - reward: 0.0129\n",
      "26 episodes - episode_reward: 9.846 [2.000, 34.000] - loss: 0.029 - mse: 39.202 - mean_q: 6.748 - mean_eps: 0.697 - ale.lives: 1.992\n",
      "\n",
      "Interval 52 (1020000 steps performed)\n",
      "20000/20000 [==============================] - 427s 21ms/step - reward: 0.0146\n",
      "27 episodes - episode_reward: 10.556 [1.000, 23.000] - loss: 0.030 - mse: 38.935 - mean_q: 6.724 - mean_eps: 0.691 - ale.lives: 2.017\n",
      "\n",
      "Interval 53 (1040000 steps performed)\n",
      "20000/20000 [==============================] - 430s 21ms/step - reward: 0.0136\n",
      "28 episodes - episode_reward: 10.071 [3.000, 17.000] - loss: 0.030 - mse: 39.746 - mean_q: 6.786 - mean_eps: 0.685 - ale.lives: 2.077\n",
      "\n",
      "Interval 54 (1060000 steps performed)\n",
      "20000/20000 [==============================] - 433s 22ms/step - reward: 0.0141\n",
      "29 episodes - episode_reward: 9.724 [5.000, 17.000] - loss: 0.028 - mse: 37.862 - mean_q: 6.625 - mean_eps: 0.679 - ale.lives: 2.123\n",
      "\n",
      "Interval 55 (1080000 steps performed)\n",
      "20000/20000 [==============================] - 435s 22ms/step - reward: 0.0167\n",
      "25 episodes - episode_reward: 12.600 [2.000, 33.000] - loss: 0.027 - mse: 36.291 - mean_q: 6.493 - mean_eps: 0.673 - ale.lives: 2.159\n",
      "\n",
      "Interval 56 (1100000 steps performed)\n",
      "20000/20000 [==============================] - 439s 22ms/step - reward: 0.0154\n",
      "25 episodes - episode_reward: 13.000 [4.000, 24.000] - loss: 0.029 - mse: 37.452 - mean_q: 6.592 - mean_eps: 0.667 - ale.lives: 2.051\n",
      "\n",
      "Interval 57 (1120000 steps performed)\n",
      "20000/20000 [==============================] - 442s 22ms/step - reward: 0.0149\n",
      "28 episodes - episode_reward: 10.714 [3.000, 22.000] - loss: 0.029 - mse: 37.867 - mean_q: 6.632 - mean_eps: 0.661 - ale.lives: 1.976\n",
      "\n",
      "Interval 58 (1140000 steps performed)\n",
      "20000/20000 [==============================] - 445s 22ms/step - reward: 0.0144\n",
      "27 episodes - episode_reward: 10.444 [4.000, 17.000] - loss: 0.028 - mse: 37.626 - mean_q: 6.625 - mean_eps: 0.655 - ale.lives: 2.065\n",
      "\n",
      "Interval 59 (1160000 steps performed)\n",
      "20000/20000 [==============================] - 448s 22ms/step - reward: 0.0143\n",
      "25 episodes - episode_reward: 11.720 [2.000, 27.000] - loss: 0.026 - mse: 35.348 - mean_q: 6.396 - mean_eps: 0.649 - ale.lives: 2.041\n",
      "\n",
      "Interval 60 (1180000 steps performed)\n",
      "20000/20000 [==============================] - 450s 22ms/step - reward: 0.0154\n",
      "25 episodes - episode_reward: 12.120 [2.000, 24.000] - loss: 0.026 - mse: 34.289 - mean_q: 6.306 - mean_eps: 0.643 - ale.lives: 2.055\n",
      "\n",
      "Interval 61 (1200000 steps performed)\n",
      "20000/20000 [==============================] - 453s 23ms/step - reward: 0.0146\n",
      "25 episodes - episode_reward: 12.000 [5.000, 28.000] - loss: 0.028 - mse: 39.157 - mean_q: 6.732 - mean_eps: 0.637 - ale.lives: 2.026\n",
      "\n",
      "Interval 62 (1220000 steps performed)\n",
      "20000/20000 [==============================] - 456s 23ms/step - reward: 0.0143\n",
      "27 episodes - episode_reward: 10.370 [6.000, 26.000] - loss: 0.027 - mse: 38.357 - mean_q: 6.666 - mean_eps: 0.631 - ale.lives: 2.115\n",
      "\n",
      "Interval 63 (1240000 steps performed)\n",
      "20000/20000 [==============================] - 459s 23ms/step - reward: 0.0144\n",
      "26 episodes - episode_reward: 11.077 [4.000, 20.000] - loss: 0.028 - mse: 37.883 - mean_q: 6.646 - mean_eps: 0.625 - ale.lives: 1.914\n",
      "\n",
      "Interval 64 (1260000 steps performed)\n",
      "20000/20000 [==============================] - 463s 23ms/step - reward: 0.0140\n",
      "27 episodes - episode_reward: 10.333 [4.000, 16.000] - loss: 0.027 - mse: 34.573 - mean_q: 6.340 - mean_eps: 0.619 - ale.lives: 2.040\n",
      "\n",
      "Interval 65 (1280000 steps performed)\n",
      "20000/20000 [==============================] - 466s 23ms/step - reward: 0.0147\n",
      "25 episodes - episode_reward: 11.800 [5.000, 27.000] - loss: 0.028 - mse: 36.830 - mean_q: 6.539 - mean_eps: 0.613 - ale.lives: 2.048\n",
      "\n",
      "Interval 66 (1300000 steps performed)\n",
      "20000/20000 [==============================] - 469s 23ms/step - reward: 0.0135\n",
      "27 episodes - episode_reward: 10.259 [4.000, 22.000] - loss: 0.027 - mse: 35.977 - mean_q: 6.469 - mean_eps: 0.607 - ale.lives: 1.911\n",
      "\n",
      "Interval 67 (1320000 steps performed)\n",
      "20000/20000 [==============================] - 472s 24ms/step - reward: 0.0143\n",
      "26 episodes - episode_reward: 11.038 [4.000, 19.000] - loss: 0.026 - mse: 36.846 - mean_q: 6.549 - mean_eps: 0.601 - ale.lives: 2.087\n",
      "\n",
      "Interval 68 (1340000 steps performed)\n",
      "20000/20000 [==============================] - 474s 24ms/step - reward: 0.0152\n",
      "22 episodes - episode_reward: 13.591 [5.000, 25.000] - loss: 0.029 - mse: 36.950 - mean_q: 6.575 - mean_eps: 0.595 - ale.lives: 2.094\n",
      "\n",
      "Interval 69 (1360000 steps performed)\n",
      "20000/20000 [==============================] - 478s 24ms/step - reward: 0.0150\n",
      "25 episodes - episode_reward: 11.960 [5.000, 21.000] - loss: 0.027 - mse: 38.433 - mean_q: 6.691 - mean_eps: 0.589 - ale.lives: 2.102\n",
      "\n",
      "Interval 70 (1380000 steps performed)\n",
      "20000/20000 [==============================] - 481s 24ms/step - reward: 0.0153\n",
      "27 episodes - episode_reward: 11.222 [6.000, 28.000] - loss: 0.027 - mse: 39.807 - mean_q: 6.797 - mean_eps: 0.583 - ale.lives: 1.984\n",
      "\n",
      "Interval 71 (1400000 steps performed)\n",
      "20000/20000 [==============================] - 484s 24ms/step - reward: 0.0145\n",
      "28 episodes - episode_reward: 10.464 [3.000, 25.000] - loss: 0.029 - mse: 36.574 - mean_q: 6.521 - mean_eps: 0.577 - ale.lives: 2.056\n",
      "\n",
      "Interval 72 (1420000 steps performed)\n",
      "20000/20000 [==============================] - 486s 24ms/step - reward: 0.0149\n",
      "26 episodes - episode_reward: 11.500 [5.000, 28.000] - loss: 0.027 - mse: 37.265 - mean_q: 6.574 - mean_eps: 0.571 - ale.lives: 2.011\n",
      "\n",
      "Interval 73 (1440000 steps performed)\n",
      "20000/20000 [==============================] - 490s 24ms/step - reward: 0.0132\n",
      "27 episodes - episode_reward: 9.815 [4.000, 17.000] - loss: 0.027 - mse: 36.776 - mean_q: 6.533 - mean_eps: 0.565 - ale.lives: 2.070\n",
      "\n",
      "Interval 74 (1460000 steps performed)\n",
      "20000/20000 [==============================] - 493s 25ms/step - reward: 0.0144\n",
      "27 episodes - episode_reward: 10.667 [5.000, 27.000] - loss: 0.029 - mse: 37.789 - mean_q: 6.616 - mean_eps: 0.559 - ale.lives: 1.949\n",
      "\n",
      "Interval 75 (1480000 steps performed)\n",
      "20000/20000 [==============================] - 495s 25ms/step - reward: 0.0138\n",
      "28 episodes - episode_reward: 10.071 [4.000, 25.000] - loss: 0.025 - mse: 34.545 - mean_q: 6.334 - mean_eps: 0.553 - ale.lives: 1.996\n",
      "\n",
      "Interval 76 (1500000 steps performed)\n",
      "20000/20000 [==============================] - 498s 25ms/step - reward: 0.0135\n",
      "26 episodes - episode_reward: 10.154 [4.000, 24.000] - loss: 0.025 - mse: 33.112 - mean_q: 6.198 - mean_eps: 0.547 - ale.lives: 2.023\n",
      "\n",
      "Interval 77 (1520000 steps performed)\n",
      "20000/20000 [==============================] - 502s 25ms/step - reward: 0.0149\n",
      "27 episodes - episode_reward: 11.074 [3.000, 25.000] - loss: 0.026 - mse: 35.469 - mean_q: 6.416 - mean_eps: 0.541 - ale.lives: 2.045\n",
      "\n",
      "Interval 78 (1540000 steps performed)\n",
      "20000/20000 [==============================] - 504s 25ms/step - reward: 0.0141\n",
      "25 episodes - episode_reward: 11.440 [2.000, 27.000] - loss: 0.027 - mse: 35.211 - mean_q: 6.375 - mean_eps: 0.535 - ale.lives: 1.903\n",
      "\n",
      "Interval 79 (1560000 steps performed)\n",
      "20000/20000 [==============================] - 508s 25ms/step - reward: 0.0150\n",
      "26 episodes - episode_reward: 11.462 [4.000, 24.000] - loss: 0.030 - mse: 39.004 - mean_q: 6.735 - mean_eps: 0.529 - ale.lives: 1.964\n",
      "\n",
      "Interval 80 (1580000 steps performed)\n",
      "20000/20000 [==============================] - 511s 26ms/step - reward: 0.0150\n",
      "23 episodes - episode_reward: 13.304 [4.000, 30.000] - loss: 0.029 - mse: 37.848 - mean_q: 6.618 - mean_eps: 0.523 - ale.lives: 2.098\n",
      "\n",
      "Interval 81 (1600000 steps performed)\n",
      "20000/20000 [==============================] - 514s 26ms/step - reward: 0.0143\n",
      "26 episodes - episode_reward: 10.808 [5.000, 19.000] - loss: 0.030 - mse: 38.214 - mean_q: 6.669 - mean_eps: 0.517 - ale.lives: 2.009\n",
      "\n",
      "Interval 82 (1620000 steps performed)\n",
      "20000/20000 [==============================] - 517s 26ms/step - reward: 0.0148\n",
      "25 episodes - episode_reward: 11.520 [5.000, 27.000] - loss: 0.025 - mse: 35.233 - mean_q: 6.394 - mean_eps: 0.511 - ale.lives: 1.987\n",
      "\n",
      "Interval 83 (1640000 steps performed)\n",
      "20000/20000 [==============================] - 521s 26ms/step - reward: 0.0149\n",
      "24 episodes - episode_reward: 12.542 [4.000, 27.000] - loss: 0.027 - mse: 36.779 - mean_q: 6.527 - mean_eps: 0.505 - ale.lives: 1.954\n",
      "\n",
      "Interval 84 (1660000 steps performed)\n",
      "20000/20000 [==============================] - 522s 26ms/step - reward: 0.0139\n",
      "25 episodes - episode_reward: 11.560 [6.000, 32.000] - loss: 0.025 - mse: 36.523 - mean_q: 6.501 - mean_eps: 0.499 - ale.lives: 2.033\n",
      "\n",
      "Interval 85 (1680000 steps performed)\n",
      "20000/20000 [==============================] - 524s 26ms/step - reward: 0.0155\n",
      "24 episodes - episode_reward: 11.958 [3.000, 27.000] - loss: 0.025 - mse: 39.671 - mean_q: 6.771 - mean_eps: 0.493 - ale.lives: 2.075\n",
      "\n",
      "Interval 86 (1700000 steps performed)\n",
      "20000/20000 [==============================] - 525s 26ms/step - reward: 0.0129\n",
      "25 episodes - episode_reward: 11.000 [4.000, 25.000] - loss: 0.027 - mse: 37.334 - mean_q: 6.580 - mean_eps: 0.487 - ale.lives: 2.045\n",
      "\n",
      "Interval 87 (1720000 steps performed)\n",
      "20000/20000 [==============================] - 527s 26ms/step - reward: 0.0140\n",
      "24 episodes - episode_reward: 11.708 [4.000, 20.000] - loss: 0.025 - mse: 33.614 - mean_q: 6.251 - mean_eps: 0.481 - ale.lives: 1.947\n",
      "\n",
      "Interval 88 (1740000 steps performed)\n",
      "20000/20000 [==============================] - 531s 27ms/step - reward: 0.0141\n",
      "27 episodes - episode_reward: 10.519 [3.000, 20.000] - loss: 0.026 - mse: 33.994 - mean_q: 6.255 - mean_eps: 0.475 - ale.lives: 1.990\n",
      "\n",
      "Interval 89 (1760000 steps performed)\n",
      "20000/20000 [==============================] - 533s 27ms/step - reward: 0.0148\n",
      "24 episodes - episode_reward: 12.250 [5.000, 17.000] - loss: 0.026 - mse: 36.119 - mean_q: 6.475 - mean_eps: 0.469 - ale.lives: 2.064\n",
      "\n",
      "Interval 90 (1780000 steps performed)\n",
      "20000/20000 [==============================] - 536s 27ms/step - reward: 0.0131\n",
      "23 episodes - episode_reward: 11.130 [4.000, 30.000] - loss: 0.026 - mse: 37.769 - mean_q: 6.622 - mean_eps: 0.463 - ale.lives: 1.954\n",
      "\n",
      "Interval 91 (1800000 steps performed)\n",
      "20000/20000 [==============================] - 540s 27ms/step - reward: 0.0140\n",
      "26 episodes - episode_reward: 11.154 [4.000, 23.000] - loss: 0.027 - mse: 36.586 - mean_q: 6.513 - mean_eps: 0.457 - ale.lives: 2.039\n",
      "\n",
      "Interval 92 (1820000 steps performed)\n",
      "20000/20000 [==============================] - 541s 27ms/step - reward: 0.0123\n",
      "26 episodes - episode_reward: 9.577 [5.000, 21.000] - loss: 0.026 - mse: 37.877 - mean_q: 6.612 - mean_eps: 0.451 - ale.lives: 2.051\n",
      "\n",
      "Interval 93 (1840000 steps performed)\n",
      "20000/20000 [==============================] - 545s 27ms/step - reward: 0.0153\n",
      "25 episodes - episode_reward: 11.920 [5.000, 32.000] - loss: 0.026 - mse: 36.533 - mean_q: 6.515 - mean_eps: 0.445 - ale.lives: 2.070\n",
      "\n",
      "Interval 94 (1860000 steps performed)\n",
      "20000/20000 [==============================] - 548s 27ms/step - reward: 0.0138\n",
      "25 episodes - episode_reward: 11.160 [6.000, 18.000] - loss: 0.025 - mse: 37.982 - mean_q: 6.632 - mean_eps: 0.439 - ale.lives: 1.952\n",
      "\n",
      "Interval 95 (1880000 steps performed)\n",
      "20000/20000 [==============================] - 551s 28ms/step - reward: 0.0155\n",
      "24 episodes - episode_reward: 12.792 [3.000, 31.000] - loss: 0.024 - mse: 31.239 - mean_q: 6.014 - mean_eps: 0.433 - ale.lives: 2.022\n",
      "\n",
      "Interval 96 (1900000 steps performed)\n",
      "20000/20000 [==============================] - 555s 28ms/step - reward: 0.0159\n",
      "23 episodes - episode_reward: 13.870 [5.000, 27.000] - loss: 0.025 - mse: 32.916 - mean_q: 6.169 - mean_eps: 0.427 - ale.lives: 1.956\n",
      "\n",
      "Interval 97 (1920000 steps performed)\n",
      "20000/20000 [==============================] - 558s 28ms/step - reward: 0.0133\n",
      "26 episodes - episode_reward: 9.885 [4.000, 17.000] - loss: 0.024 - mse: 31.302 - mean_q: 5.996 - mean_eps: 0.421 - ale.lives: 1.979\n",
      "\n",
      "Interval 98 (1940000 steps performed)\n",
      "20000/20000 [==============================] - 562s 28ms/step - reward: 0.0146\n",
      "21 episodes - episode_reward: 13.857 [6.000, 27.000] - loss: 0.024 - mse: 33.618 - mean_q: 6.235 - mean_eps: 0.415 - ale.lives: 1.878\n",
      "\n",
      "Interval 99 (1960000 steps performed)\n",
      "20000/20000 [==============================] - 566s 28ms/step - reward: 0.0139\n",
      "29 episodes - episode_reward: 10.138 [5.000, 22.000] - loss: 0.027 - mse: 35.411 - mean_q: 6.408 - mean_eps: 0.409 - ale.lives: 2.027\n",
      "\n",
      "Interval 100 (1980000 steps performed)\n",
      "20000/20000 [==============================] - 570s 28ms/step - reward: 0.0162\n",
      "24 episodes - episode_reward: 12.917 [5.000, 33.000] - loss: 0.024 - mse: 35.821 - mean_q: 6.440 - mean_eps: 0.403 - ale.lives: 2.120\n",
      "\n",
      "Interval 101 (2000000 steps performed)\n",
      "20000/20000 [==============================] - 574s 29ms/step - reward: 0.0147\n",
      "24 episodes - episode_reward: 12.417 [5.000, 32.000] - loss: 0.024 - mse: 33.535 - mean_q: 6.234 - mean_eps: 0.397 - ale.lives: 1.988\n",
      "\n",
      "Interval 102 (2020000 steps performed)\n",
      "20000/20000 [==============================] - 585s 29ms/step - reward: 0.0141\n",
      "25 episodes - episode_reward: 11.360 [5.000, 20.000] - loss: 0.026 - mse: 34.710 - mean_q: 6.331 - mean_eps: 0.391 - ale.lives: 1.978\n",
      "\n",
      "Interval 103 (2040000 steps performed)\n",
      "20000/20000 [==============================] - 587s 29ms/step - reward: 0.0140\n",
      "26 episodes - episode_reward: 11.115 [4.000, 26.000] - loss: 0.024 - mse: 33.860 - mean_q: 6.264 - mean_eps: 0.385 - ale.lives: 2.040\n",
      "\n",
      "Interval 104 (2060000 steps performed)\n",
      "20000/20000 [==============================] - 594s 30ms/step - reward: 0.0149\n",
      "23 episodes - episode_reward: 12.522 [6.000, 21.000] - loss: 0.025 - mse: 33.003 - mean_q: 6.189 - mean_eps: 0.379 - ale.lives: 2.022\n",
      "\n",
      "Interval 105 (2080000 steps performed)\n",
      "20000/20000 [==============================] - 600s 30ms/step - reward: 0.0144\n",
      "24 episodes - episode_reward: 12.167 [4.000, 21.000] - loss: 0.022 - mse: 30.721 - mean_q: 5.941 - mean_eps: 0.373 - ale.lives: 1.989\n",
      "\n",
      "Interval 106 (2100000 steps performed)\n",
      "20000/20000 [==============================] - 605s 30ms/step - reward: 0.0138\n",
      "25 episodes - episode_reward: 10.840 [3.000, 25.000] - loss: 0.023 - mse: 31.909 - mean_q: 6.066 - mean_eps: 0.367 - ale.lives: 1.959\n",
      "\n",
      "Interval 107 (2120000 steps performed)\n",
      "20000/20000 [==============================] - 608s 30ms/step - reward: 0.0155\n",
      "22 episodes - episode_reward: 14.409 [3.000, 29.000] - loss: 0.021 - mse: 29.529 - mean_q: 5.824 - mean_eps: 0.361 - ale.lives: 2.024\n",
      "\n",
      "Interval 108 (2140000 steps performed)\n",
      "20000/20000 [==============================] - 614s 31ms/step - reward: 0.0132\n",
      "22 episodes - episode_reward: 11.955 [4.000, 23.000] - loss: 0.023 - mse: 31.011 - mean_q: 5.994 - mean_eps: 0.355 - ale.lives: 1.963\n",
      "\n",
      "Interval 109 (2160000 steps performed)\n",
      "20000/20000 [==============================] - 618s 31ms/step - reward: 0.0152\n",
      "24 episodes - episode_reward: 12.583 [5.000, 23.000] - loss: 0.025 - mse: 31.660 - mean_q: 6.055 - mean_eps: 0.349 - ale.lives: 2.043\n",
      "\n",
      "Interval 110 (2180000 steps performed)\n",
      "20000/20000 [==============================] - 620s 31ms/step - reward: 0.0141\n",
      "27 episodes - episode_reward: 10.704 [4.000, 24.000] - loss: 0.023 - mse: 35.788 - mean_q: 6.433 - mean_eps: 0.343 - ale.lives: 2.152\n",
      "\n",
      "Interval 111 (2200000 steps performed)\n",
      "20000/20000 [==============================] - 625s 31ms/step - reward: 0.0138\n",
      "24 episodes - episode_reward: 11.167 [5.000, 24.000] - loss: 0.024 - mse: 33.857 - mean_q: 6.260 - mean_eps: 0.337 - ale.lives: 1.941\n",
      "\n",
      "Interval 112 (2220000 steps performed)\n",
      "20000/20000 [==============================] - 629s 31ms/step - reward: 0.0144\n",
      "28 episodes - episode_reward: 10.321 [3.000, 23.000] - loss: 0.023 - mse: 31.608 - mean_q: 6.045 - mean_eps: 0.331 - ale.lives: 2.015\n",
      "\n",
      "Interval 113 (2240000 steps performed)\n",
      "20000/20000 [==============================] - 633s 32ms/step - reward: 0.0140\n",
      "26 episodes - episode_reward: 10.615 [4.000, 18.000] - loss: 0.027 - mse: 35.131 - mean_q: 6.386 - mean_eps: 0.325 - ale.lives: 2.044\n",
      "\n",
      "Interval 114 (2260000 steps performed)\n",
      "20000/20000 [==============================] - 643s 32ms/step - reward: 0.0152\n",
      "21 episodes - episode_reward: 14.714 [8.000, 28.000] - loss: 0.026 - mse: 38.512 - mean_q: 6.665 - mean_eps: 0.319 - ale.lives: 1.946\n",
      "\n",
      "Interval 115 (2280000 steps performed)\n",
      "20000/20000 [==============================] - 649s 32ms/step - reward: 0.0149\n",
      "21 episodes - episode_reward: 14.095 [5.000, 27.000] - loss: 0.024 - mse: 33.429 - mean_q: 6.211 - mean_eps: 0.313 - ale.lives: 2.142\n",
      "\n",
      "Interval 116 (2300000 steps performed)\n",
      "20000/20000 [==============================] - 650s 33ms/step - reward: 0.0158\n",
      "25 episodes - episode_reward: 12.720 [4.000, 24.000] - loss: 0.024 - mse: 31.697 - mean_q: 6.061 - mean_eps: 0.307 - ale.lives: 2.063\n",
      "\n",
      "Interval 117 (2320000 steps performed)\n",
      "20000/20000 [==============================] - 651s 33ms/step - reward: 0.0138\n",
      "28 episodes - episode_reward: 9.857 [4.000, 27.000] - loss: 0.020 - mse: 29.411 - mean_q: 5.826 - mean_eps: 0.301 - ale.lives: 2.022\n",
      "\n",
      "Interval 118 (2340000 steps performed)\n",
      "20000/20000 [==============================] - 653s 33ms/step - reward: 0.0148\n",
      "25 episodes - episode_reward: 12.080 [3.000, 21.000] - loss: 0.024 - mse: 33.511 - mean_q: 6.225 - mean_eps: 0.295 - ale.lives: 1.912\n",
      "\n",
      "Interval 119 (2360000 steps performed)\n",
      "20000/20000 [==============================] - 663s 33ms/step - reward: 0.0147\n",
      "22 episodes - episode_reward: 13.227 [7.000, 24.000] - loss: 0.023 - mse: 34.481 - mean_q: 6.303 - mean_eps: 0.289 - ale.lives: 1.974\n",
      "\n",
      "Interval 120 (2380000 steps performed)\n",
      "20000/20000 [==============================] - 669s 33ms/step - reward: 0.0152\n",
      "25 episodes - episode_reward: 12.120 [5.000, 23.000] - loss: 0.021 - mse: 31.972 - mean_q: 6.074 - mean_eps: 0.283 - ale.lives: 1.990\n",
      "\n",
      "Interval 121 (2400000 steps performed)\n",
      "20000/20000 [==============================] - 671s 34ms/step - reward: 0.0148\n",
      "22 episodes - episode_reward: 13.318 [6.000, 28.000] - loss: 0.020 - mse: 29.320 - mean_q: 5.803 - mean_eps: 0.277 - ale.lives: 2.040\n",
      "\n",
      "Interval 122 (2420000 steps performed)\n",
      "20000/20000 [==============================] - 673s 34ms/step - reward: 0.0150\n",
      "24 episodes - episode_reward: 12.667 [6.000, 23.000] - loss: 0.020 - mse: 28.182 - mean_q: 5.692 - mean_eps: 0.271 - ale.lives: 2.047\n",
      "\n",
      "Interval 123 (2440000 steps performed)\n",
      "20000/20000 [==============================] - 676s 34ms/step - reward: 0.0148\n",
      "26 episodes - episode_reward: 11.154 [5.000, 27.000] - loss: 0.020 - mse: 27.566 - mean_q: 5.627 - mean_eps: 0.265 - ale.lives: 2.013\n",
      "\n",
      "Interval 124 (2460000 steps performed)\n",
      "20000/20000 [==============================] - 681s 34ms/step - reward: 0.0141\n",
      "26 episodes - episode_reward: 11.154 [3.000, 23.000] - loss: 0.021 - mse: 29.575 - mean_q: 5.844 - mean_eps: 0.259 - ale.lives: 2.040\n",
      "\n",
      "Interval 125 (2480000 steps performed)\n",
      "20000/20000 [==============================] - 687s 34ms/step - reward: 0.0150\n",
      "23 episodes - episode_reward: 12.696 [5.000, 23.000] - loss: 0.021 - mse: 28.655 - mean_q: 5.743 - mean_eps: 0.253 - ale.lives: 2.054\n",
      "\n",
      "Interval 126 (2500000 steps performed)\n",
      "20000/20000 [==============================] - 697s 35ms/step - reward: 0.0154\n",
      "22 episodes - episode_reward: 13.955 [5.000, 30.000] - loss: 0.023 - mse: 32.436 - mean_q: 6.114 - mean_eps: 0.247 - ale.lives: 2.095\n",
      "\n",
      "Interval 127 (2520000 steps performed)\n",
      "20000/20000 [==============================] - 709s 35ms/step - reward: 0.0149\n",
      "24 episodes - episode_reward: 13.042 [6.000, 22.000] - loss: 0.025 - mse: 34.239 - mean_q: 6.307 - mean_eps: 0.241 - ale.lives: 2.002\n",
      "\n",
      "Interval 128 (2540000 steps performed)\n",
      "20000/20000 [==============================] - 706s 35ms/step - reward: 0.0158\n",
      "25 episodes - episode_reward: 12.680 [1.000, 28.000] - loss: 0.023 - mse: 30.778 - mean_q: 5.972 - mean_eps: 0.235 - ale.lives: 1.970\n",
      "\n",
      "Interval 129 (2560000 steps performed)\n",
      "20000/20000 [==============================] - 711s 36ms/step - reward: 0.0166\n",
      "23 episodes - episode_reward: 14.304 [7.000, 23.000] - loss: 0.022 - mse: 32.253 - mean_q: 6.081 - mean_eps: 0.229 - ale.lives: 2.019\n",
      "\n",
      "Interval 130 (2580000 steps performed)\n",
      "20000/20000 [==============================] - 706s 35ms/step - reward: 0.0155\n",
      "26 episodes - episode_reward: 11.731 [4.000, 21.000] - loss: 0.021 - mse: 30.275 - mean_q: 5.897 - mean_eps: 0.223 - ale.lives: 1.968\n",
      "\n",
      "Interval 131 (2600000 steps performed)\n",
      "20000/20000 [==============================] - 708s 35ms/step - reward: 0.0163\n",
      "24 episodes - episode_reward: 13.375 [4.000, 27.000] - loss: 0.023 - mse: 30.859 - mean_q: 5.952 - mean_eps: 0.217 - ale.lives: 2.098\n",
      "\n",
      "Interval 132 (2620000 steps performed)\n",
      "20000/20000 [==============================] - 710s 36ms/step - reward: 0.0158\n",
      "26 episodes - episode_reward: 12.538 [5.000, 29.000] - loss: 0.021 - mse: 30.763 - mean_q: 5.950 - mean_eps: 0.211 - ale.lives: 2.051\n",
      "\n",
      "Interval 133 (2640000 steps performed)\n",
      "20000/20000 [==============================] - 714s 36ms/step - reward: 0.0141\n",
      "24 episodes - episode_reward: 11.625 [5.000, 32.000] - loss: 0.023 - mse: 30.477 - mean_q: 5.922 - mean_eps: 0.205 - ale.lives: 1.973\n",
      "\n",
      "Interval 134 (2660000 steps performed)\n",
      "20000/20000 [==============================] - 723s 36ms/step - reward: 0.0161\n",
      "24 episodes - episode_reward: 13.292 [6.000, 25.000] - loss: 0.022 - mse: 30.523 - mean_q: 5.934 - mean_eps: 0.199 - ale.lives: 2.027\n",
      "\n",
      "Interval 135 (2680000 steps performed)\n",
      "20000/20000 [==============================] - 727s 36ms/step - reward: 0.0155\n",
      "24 episodes - episode_reward: 13.250 [4.000, 26.000] - loss: 0.021 - mse: 30.385 - mean_q: 5.920 - mean_eps: 0.193 - ale.lives: 2.102\n",
      "\n",
      "Interval 136 (2700000 steps performed)\n",
      "20000/20000 [==============================] - 733s 37ms/step - reward: 0.0155\n",
      "23 episodes - episode_reward: 13.174 [5.000, 29.000] - loss: 0.023 - mse: 31.105 - mean_q: 5.986 - mean_eps: 0.187 - ale.lives: 2.102\n",
      "\n",
      "Interval 137 (2720000 steps performed)\n",
      "20000/20000 [==============================] - 772s 39ms/step - reward: 0.0169\n",
      "27 episodes - episode_reward: 12.630 [4.000, 19.000] - loss: 0.021 - mse: 30.172 - mean_q: 5.894 - mean_eps: 0.181 - ale.lives: 2.025\n",
      "\n",
      "Interval 138 (2740000 steps performed)\n",
      "20000/20000 [==============================] - 727s 36ms/step - reward: 0.0149\n",
      "22 episodes - episode_reward: 13.727 [6.000, 24.000] - loss: 0.021 - mse: 28.962 - mean_q: 5.780 - mean_eps: 0.175 - ale.lives: 1.918\n",
      "\n",
      "Interval 139 (2760000 steps performed)\n",
      "20000/20000 [==============================] - 741s 37ms/step - reward: 0.0143\n",
      "24 episodes - episode_reward: 11.375 [5.000, 25.000] - loss: 0.023 - mse: 30.367 - mean_q: 5.938 - mean_eps: 0.169 - ale.lives: 1.995\n",
      "\n",
      "Interval 140 (2780000 steps performed)\n",
      "20000/20000 [==============================] - 829s 41ms/step - reward: 0.0158\n",
      "24 episodes - episode_reward: 13.542 [4.000, 23.000] - loss: 0.021 - mse: 31.089 - mean_q: 5.988 - mean_eps: 0.163 - ale.lives: 1.932\n",
      "\n",
      "Interval 141 (2800000 steps performed)\n",
      "20000/20000 [==============================] - 918s 46ms/step - reward: 0.0135\n",
      "25 episodes - episode_reward: 10.680 [1.000, 20.000] - loss: 0.021 - mse: 30.351 - mean_q: 5.909 - mean_eps: 0.157 - ale.lives: 1.952\n",
      "\n",
      "Interval 142 (2820000 steps performed)\n",
      "20000/20000 [==============================] - 755s 38ms/step - reward: 0.0162\n",
      "28 episodes - episode_reward: 11.536 [6.000, 22.000] - loss: 0.021 - mse: 29.051 - mean_q: 5.781 - mean_eps: 0.151 - ale.lives: 1.939\n",
      "\n",
      "Interval 143 (2840000 steps performed)\n",
      "20000/20000 [==============================] - 758s 38ms/step - reward: 0.0140\n",
      "26 episodes - episode_reward: 11.077 [2.000, 27.000] - loss: 0.019 - mse: 26.805 - mean_q: 5.547 - mean_eps: 0.145 - ale.lives: 2.099\n",
      "\n",
      "Interval 144 (2860000 steps performed)\n",
      "20000/20000 [==============================] - 764s 38ms/step - reward: 0.0155\n",
      "23 episodes - episode_reward: 13.304 [6.000, 26.000] - loss: 0.023 - mse: 30.384 - mean_q: 5.931 - mean_eps: 0.139 - ale.lives: 2.034\n",
      "\n",
      "Interval 145 (2880000 steps performed)\n",
      "20000/20000 [==============================] - 771s 39ms/step - reward: 0.0152\n",
      "24 episodes - episode_reward: 12.458 [4.000, 25.000] - loss: 0.023 - mse: 35.966 - mean_q: 6.445 - mean_eps: 0.133 - ale.lives: 2.038\n",
      "\n",
      "Interval 146 (2900000 steps performed)\n",
      "20000/20000 [==============================] - 775s 39ms/step - reward: 0.0162\n",
      "24 episodes - episode_reward: 13.250 [6.000, 25.000] - loss: 0.021 - mse: 30.796 - mean_q: 5.962 - mean_eps: 0.127 - ale.lives: 2.046\n",
      "\n",
      "Interval 147 (2920000 steps performed)\n",
      "20000/20000 [==============================] - 784s 39ms/step - reward: 0.0158\n",
      "25 episodes - episode_reward: 13.280 [3.000, 35.000] - loss: 0.021 - mse: 28.014 - mean_q: 5.670 - mean_eps: 0.121 - ale.lives: 1.996\n",
      "\n",
      "Interval 148 (2940000 steps performed)\n",
      "20000/20000 [==============================] - 784s 39ms/step - reward: 0.0149\n",
      "25 episodes - episode_reward: 11.560 [3.000, 25.000] - loss: 0.021 - mse: 29.516 - mean_q: 5.825 - mean_eps: 0.115 - ale.lives: 2.095\n",
      "\n",
      "Interval 149 (2960000 steps performed)\n",
      "20000/20000 [==============================] - 792s 40ms/step - reward: 0.0149\n",
      "24 episodes - episode_reward: 12.417 [5.000, 21.000] - loss: 0.019 - mse: 25.956 - mean_q: 5.447 - mean_eps: 0.109 - ale.lives: 1.956\n",
      "\n",
      "Interval 150 (2980000 steps performed)\n",
      "20000/20000 [==============================] - 800s 40ms/step - reward: 0.0157\n",
      "done, took 76256.671 seconds\n"
     ]
    }
   ],
   "source": [
    "#dqn.fit(env, callbacks=callbacks, nb_steps=1750000, log_interval=10000, visualize=False)\n",
    "#dqn.fit(env, callbacks=callbacks, nb_steps=2000000, log_interval=2000, visualize=True)\n",
    "dqn.fit(env, callbacks=callbacks, nb_steps=3000000, log_interval=20000, visualize=True)\n",
    "\n",
    "dqn.save_weights(weights_filename, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "OHYryKd1Gb2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 11.000, steps: 795\n",
      "Episode 2: reward: 11.000, steps: 812\n",
      "Episode 3: reward: 12.000, steps: 857\n",
      "Episode 4: reward: 11.000, steps: 798\n",
      "Episode 5: reward: 12.000, steps: 822\n",
      "Episode 6: reward: 12.000, steps: 854\n",
      "Episode 7: reward: 6.000, steps: 653\n",
      "Episode 8: reward: 9.000, steps: 784\n",
      "Episode 9: reward: 6.000, steps: 652\n",
      "Episode 10: reward: 12.000, steps: 848\n",
      "Media de la recompensa obtenida en 10 episodios: 10.2\n",
      "Desviación estandar de la recompensa obtenida en 10 episodios: 2.2715633383201093\n"
     ]
    }
   ],
   "source": [
    "# Testing part to calculate the mean reward\n",
    "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
    "dqn.load_weights(weights_filename)\n",
    "history = dqn.test(env, nb_episodes=10, visualize=False)\n",
    "recompensas = history.history.get('episode_reward', [])\n",
    "print(f\"Media de la recompensa obtenida en 10 episodios: {np.mean(np.array(recompensas))}\")\n",
    "print(f\"Desviación estandar de la recompensa obtenida en 10 episodios: {np.std(np.array(recompensas))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NAlu8b1Gb2b"
   },
   "source": [
    "3. Justificación de los parámetros seleccionados y de los resultados obtenidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANFQiicXK3sO"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
