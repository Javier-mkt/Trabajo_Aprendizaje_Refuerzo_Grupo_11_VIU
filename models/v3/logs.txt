   50270/2000000: episode: 74, duration: 13.334s, episode steps: 519, steps per second:  39, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.006156, mae: 0.014239, mean_q: 0.020979, mean_eps: 0.968247
   51209/2000000: episode: 75, duration: 36.877s, episode steps: 939, steps per second:  25, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.006686, mae: 0.013210, mean_q: 0.013179, mean_eps: 0.967865
   51622/2000000: episode: 76, duration: 16.190s, episode steps: 413, steps per second:  26, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.006196, mae: 0.012740, mean_q: 0.008496, mean_eps: 0.967437
   52388/2000000: episode: 77, duration: 31.584s, episode steps: 766, steps per second:  24, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.006532, mae: 0.012890, mean_q: 0.017494, mean_eps: 0.967064
   52797/2000000: episode: 78, duration: 16.439s, episode steps: 409, steps per second:  25, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.007061, mae: 0.012769, mean_q: 0.006376, mean_eps: 0.966692
   53281/2000000: episode: 79, duration: 19.129s, episode steps: 484, steps per second:  25, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.007003, mae: 0.015251, mean_q: 0.015980, mean_eps: 0.966408
   53941/2000000: episode: 80, duration: 25.936s, episode steps: 660, steps per second:  25, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.006324, mae: 0.012808, mean_q: 0.008469, mean_eps: 0.966046
   54729/2000000: episode: 81, duration: 31.160s, episode steps: 788, steps per second:  25, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.006301, mae: 0.012768, mean_q: 0.001648, mean_eps: 0.965587
   55221/2000000: episode: 82, duration: 19.376s, episode steps: 492, steps per second:  25, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.005817, mae: 0.013655, mean_q: 0.007470, mean_eps: 0.965182
   55960/2000000: episode: 83, duration: 28.594s, episode steps: 739, steps per second:  26, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.006653, mae: 0.011201, mean_q: 0.007396, mean_eps: 0.964793
   56735/2000000: episode: 84, duration: 29.952s, episode steps: 775, steps per second:  26, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.007178, mae: 0.014693, mean_q: 0.012991, mean_eps: 0.964314
   57801/2000000: episode: 85, duration: 40.454s, episode steps: 1066, steps per second:  26, episode reward:  8.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.006234, mae: 0.012925, mean_q: 0.015381, mean_eps: 0.963730
   58583/2000000: episode: 86, duration: 29.482s, episode steps: 782, steps per second:  27, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.005999, mae: 0.014548, mean_q: 0.019218, mean_eps: 0.963145
   59016/2000000: episode: 87, duration: 16.486s, episode steps: 433, steps per second:  26, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.006467, mae: 0.016340, mean_q: 0.016408, mean_eps: 0.962761
   60132/2000000: episode: 88, duration: 42.315s, episode steps: 1116, steps per second:  26, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.006236, mae: 0.016486, mean_q: 0.019692, mean_eps: 0.962271
   60540/2000000: episode: 89, duration: 15.506s, episode steps: 408, steps per second:  26, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.005877, mae: 0.028808, mean_q: 0.038263, mean_eps: 0.961788
   61052/2000000: episode: 90, duration: 19.513s, episode steps: 512, steps per second:  26, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.006571, mae: 0.031374, mean_q: 0.040533, mean_eps: 0.961497
   61764/2000000: episode: 91, duration: 27.073s, episode steps: 712, steps per second:  26, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.006777, mae: 0.031432, mean_q: 0.040227, mean_eps: 0.961110
   62447/2000000: episode: 92, duration: 25.741s, episode steps: 683, steps per second:  27, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.005467, mae: 0.028944, mean_q: 0.037113, mean_eps: 0.960667
   63054/2000000: episode: 93, duration: 23.086s, episode steps: 607, steps per second:  26, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: 0.006073, mae: 0.028278, mean_q: 0.036755, mean_eps: 0.960258
   63548/2000000: episode: 94, duration: 18.805s, episode steps: 494, steps per second:  26, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.007396, mae: 0.031718, mean_q: 0.039529, mean_eps: 0.959910
   64182/2000000: episode: 95, duration: 24.228s, episode steps: 634, steps per second:  26, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.004860, mae: 0.029363, mean_q: 0.039119, mean_eps: 0.959553
   65146/2000000: episode: 96, duration: 36.337s, episode steps: 964, steps per second:  27, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.006361, mae: 0.030977, mean_q: 0.039037, mean_eps: 0.959046
   65763/2000000: episode: 97, duration: 23.265s, episode steps: 617, steps per second:  27, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.005585, mae: 0.029525, mean_q: 0.038142, mean_eps: 0.958546
   67201/2000000: episode: 98, duration: 54.226s, episode steps: 1438, steps per second:  27, episode reward: 12.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.005690, mae: 0.029049, mean_q: 0.039914, mean_eps: 0.957895
   67972/2000000: episode: 99, duration: 29.208s, episode steps: 771, steps per second:  26, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.006131, mae: 0.030446, mean_q: 0.041803, mean_eps: 0.957196
   68365/2000000: episode: 100, duration: 14.957s, episode steps: 393, steps per second:  26, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: 0.005624, mae: 0.029065, mean_q: 0.038716, mean_eps: 0.956827
   69076/2000000: episode: 101, duration: 26.822s, episode steps: 711, steps per second:  27, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: 0.006332, mae: 0.029483, mean_q: 0.039095, mean_eps: 0.956477
   69570/2000000: episode: 102, duration: 18.694s, episode steps: 494, steps per second:  26, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.007500, mae: 0.032838, mean_q: 0.043564, mean_eps: 0.956096
   70424/2000000: episode: 103, duration: 32.380s, episode steps: 854, steps per second:  26, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.005830, mae: 0.037828, mean_q: 0.051683, mean_eps: 0.955669
   71160/2000000: episode: 104, duration: 27.874s, episode steps: 736, steps per second:  26, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.006328, mae: 0.047575, mean_q: 0.063884, mean_eps: 0.955166
   71865/2000000: episode: 105, duration: 26.718s, episode steps: 705, steps per second:  26, episode reward:  5.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.006126, mae: 0.047895, mean_q: 0.063855, mean_eps: 0.954709
   72671/2000000: episode: 106, duration: 30.378s, episode steps: 806, steps per second:  27, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.005647, mae: 0.045377, mean_q: 0.061021, mean_eps: 0.954230
   73047/2000000: episode: 107, duration: 14.161s, episode steps: 376, steps per second:  27, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.006477, mae: 0.046532, mean_q: 0.062400, mean_eps: 0.953857
   73803/2000000: episode: 108, duration: 28.538s, episode steps: 756, steps per second:  26, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.006392, mae: 0.046669, mean_q: 0.063969, mean_eps: 0.953498
   74451/2000000: episode: 109, duration: 24.455s, episode steps: 648, steps per second:  26, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.005992, mae: 0.046280, mean_q: 0.061459, mean_eps: 0.953054
   75117/2000000: episode: 110, duration: 25.683s, episode steps: 666, steps per second:  26, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.006823, mae: 0.049412, mean_q: 0.066093, mean_eps: 0.952637
   76598/2000000: episode: 111, duration: 56.656s, episode steps: 1481, steps per second:  26, episode reward: 15.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.006224, mae: 0.047572, mean_q: 0.064952, mean_eps: 0.951957
   76999/2000000: episode: 112, duration: 15.281s, episode steps: 401, steps per second:  26, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.005568, mae: 0.044915, mean_q: 0.061003, mean_eps: 0.951361
   77906/2000000: episode: 113, duration: 34.612s, episode steps: 907, steps per second:  26, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.005507, mae: 0.044595, mean_q: 0.061877, mean_eps: 0.950947
   78702/2000000: episode: 114, duration: 30.439s, episode steps: 796, steps per second:  26, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.005821, mae: 0.045888, mean_q: 0.066091, mean_eps: 0.950407
   79205/2000000: episode: 115, duration: 19.431s, episode steps: 503, steps per second:  26, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.005601, mae: 0.045391, mean_q: 0.062460, mean_eps: 0.949996
   79586/2000000: episode: 116, duration: 14.523s, episode steps: 381, steps per second:  26, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.006338, mae: 0.047767, mean_q: 0.064232, mean_eps: 0.949716
   80065/2000000: episode: 117, duration: 18.480s, episode steps: 479, steps per second:  26, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.005216, mae: 0.046279, mean_q: 0.064739, mean_eps: 0.949444
   80568/2000000: episode: 118, duration: 19.256s, episode steps: 503, steps per second:  26, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.005774, mae: 0.061345, mean_q: 0.084476, mean_eps: 0.949133
   81357/2000000: episode: 119, duration: 30.380s, episode steps: 789, steps per second:  26, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.005656, mae: 0.058118, mean_q: 0.078997, mean_eps: 0.948724
   82348/2000000: episode: 120, duration: 38.088s, episode steps: 991, steps per second:  26, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.006035, mae: 0.059185, mean_q: 0.080616, mean_eps: 0.948160
   83055/2000000: episode: 121, duration: 27.177s, episode steps: 707, steps per second:  26, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.005962, mae: 0.057104, mean_q: 0.076645, mean_eps: 0.947623
   83487/2000000: episode: 122, duration: 16.464s, episode steps: 432, steps per second:  26, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.004873, mae: 0.057275, mean_q: 0.078115, mean_eps: 0.947262
   84334/2000000: episode: 123, duration: 32.519s, episode steps: 847, steps per second:  26, episode reward:  8.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.005755, mae: 0.057201, mean_q: 0.079704, mean_eps: 0.946857
   85189/2000000: episode: 124, duration: 33.732s, episode steps: 855, steps per second:  25, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.005992, mae: 0.059546, mean_q: 0.082683, mean_eps: 0.946317
   86016/2000000: episode: 125, duration: 31.794s, episode steps: 827, steps per second:  26, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.006199, mae: 0.059839, mean_q: 0.083178, mean_eps: 0.945785
   86821/2000000: episode: 126, duration: 31.270s, episode steps: 805, steps per second:  26, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.005550, mae: 0.057330, mean_q: 0.081020, mean_eps: 0.945269
   87729/2000000: episode: 127, duration: 35.196s, episode steps: 908, steps per second:  26, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.005292, mae: 0.057954, mean_q: 0.081740, mean_eps: 0.944725
   88113/2000000: episode: 128, duration: 14.757s, episode steps: 384, steps per second:  26, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.004820, mae: 0.056716, mean_q: 0.079557, mean_eps: 0.944316
   88847/2000000: episode: 129, duration: 28.269s, episode steps: 734, steps per second:  26, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.004937, mae: 0.056754, mean_q: 0.080258, mean_eps: 0.943963
   89428/2000000: episode: 130, duration: 22.549s, episode steps: 581, steps per second:  26, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.006467, mae: 0.060348, mean_q: 0.084440, mean_eps: 0.943547
   89996/2000000: episode: 131, duration: 21.990s, episode steps: 568, steps per second:  26, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.004545, mae: 0.055732, mean_q: 0.079708, mean_eps: 0.943184
   90665/2000000: episode: 132, duration: 25.827s, episode steps: 669, steps per second:  26, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.005412, mae: 0.075074, mean_q: 0.104501, mean_eps: 0.942791
   91274/2000000: episode: 133, duration: 23.406s, episode steps: 609, steps per second:  26, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.005450, mae: 0.074788, mean_q: 0.103917, mean_eps: 0.942386
   92246/2000000: episode: 134, duration: 37.272s, episode steps: 972, steps per second:  26, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.005258, mae: 0.072772, mean_q: 0.100976, mean_eps: 0.941885
   92983/2000000: episode: 135, duration: 28.347s, episode steps: 737, steps per second:  26, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.006297, mae: 0.075344, mean_q: 0.102039, mean_eps: 0.941344
   93470/2000000: episode: 136, duration: 18.806s, episode steps: 487, steps per second:  26, episode reward:  1.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.005478, mae: 0.075000, mean_q: 0.103821, mean_eps: 0.940957
   94214/2000000: episode: 137, duration: 28.691s, episode steps: 744, steps per second:  26, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.005494, mae: 0.075273, mean_q: 0.104412, mean_eps: 0.940567
   95094/2000000: episode: 138, duration: 33.857s, episode steps: 880, steps per second:  26, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.005790, mae: 0.075554, mean_q: 0.106194, mean_eps: 0.940052
   95883/2000000: episode: 139, duration: 30.457s, episode steps: 789, steps per second:  26, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.004853, mae: 0.070614, mean_q: 0.098701, mean_eps: 0.939524
   96599/2000000: episode: 140, duration: 27.642s, episode steps: 716, steps per second:  26, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.005652, mae: 0.073502, mean_q: 0.104224, mean_eps: 0.939048
   97188/2000000: episode: 141, duration: 22.813s, episode steps: 589, steps per second:  26, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.635 [0.000, 5.000],  loss: 0.004868, mae: 0.074108, mean_q: 0.102749, mean_eps: 0.938635
   98078/2000000: episode: 142, duration: 34.318s, episode steps: 890, steps per second:  26, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.005611, mae: 0.074662, mean_q: 0.104442, mean_eps: 0.938166
   98795/2000000: episode: 143, duration: 27.630s, episode steps: 717, steps per second:  26, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.005035, mae: 0.074604, mean_q: 0.104379, mean_eps: 0.937657
   99522/2000000: episode: 144, duration: 27.960s, episode steps: 727, steps per second:  26, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.005079, mae: 0.073693, mean_q: 0.102013, mean_eps: 0.937200
  100073/2000000: episode: 145, duration: 22.322s, episode steps: 551, steps per second:  25, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.005338, mae: 0.079218, mean_q: 0.109034, mean_eps: 0.936795
  100476/2000000: episode: 146, duration: 15.586s, episode steps: 403, steps per second:  26, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.006808, mae: 0.104838, mean_q: 0.142392, mean_eps: 0.936493
  100979/2000000: episode: 147, duration: 19.405s, episode steps: 503, steps per second:  26, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.007214, mae: 0.106352, mean_q: 0.145198, mean_eps: 0.936207
  101778/2000000: episode: 148, duration: 30.847s, episode steps: 799, steps per second:  26, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.006093, mae: 0.103086, mean_q: 0.140061, mean_eps: 0.935794
  102421/2000000: episode: 149, duration: 24.831s, episode steps: 643, steps per second:  26, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.006683, mae: 0.101618, mean_q: 0.137845, mean_eps: 0.935337
  103081/2000000: episode: 150, duration: 25.479s, episode steps: 660, steps per second:  26, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.005494, mae: 0.099534, mean_q: 0.138098, mean_eps: 0.934924
  104108/2000000: episode: 151, duration: 39.827s, episode steps: 1027, steps per second:  26, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.005474, mae: 0.098977, mean_q: 0.135184, mean_eps: 0.934390
  104773/2000000: episode: 152, duration: 25.744s, episode steps: 665, steps per second:  26, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.005548, mae: 0.099731, mean_q: 0.135363, mean_eps: 0.933855
  105510/2000000: episode: 153, duration: 28.415s, episode steps: 737, steps per second:  26, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.685 [0.000, 5.000],  loss: 0.005415, mae: 0.099429, mean_q: 0.135682, mean_eps: 0.933410
  105915/2000000: episode: 154, duration: 15.581s, episode steps: 405, steps per second:  26, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.005270, mae: 0.099424, mean_q: 0.136346, mean_eps: 0.933049
  106555/2000000: episode: 155, duration: 24.708s, episode steps: 640, steps per second:  26, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.670 [0.000, 5.000],  loss: 0.006541, mae: 0.104171, mean_q: 0.140178, mean_eps: 0.932718
  107065/2000000: episode: 156, duration: 19.758s, episode steps: 510, steps per second:  26, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.005100, mae: 0.096519, mean_q: 0.132442, mean_eps: 0.932354
  107650/2000000: episode: 157, duration: 22.626s, episode steps: 585, steps per second:  26, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.005697, mae: 0.101870, mean_q: 0.138424, mean_eps: 0.932007
  108431/2000000: episode: 158, duration: 30.268s, episode steps: 781, steps per second:  26, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.005009, mae: 0.098884, mean_q: 0.134912, mean_eps: 0.931575
  109173/2000000: episode: 159, duration: 28.842s, episode steps: 742, steps per second:  26, episode reward:  6.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.005483, mae: 0.100732, mean_q: 0.137052, mean_eps: 0.931092
  109933/2000000: episode: 160, duration: 29.674s, episode steps: 760, steps per second:  26, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.005187, mae: 0.097467, mean_q: 0.132875, mean_eps: 0.930616
  110561/2000000: episode: 161, duration: 24.334s, episode steps: 628, steps per second:  26, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.007305, mae: 0.130654, mean_q: 0.175848, mean_eps: 0.930176
  111118/2000000: episode: 162, duration: 21.479s, episode steps: 557, steps per second:  26, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.006217, mae: 0.132234, mean_q: 0.176741, mean_eps: 0.929801
  111780/2000000: episode: 163, duration: 25.627s, episode steps: 662, steps per second:  26, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.006682, mae: 0.132887, mean_q: 0.177190, mean_eps: 0.929416
  112589/2000000: episode: 164, duration: 31.355s, episode steps: 809, steps per second:  26, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.007347, mae: 0.132962, mean_q: 0.177449, mean_eps: 0.928950
  113058/2000000: episode: 165, duration: 18.176s, episode steps: 469, steps per second:  26, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.006679, mae: 0.129057, mean_q: 0.172545, mean_eps: 0.928545
  113560/2000000: episode: 166, duration: 19.552s, episode steps: 502, steps per second:  26, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.006190, mae: 0.132008, mean_q: 0.176362, mean_eps: 0.928238
  114664/2000000: episode: 167, duration: 42.923s, episode steps: 1104, steps per second:  26, episode reward: 13.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.006297, mae: 0.131017, mean_q: 0.176187, mean_eps: 0.927730
  115168/2000000: episode: 168, duration: 19.774s, episode steps: 504, steps per second:  25, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.005855, mae: 0.127410, mean_q: 0.171606, mean_eps: 0.927221
  115997/2000000: episode: 169, duration: 32.149s, episode steps: 829, steps per second:  26, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.006129, mae: 0.131206, mean_q: 0.174766, mean_eps: 0.926798
  116663/2000000: episode: 170, duration: 25.722s, episode steps: 666, steps per second:  26, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.006201, mae: 0.132729, mean_q: 0.177848, mean_eps: 0.926324
  117695/2000000: episode: 171, duration: 40.166s, episode steps: 1032, steps per second:  26, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.005324, mae: 0.127705, mean_q: 0.171359, mean_eps: 0.925787
  118286/2000000: episode: 172, duration: 22.917s, episode steps: 591, steps per second:  26, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.005929, mae: 0.130392, mean_q: 0.175534, mean_eps: 0.925273
  118719/2000000: episode: 173, duration: 16.774s, episode steps: 433, steps per second:  26, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.006066, mae: 0.131992, mean_q: 0.176356, mean_eps: 0.924949
  119218/2000000: episode: 174, duration: 19.388s, episode steps: 499, steps per second:  26, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.005878, mae: 0.129648, mean_q: 0.173919, mean_eps: 0.924654
  119883/2000000: episode: 175, duration: 25.876s, episode steps: 665, steps per second:  26, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.006428, mae: 0.131077, mean_q: 0.176673, mean_eps: 0.924285
  120674/2000000: episode: 176, duration: 30.819s, episode steps: 791, steps per second:  26, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.007066, mae: 0.159492, mean_q: 0.213422, mean_eps: 0.923824
  121523/2000000: episode: 177, duration: 32.865s, episode steps: 849, steps per second:  26, episode reward:  6.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.007402, mae: 0.165538, mean_q: 0.220471, mean_eps: 0.923305
  122243/2000000: episode: 178, duration: 27.899s, episode steps: 720, steps per second:  26, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.007338, mae: 0.166918, mean_q: 0.221364, mean_eps: 0.922808
  123112/2000000: episode: 179, duration: 33.819s, episode steps: 869, steps per second:  26, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.006946, mae: 0.164980, mean_q: 0.217361, mean_eps: 0.922305
  123786/2000000: episode: 180, duration: 26.191s, episode steps: 674, steps per second:  26, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.006890, mae: 0.164140, mean_q: 0.218600, mean_eps: 0.921816
  124595/2000000: episode: 181, duration: 31.345s, episode steps: 809, steps per second:  26, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.006487, mae: 0.164017, mean_q: 0.217863, mean_eps: 0.921346
  124987/2000000: episode: 182, duration: 15.208s, episode steps: 392, steps per second:  26, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.006801, mae: 0.162265, mean_q: 0.215288, mean_eps: 0.920966
  126040/2000000: episode: 183, duration: 40.964s, episode steps: 1053, steps per second:  26, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.006475, mae: 0.161519, mean_q: 0.213619, mean_eps: 0.920509
  126663/2000000: episode: 184, duration: 24.364s, episode steps: 623, steps per second:  26, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.005743, mae: 0.158676, mean_q: 0.210424, mean_eps: 0.919978
  127429/2000000: episode: 185, duration: 29.785s, episode steps: 766, steps per second:  26, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.006518, mae: 0.162143, mean_q: 0.215427, mean_eps: 0.919538
  128382/2000000: episode: 186, duration: 36.898s, episode steps: 953, steps per second:  26, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.006239, mae: 0.160652, mean_q: 0.213583, mean_eps: 0.918993
  129036/2000000: episode: 187, duration: 25.636s, episode steps: 654, steps per second:  26, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.006995, mae: 0.164676, mean_q: 0.219009, mean_eps: 0.918485
  129893/2000000: episode: 188, duration: 33.510s, episode steps: 857, steps per second:  26, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.006087, mae: 0.163763, mean_q: 0.216121, mean_eps: 0.918006
  130592/2000000: episode: 189, duration: 27.129s, episode steps: 699, steps per second:  26, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.007484, mae: 0.192356, mean_q: 0.252105, mean_eps: 0.917513
  131121/2000000: episode: 190, duration: 20.624s, episode steps: 529, steps per second:  26, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.006255, mae: 0.192592, mean_q: 0.254561, mean_eps: 0.917125
  132235/2000000: episode: 191, duration: 43.760s, episode steps: 1114, steps per second:  25, episode reward: 11.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.006797, mae: 0.197316, mean_q: 0.258860, mean_eps: 0.916604
  133021/2000000: episode: 192, duration: 30.697s, episode steps: 786, steps per second:  26, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.006628, mae: 0.198351, mean_q: 0.259407, mean_eps: 0.916002
  133827/2000000: episode: 193, duration: 31.456s, episode steps: 806, steps per second:  26, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.007025, mae: 0.196275, mean_q: 0.256323, mean_eps: 0.915498
  134673/2000000: episode: 194, duration: 33.101s, episode steps: 846, steps per second:  26, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.007699, mae: 0.198053, mean_q: 0.259111, mean_eps: 0.914975
  135495/2000000: episode: 195, duration: 32.044s, episode steps: 822, steps per second:  26, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.006964, mae: 0.195358, mean_q: 0.257718, mean_eps: 0.914447
  136298/2000000: episode: 196, duration: 31.301s, episode steps: 803, steps per second:  26, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.006955, mae: 0.194329, mean_q: 0.255810, mean_eps: 0.913933
  137315/2000000: episode: 197, duration: 39.636s, episode steps: 1017, steps per second:  26, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.006224, mae: 0.193527, mean_q: 0.253429, mean_eps: 0.913356
  137897/2000000: episode: 198, duration: 22.791s, episode steps: 582, steps per second:  26, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.006356, mae: 0.197073, mean_q: 0.259908, mean_eps: 0.912850
  138468/2000000: episode: 199, duration: 22.341s, episode steps: 571, steps per second:  26, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.007335, mae: 0.196697, mean_q: 0.260749, mean_eps: 0.912485
  139351/2000000: episode: 200, duration: 34.479s, episode steps: 883, steps per second:  26, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.005852, mae: 0.193548, mean_q: 0.256060, mean_eps: 0.912025
  140164/2000000: episode: 201, duration: 32.028s, episode steps: 813, steps per second:  25, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.006890, mae: 0.206238, mean_q: 0.272008, mean_eps: 0.911488
  140910/2000000: episode: 202, duration: 29.267s, episode steps: 746, steps per second:  25, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.007310, mae: 0.239728, mean_q: 0.313692, mean_eps: 0.910994
  142130/2000000: episode: 203, duration: 47.580s, episode steps: 1220, steps per second:  26, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.007096, mae: 0.239027, mean_q: 0.312077, mean_eps: 0.910371
  142685/2000000: episode: 204, duration: 21.654s, episode steps: 555, steps per second:  26, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.007919, mae: 0.245533, mean_q: 0.320200, mean_eps: 0.909808
  143096/2000000: episode: 205, duration: 16.107s, episode steps: 411, steps per second:  26, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.007107, mae: 0.240222, mean_q: 0.312581, mean_eps: 0.909503
  143853/2000000: episode: 206, duration: 29.716s, episode steps: 757, steps per second:  25, episode reward:  6.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.007666, mae: 0.245046, mean_q: 0.319314, mean_eps: 0.909133
  144804/2000000: episode: 207, duration: 37.501s, episode steps: 951, steps per second:  25, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.007443, mae: 0.239063, mean_q: 0.310629, mean_eps: 0.908592
  145650/2000000: episode: 208, duration: 33.008s, episode steps: 846, steps per second:  26, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.008132, mae: 0.239935, mean_q: 0.311876, mean_eps: 0.908024
  146060/2000000: episode: 209, duration: 16.112s, episode steps: 410, steps per second:  25, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.006550, mae: 0.236214, mean_q: 0.308802, mean_eps: 0.907626
  146523/2000000: episode: 210, duration: 18.207s, episode steps: 463, steps per second:  25, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.007885, mae: 0.240119, mean_q: 0.313730, mean_eps: 0.907350
  147212/2000000: episode: 211, duration: 26.990s, episode steps: 689, steps per second:  26, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.007044, mae: 0.239161, mean_q: 0.311455, mean_eps: 0.906985
  147743/2000000: episode: 212, duration: 20.832s, episode steps: 531, steps per second:  25, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.670 [0.000, 5.000],  loss: 0.006351, mae: 0.236920, mean_q: 0.307237, mean_eps: 0.906599
  148215/2000000: episode: 213, duration: 18.459s, episode steps: 472, steps per second:  26, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.006644, mae: 0.235183, mean_q: 0.305033, mean_eps: 0.906281
  149113/2000000: episode: 214, duration: 35.106s, episode steps: 898, steps per second:  26, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.006550, mae: 0.240999, mean_q: 0.313567, mean_eps: 0.905846
  149899/2000000: episode: 215, duration: 30.644s, episode steps: 786, steps per second:  26, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.006739, mae: 0.239251, mean_q: 0.313044, mean_eps: 0.905313
  150545/2000000: episode: 216, duration: 26.628s, episode steps: 646, steps per second:  24, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.007027, mae: 0.257355, mean_q: 0.334554, mean_eps: 0.904859
  151548/2000000: episode: 217, duration: 39.165s, episode steps: 1003, steps per second:  26, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.007178, mae: 0.262595, mean_q: 0.340994, mean_eps: 0.904338
  151966/2000000: episode: 218, duration: 16.451s, episode steps: 418, steps per second:  25, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.007160, mae: 0.259387, mean_q: 0.336198, mean_eps: 0.903888
  153573/2000000: episode: 219, duration: 62.843s, episode steps: 1607, steps per second:  26, episode reward: 16.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.007050, mae: 0.264300, mean_q: 0.344137, mean_eps: 0.903246
  154672/2000000: episode: 220, duration: 43.201s, episode steps: 1099, steps per second:  25, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.007663, mae: 0.265233, mean_q: 0.344923, mean_eps: 0.902389
  155658/2000000: episode: 221, duration: 38.790s, episode steps: 986, steps per second:  25, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.006962, mae: 0.261492, mean_q: 0.338685, mean_eps: 0.901729
  156412/2000000: episode: 222, duration: 29.883s, episode steps: 754, steps per second:  25, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.006656, mae: 0.262862, mean_q: 0.340934, mean_eps: 0.901178
  157048/2000000: episode: 223, duration: 25.101s, episode steps: 636, steps per second:  25, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.006529, mae: 0.260218, mean_q: 0.337232, mean_eps: 0.900739
  158376/2000000: episode: 224, duration: 53.006s, episode steps: 1328, steps per second:  25, episode reward: 17.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.007030, mae: 0.264268, mean_q: 0.343926, mean_eps: 0.900117
  158941/2000000: episode: 225, duration: 22.538s, episode steps: 565, steps per second:  25, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.006639, mae: 0.267876, mean_q: 0.349156, mean_eps: 0.899517
  159653/2000000: episode: 226, duration: 28.239s, episode steps: 712, steps per second:  25, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.007237, mae: 0.261619, mean_q: 0.339866, mean_eps: 0.899111
  160454/2000000: episode: 227, duration: 31.848s, episode steps: 801, steps per second:  25, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.007674, mae: 0.277818, mean_q: 0.360350, mean_eps: 0.898632
  161296/2000000: episode: 228, duration: 33.652s, episode steps: 842, steps per second:  25, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.007132, mae: 0.290904, mean_q: 0.375338, mean_eps: 0.898113
  162152/2000000: episode: 229, duration: 33.971s, episode steps: 856, steps per second:  25, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.007485, mae: 0.288602, mean_q: 0.374443, mean_eps: 0.897576
  162612/2000000: episode: 230, duration: 18.385s, episode steps: 460, steps per second:  25, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.007240, mae: 0.288490, mean_q: 0.372827, mean_eps: 0.897159
  163473/2000000: episode: 231, duration: 34.648s, episode steps: 861, steps per second:  25, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.007391, mae: 0.290652, mean_q: 0.376428, mean_eps: 0.896740
  164249/2000000: episode: 232, duration: 30.978s, episode steps: 776, steps per second:  25, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.007389, mae: 0.288075, mean_q: 0.373272, mean_eps: 0.896221
  164828/2000000: episode: 233, duration: 23.091s, episode steps: 579, steps per second:  25, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.007511, mae: 0.291042, mean_q: 0.375531, mean_eps: 0.895793
  165504/2000000: episode: 234, duration: 28.205s, episode steps: 676, steps per second:  24, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.006879, mae: 0.286888, mean_q: 0.372010, mean_eps: 0.895396
  166185/2000000: episode: 235, duration: 29.045s, episode steps: 681, steps per second:  23, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.006765, mae: 0.287242, mean_q: 0.371979, mean_eps: 0.894965
  166905/2000000: episode: 236, duration: 30.076s, episode steps: 720, steps per second:  24, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.006935, mae: 0.289593, mean_q: 0.373938, mean_eps: 0.894521
  167561/2000000: episode: 237, duration: 26.534s, episode steps: 656, steps per second:  25, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.007557, mae: 0.283427, mean_q: 0.365841, mean_eps: 0.894085
  168049/2000000: episode: 238, duration: 22.411s, episode steps: 488, steps per second:  22, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.007888, mae: 0.287609, mean_q: 0.372050, mean_eps: 0.893723
  168878/2000000: episode: 239, duration: 34.255s, episode steps: 829, steps per second:  24, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.006741, mae: 0.285720, mean_q: 0.370459, mean_eps: 0.893306
  169215/2000000: episode: 240, duration: 14.548s, episode steps: 337, steps per second:  23, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.312 [0.000, 5.000],  loss: 0.006958, mae: 0.289451, mean_q: 0.375026, mean_eps: 0.892938
  169915/2000000: episode: 241, duration: 28.715s, episode steps: 700, steps per second:  24, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.006975, mae: 0.286293, mean_q: 0.371734, mean_eps: 0.892609
  170696/2000000: episode: 242, duration: 31.915s, episode steps: 781, steps per second:  24, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.007852, mae: 0.312968, mean_q: 0.405922, mean_eps: 0.892141
  171555/2000000: episode: 243, duration: 34.493s, episode steps: 859, steps per second:  25, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.007923, mae: 0.311038, mean_q: 0.404512, mean_eps: 0.891621
  172170/2000000: episode: 244, duration: 24.723s, episode steps: 615, steps per second:  25, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.008249, mae: 0.311635, mean_q: 0.404874, mean_eps: 0.891154
  172951/2000000: episode: 245, duration: 31.337s, episode steps: 781, steps per second:  25, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.007730, mae: 0.313288, mean_q: 0.406308, mean_eps: 0.890712
  173820/2000000: episode: 246, duration: 34.735s, episode steps: 869, steps per second:  25, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.007575, mae: 0.312103, mean_q: 0.403391, mean_eps: 0.890190
  174290/2000000: episode: 247, duration: 18.800s, episode steps: 470, steps per second:  25, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.007763, mae: 0.310264, mean_q: 0.402374, mean_eps: 0.889766
  175550/2000000: episode: 248, duration: 50.472s, episode steps: 1260, steps per second:  25, episode reward: 18.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.007972, mae: 0.309660, mean_q: 0.399714, mean_eps: 0.889217
  176019/2000000: episode: 249, duration: 18.771s, episode steps: 469, steps per second:  25, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.007694, mae: 0.310231, mean_q: 0.400184, mean_eps: 0.888670
  176898/2000000: episode: 250, duration: 36.213s, episode steps: 879, steps per second:  24, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.007758, mae: 0.309183, mean_q: 0.399702, mean_eps: 0.888243
  177827/2000000: episode: 251, duration: 37.263s, episode steps: 929, steps per second:  25, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.007904, mae: 0.310794, mean_q: 0.402908, mean_eps: 0.887671
  178525/2000000: episode: 252, duration: 28.179s, episode steps: 698, steps per second:  25, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.007550, mae: 0.308452, mean_q: 0.399549, mean_eps: 0.887155
  179272/2000000: episode: 253, duration: 30.212s, episode steps: 747, steps per second:  25, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.007130, mae: 0.310759, mean_q: 0.403136, mean_eps: 0.886698
  179983/2000000: episode: 254, duration: 28.575s, episode steps: 711, steps per second:  25, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.007577, mae: 0.311694, mean_q: 0.402057, mean_eps: 0.886237
  180599/2000000: episode: 255, duration: 24.669s, episode steps: 616, steps per second:  25, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.007188, mae: 0.343647, mean_q: 0.443481, mean_eps: 0.885816
  181463/2000000: episode: 256, duration: 34.717s, episode steps: 864, steps per second:  25, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.007621, mae: 0.337613, mean_q: 0.436833, mean_eps: 0.885348
  182283/2000000: episode: 257, duration: 32.951s, episode steps: 820, steps per second:  25, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.008041, mae: 0.345919, mean_q: 0.445926, mean_eps: 0.884814
  182820/2000000: episode: 258, duration: 21.610s, episode steps: 537, steps per second:  25, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.007937, mae: 0.341597, mean_q: 0.440304, mean_eps: 0.884385
  183324/2000000: episode: 259, duration: 20.410s, episode steps: 504, steps per second:  25, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.008038, mae: 0.341543, mean_q: 0.439606, mean_eps: 0.884056
  183711/2000000: episode: 260, duration: 15.588s, episode steps: 387, steps per second:  25, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.007160, mae: 0.347649, mean_q: 0.450134, mean_eps: 0.883773
  184424/2000000: episode: 261, duration: 28.642s, episode steps: 713, steps per second:  25, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.008478, mae: 0.340216, mean_q: 0.437637, mean_eps: 0.883425
  184946/2000000: episode: 262, duration: 20.971s, episode steps: 522, steps per second:  25, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.008186, mae: 0.338612, mean_q: 0.434923, mean_eps: 0.883033
  186086/2000000: episode: 263, duration: 45.635s, episode steps: 1140, steps per second:  25, episode reward: 14.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: 0.007816, mae: 0.344125, mean_q: 0.443215, mean_eps: 0.882507
  186710/2000000: episode: 264, duration: 24.901s, episode steps: 624, steps per second:  25, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.008534, mae: 0.337913, mean_q: 0.436639, mean_eps: 0.881948
  187403/2000000: episode: 265, duration: 27.656s, episode steps: 693, steps per second:  25, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.008038, mae: 0.341626, mean_q: 0.438820, mean_eps: 0.881531
  188361/2000000: episode: 266, duration: 38.398s, episode steps: 958, steps per second:  25, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.007656, mae: 0.342104, mean_q: 0.439129, mean_eps: 0.881008
  189070/2000000: episode: 267, duration: 28.451s, episode steps: 709, steps per second:  25, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.007764, mae: 0.345535, mean_q: 0.445435, mean_eps: 0.880480
  189719/2000000: episode: 268, duration: 26.051s, episode steps: 649, steps per second:  25, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.008313, mae: 0.338416, mean_q: 0.435234, mean_eps: 0.880050
  190633/2000000: episode: 269, duration: 36.916s, episode steps: 914, steps per second:  25, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.008635, mae: 0.355992, mean_q: 0.457705, mean_eps: 0.879555
  191753/2000000: episode: 270, duration: 44.881s, episode steps: 1120, steps per second:  25, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.008122, mae: 0.357067, mean_q: 0.459240, mean_eps: 0.878910
  192537/2000000: episode: 271, duration: 31.440s, episode steps: 784, steps per second:  25, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.007720, mae: 0.361365, mean_q: 0.465242, mean_eps: 0.878308
  193251/2000000: episode: 272, duration: 28.637s, episode steps: 714, steps per second:  25, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.008257, mae: 0.359786, mean_q: 0.462845, mean_eps: 0.877834
  193766/2000000: episode: 273, duration: 20.702s, episode steps: 515, steps per second:  25, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.008151, mae: 0.357982, mean_q: 0.460565, mean_eps: 0.877445
  194582/2000000: episode: 274, duration: 32.763s, episode steps: 816, steps per second:  25, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.008671, mae: 0.361631, mean_q: 0.464758, mean_eps: 0.877023
  195598/2000000: episode: 275, duration: 40.836s, episode steps: 1016, steps per second:  25, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.007804, mae: 0.360921, mean_q: 0.465176, mean_eps: 0.876443
  196272/2000000: episode: 276, duration: 27.115s, episode steps: 674, steps per second:  25, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.007527, mae: 0.363141, mean_q: 0.467535, mean_eps: 0.875908
  196795/2000000: episode: 277, duration: 21.142s, episode steps: 523, steps per second:  25, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.007811, mae: 0.353940, mean_q: 0.453424, mean_eps: 0.875530
  197306/2000000: episode: 278, duration: 20.597s, episode steps: 511, steps per second:  25, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.307 [0.000, 5.000],  loss: 0.007495, mae: 0.357181, mean_q: 0.458990, mean_eps: 0.875202
  197964/2000000: episode: 279, duration: 26.632s, episode steps: 658, steps per second:  25, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.007838, mae: 0.363115, mean_q: 0.466486, mean_eps: 0.874832
  198644/2000000: episode: 280, duration: 28.079s, episode steps: 680, steps per second:  24, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.007495, mae: 0.352269, mean_q: 0.452301, mean_eps: 0.874409
  199212/2000000: episode: 281, duration: 23.061s, episode steps: 568, steps per second:  25, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.007719, mae: 0.363984, mean_q: 0.468303, mean_eps: 0.874014
  199996/2000000: episode: 282, duration: 31.769s, episode steps: 784, steps per second:  25, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.007155, mae: 0.358406, mean_q: 0.460536, mean_eps: 0.873585
  200763/2000000: episode: 283, duration: 32.198s, episode steps: 767, steps per second:  24, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.007717, mae: 0.392113, mean_q: 0.502790, mean_eps: 0.873094
  201244/2000000: episode: 284, duration: 19.500s, episode steps: 481, steps per second:  25, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.287 [0.000, 5.000],  loss: 0.008876, mae: 0.396084, mean_q: 0.508776, mean_eps: 0.872699
  201976/2000000: episode: 285, duration: 29.548s, episode steps: 732, steps per second:  25, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.008263, mae: 0.401877, mean_q: 0.515330, mean_eps: 0.872315
  202375/2000000: episode: 286, duration: 16.070s, episode steps: 399, steps per second:  25, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.008726, mae: 0.392594, mean_q: 0.500523, mean_eps: 0.871956
  202926/2000000: episode: 287, duration: 22.102s, episode steps: 551, steps per second:  25, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.008128, mae: 0.388868, mean_q: 0.497559, mean_eps: 0.871655
  203588/2000000: episode: 288, duration: 26.669s, episode steps: 662, steps per second:  25, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.008221, mae: 0.394117, mean_q: 0.504956, mean_eps: 0.871271
  204192/2000000: episode: 289, duration: 24.569s, episode steps: 604, steps per second:  25, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.009322, mae: 0.398363, mean_q: 0.510531, mean_eps: 0.870871
  204889/2000000: episode: 290, duration: 28.246s, episode steps: 697, steps per second:  25, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.007737, mae: 0.394963, mean_q: 0.505495, mean_eps: 0.870458
  205753/2000000: episode: 291, duration: 34.784s, episode steps: 864, steps per second:  25, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.008453, mae: 0.395328, mean_q: 0.506185, mean_eps: 0.869963
  206610/2000000: episode: 292, duration: 34.537s, episode steps: 857, steps per second:  25, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.008803, mae: 0.393448, mean_q: 0.503969, mean_eps: 0.869418
  207194/2000000: episode: 293, duration: 23.491s, episode steps: 584, steps per second:  25, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.007905, mae: 0.396143, mean_q: 0.507650, mean_eps: 0.868962
  207840/2000000: episode: 294, duration: 26.142s, episode steps: 646, steps per second:  25, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.008432, mae: 0.396075, mean_q: 0.506958, mean_eps: 0.868573
  208513/2000000: episode: 295, duration: 27.084s, episode steps: 673, steps per second:  25, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.007961, mae: 0.392970, mean_q: 0.503142, mean_eps: 0.868155
  209331/2000000: episode: 296, duration: 32.864s, episode steps: 818, steps per second:  25, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.008110, mae: 0.393397, mean_q: 0.505099, mean_eps: 0.867683
  209891/2000000: episode: 297, duration: 22.591s, episode steps: 560, steps per second:  25, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.008239, mae: 0.395461, mean_q: 0.507427, mean_eps: 0.867247
  210275/2000000: episode: 298, duration: 15.488s, episode steps: 384, steps per second:  25, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.008872, mae: 0.425723, mean_q: 0.544161, mean_eps: 0.866948
  210772/2000000: episode: 299, duration: 20.154s, episode steps: 497, steps per second:  25, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.009164, mae: 0.440630, mean_q: 0.562172, mean_eps: 0.866669
  211402/2000000: episode: 300, duration: 25.512s, episode steps: 630, steps per second:  25, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.009077, mae: 0.435585, mean_q: 0.555606, mean_eps: 0.866312
  212101/2000000: episode: 301, duration: 28.400s, episode steps: 699, steps per second:  25, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 0.009133, mae: 0.433843, mean_q: 0.555177, mean_eps: 0.865890
  212667/2000000: episode: 302, duration: 22.839s, episode steps: 566, steps per second:  25, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.008897, mae: 0.440411, mean_q: 0.562414, mean_eps: 0.865490
  213347/2000000: episode: 303, duration: 27.367s, episode steps: 680, steps per second:  25, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.008541, mae: 0.436383, mean_q: 0.556722, mean_eps: 0.865096
  214025/2000000: episode: 304, duration: 27.499s, episode steps: 678, steps per second:  25, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.008540, mae: 0.434650, mean_q: 0.555722, mean_eps: 0.864666
  214630/2000000: episode: 305, duration: 24.394s, episode steps: 605, steps per second:  25, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.008087, mae: 0.436893, mean_q: 0.558400, mean_eps: 0.864259
  215187/2000000: episode: 306, duration: 22.518s, episode steps: 557, steps per second:  25, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.008265, mae: 0.425357, mean_q: 0.542794, mean_eps: 0.863892
  215859/2000000: episode: 307, duration: 28.004s, episode steps: 672, steps per second:  24, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.646 [0.000, 5.000],  loss: 0.008471, mae: 0.436678, mean_q: 0.558662, mean_eps: 0.863503
  216381/2000000: episode: 308, duration: 22.808s, episode steps: 522, steps per second:  23, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.008883, mae: 0.432206, mean_q: 0.552372, mean_eps: 0.863124
  217040/2000000: episode: 309, duration: 29.046s, episode steps: 659, steps per second:  23, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.008174, mae: 0.424605, mean_q: 0.541364, mean_eps: 0.862750
  217578/2000000: episode: 310, duration: 23.563s, episode steps: 538, steps per second:  23, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.007848, mae: 0.435116, mean_q: 0.555205, mean_eps: 0.862372
  218140/2000000: episode: 311, duration: 23.466s, episode steps: 562, steps per second:  24, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.007400, mae: 0.432269, mean_q: 0.552497, mean_eps: 0.862023
  219251/2000000: episode: 312, duration: 47.625s, episode steps: 1111, steps per second:  23, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.008171, mae: 0.436272, mean_q: 0.555557, mean_eps: 0.861494
  220752/2000000: episode: 313, duration: 65.686s, episode steps: 1501, steps per second:  23, episode reward: 33.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.009238, mae: 0.457882, mean_q: 0.583954, mean_eps: 0.860667
  221773/2000000: episode: 314, duration: 46.547s, episode steps: 1021, steps per second:  22, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.009122, mae: 0.471826, mean_q: 0.603315, mean_eps: 0.859867
  222146/2000000: episode: 315, duration: 16.365s, episode steps: 373, steps per second:  23, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.009221, mae: 0.477707, mean_q: 0.609777, mean_eps: 0.859425
  223093/2000000: episode: 316, duration: 43.057s, episode steps: 947, steps per second:  22, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.009310, mae: 0.464462, mean_q: 0.592515, mean_eps: 0.859007
  223716/2000000: episode: 317, duration: 29.401s, episode steps: 623, steps per second:  21, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.009511, mae: 0.471308, mean_q: 0.602291, mean_eps: 0.858511
  224426/2000000: episode: 318, duration: 32.613s, episode steps: 710, steps per second:  22, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.009380, mae: 0.474478, mean_q: 0.606171, mean_eps: 0.858089
  224927/2000000: episode: 319, duration: 22.703s, episode steps: 501, steps per second:  22, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.008998, mae: 0.463579, mean_q: 0.592353, mean_eps: 0.857705
  225535/2000000: episode: 320, duration: 26.566s, episode steps: 608, steps per second:  23, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.008842, mae: 0.471960, mean_q: 0.603173, mean_eps: 0.857354
  226025/2000000: episode: 321, duration: 20.775s, episode steps: 490, steps per second:  24, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.639 [0.000, 5.000],  loss: 0.009071, mae: 0.467137, mean_q: 0.598509, mean_eps: 0.857006
  226513/2000000: episode: 322, duration: 21.268s, episode steps: 488, steps per second:  23, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.009122, mae: 0.465071, mean_q: 0.592552, mean_eps: 0.856696
  227253/2000000: episode: 323, duration: 31.380s, episode steps: 740, steps per second:  24, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.009199, mae: 0.464167, mean_q: 0.593177, mean_eps: 0.856307
  228039/2000000: episode: 324, duration: 32.889s, episode steps: 786, steps per second:  24, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.008621, mae: 0.469745, mean_q: 0.599103, mean_eps: 0.855824
  228460/2000000: episode: 325, duration: 16.965s, episode steps: 421, steps per second:  25, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.008794, mae: 0.466040, mean_q: 0.595174, mean_eps: 0.855443
  229567/2000000: episode: 326, duration: 44.151s, episode steps: 1107, steps per second:  25, episode reward: 12.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.008369, mae: 0.465803, mean_q: 0.593761, mean_eps: 0.854959
  230393/2000000: episode: 327, duration: 32.516s, episode steps: 826, steps per second:  25, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.009362, mae: 0.490875, mean_q: 0.626409, mean_eps: 0.854346
  230995/2000000: episode: 328, duration: 23.554s, episode steps: 602, steps per second:  26, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.010847, mae: 0.513838, mean_q: 0.655457, mean_eps: 0.853894
  231663/2000000: episode: 329, duration: 27.763s, episode steps: 668, steps per second:  24, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.010109, mae: 0.505580, mean_q: 0.644883, mean_eps: 0.853492
  232270/2000000: episode: 330, duration: 25.265s, episode steps: 607, steps per second:  24, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.010147, mae: 0.505398, mean_q: 0.645044, mean_eps: 0.853088
  233141/2000000: episode: 331, duration: 34.666s, episode steps: 871, steps per second:  25, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.009704, mae: 0.506001, mean_q: 0.646394, mean_eps: 0.852620
  233790/2000000: episode: 332, duration: 25.663s, episode steps: 649, steps per second:  25, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.009810, mae: 0.502170, mean_q: 0.641625, mean_eps: 0.852138
  234328/2000000: episode: 333, duration: 21.427s, episode steps: 538, steps per second:  25, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.010215, mae: 0.500387, mean_q: 0.638644, mean_eps: 0.851763
  234800/2000000: episode: 334, duration: 18.644s, episode steps: 472, steps per second:  25, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.009100, mae: 0.510262, mean_q: 0.651737, mean_eps: 0.851444
  235180/2000000: episode: 335, duration: 15.102s, episode steps: 380, steps per second:  25, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.008997, mae: 0.505209, mean_q: 0.643873, mean_eps: 0.851174
  235733/2000000: episode: 336, duration: 21.833s, episode steps: 553, steps per second:  25, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.009493, mae: 0.511957, mean_q: 0.653467, mean_eps: 0.850878
  236281/2000000: episode: 337, duration: 21.558s, episode steps: 548, steps per second:  25, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.009566, mae: 0.504725, mean_q: 0.645353, mean_eps: 0.850528
  236966/2000000: episode: 338, duration: 27.122s, episode steps: 685, steps per second:  25, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.009316, mae: 0.507311, mean_q: 0.647243, mean_eps: 0.850138
  237603/2000000: episode: 339, duration: 25.067s, episode steps: 637, steps per second:  25, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.009579, mae: 0.499322, mean_q: 0.637109, mean_eps: 0.849720
  238228/2000000: episode: 340, duration: 25.010s, episode steps: 625, steps per second:  25, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.009532, mae: 0.507773, mean_q: 0.647500, mean_eps: 0.849321
  238829/2000000: episode: 341, duration: 23.840s, episode steps: 601, steps per second:  25, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.008984, mae: 0.498759, mean_q: 0.635275, mean_eps: 0.848932
  239373/2000000: episode: 342, duration: 21.658s, episode steps: 544, steps per second:  25, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.008680, mae: 0.496288, mean_q: 0.632437, mean_eps: 0.848569
  240283/2000000: episode: 343, duration: 35.769s, episode steps: 910, steps per second:  25, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.009538, mae: 0.517818, mean_q: 0.659925, mean_eps: 0.848109
  240949/2000000: episode: 344, duration: 26.241s, episode steps: 666, steps per second:  25, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.010483, mae: 0.536850, mean_q: 0.684488, mean_eps: 0.847610
  241658/2000000: episode: 345, duration: 27.778s, episode steps: 709, steps per second:  26, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.010291, mae: 0.538074, mean_q: 0.682796, mean_eps: 0.847174
  242474/2000000: episode: 346, duration: 32.136s, episode steps: 816, steps per second:  25, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.009410, mae: 0.543798, mean_q: 0.692174, mean_eps: 0.846692
  243009/2000000: episode: 347, duration: 21.160s, episode steps: 535, steps per second:  25, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.008842, mae: 0.533014, mean_q: 0.678375, mean_eps: 0.846263
  243687/2000000: episode: 348, duration: 26.687s, episode steps: 678, steps per second:  25, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.010315, mae: 0.539576, mean_q: 0.687749, mean_eps: 0.845880
  244363/2000000: episode: 349, duration: 27.572s, episode steps: 676, steps per second:  25, episode reward:  5.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.009970, mae: 0.539171, mean_q: 0.686824, mean_eps: 0.845451
  245004/2000000: episode: 350, duration: 27.251s, episode steps: 641, steps per second:  24, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.009568, mae: 0.536754, mean_q: 0.683190, mean_eps: 0.845035
  245620/2000000: episode: 351, duration: 26.580s, episode steps: 616, steps per second:  23, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.009386, mae: 0.537750, mean_q: 0.682310, mean_eps: 0.844637
  245997/2000000: episode: 352, duration: 16.168s, episode steps: 377, steps per second:  23, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.008738, mae: 0.535174, mean_q: 0.679049, mean_eps: 0.844322
  246548/2000000: episode: 353, duration: 22.136s, episode steps: 551, steps per second:  25, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: 0.008546, mae: 0.536595, mean_q: 0.682913, mean_eps: 0.844028
  247039/2000000: episode: 354, duration: 19.763s, episode steps: 491, steps per second:  25, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.009422, mae: 0.538224, mean_q: 0.685451, mean_eps: 0.843698
  247830/2000000: episode: 355, duration: 31.727s, episode steps: 791, steps per second:  25, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.010159, mae: 0.541396, mean_q: 0.689211, mean_eps: 0.843292
  248529/2000000: episode: 356, duration: 29.424s, episode steps: 699, steps per second:  24, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.009428, mae: 0.537671, mean_q: 0.684609, mean_eps: 0.842819
  249084/2000000: episode: 357, duration: 22.318s, episode steps: 555, steps per second:  25, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.009138, mae: 0.533904, mean_q: 0.679287, mean_eps: 0.842423
  250048/2000000: episode: 358, duration: 39.742s, episode steps: 964, steps per second:  24, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.009180, mae: 0.536528, mean_q: 0.682422, mean_eps: 0.841943
  250615/2000000: episode: 359, duration: 22.756s, episode steps: 567, steps per second:  25, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.009191, mae: 0.572361, mean_q: 0.727970, mean_eps: 0.841458
  251270/2000000: episode: 360, duration: 26.159s, episode steps: 655, steps per second:  25, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.010588, mae: 0.574299, mean_q: 0.729167, mean_eps: 0.841070
  252375/2000000: episode: 361, duration: 45.277s, episode steps: 1105, steps per second:  24, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.010674, mae: 0.574926, mean_q: 0.730833, mean_eps: 0.840513
  253000/2000000: episode: 362, duration: 30.384s, episode steps: 625, steps per second:  21, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.010773, mae: 0.573317, mean_q: 0.727698, mean_eps: 0.839966
  253849/2000000: episode: 363, duration: 37.651s, episode steps: 849, steps per second:  23, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.012399, mae: 0.573115, mean_q: 0.727101, mean_eps: 0.839498
  254567/2000000: episode: 364, duration: 29.957s, episode steps: 718, steps per second:  24, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.010239, mae: 0.572956, mean_q: 0.727513, mean_eps: 0.839002
  255056/2000000: episode: 365, duration: 20.417s, episode steps: 489, steps per second:  24, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.009804, mae: 0.578036, mean_q: 0.734125, mean_eps: 0.838620
  255690/2000000: episode: 366, duration: 26.297s, episode steps: 634, steps per second:  24, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 0.009691, mae: 0.567570, mean_q: 0.721306, mean_eps: 0.838264
  256388/2000000: episode: 367, duration: 28.864s, episode steps: 698, steps per second:  24, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.009480, mae: 0.575105, mean_q: 0.732091, mean_eps: 0.837843
  256979/2000000: episode: 368, duration: 24.305s, episode steps: 591, steps per second:  24, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.009903, mae: 0.575338, mean_q: 0.732839, mean_eps: 0.837435
  257752/2000000: episode: 369, duration: 32.027s, episode steps: 773, steps per second:  24, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.010423, mae: 0.565069, mean_q: 0.720497, mean_eps: 0.837003
  258497/2000000: episode: 370, duration: 30.781s, episode steps: 745, steps per second:  24, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.009813, mae: 0.570702, mean_q: 0.725223, mean_eps: 0.836521
  259181/2000000: episode: 371, duration: 28.364s, episode steps: 684, steps per second:  24, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.011434, mae: 0.580005, mean_q: 0.737055, mean_eps: 0.836068
  259516/2000000: episode: 372, duration: 13.926s, episode steps: 335, steps per second:  24, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.009779, mae: 0.573051, mean_q: 0.727488, mean_eps: 0.835746
  260754/2000000: episode: 373, duration: 51.172s, episode steps: 1238, steps per second:  24, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.009934, mae: 0.589597, mean_q: 0.749624, mean_eps: 0.835248
  261521/2000000: episode: 374, duration: 31.650s, episode steps: 767, steps per second:  24, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.010768, mae: 0.597941, mean_q: 0.760229, mean_eps: 0.834613
  262012/2000000: episode: 375, duration: 20.198s, episode steps: 491, steps per second:  24, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.010464, mae: 0.598359, mean_q: 0.759739, mean_eps: 0.834215
  262719/2000000: episode: 376, duration: 29.127s, episode steps: 707, steps per second:  24, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.009984, mae: 0.593609, mean_q: 0.753912, mean_eps: 0.833836
  263625/2000000: episode: 377, duration: 37.261s, episode steps: 906, steps per second:  24, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.010148, mae: 0.600326, mean_q: 0.763350, mean_eps: 0.833324
  264439/2000000: episode: 378, duration: 34.633s, episode steps: 814, steps per second:  24, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.010736, mae: 0.593294, mean_q: 0.752559, mean_eps: 0.832780
  264954/2000000: episode: 379, duration: 21.558s, episode steps: 515, steps per second:  24, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.010600, mae: 0.592893, mean_q: 0.753710, mean_eps: 0.832359
  265660/2000000: episode: 380, duration: 29.617s, episode steps: 706, steps per second:  24, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.010419, mae: 0.600199, mean_q: 0.763405, mean_eps: 0.831973
  266299/2000000: episode: 381, duration: 28.233s, episode steps: 639, steps per second:  23, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.010073, mae: 0.592733, mean_q: 0.751712, mean_eps: 0.831547
  266959/2000000: episode: 382, duration: 29.159s, episode steps: 660, steps per second:  23, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.011431, mae: 0.603193, mean_q: 0.765800, mean_eps: 0.831136
  267477/2000000: episode: 383, duration: 23.007s, episode steps: 518, steps per second:  23, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.011395, mae: 0.590361, mean_q: 0.748820, mean_eps: 0.830762
  267831/2000000: episode: 384, duration: 17.510s, episode steps: 354, steps per second:  20, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.010037, mae: 0.589196, mean_q: 0.747823, mean_eps: 0.830486
  268610/2000000: episode: 385, duration: 32.664s, episode steps: 779, steps per second:  24, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.010733, mae: 0.591976, mean_q: 0.750568, mean_eps: 0.830127
  269000/2000000: episode: 386, duration: 16.246s, episode steps: 390, steps per second:  24, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.010576, mae: 0.593732, mean_q: 0.753557, mean_eps: 0.829757
  270090/2000000: episode: 387, duration: 46.157s, episode steps: 1090, steps per second:  24, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.013046, mae: 0.605305, mean_q: 0.766564, mean_eps: 0.829289
  271246/2000000: episode: 388, duration: 47.707s, episode steps: 1156, steps per second:  24, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.011283, mae: 0.642808, mean_q: 0.814678, mean_eps: 0.828577
  271774/2000000: episode: 389, duration: 22.689s, episode steps: 528, steps per second:  23, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.010398, mae: 0.643314, mean_q: 0.815249, mean_eps: 0.828044
  272417/2000000: episode: 390, duration: 26.914s, episode steps: 643, steps per second:  24, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.010989, mae: 0.644728, mean_q: 0.818042, mean_eps: 0.827673
  272806/2000000: episode: 391, duration: 16.206s, episode steps: 389, steps per second:  24, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.648 [0.000, 5.000],  loss: 0.010579, mae: 0.648202, mean_q: 0.821441, mean_eps: 0.827346
  273341/2000000: episode: 392, duration: 22.192s, episode steps: 535, steps per second:  24, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.010987, mae: 0.644722, mean_q: 0.816750, mean_eps: 0.827053
  274391/2000000: episode: 393, duration: 43.453s, episode steps: 1050, steps per second:  24, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.010963, mae: 0.641404, mean_q: 0.813737, mean_eps: 0.826552
  275276/2000000: episode: 394, duration: 36.725s, episode steps: 885, steps per second:  24, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.011208, mae: 0.647844, mean_q: 0.820764, mean_eps: 0.825940
  275627/2000000: episode: 395, duration: 14.744s, episode steps: 351, steps per second:  24, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.010678, mae: 0.647935, mean_q: 0.822240, mean_eps: 0.825548
  276036/2000000: episode: 396, duration: 17.186s, episode steps: 409, steps per second:  24, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.010419, mae: 0.638648, mean_q: 0.810579, mean_eps: 0.825308
  276533/2000000: episode: 397, duration: 20.708s, episode steps: 497, steps per second:  24, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.010331, mae: 0.640114, mean_q: 0.811767, mean_eps: 0.825020
  277135/2000000: episode: 398, duration: 24.899s, episode steps: 602, steps per second:  24, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.010394, mae: 0.634573, mean_q: 0.804621, mean_eps: 0.824672
  277656/2000000: episode: 399, duration: 21.726s, episode steps: 521, steps per second:  24, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.010417, mae: 0.633796, mean_q: 0.803680, mean_eps: 0.824317
  278621/2000000: episode: 400, duration: 40.211s, episode steps: 965, steps per second:  24, episode reward: 12.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.010387, mae: 0.639151, mean_q: 0.808556, mean_eps: 0.823846
  279228/2000000: episode: 401, duration: 25.225s, episode steps: 607, steps per second:  24, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.641 [0.000, 5.000],  loss: 0.010607, mae: 0.636684, mean_q: 0.805957, mean_eps: 0.823348
  279742/2000000: episode: 402, duration: 21.464s, episode steps: 514, steps per second:  24, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.636 [0.000, 5.000],  loss: 0.010320, mae: 0.642270, mean_q: 0.812920, mean_eps: 0.822993
  280185/2000000: episode: 403, duration: 18.574s, episode steps: 443, steps per second:  24, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.011702, mae: 0.658490, mean_q: 0.832858, mean_eps: 0.822689
  280868/2000000: episode: 404, duration: 28.532s, episode steps: 683, steps per second:  24, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.010941, mae: 0.678765, mean_q: 0.860837, mean_eps: 0.822334
  281548/2000000: episode: 405, duration: 28.641s, episode steps: 680, steps per second:  24, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.012308, mae: 0.682389, mean_q: 0.865703, mean_eps: 0.821903
  282951/2000000: episode: 406, duration: 58.661s, episode steps: 1403, steps per second:  24, episode reward: 12.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.010985, mae: 0.666111, mean_q: 0.843638, mean_eps: 0.821243
  283732/2000000: episode: 407, duration: 32.562s, episode steps: 781, steps per second:  24, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.011581, mae: 0.677713, mean_q: 0.859128, mean_eps: 0.820551
  284121/2000000: episode: 408, duration: 16.346s, episode steps: 389, steps per second:  24, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.010526, mae: 0.671105, mean_q: 0.849833, mean_eps: 0.820180
  284743/2000000: episode: 409, duration: 25.876s, episode steps: 622, steps per second:  24, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.011346, mae: 0.665778, mean_q: 0.842870, mean_eps: 0.819860
  285624/2000000: episode: 410, duration: 36.869s, episode steps: 881, steps per second:  24, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.011471, mae: 0.671994, mean_q: 0.849891, mean_eps: 0.819385
  286527/2000000: episode: 411, duration: 37.691s, episode steps: 903, steps per second:  24, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.011420, mae: 0.669315, mean_q: 0.848695, mean_eps: 0.818820
  287158/2000000: episode: 412, duration: 26.493s, episode steps: 631, steps per second:  24, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.011213, mae: 0.672068, mean_q: 0.850800, mean_eps: 0.818333
  288398/2000000: episode: 413, duration: 51.642s, episode steps: 1240, steps per second:  24, episode reward: 17.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.011424, mae: 0.677427, mean_q: 0.857815, mean_eps: 0.817741
  289073/2000000: episode: 414, duration: 28.163s, episode steps: 675, steps per second:  24, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.011170, mae: 0.671823, mean_q: 0.851049, mean_eps: 0.817134
  289510/2000000: episode: 415, duration: 18.271s, episode steps: 437, steps per second:  24, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.010665, mae: 0.669009, mean_q: 0.846408, mean_eps: 0.816782
  290148/2000000: episode: 416, duration: 26.790s, episode steps: 638, steps per second:  24, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.010240, mae: 0.682012, mean_q: 0.862749, mean_eps: 0.816442
  290686/2000000: episode: 417, duration: 22.537s, episode steps: 538, steps per second:  24, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.010953, mae: 0.719803, mean_q: 0.909443, mean_eps: 0.816070
  291276/2000000: episode: 418, duration: 24.833s, episode steps: 590, steps per second:  24, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.636 [0.000, 5.000],  loss: 0.011547, mae: 0.710275, mean_q: 0.898053, mean_eps: 0.815713
  292338/2000000: episode: 419, duration: 44.604s, episode steps: 1062, steps per second:  24, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.011324, mae: 0.720189, mean_q: 0.911647, mean_eps: 0.815190
  292996/2000000: episode: 420, duration: 27.792s, episode steps: 658, steps per second:  24, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.011637, mae: 0.706853, mean_q: 0.891571, mean_eps: 0.814645
  293611/2000000: episode: 421, duration: 25.816s, episode steps: 615, steps per second:  24, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.012117, mae: 0.714936, mean_q: 0.902985, mean_eps: 0.814242
  294106/2000000: episode: 422, duration: 20.812s, episode steps: 495, steps per second:  24, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.717 [0.000, 5.000],  loss: 0.011711, mae: 0.715760, mean_q: 0.905339, mean_eps: 0.813890
  294813/2000000: episode: 423, duration: 29.533s, episode steps: 707, steps per second:  24, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.011813, mae: 0.714666, mean_q: 0.904206, mean_eps: 0.813509
  295327/2000000: episode: 424, duration: 21.482s, episode steps: 514, steps per second:  24, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.011086, mae: 0.710317, mean_q: 0.895876, mean_eps: 0.813122
  296070/2000000: episode: 425, duration: 31.300s, episode steps: 743, steps per second:  24, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.011465, mae: 0.713098, mean_q: 0.898666, mean_eps: 0.812725
  297015/2000000: episode: 426, duration: 39.591s, episode steps: 945, steps per second:  24, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.011237, mae: 0.715782, mean_q: 0.902853, mean_eps: 0.812190
  297700/2000000: episode: 427, duration: 28.955s, episode steps: 685, steps per second:  24, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.011278, mae: 0.713588, mean_q: 0.898848, mean_eps: 0.811675
  298291/2000000: episode: 428, duration: 24.815s, episode steps: 591, steps per second:  24, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.011340, mae: 0.712159, mean_q: 0.898396, mean_eps: 0.811270
  299253/2000000: episode: 429, duration: 40.327s, episode steps: 962, steps per second:  24, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.011618, mae: 0.710291, mean_q: 0.896999, mean_eps: 0.810778
  299703/2000000: episode: 430, duration: 18.770s, episode steps: 450, steps per second:  24, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.011580, mae: 0.708919, mean_q: 0.894912, mean_eps: 0.810331
  300768/2000000: episode: 431, duration: 45.846s, episode steps: 1065, steps per second:  23, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.010810, mae: 0.738563, mean_q: 0.932279, mean_eps: 0.809852
  301670/2000000: episode: 432, duration: 38.075s, episode steps: 902, steps per second:  24, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.011316, mae: 0.744530, mean_q: 0.939312, mean_eps: 0.809229
  302494/2000000: episode: 433, duration: 34.843s, episode steps: 824, steps per second:  24, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.012301, mae: 0.739091, mean_q: 0.931777, mean_eps: 0.808681
  303302/2000000: episode: 434, duration: 34.007s, episode steps: 808, steps per second:  24, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.011780, mae: 0.749910, mean_q: 0.945305, mean_eps: 0.808165
  303955/2000000: episode: 435, duration: 27.444s, episode steps: 653, steps per second:  24, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.714 [0.000, 5.000],  loss: 0.012182, mae: 0.744551, mean_q: 0.937537, mean_eps: 0.807702
  304511/2000000: episode: 436, duration: 23.427s, episode steps: 556, steps per second:  24, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.011519, mae: 0.739689, mean_q: 0.932679, mean_eps: 0.807320
  305161/2000000: episode: 437, duration: 27.625s, episode steps: 650, steps per second:  24, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.011402, mae: 0.750544, mean_q: 0.946980, mean_eps: 0.806937
  306184/2000000: episode: 438, duration: 43.065s, episode steps: 1023, steps per second:  24, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.011704, mae: 0.742011, mean_q: 0.935583, mean_eps: 0.806408
  306689/2000000: episode: 439, duration: 21.467s, episode steps: 505, steps per second:  24, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.010973, mae: 0.737434, mean_q: 0.930195, mean_eps: 0.805924
  307746/2000000: episode: 440, duration: 44.510s, episode steps: 1057, steps per second:  24, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.011062, mae: 0.733991, mean_q: 0.926565, mean_eps: 0.805429
  308295/2000000: episode: 441, duration: 23.096s, episode steps: 549, steps per second:  24, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.011760, mae: 0.734512, mean_q: 0.927311, mean_eps: 0.804921
  308642/2000000: episode: 442, duration: 14.858s, episode steps: 347, steps per second:  23, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.011850, mae: 0.734575, mean_q: 0.925245, mean_eps: 0.804637
  309310/2000000: episode: 443, duration: 28.393s, episode steps: 668, steps per second:  24, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.011115, mae: 0.740180, mean_q: 0.933282, mean_eps: 0.804315
  310332/2000000: episode: 444, duration: 43.162s, episode steps: 1022, steps per second:  24, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.011332, mae: 0.761128, mean_q: 0.959250, mean_eps: 0.803781
  310999/2000000: episode: 445, duration: 28.279s, episode steps: 667, steps per second:  24, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.011849, mae: 0.794033, mean_q: 1.000687, mean_eps: 0.803246
  311677/2000000: episode: 446, duration: 28.784s, episode steps: 678, steps per second:  24, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.012504, mae: 0.798611, mean_q: 1.004926, mean_eps: 0.802819
  312191/2000000: episode: 447, duration: 21.644s, episode steps: 514, steps per second:  24, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.012469, mae: 0.799549, mean_q: 1.006854, mean_eps: 0.802442
  312907/2000000: episode: 448, duration: 30.221s, episode steps: 716, steps per second:  24, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.012631, mae: 0.804732, mean_q: 1.009544, mean_eps: 0.802053
  313449/2000000: episode: 449, duration: 23.046s, episode steps: 542, steps per second:  24, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.012012, mae: 0.802653, mean_q: 1.009807, mean_eps: 0.801654
  314492/2000000: episode: 450, duration: 44.309s, episode steps: 1043, steps per second:  24, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.012117, mae: 0.798153, mean_q: 1.004748, mean_eps: 0.801152
  314968/2000000: episode: 451, duration: 20.336s, episode steps: 476, steps per second:  23, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.012428, mae: 0.805964, mean_q: 1.013792, mean_eps: 0.800672
  315772/2000000: episode: 452, duration: 34.667s, episode steps: 804, steps per second:  23, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.012137, mae: 0.798472, mean_q: 1.002642, mean_eps: 0.800267
  316267/2000000: episode: 453, duration: 21.676s, episode steps: 495, steps per second:  23, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.012320, mae: 0.805212, mean_q: 1.012888, mean_eps: 0.799855
  316951/2000000: episode: 454, duration: 29.889s, episode steps: 684, steps per second:  23, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.012513, mae: 0.804569, mean_q: 1.011380, mean_eps: 0.799482
  317590/2000000: episode: 455, duration: 28.161s, episode steps: 639, steps per second:  23, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.011346, mae: 0.798885, mean_q: 1.005990, mean_eps: 0.799062
  318266/2000000: episode: 456, duration: 29.787s, episode steps: 676, steps per second:  23, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.012113, mae: 0.801002, mean_q: 1.009084, mean_eps: 0.798646
  319198/2000000: episode: 457, duration: 40.572s, episode steps: 932, steps per second:  23, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.012133, mae: 0.793883, mean_q: 1.000509, mean_eps: 0.798136
  319722/2000000: episode: 458, duration: 22.839s, episode steps: 524, steps per second:  23, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.011528, mae: 0.804530, mean_q: 1.011088, mean_eps: 0.797675
  320471/2000000: episode: 459, duration: 32.503s, episode steps: 749, steps per second:  23, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.628 [0.000, 5.000],  loss: 0.012662, mae: 0.831309, mean_q: 1.044996, mean_eps: 0.797273
  321203/2000000: episode: 460, duration: 31.864s, episode steps: 732, steps per second:  23, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.011844, mae: 0.853150, mean_q: 1.073303, mean_eps: 0.796804
  321815/2000000: episode: 461, duration: 26.764s, episode steps: 612, steps per second:  23, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.013051, mae: 0.846272, mean_q: 1.062616, mean_eps: 0.796378
  322608/2000000: episode: 462, duration: 34.554s, episode steps: 793, steps per second:  23, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.011608, mae: 0.834773, mean_q: 1.048656, mean_eps: 0.795934
  323425/2000000: episode: 463, duration: 35.734s, episode steps: 817, steps per second:  23, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.012478, mae: 0.837812, mean_q: 1.054382, mean_eps: 0.795423
  323948/2000000: episode: 464, duration: 22.851s, episode steps: 523, steps per second:  23, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.012059, mae: 0.835927, mean_q: 1.049320, mean_eps: 0.794999
  324650/2000000: episode: 465, duration: 30.567s, episode steps: 702, steps per second:  23, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.012079, mae: 0.840950, mean_q: 1.056980, mean_eps: 0.794611
  325543/2000000: episode: 466, duration: 38.876s, episode steps: 893, steps per second:  23, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.012864, mae: 0.847516, mean_q: 1.062993, mean_eps: 0.794106
  326484/2000000: episode: 467, duration: 40.819s, episode steps: 941, steps per second:  23, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.012145, mae: 0.842142, mean_q: 1.056634, mean_eps: 0.793526
  327460/2000000: episode: 468, duration: 42.214s, episode steps: 976, steps per second:  23, episode reward: 11.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.011677, mae: 0.847236, mean_q: 1.063727, mean_eps: 0.792919
  328046/2000000: episode: 469, duration: 25.379s, episode steps: 586, steps per second:  23, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.012317, mae: 0.844240, mean_q: 1.060155, mean_eps: 0.792424
  328556/2000000: episode: 470, duration: 22.166s, episode steps: 510, steps per second:  23, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.010966, mae: 0.830836, mean_q: 1.041709, mean_eps: 0.792077
  328987/2000000: episode: 471, duration: 18.660s, episode steps: 431, steps per second:  23, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.012897, mae: 0.836837, mean_q: 1.049685, mean_eps: 0.791779
  329602/2000000: episode: 472, duration: 26.784s, episode steps: 615, steps per second:  23, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.011653, mae: 0.851899, mean_q: 1.068419, mean_eps: 0.791447
  330311/2000000: episode: 473, duration: 30.643s, episode steps: 709, steps per second:  23, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.012600, mae: 0.865689, mean_q: 1.084512, mean_eps: 0.791028
  330812/2000000: episode: 474, duration: 22.093s, episode steps: 501, steps per second:  23, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.012676, mae: 0.896005, mean_q: 1.123154, mean_eps: 0.790645
  331350/2000000: episode: 475, duration: 23.776s, episode steps: 538, steps per second:  23, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.012002, mae: 0.900806, mean_q: 1.132334, mean_eps: 0.790316
  331912/2000000: episode: 476, duration: 24.693s, episode steps: 562, steps per second:  23, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.013360, mae: 0.897665, mean_q: 1.125984, mean_eps: 0.789968
  332427/2000000: episode: 477, duration: 22.975s, episode steps: 515, steps per second:  22, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.011556, mae: 0.895921, mean_q: 1.123288, mean_eps: 0.789627
  333086/2000000: episode: 478, duration: 28.831s, episode steps: 659, steps per second:  23, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.013156, mae: 0.910468, mean_q: 1.143070, mean_eps: 0.789255
  334156/2000000: episode: 479, duration: 47.104s, episode steps: 1070, steps per second:  23, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.012163, mae: 0.900100, mean_q: 1.128374, mean_eps: 0.788707
  334750/2000000: episode: 480, duration: 26.198s, episode steps: 594, steps per second:  23, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.011937, mae: 0.905895, mean_q: 1.135779, mean_eps: 0.788180
  335154/2000000: episode: 481, duration: 17.702s, episode steps: 404, steps per second:  23, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.011906, mae: 0.880020, mean_q: 1.102609, mean_eps: 0.787864
  335580/2000000: episode: 482, duration: 18.697s, episode steps: 426, steps per second:  23, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.013003, mae: 0.903482, mean_q: 1.131390, mean_eps: 0.787602
  336225/2000000: episode: 483, duration: 28.327s, episode steps: 645, steps per second:  23, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.012491, mae: 0.895806, mean_q: 1.122505, mean_eps: 0.787262
  336682/2000000: episode: 484, duration: 20.234s, episode steps: 457, steps per second:  23, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.011898, mae: 0.895690, mean_q: 1.121760, mean_eps: 0.786912
  337216/2000000: episode: 485, duration: 23.719s, episode steps: 534, steps per second:  23, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.011942, mae: 0.905477, mean_q: 1.133543, mean_eps: 0.786600
  337891/2000000: episode: 486, duration: 29.740s, episode steps: 675, steps per second:  23, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.012797, mae: 0.893607, mean_q: 1.117893, mean_eps: 0.786217
  338758/2000000: episode: 487, duration: 37.700s, episode steps: 867, steps per second:  23, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.012652, mae: 0.892968, mean_q: 1.118794, mean_eps: 0.785728
  339304/2000000: episode: 488, duration: 23.768s, episode steps: 546, steps per second:  23, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.013077, mae: 0.907660, mean_q: 1.135883, mean_eps: 0.785281
  339897/2000000: episode: 489, duration: 25.800s, episode steps: 593, steps per second:  23, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.707 [0.000, 5.000],  loss: 0.011600, mae: 0.896122, mean_q: 1.121726, mean_eps: 0.784920
  340929/2000000: episode: 490, duration: 44.704s, episode steps: 1032, steps per second:  23, episode reward: 11.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.012670, mae: 0.967239, mean_q: 1.209129, mean_eps: 0.784404
  341469/2000000: episode: 491, duration: 23.322s, episode steps: 540, steps per second:  23, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: 0.012956, mae: 0.971394, mean_q: 1.214230, mean_eps: 0.783907
  342277/2000000: episode: 492, duration: 34.949s, episode steps: 808, steps per second:  23, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.013026, mae: 0.975286, mean_q: 1.219866, mean_eps: 0.783480
  343129/2000000: episode: 493, duration: 36.964s, episode steps: 852, steps per second:  23, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.636 [0.000, 5.000],  loss: 0.012592, mae: 0.973738, mean_q: 1.217236, mean_eps: 0.782954
  343705/2000000: episode: 494, duration: 25.005s, episode steps: 576, steps per second:  23, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.013185, mae: 0.974740, mean_q: 1.218806, mean_eps: 0.782502
  344504/2000000: episode: 495, duration: 34.809s, episode steps: 799, steps per second:  23, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.012606, mae: 0.973049, mean_q: 1.216449, mean_eps: 0.782067
  345303/2000000: episode: 496, duration: 34.787s, episode steps: 799, steps per second:  23, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.013123, mae: 0.975847, mean_q: 1.217723, mean_eps: 0.781562
  345933/2000000: episode: 497, duration: 27.726s, episode steps: 630, steps per second:  23, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.012646, mae: 0.973404, mean_q: 1.217900, mean_eps: 0.781109
  346459/2000000: episode: 498, duration: 22.764s, episode steps: 526, steps per second:  23, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.012076, mae: 0.962149, mean_q: 1.202021, mean_eps: 0.780743
  347072/2000000: episode: 499, duration: 26.763s, episode steps: 613, steps per second:  23, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.012544, mae: 0.969488, mean_q: 1.208662, mean_eps: 0.780383
  348158/2000000: episode: 500, duration: 47.365s, episode steps: 1086, steps per second:  23, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.651 [0.000, 5.000],  loss: 0.012580, mae: 0.973069, mean_q: 1.215073, mean_eps: 0.779844
  348922/2000000: episode: 501, duration: 34.921s, episode steps: 764, steps per second:  22, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.012584, mae: 0.968086, mean_q: 1.209813, mean_eps: 0.779258
  349574/2000000: episode: 502, duration: 28.375s, episode steps: 652, steps per second:  23, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.011926, mae: 0.962532, mean_q: 1.200146, mean_eps: 0.778810
  350223/2000000: episode: 503, duration: 29.289s, episode steps: 649, steps per second:  22, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.011399, mae: 0.973256, mean_q: 1.214953, mean_eps: 0.778398
  350913/2000000: episode: 504, duration: 30.346s, episode steps: 690, steps per second:  23, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.011960, mae: 0.995109, mean_q: 1.243478, mean_eps: 0.777974
  351546/2000000: episode: 505, duration: 27.698s, episode steps: 633, steps per second:  23, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.012380, mae: 0.997272, mean_q: 1.245266, mean_eps: 0.777554
  352202/2000000: episode: 506, duration: 28.643s, episode steps: 656, steps per second:  23, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.013064, mae: 1.003164, mean_q: 1.251850, mean_eps: 0.777146
  352697/2000000: episode: 507, duration: 21.806s, episode steps: 495, steps per second:  23, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.013299, mae: 1.000935, mean_q: 1.247844, mean_eps: 0.776782
  353333/2000000: episode: 508, duration: 27.930s, episode steps: 636, steps per second:  23, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.012867, mae: 0.992611, mean_q: 1.236538, mean_eps: 0.776423
  354607/2000000: episode: 509, duration: 55.639s, episode steps: 1274, steps per second:  23, episode reward: 15.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.013461, mae: 1.001358, mean_q: 1.248993, mean_eps: 0.775819
  355565/2000000: episode: 510, duration: 41.984s, episode steps: 958, steps per second:  23, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.012385, mae: 0.988496, mean_q: 1.232802, mean_eps: 0.775112
  355918/2000000: episode: 511, duration: 15.405s, episode steps: 353, steps per second:  23, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.012291, mae: 1.001353, mean_q: 1.248501, mean_eps: 0.774697
  356764/2000000: episode: 512, duration: 37.004s, episode steps: 846, steps per second:  23, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.012757, mae: 0.990043, mean_q: 1.234147, mean_eps: 0.774318
  357449/2000000: episode: 513, duration: 30.168s, episode steps: 685, steps per second:  23, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.012420, mae: 0.985881, mean_q: 1.228037, mean_eps: 0.773833
  358114/2000000: episode: 514, duration: 29.052s, episode steps: 665, steps per second:  23, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.326 [0.000, 5.000],  loss: 0.013078, mae: 1.001203, mean_q: 1.248454, mean_eps: 0.773405
  358670/2000000: episode: 515, duration: 24.368s, episode steps: 556, steps per second:  23, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.013130, mae: 0.993885, mean_q: 1.238126, mean_eps: 0.773018
  359287/2000000: episode: 516, duration: 26.946s, episode steps: 617, steps per second:  23, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.012728, mae: 0.994968, mean_q: 1.239712, mean_eps: 0.772647
  360087/2000000: episode: 517, duration: 35.205s, episode steps: 800, steps per second:  23, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.012984, mae: 1.003974, mean_q: 1.250580, mean_eps: 0.772199
  360526/2000000: episode: 518, duration: 19.215s, episode steps: 439, steps per second:  23, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.809 [0.000, 5.000],  loss: 0.013317, mae: 1.056968, mean_q: 1.315386, mean_eps: 0.771806
  361511/2000000: episode: 519, duration: 43.084s, episode steps: 985, steps per second:  23, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.013780, mae: 1.059866, mean_q: 1.318867, mean_eps: 0.771355
  362128/2000000: episode: 520, duration: 27.165s, episode steps: 617, steps per second:  23, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.013342, mae: 1.052944, mean_q: 1.311140, mean_eps: 0.770849
  363322/2000000: episode: 521, duration: 52.646s, episode steps: 1194, steps per second:  23, episode reward: 17.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.013452, mae: 1.048701, mean_q: 1.304435, mean_eps: 0.770275
  363803/2000000: episode: 522, duration: 21.064s, episode steps: 481, steps per second:  23, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.012333, mae: 1.040158, mean_q: 1.292704, mean_eps: 0.769744
  364389/2000000: episode: 523, duration: 25.806s, episode steps: 586, steps per second:  23, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.012774, mae: 1.037171, mean_q: 1.290419, mean_eps: 0.769406
  364753/2000000: episode: 524, duration: 16.008s, episode steps: 364, steps per second:  23, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.014086, mae: 1.043108, mean_q: 1.295875, mean_eps: 0.769104
  365376/2000000: episode: 525, duration: 27.636s, episode steps: 623, steps per second:  23, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.013201, mae: 1.046997, mean_q: 1.302378, mean_eps: 0.768793
  366480/2000000: episode: 526, duration: 48.691s, episode steps: 1104, steps per second:  23, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.012803, mae: 1.051396, mean_q: 1.307429, mean_eps: 0.768247
  367117/2000000: episode: 527, duration: 28.023s, episode steps: 637, steps per second:  23, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.013869, mae: 1.060455, mean_q: 1.317919, mean_eps: 0.767695
  367667/2000000: episode: 528, duration: 23.973s, episode steps: 550, steps per second:  23, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.012915, mae: 1.040091, mean_q: 1.292701, mean_eps: 0.767318
  368294/2000000: episode: 529, duration: 27.584s, episode steps: 627, steps per second:  23, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.837 [0.000, 5.000],  loss: 0.013402, mae: 1.047875, mean_q: 1.301735, mean_eps: 0.766946
  369119/2000000: episode: 530, duration: 36.259s, episode steps: 825, steps per second:  23, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.013077, mae: 1.049763, mean_q: 1.305363, mean_eps: 0.766486
  369838/2000000: episode: 531, duration: 31.659s, episode steps: 719, steps per second:  23, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.012926, mae: 1.047586, mean_q: 1.303599, mean_eps: 0.765997
  370335/2000000: episode: 532, duration: 21.807s, episode steps: 497, steps per second:  23, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.013527, mae: 1.100449, mean_q: 1.367652, mean_eps: 0.765612
  370830/2000000: episode: 533, duration: 22.056s, episode steps: 495, steps per second:  22, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.014490, mae: 1.138646, mean_q: 1.413971, mean_eps: 0.765298
  371643/2000000: episode: 534, duration: 35.840s, episode steps: 813, steps per second:  23, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.014740, mae: 1.127219, mean_q: 1.399969, mean_eps: 0.764884
  372053/2000000: episode: 535, duration: 18.302s, episode steps: 410, steps per second:  22, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.015166, mae: 1.130084, mean_q: 1.401336, mean_eps: 0.764496
  372748/2000000: episode: 536, duration: 30.811s, episode steps: 695, steps per second:  23, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.013816, mae: 1.124687, mean_q: 1.397511, mean_eps: 0.764147
  373268/2000000: episode: 537, duration: 23.175s, episode steps: 520, steps per second:  22, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.014761, mae: 1.128534, mean_q: 1.399784, mean_eps: 0.763763
  373942/2000000: episode: 538, duration: 29.876s, episode steps: 674, steps per second:  23, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.014666, mae: 1.119718, mean_q: 1.389921, mean_eps: 0.763384
  375041/2000000: episode: 539, duration: 48.683s, episode steps: 1099, steps per second:  23, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.014789, mae: 1.135287, mean_q: 1.407284, mean_eps: 0.762822
  375398/2000000: episode: 540, duration: 15.855s, episode steps: 357, steps per second:  23, episode reward:  7.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.014246, mae: 1.128025, mean_q: 1.399387, mean_eps: 0.762361
  375942/2000000: episode: 541, duration: 24.194s, episode steps: 544, steps per second:  22, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.824 [0.000, 5.000],  loss: 0.015345, mae: 1.122341, mean_q: 1.391452, mean_eps: 0.762076
  376446/2000000: episode: 542, duration: 22.270s, episode steps: 504, steps per second:  23, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.014037, mae: 1.116998, mean_q: 1.382915, mean_eps: 0.761744
  377061/2000000: episode: 543, duration: 27.344s, episode steps: 615, steps per second:  22, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.014034, mae: 1.126306, mean_q: 1.396726, mean_eps: 0.761389
  377914/2000000: episode: 544, duration: 37.727s, episode steps: 853, steps per second:  23, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.014332, mae: 1.127038, mean_q: 1.396715, mean_eps: 0.760924
  378460/2000000: episode: 545, duration: 24.201s, episode steps: 546, steps per second:  23, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.013771, mae: 1.130450, mean_q: 1.401749, mean_eps: 0.760482
  379466/2000000: episode: 546, duration: 44.513s, episode steps: 1006, steps per second:  23, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.696 [0.000, 5.000],  loss: 0.014255, mae: 1.122662, mean_q: 1.393456, mean_eps: 0.759991
  379812/2000000: episode: 547, duration: 15.461s, episode steps: 346, steps per second:  22, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.013060, mae: 1.130832, mean_q: 1.401006, mean_eps: 0.759563
  380854/2000000: episode: 548, duration: 46.160s, episode steps: 1042, steps per second:  23, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.014924, mae: 1.174849, mean_q: 1.456695, mean_eps: 0.759123
  381928/2000000: episode: 549, duration: 47.623s, episode steps: 1074, steps per second:  23, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.015138, mae: 1.163676, mean_q: 1.442250, mean_eps: 0.758453
  382515/2000000: episode: 550, duration: 26.071s, episode steps: 587, steps per second:  23, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.015197, mae: 1.156578, mean_q: 1.435203, mean_eps: 0.757927
  382910/2000000: episode: 551, duration: 17.681s, episode steps: 395, steps per second:  22, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.014924, mae: 1.167380, mean_q: 1.445750, mean_eps: 0.757616
  383491/2000000: episode: 552, duration: 25.674s, episode steps: 581, steps per second:  23, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.014104, mae: 1.169645, mean_q: 1.449960, mean_eps: 0.757307
  384330/2000000: episode: 553, duration: 37.348s, episode steps: 839, steps per second:  22, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.015642, mae: 1.164255, mean_q: 1.442040, mean_eps: 0.756857
  384984/2000000: episode: 554, duration: 29.336s, episode steps: 654, steps per second:  22, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.015626, mae: 1.175169, mean_q: 1.457630, mean_eps: 0.756385
  385721/2000000: episode: 555, duration: 32.683s, episode steps: 737, steps per second:  23, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.014739, mae: 1.165969, mean_q: 1.444530, mean_eps: 0.755944
  386190/2000000: episode: 556, duration: 20.780s, episode steps: 469, steps per second:  23, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.015440, mae: 1.170751, mean_q: 1.449366, mean_eps: 0.755561
  386692/2000000: episode: 557, duration: 22.644s, episode steps: 502, steps per second:  22, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.014559, mae: 1.170270, mean_q: 1.450922, mean_eps: 0.755255
  387171/2000000: episode: 558, duration: 21.277s, episode steps: 479, steps per second:  23, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.014146, mae: 1.152890, mean_q: 1.427507, mean_eps: 0.754944
  388038/2000000: episode: 559, duration: 38.414s, episode steps: 867, steps per second:  23, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.014721, mae: 1.166202, mean_q: 1.442055, mean_eps: 0.754517
  388671/2000000: episode: 560, duration: 28.292s, episode steps: 633, steps per second:  22, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.014511, mae: 1.165813, mean_q: 1.443808, mean_eps: 0.754042
  389145/2000000: episode: 561, duration: 21.302s, episode steps: 474, steps per second:  22, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.013825, mae: 1.162787, mean_q: 1.437149, mean_eps: 0.753692
  389518/2000000: episode: 562, duration: 16.745s, episode steps: 373, steps per second:  22, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.013540, mae: 1.160974, mean_q: 1.437353, mean_eps: 0.753423
  390333/2000000: episode: 563, duration: 36.292s, episode steps: 815, steps per second:  22, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.015521, mae: 1.190922, mean_q: 1.474637, mean_eps: 0.753047
  390935/2000000: episode: 564, duration: 26.838s, episode steps: 602, steps per second:  22, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.014772, mae: 1.213952, mean_q: 1.502559, mean_eps: 0.752598
  391507/2000000: episode: 565, duration: 25.742s, episode steps: 572, steps per second:  22, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.014829, mae: 1.197807, mean_q: 1.482730, mean_eps: 0.752227
  392172/2000000: episode: 566, duration: 29.803s, episode steps: 665, steps per second:  22, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.015440, mae: 1.211494, mean_q: 1.499486, mean_eps: 0.751836
  392645/2000000: episode: 567, duration: 21.284s, episode steps: 473, steps per second:  22, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.015746, mae: 1.210584, mean_q: 1.499029, mean_eps: 0.751475
  393774/2000000: episode: 568, duration: 50.550s, episode steps: 1129, steps per second:  22, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.014935, mae: 1.197393, mean_q: 1.480982, mean_eps: 0.750967
  394450/2000000: episode: 569, duration: 30.236s, episode steps: 676, steps per second:  22, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.654 [0.000, 5.000],  loss: 0.015329, mae: 1.206268, mean_q: 1.492024, mean_eps: 0.750396
  395199/2000000: episode: 570, duration: 33.407s, episode steps: 749, steps per second:  22, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.014755, mae: 1.204116, mean_q: 1.488069, mean_eps: 0.749945
  395989/2000000: episode: 571, duration: 35.232s, episode steps: 790, steps per second:  22, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.016003, mae: 1.210173, mean_q: 1.496754, mean_eps: 0.749457
  396905/2000000: episode: 572, duration: 40.839s, episode steps: 916, steps per second:  22, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.016220, mae: 1.204280, mean_q: 1.488379, mean_eps: 0.748916
  397643/2000000: episode: 573, duration: 33.086s, episode steps: 738, steps per second:  22, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.015169, mae: 1.200263, mean_q: 1.484369, mean_eps: 0.748393
  398198/2000000: episode: 574, duration: 24.918s, episode steps: 555, steps per second:  22, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.254 [0.000, 5.000],  loss: 0.015727, mae: 1.204696, mean_q: 1.487810, mean_eps: 0.747984
  398921/2000000: episode: 575, duration: 32.559s, episode steps: 723, steps per second:  22, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.014677, mae: 1.206920, mean_q: 1.491119, mean_eps: 0.747579
  400096/2000000: episode: 576, duration: 53.445s, episode steps: 1175, steps per second:  22, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.014782, mae: 1.217133, mean_q: 1.503968, mean_eps: 0.746978
  400688/2000000: episode: 577, duration: 26.552s, episode steps: 592, steps per second:  22, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.014297, mae: 1.290638, mean_q: 1.595447, mean_eps: 0.746420
  401136/2000000: episode: 578, duration: 20.160s, episode steps: 448, steps per second:  22, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.015201, mae: 1.286516, mean_q: 1.587315, mean_eps: 0.746090
  402206/2000000: episode: 579, duration: 47.539s, episode steps: 1070, steps per second:  23, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.016358, mae: 1.279182, mean_q: 1.579431, mean_eps: 0.745609
  402694/2000000: episode: 580, duration: 21.740s, episode steps: 488, steps per second:  22, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.016337, mae: 1.281765, mean_q: 1.582322, mean_eps: 0.745115
  403146/2000000: episode: 581, duration: 20.168s, episode steps: 452, steps per second:  22, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.016518, mae: 1.292138, mean_q: 1.595645, mean_eps: 0.744817
  403735/2000000: episode: 582, duration: 26.131s, episode steps: 589, steps per second:  23, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.015452, mae: 1.290985, mean_q: 1.593995, mean_eps: 0.744488
  404441/2000000: episode: 583, duration: 31.503s, episode steps: 706, steps per second:  22, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.016121, mae: 1.273620, mean_q: 1.571341, mean_eps: 0.744078
  405536/2000000: episode: 584, duration: 48.834s, episode steps: 1095, steps per second:  22, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.015902, mae: 1.277408, mean_q: 1.576541, mean_eps: 0.743508
  406597/2000000: episode: 585, duration: 47.394s, episode steps: 1061, steps per second:  22, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.016041, mae: 1.281319, mean_q: 1.582086, mean_eps: 0.742825
  407167/2000000: episode: 586, duration: 25.493s, episode steps: 570, steps per second:  22, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.015473, mae: 1.283874, mean_q: 1.583750, mean_eps: 0.742308
  407737/2000000: episode: 587, duration: 25.468s, episode steps: 570, steps per second:  22, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.658 [0.000, 5.000],  loss: 0.015866, mae: 1.270713, mean_q: 1.569154, mean_eps: 0.741947
  408167/2000000: episode: 588, duration: 19.097s, episode steps: 430, steps per second:  23, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.288 [0.000, 5.000],  loss: 0.016708, mae: 1.287793, mean_q: 1.590229, mean_eps: 0.741630
  408919/2000000: episode: 589, duration: 33.609s, episode steps: 752, steps per second:  22, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.015683, mae: 1.275671, mean_q: 1.575175, mean_eps: 0.741257
  409454/2000000: episode: 590, duration: 24.427s, episode steps: 535, steps per second:  22, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.015976, mae: 1.269895, mean_q: 1.567233, mean_eps: 0.740849
  410180/2000000: episode: 591, duration: 32.701s, episode steps: 726, steps per second:  22, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.015063, mae: 1.288643, mean_q: 1.592626, mean_eps: 0.740450
  410554/2000000: episode: 592, duration: 16.999s, episode steps: 374, steps per second:  22, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.015734, mae: 1.325387, mean_q: 1.637743, mean_eps: 0.740102
  411085/2000000: episode: 593, duration: 23.878s, episode steps: 531, steps per second:  22, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.014873, mae: 1.314876, mean_q: 1.625236, mean_eps: 0.739814
  411725/2000000: episode: 594, duration: 28.873s, episode steps: 640, steps per second:  22, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.016660, mae: 1.314329, mean_q: 1.620976, mean_eps: 0.739443
  412214/2000000: episode: 595, duration: 22.080s, episode steps: 489, steps per second:  22, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.017584, mae: 1.318446, mean_q: 1.624361, mean_eps: 0.739086
  412845/2000000: episode: 596, duration: 28.271s, episode steps: 631, steps per second:  22, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.016522, mae: 1.316213, mean_q: 1.622441, mean_eps: 0.738731
  413870/2000000: episode: 597, duration: 45.990s, episode steps: 1025, steps per second:  22, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.016575, mae: 1.318675, mean_q: 1.624551, mean_eps: 0.738207
  414637/2000000: episode: 598, duration: 34.732s, episode steps: 767, steps per second:  22, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.016026, mae: 1.309295, mean_q: 1.613836, mean_eps: 0.737639
  415167/2000000: episode: 599, duration: 23.911s, episode steps: 530, steps per second:  22, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.015447, mae: 1.300079, mean_q: 1.602581, mean_eps: 0.737229
  415971/2000000: episode: 600, duration: 36.278s, episode steps: 804, steps per second:  22, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.015816, mae: 1.324879, mean_q: 1.633059, mean_eps: 0.736807
  416672/2000000: episode: 601, duration: 31.625s, episode steps: 701, steps per second:  22, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.017098, mae: 1.316429, mean_q: 1.621556, mean_eps: 0.736331
  417342/2000000: episode: 602, duration: 30.289s, episode steps: 670, steps per second:  22, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.015540, mae: 1.311381, mean_q: 1.616458, mean_eps: 0.735896
  418034/2000000: episode: 603, duration: 31.222s, episode steps: 692, steps per second:  22, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.015906, mae: 1.311709, mean_q: 1.616699, mean_eps: 0.735464
  418655/2000000: episode: 604, duration: 27.952s, episode steps: 621, steps per second:  22, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.015895, mae: 1.307837, mean_q: 1.612991, mean_eps: 0.735049
  419203/2000000: episode: 605, duration: 24.340s, episode steps: 548, steps per second:  23, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.015279, mae: 1.317315, mean_q: 1.622362, mean_eps: 0.734679
  419581/2000000: episode: 606, duration: 16.891s, episode steps: 378, steps per second:  22, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.015604, mae: 1.307918, mean_q: 1.611905, mean_eps: 0.734385
  420079/2000000: episode: 607, duration: 22.246s, episode steps: 498, steps per second:  22, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.016405, mae: 1.316985, mean_q: 1.621763, mean_eps: 0.734108
  420960/2000000: episode: 608, duration: 39.174s, episode steps: 881, steps per second:  22, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.016427, mae: 1.353465, mean_q: 1.668671, mean_eps: 0.733672
  421586/2000000: episode: 609, duration: 27.869s, episode steps: 626, steps per second:  22, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.016186, mae: 1.360084, mean_q: 1.674190, mean_eps: 0.733194
  422226/2000000: episode: 610, duration: 28.351s, episode steps: 640, steps per second:  23, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.233 [0.000, 5.000],  loss: 0.016252, mae: 1.342199, mean_q: 1.652982, mean_eps: 0.732793
  422853/2000000: episode: 611, duration: 27.963s, episode steps: 627, steps per second:  22, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.016127, mae: 1.342785, mean_q: 1.654808, mean_eps: 0.732391
  423571/2000000: episode: 612, duration: 32.026s, episode steps: 718, steps per second:  22, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.016222, mae: 1.349096, mean_q: 1.662904, mean_eps: 0.731966
  424165/2000000: episode: 613, duration: 26.520s, episode steps: 594, steps per second:  22, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.699 [0.000, 5.000],  loss: 0.016343, mae: 1.350972, mean_q: 1.662187, mean_eps: 0.731550
  424578/2000000: episode: 614, duration: 18.627s, episode steps: 413, steps per second:  22, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.016950, mae: 1.351288, mean_q: 1.662166, mean_eps: 0.731231
  425236/2000000: episode: 615, duration: 29.647s, episode steps: 658, steps per second:  22, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.017294, mae: 1.347545, mean_q: 1.659898, mean_eps: 0.730893
  425608/2000000: episode: 616, duration: 17.019s, episode steps: 372, steps per second:  22, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.656 [0.000, 5.000],  loss: 0.016466, mae: 1.339206, mean_q: 1.648648, mean_eps: 0.730567
  426304/2000000: episode: 617, duration: 31.696s, episode steps: 696, steps per second:  22, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.016595, mae: 1.333562, mean_q: 1.641037, mean_eps: 0.730229
  426834/2000000: episode: 618, duration: 24.995s, episode steps: 530, steps per second:  21, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.015672, mae: 1.350767, mean_q: 1.663160, mean_eps: 0.729840
  427344/2000000: episode: 619, duration: 23.230s, episode steps: 510, steps per second:  22, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.017225, mae: 1.348856, mean_q: 1.660435, mean_eps: 0.729511
  428053/2000000: episode: 620, duration: 32.081s, episode steps: 709, steps per second:  22, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.015844, mae: 1.328068, mean_q: 1.633918, mean_eps: 0.729125
  428471/2000000: episode: 621, duration: 18.795s, episode steps: 418, steps per second:  22, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.015471, mae: 1.336614, mean_q: 1.645709, mean_eps: 0.728767
  428991/2000000: episode: 622, duration: 23.349s, episode steps: 520, steps per second:  22, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.018338, mae: 1.346657, mean_q: 1.656155, mean_eps: 0.728471
  429618/2000000: episode: 623, duration: 28.628s, episode steps: 627, steps per second:  22, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.673 [0.000, 5.000],  loss: 0.016140, mae: 1.347779, mean_q: 1.658306, mean_eps: 0.728107
  429947/2000000: episode: 624, duration: 14.809s, episode steps: 329, steps per second:  22, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.016708, mae: 1.349856, mean_q: 1.663103, mean_eps: 0.727805
  430634/2000000: episode: 625, duration: 31.350s, episode steps: 687, steps per second:  22, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.014691, mae: 1.381060, mean_q: 1.701043, mean_eps: 0.727483
  431155/2000000: episode: 626, duration: 23.833s, episode steps: 521, steps per second:  22, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.309 [0.000, 5.000],  loss: 0.016326, mae: 1.375612, mean_q: 1.691788, mean_eps: 0.727100
  431908/2000000: episode: 627, duration: 34.283s, episode steps: 753, steps per second:  22, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.018010, mae: 1.375314, mean_q: 1.692443, mean_eps: 0.726698
  432518/2000000: episode: 628, duration: 27.993s, episode steps: 610, steps per second:  22, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.016290, mae: 1.382085, mean_q: 1.701490, mean_eps: 0.726266
  433052/2000000: episode: 629, duration: 24.448s, episode steps: 534, steps per second:  22, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.018018, mae: 1.390411, mean_q: 1.713076, mean_eps: 0.725903
  433967/2000000: episode: 630, duration: 41.456s, episode steps: 915, steps per second:  22, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.017299, mae: 1.373917, mean_q: 1.690895, mean_eps: 0.725445
  434697/2000000: episode: 631, duration: 33.207s, episode steps: 730, steps per second:  22, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.016748, mae: 1.370186, mean_q: 1.686393, mean_eps: 0.724923
  435251/2000000: episode: 632, duration: 25.340s, episode steps: 554, steps per second:  22, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.016624, mae: 1.380157, mean_q: 1.697506, mean_eps: 0.724516
  435892/2000000: episode: 633, duration: 30.326s, episode steps: 641, steps per second:  21, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.016353, mae: 1.363547, mean_q: 1.676419, mean_eps: 0.724139
  436303/2000000: episode: 634, duration: 23.372s, episode steps: 411, steps per second:  18, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.676 [0.000, 5.000],  loss: 0.018243, mae: 1.356538, mean_q: 1.667988, mean_eps: 0.723806
  436995/2000000: episode: 635, duration: 42.899s, episode steps: 692, steps per second:  16, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.018269, mae: 1.387343, mean_q: 1.706619, mean_eps: 0.723456
  437970/2000000: episode: 636, duration: 50.265s, episode steps: 975, steps per second:  19, episode reward: 12.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.017678, mae: 1.378951, mean_q: 1.696287, mean_eps: 0.722928
  438363/2000000: episode: 637, duration: 17.667s, episode steps: 393, steps per second:  22, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.017940, mae: 1.407177, mean_q: 1.732136, mean_eps: 0.722495
  439000/2000000: episode: 638, duration: 28.791s, episode steps: 637, steps per second:  22, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.017098, mae: 1.391851, mean_q: 1.710623, mean_eps: 0.722169
  439556/2000000: episode: 639, duration: 25.211s, episode steps: 556, steps per second:  22, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.016380, mae: 1.373136, mean_q: 1.689862, mean_eps: 0.721792
  440193/2000000: episode: 640, duration: 28.736s, episode steps: 637, steps per second:  22, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.017815, mae: 1.390402, mean_q: 1.708240, mean_eps: 0.721413
  440845/2000000: episode: 641, duration: 29.187s, episode steps: 652, steps per second:  22, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.016181, mae: 1.459744, mean_q: 1.793682, mean_eps: 0.721004
  441581/2000000: episode: 642, duration: 33.166s, episode steps: 736, steps per second:  22, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.015695, mae: 1.440188, mean_q: 1.769576, mean_eps: 0.720564
  442552/2000000: episode: 643, duration: 43.688s, episode steps: 971, steps per second:  22, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.017073, mae: 1.442531, mean_q: 1.772150, mean_eps: 0.720025
  443180/2000000: episode: 644, duration: 28.262s, episode steps: 628, steps per second:  22, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.016486, mae: 1.438613, mean_q: 1.767414, mean_eps: 0.719519
  443591/2000000: episode: 645, duration: 18.615s, episode steps: 411, steps per second:  22, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.255 [0.000, 5.000],  loss: 0.016503, mae: 1.449228, mean_q: 1.779631, mean_eps: 0.719190
  444196/2000000: episode: 646, duration: 27.315s, episode steps: 605, steps per second:  22, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.702 [0.000, 5.000],  loss: 0.015367, mae: 1.449897, mean_q: 1.782190, mean_eps: 0.718868
  445131/2000000: episode: 647, duration: 42.256s, episode steps: 935, steps per second:  22, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.017195, mae: 1.459614, mean_q: 1.794900, mean_eps: 0.718381
  445773/2000000: episode: 648, duration: 28.993s, episode steps: 642, steps per second:  22, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.017511, mae: 1.443359, mean_q: 1.773911, mean_eps: 0.717880
  446586/2000000: episode: 649, duration: 36.637s, episode steps: 813, steps per second:  22, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.017344, mae: 1.442451, mean_q: 1.772713, mean_eps: 0.717419
  447513/2000000: episode: 650, duration: 41.637s, episode steps: 927, steps per second:  22, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.016553, mae: 1.439243, mean_q: 1.769066, mean_eps: 0.716868
  447953/2000000: episode: 651, duration: 20.009s, episode steps: 440, steps per second:  22, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.016842, mae: 1.449868, mean_q: 1.781555, mean_eps: 0.716435
  448816/2000000: episode: 652, duration: 38.822s, episode steps: 863, steps per second:  22, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.017494, mae: 1.434332, mean_q: 1.762540, mean_eps: 0.716023
  449663/2000000: episode: 653, duration: 38.255s, episode steps: 847, steps per second:  22, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.017539, mae: 1.446631, mean_q: 1.776737, mean_eps: 0.715483
  450467/2000000: episode: 654, duration: 37.277s, episode steps: 804, steps per second:  22, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.016504, mae: 1.476625, mean_q: 1.812948, mean_eps: 0.714959
  451145/2000000: episode: 655, duration: 30.670s, episode steps: 678, steps per second:  22, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.016444, mae: 1.489838, mean_q: 1.831121, mean_eps: 0.714490
  451984/2000000: episode: 656, duration: 37.875s, episode steps: 839, steps per second:  22, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.017081, mae: 1.499733, mean_q: 1.842788, mean_eps: 0.714009
  452347/2000000: episode: 657, duration: 16.399s, episode steps: 363, steps per second:  22, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.711 [0.000, 5.000],  loss: 0.017879, mae: 1.531311, mean_q: 1.886579, mean_eps: 0.713629
  453117/2000000: episode: 658, duration: 34.860s, episode steps: 770, steps per second:  22, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.018143, mae: 1.504995, mean_q: 1.849731, mean_eps: 0.713270
  453888/2000000: episode: 659, duration: 34.826s, episode steps: 771, steps per second:  22, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.018644, mae: 1.485394, mean_q: 1.825294, mean_eps: 0.712782
  454420/2000000: episode: 660, duration: 24.326s, episode steps: 532, steps per second:  22, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.018231, mae: 1.496911, mean_q: 1.837193, mean_eps: 0.712370
  455085/2000000: episode: 661, duration: 30.107s, episode steps: 665, steps per second:  22, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.015951, mae: 1.497882, mean_q: 1.842487, mean_eps: 0.711990
  455745/2000000: episode: 662, duration: 30.010s, episode steps: 660, steps per second:  22, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.018497, mae: 1.481040, mean_q: 1.819073, mean_eps: 0.711570
  456141/2000000: episode: 663, duration: 17.991s, episode steps: 396, steps per second:  22, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.018134, mae: 1.501651, mean_q: 1.843345, mean_eps: 0.711235
  456826/2000000: episode: 664, duration: 30.986s, episode steps: 685, steps per second:  22, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.017717, mae: 1.494014, mean_q: 1.835704, mean_eps: 0.710893
  457631/2000000: episode: 665, duration: 36.380s, episode steps: 805, steps per second:  22, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.017537, mae: 1.477934, mean_q: 1.815194, mean_eps: 0.710422
  458413/2000000: episode: 666, duration: 35.264s, episode steps: 782, steps per second:  22, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.017527, mae: 1.486536, mean_q: 1.827531, mean_eps: 0.709919
  459237/2000000: episode: 667, duration: 37.406s, episode steps: 824, steps per second:  22, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.016326, mae: 1.484329, mean_q: 1.823967, mean_eps: 0.709410
  459876/2000000: episode: 668, duration: 29.098s, episode steps: 639, steps per second:  22, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.017796, mae: 1.485536, mean_q: 1.824325, mean_eps: 0.708948
  460547/2000000: episode: 669, duration: 30.465s, episode steps: 671, steps per second:  22, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.016158, mae: 1.507680, mean_q: 1.855116, mean_eps: 0.708534
  461326/2000000: episode: 670, duration: 35.284s, episode steps: 779, steps per second:  22, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.016796, mae: 1.489900, mean_q: 1.833038, mean_eps: 0.708074
  461998/2000000: episode: 671, duration: 30.560s, episode steps: 672, steps per second:  22, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.017618, mae: 1.513689, mean_q: 1.862544, mean_eps: 0.707614
  462589/2000000: episode: 672, duration: 27.080s, episode steps: 591, steps per second:  22, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.017370, mae: 1.503861, mean_q: 1.848493, mean_eps: 0.707214
  463184/2000000: episode: 673, duration: 27.283s, episode steps: 595, steps per second:  22, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.016829, mae: 1.496520, mean_q: 1.836916, mean_eps: 0.706839
  463783/2000000: episode: 674, duration: 27.513s, episode steps: 599, steps per second:  22, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.019630, mae: 1.502877, mean_q: 1.846349, mean_eps: 0.706461
  464493/2000000: episode: 675, duration: 32.484s, episode steps: 710, steps per second:  22, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.642 [0.000, 5.000],  loss: 0.017676, mae: 1.492335, mean_q: 1.833660, mean_eps: 0.706046
  465538/2000000: episode: 676, duration: 47.790s, episode steps: 1045, steps per second:  22, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.016996, mae: 1.507772, mean_q: 1.854258, mean_eps: 0.705490
  466306/2000000: episode: 677, duration: 35.405s, episode steps: 768, steps per second:  22, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.016827, mae: 1.497190, mean_q: 1.840608, mean_eps: 0.704916
  466899/2000000: episode: 678, duration: 27.114s, episode steps: 593, steps per second:  22, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.016274, mae: 1.499803, mean_q: 1.842721, mean_eps: 0.704485
  467453/2000000: episode: 679, duration: 25.399s, episode steps: 554, steps per second:  22, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.018185, mae: 1.500228, mean_q: 1.841319, mean_eps: 0.704122
  467971/2000000: episode: 680, duration: 23.662s, episode steps: 518, steps per second:  22, episode reward: 13.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.017004, mae: 1.501392, mean_q: 1.842617, mean_eps: 0.703782
  468587/2000000: episode: 681, duration: 28.212s, episode steps: 616, steps per second:  22, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.017852, mae: 1.516784, mean_q: 1.863203, mean_eps: 0.703424
  469425/2000000: episode: 682, duration: 38.644s, episode steps: 838, steps per second:  22, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.016802, mae: 1.497910, mean_q: 1.837312, mean_eps: 0.702963
  470126/2000000: episode: 683, duration: 32.443s, episode steps: 701, steps per second:  22, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.017432, mae: 1.517860, mean_q: 1.863704, mean_eps: 0.702475
  470801/2000000: episode: 684, duration: 31.305s, episode steps: 675, steps per second:  22, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.017676, mae: 1.559218, mean_q: 1.914801, mean_eps: 0.702039
  471446/2000000: episode: 685, duration: 30.029s, episode steps: 645, steps per second:  21, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.017869, mae: 1.558972, mean_q: 1.913159, mean_eps: 0.701621
  472094/2000000: episode: 686, duration: 29.971s, episode steps: 648, steps per second:  22, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.016476, mae: 1.537243, mean_q: 1.886280, mean_eps: 0.701212
  472856/2000000: episode: 687, duration: 35.615s, episode steps: 762, steps per second:  21, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.016850, mae: 1.558841, mean_q: 1.913649, mean_eps: 0.700766
  473609/2000000: episode: 688, duration: 35.138s, episode steps: 753, steps per second:  21, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.017298, mae: 1.550126, mean_q: 1.900998, mean_eps: 0.700286
  473972/2000000: episode: 689, duration: 17.035s, episode steps: 363, steps per second:  21, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.016411, mae: 1.565693, mean_q: 1.919766, mean_eps: 0.699933
  474467/2000000: episode: 690, duration: 23.155s, episode steps: 495, steps per second:  21, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.016739, mae: 1.534590, mean_q: 1.882459, mean_eps: 0.699662
  475221/2000000: episode: 691, duration: 35.056s, episode steps: 754, steps per second:  22, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.017521, mae: 1.560962, mean_q: 1.914739, mean_eps: 0.699265
  475868/2000000: episode: 692, duration: 30.337s, episode steps: 647, steps per second:  21, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.017090, mae: 1.556613, mean_q: 1.909758, mean_eps: 0.698822
  476250/2000000: episode: 693, duration: 17.699s, episode steps: 382, steps per second:  22, episode reward:  9.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.017491, mae: 1.563137, mean_q: 1.917206, mean_eps: 0.698497
  476763/2000000: episode: 694, duration: 23.631s, episode steps: 513, steps per second:  22, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.018348, mae: 1.550244, mean_q: 1.901264, mean_eps: 0.698213
  477309/2000000: episode: 695, duration: 25.216s, episode steps: 546, steps per second:  22, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.017573, mae: 1.554233, mean_q: 1.908429, mean_eps: 0.697877
  477933/2000000: episode: 696, duration: 28.878s, episode steps: 624, steps per second:  22, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.018995, mae: 1.548816, mean_q: 1.898581, mean_eps: 0.697506
  478745/2000000: episode: 697, duration: 37.632s, episode steps: 812, steps per second:  22, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.018809, mae: 1.550688, mean_q: 1.903084, mean_eps: 0.697051
  479377/2000000: episode: 698, duration: 29.291s, episode steps: 632, steps per second:  22, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.018236, mae: 1.543660, mean_q: 1.897981, mean_eps: 0.696594
  480452/2000000: episode: 699, duration: 49.906s, episode steps: 1075, steps per second:  22, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.017252, mae: 1.567559, mean_q: 1.925422, mean_eps: 0.696054
  481337/2000000: episode: 700, duration: 41.669s, episode steps: 885, steps per second:  21, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.017725, mae: 1.595074, mean_q: 1.958233, mean_eps: 0.695434
  481709/2000000: episode: 701, duration: 17.799s, episode steps: 372, steps per second:  21, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.194 [0.000, 5.000],  loss: 0.017525, mae: 1.577798, mean_q: 1.937442, mean_eps: 0.695035
  482344/2000000: episode: 702, duration: 30.060s, episode steps: 635, steps per second:  21, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.018170, mae: 1.588298, mean_q: 1.950650, mean_eps: 0.694717
  482862/2000000: episode: 703, duration: 24.469s, episode steps: 518, steps per second:  21, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.018827, mae: 1.615209, mean_q: 1.980633, mean_eps: 0.694352
  483434/2000000: episode: 704, duration: 26.972s, episode steps: 572, steps per second:  21, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.018063, mae: 1.595892, mean_q: 1.956761, mean_eps: 0.694006
  484100/2000000: episode: 705, duration: 31.464s, episode steps: 666, steps per second:  21, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.018057, mae: 1.589800, mean_q: 1.950694, mean_eps: 0.693615
  484716/2000000: episode: 706, duration: 29.165s, episode steps: 616, steps per second:  21, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.695 [0.000, 5.000],  loss: 0.019425, mae: 1.602584, mean_q: 1.965704, mean_eps: 0.693210
  485240/2000000: episode: 707, duration: 24.792s, episode steps: 524, steps per second:  21, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.017262, mae: 1.599040, mean_q: 1.963066, mean_eps: 0.692849
  486116/2000000: episode: 708, duration: 40.726s, episode steps: 876, steps per second:  22, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.017697, mae: 1.585412, mean_q: 1.943411, mean_eps: 0.692405
  487173/2000000: episode: 709, duration: 48.804s, episode steps: 1057, steps per second:  22, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.017888, mae: 1.603110, mean_q: 1.965763, mean_eps: 0.691792
  488221/2000000: episode: 710, duration: 48.468s, episode steps: 1048, steps per second:  22, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.017259, mae: 1.581501, mean_q: 1.941861, mean_eps: 0.691125
  488582/2000000: episode: 711, duration: 16.703s, episode steps: 361, steps per second:  22, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: 0.019249, mae: 1.598339, mean_q: 1.962773, mean_eps: 0.690679
  489585/2000000: episode: 712, duration: 46.838s, episode steps: 1003, steps per second:  21, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.018138, mae: 1.609775, mean_q: 1.974287, mean_eps: 0.690247
  490080/2000000: episode: 713, duration: 23.035s, episode steps: 495, steps per second:  21, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.017563, mae: 1.594391, mean_q: 1.955782, mean_eps: 0.689773
  490599/2000000: episode: 714, duration: 24.106s, episode steps: 519, steps per second:  22, episode reward: 13.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.017809, mae: 1.651712, mean_q: 2.026098, mean_eps: 0.689453
  491176/2000000: episode: 715, duration: 27.043s, episode steps: 577, steps per second:  21, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.018600, mae: 1.639632, mean_q: 2.011776, mean_eps: 0.689106
  492065/2000000: episode: 716, duration: 41.470s, episode steps: 889, steps per second:  21, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.019065, mae: 1.651656, mean_q: 2.025572, mean_eps: 0.688641
  492745/2000000: episode: 717, duration: 31.841s, episode steps: 680, steps per second:  21, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.018195, mae: 1.644185, mean_q: 2.015727, mean_eps: 0.688143
  493406/2000000: episode: 718, duration: 31.046s, episode steps: 661, steps per second:  21, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.019221, mae: 1.641104, mean_q: 2.011339, mean_eps: 0.687719
  493957/2000000: episode: 719, duration: 25.819s, episode steps: 551, steps per second:  21, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.020058, mae: 1.638638, mean_q: 2.007941, mean_eps: 0.687335
  494628/2000000: episode: 720, duration: 31.454s, episode steps: 671, steps per second:  21, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.017117, mae: 1.652710, mean_q: 2.026032, mean_eps: 0.686948
  495122/2000000: episode: 721, duration: 23.337s, episode steps: 494, steps per second:  21, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.265 [0.000, 5.000],  loss: 0.018685, mae: 1.652692, mean_q: 2.026549, mean_eps: 0.686580
  496324/2000000: episode: 722, duration: 56.790s, episode steps: 1202, steps per second:  21, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.662 [0.000, 5.000],  loss: 0.018853, mae: 1.640194, mean_q: 2.011282, mean_eps: 0.686043
  497591/2000000: episode: 723, duration: 59.501s, episode steps: 1267, steps per second:  21, episode reward: 22.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.019044, mae: 1.652802, mean_q: 2.024283, mean_eps: 0.685261
  498108/2000000: episode: 724, duration: 24.190s, episode steps: 517, steps per second:  21, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.019850, mae: 1.628073, mean_q: 1.994274, mean_eps: 0.684696
  498870/2000000: episode: 725, duration: 35.592s, episode steps: 762, steps per second:  21, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.019767, mae: 1.649565, mean_q: 2.021673, mean_eps: 0.684291
  500094/2000000: episode: 726, duration: 57.924s, episode steps: 1224, steps per second:  21, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.018819, mae: 1.642416, mean_q: 2.010801, mean_eps: 0.683661
  501001/2000000: episode: 727, duration: 42.360s, episode steps: 907, steps per second:  21, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.017844, mae: 1.663456, mean_q: 2.037953, mean_eps: 0.682986
  502278/2000000: episode: 728, duration: 59.515s, episode steps: 1277, steps per second:  21, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.312 [0.000, 5.000],  loss: 0.018878, mae: 1.660263, mean_q: 2.033208, mean_eps: 0.682295
  502784/2000000: episode: 729, duration: 23.729s, episode steps: 506, steps per second:  21, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.018937, mae: 1.655471, mean_q: 2.027779, mean_eps: 0.681731
  503699/2000000: episode: 730, duration: 42.930s, episode steps: 915, steps per second:  21, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.019104, mae: 1.663943, mean_q: 2.038307, mean_eps: 0.681281
  504189/2000000: episode: 731, duration: 23.197s, episode steps: 490, steps per second:  21, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.019114, mae: 1.649062, mean_q: 2.017092, mean_eps: 0.680835
  505053/2000000: episode: 732, duration: 40.708s, episode steps: 864, steps per second:  21, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.018407, mae: 1.661560, mean_q: 2.032928, mean_eps: 0.680406
  505563/2000000: episode: 733, duration: 23.723s, episode steps: 510, steps per second:  21, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.733 [0.000, 5.000],  loss: 0.017839, mae: 1.632113, mean_q: 1.996015, mean_eps: 0.679972
  506303/2000000: episode: 734, duration: 34.651s, episode steps: 740, steps per second:  21, episode reward: 21.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.672 [0.000, 5.000],  loss: 0.020450, mae: 1.667293, mean_q: 2.038411, mean_eps: 0.679576
  506999/2000000: episode: 735, duration: 33.740s, episode steps: 696, steps per second:  21, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.018924, mae: 1.646512, mean_q: 2.015997, mean_eps: 0.679122
  507534/2000000: episode: 736, duration: 25.345s, episode steps: 535, steps per second:  21, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.250 [0.000, 5.000],  loss: 0.020018, mae: 1.676276, mean_q: 2.051346, mean_eps: 0.678732
  508187/2000000: episode: 737, duration: 30.992s, episode steps: 653, steps per second:  21, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.019060, mae: 1.661094, mean_q: 2.033064, mean_eps: 0.678355
  508684/2000000: episode: 738, duration: 23.748s, episode steps: 497, steps per second:  21, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.020372, mae: 1.635561, mean_q: 2.000842, mean_eps: 0.677992
  509694/2000000: episode: 739, duration: 47.803s, episode steps: 1010, steps per second:  21, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.019656, mae: 1.658184, mean_q: 2.028988, mean_eps: 0.677514
  510094/2000000: episode: 740, duration: 19.036s, episode steps: 400, steps per second:  21, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.743 [0.000, 5.000],  loss: 0.017517, mae: 1.678849, mean_q: 2.056147, mean_eps: 0.677067
  510945/2000000: episode: 741, duration: 40.040s, episode steps: 851, steps per second:  21, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.017566, mae: 1.718253, mean_q: 2.106304, mean_eps: 0.676671
  511336/2000000: episode: 742, duration: 18.652s, episode steps: 391, steps per second:  21, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.019014, mae: 1.708838, mean_q: 2.094025, mean_eps: 0.676278
  512218/2000000: episode: 743, duration: 41.487s, episode steps: 882, steps per second:  21, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.018711, mae: 1.702945, mean_q: 2.084689, mean_eps: 0.675875
  512876/2000000: episode: 744, duration: 31.192s, episode steps: 658, steps per second:  21, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.860 [0.000, 5.000],  loss: 0.017869, mae: 1.705545, mean_q: 2.089722, mean_eps: 0.675388
  513730/2000000: episode: 745, duration: 40.274s, episode steps: 854, steps per second:  21, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.017792, mae: 1.696313, mean_q: 2.077044, mean_eps: 0.674909
  514648/2000000: episode: 746, duration: 43.221s, episode steps: 918, steps per second:  21, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.018408, mae: 1.690993, mean_q: 2.069297, mean_eps: 0.674348
  515371/2000000: episode: 747, duration: 34.280s, episode steps: 723, steps per second:  21, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.650 [0.000, 5.000],  loss: 0.020739, mae: 1.709902, mean_q: 2.093466, mean_eps: 0.673828
  515895/2000000: episode: 748, duration: 24.840s, episode steps: 524, steps per second:  21, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.334 [0.000, 5.000],  loss: 0.020345, mae: 1.714695, mean_q: 2.098216, mean_eps: 0.673433
  516428/2000000: episode: 749, duration: 25.493s, episode steps: 533, steps per second:  21, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.642 [0.000, 5.000],  loss: 0.019470, mae: 1.693427, mean_q: 2.074145, mean_eps: 0.673099
  517266/2000000: episode: 750, duration: 39.722s, episode steps: 838, steps per second:  21, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.019137, mae: 1.703404, mean_q: 2.084667, mean_eps: 0.672664
  518511/2000000: episode: 751, duration: 59.423s, episode steps: 1245, steps per second:  21, episode reward: 18.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.669 [0.000, 5.000],  loss: 0.019873, mae: 1.707747, mean_q: 2.088832, mean_eps: 0.672004
  519430/2000000: episode: 752, duration: 43.919s, episode steps: 919, steps per second:  21, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.018985, mae: 1.696889, mean_q: 2.075142, mean_eps: 0.671319
  519956/2000000: episode: 753, duration: 25.184s, episode steps: 526, steps per second:  21, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.019987, mae: 1.708323, mean_q: 2.089980, mean_eps: 0.670862
  520451/2000000: episode: 754, duration: 23.807s, episode steps: 495, steps per second:  21, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.019028, mae: 1.760384, mean_q: 2.157405, mean_eps: 0.670539
  521245/2000000: episode: 755, duration: 37.989s, episode steps: 794, steps per second:  21, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.019603, mae: 1.726653, mean_q: 2.113228, mean_eps: 0.670130
  521740/2000000: episode: 756, duration: 23.682s, episode steps: 495, steps per second:  21, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.018761, mae: 1.725201, mean_q: 2.111032, mean_eps: 0.669722
  522136/2000000: episode: 757, duration: 18.987s, episode steps: 396, steps per second:  21, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.652 [0.000, 5.000],  loss: 0.019291, mae: 1.753559, mean_q: 2.145831, mean_eps: 0.669441
  523031/2000000: episode: 758, duration: 43.008s, episode steps: 895, steps per second:  21, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.018788, mae: 1.725393, mean_q: 2.111595, mean_eps: 0.669031
  523677/2000000: episode: 759, duration: 31.512s, episode steps: 646, steps per second:  21, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.020129, mae: 1.716118, mean_q: 2.098829, mean_eps: 0.668542
  524523/2000000: episode: 760, duration: 39.820s, episode steps: 846, steps per second:  21, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.019178, mae: 1.729591, mean_q: 2.115832, mean_eps: 0.668070
  525701/2000000: episode: 761, duration: 56.114s, episode steps: 1178, steps per second:  21, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.019947, mae: 1.736398, mean_q: 2.123551, mean_eps: 0.667429
  526246/2000000: episode: 762, duration: 26.724s, episode steps: 545, steps per second:  20, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.020192, mae: 1.731046, mean_q: 2.117010, mean_eps: 0.666883
  526752/2000000: episode: 763, duration: 24.722s, episode steps: 506, steps per second:  20, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.020916, mae: 1.738432, mean_q: 2.126246, mean_eps: 0.666551
  527555/2000000: episode: 764, duration: 38.952s, episode steps: 803, steps per second:  21, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.019654, mae: 1.727202, mean_q: 2.110082, mean_eps: 0.666137
  528731/2000000: episode: 765, duration: 57.501s, episode steps: 1176, steps per second:  20, episode reward: 18.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.020632, mae: 1.733652, mean_q: 2.121669, mean_eps: 0.665510
  529531/2000000: episode: 766, duration: 39.353s, episode steps: 800, steps per second:  20, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.021773, mae: 1.742516, mean_q: 2.131208, mean_eps: 0.664884
  530802/2000000: episode: 767, duration: 63.012s, episode steps: 1271, steps per second:  20, episode reward: 25.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.019748, mae: 1.760312, mean_q: 2.155006, mean_eps: 0.664228
  531354/2000000: episode: 768, duration: 26.812s, episode steps: 552, steps per second:  21, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.018030, mae: 1.737554, mean_q: 2.126400, mean_eps: 0.663651
  531901/2000000: episode: 769, duration: 26.901s, episode steps: 547, steps per second:  20, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.018484, mae: 1.752233, mean_q: 2.143271, mean_eps: 0.663302
  532543/2000000: episode: 770, duration: 31.266s, episode steps: 642, steps per second:  21, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.020558, mae: 1.754431, mean_q: 2.146780, mean_eps: 0.662926
  532938/2000000: episode: 771, duration: 19.310s, episode steps: 395, steps per second:  20, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.018969, mae: 1.777137, mean_q: 2.173499, mean_eps: 0.662598
  533447/2000000: episode: 772, duration: 25.101s, episode steps: 509, steps per second:  20, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.018146, mae: 1.768642, mean_q: 2.163322, mean_eps: 0.662312
  534101/2000000: episode: 773, duration: 32.520s, episode steps: 654, steps per second:  20, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.775 [0.000, 5.000],  loss: 0.019618, mae: 1.761057, mean_q: 2.154912, mean_eps: 0.661943
  534747/2000000: episode: 774, duration: 32.195s, episode steps: 646, steps per second:  20, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.019045, mae: 1.770656, mean_q: 2.167380, mean_eps: 0.661531
  535418/2000000: episode: 775, duration: 33.154s, episode steps: 671, steps per second:  20, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.020620, mae: 1.764435, mean_q: 2.158268, mean_eps: 0.661115
  536357/2000000: episode: 776, duration: 46.645s, episode steps: 939, steps per second:  20, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.784 [0.000, 5.000],  loss: 0.019388, mae: 1.756912, mean_q: 2.150423, mean_eps: 0.660604
  537062/2000000: episode: 777, duration: 34.708s, episode steps: 705, steps per second:  20, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.643 [0.000, 5.000],  loss: 0.020464, mae: 1.761434, mean_q: 2.155788, mean_eps: 0.660084
  537419/2000000: episode: 778, duration: 17.766s, episode steps: 357, steps per second:  20, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.675 [0.000, 5.000],  loss: 0.020502, mae: 1.776454, mean_q: 2.174959, mean_eps: 0.659748
  538328/2000000: episode: 779, duration: 44.999s, episode steps: 909, steps per second:  20, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.019074, mae: 1.758244, mean_q: 2.150121, mean_eps: 0.659348
  538861/2000000: episode: 780, duration: 25.955s, episode steps: 533, steps per second:  21, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.019591, mae: 1.754605, mean_q: 2.143615, mean_eps: 0.658890
  539417/2000000: episode: 781, duration: 26.950s, episode steps: 556, steps per second:  21, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.019899, mae: 1.767384, mean_q: 2.160573, mean_eps: 0.658545
  540116/2000000: episode: 782, duration: 34.117s, episode steps: 699, steps per second:  20, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.019212, mae: 1.750927, mean_q: 2.142084, mean_eps: 0.658148
  541053/2000000: episode: 783, duration: 45.454s, episode steps: 937, steps per second:  21, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.018753, mae: 1.785309, mean_q: 2.184882, mean_eps: 0.657630
  542343/2000000: episode: 784, duration: 63.041s, episode steps: 1290, steps per second:  20, episode reward: 31.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.019778, mae: 1.777169, mean_q: 2.173564, mean_eps: 0.656925
  543067/2000000: episode: 785, duration: 34.417s, episode steps: 724, steps per second:  21, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 0.019494, mae: 1.795074, mean_q: 2.196799, mean_eps: 0.656287
  543760/2000000: episode: 786, duration: 33.595s, episode steps: 693, steps per second:  21, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.019966, mae: 1.764152, mean_q: 2.160374, mean_eps: 0.655839
  544779/2000000: episode: 787, duration: 50.206s, episode steps: 1019, steps per second:  20, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.020716, mae: 1.775870, mean_q: 2.173177, mean_eps: 0.655297
  545841/2000000: episode: 788, duration: 52.151s, episode steps: 1062, steps per second:  20, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.019703, mae: 1.775004, mean_q: 2.170514, mean_eps: 0.654637
  546378/2000000: episode: 789, duration: 26.463s, episode steps: 537, steps per second:  20, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.020779, mae: 1.770442, mean_q: 2.165942, mean_eps: 0.654130
  547116/2000000: episode: 790, duration: 35.967s, episode steps: 738, steps per second:  21, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.019618, mae: 1.766413, mean_q: 2.162100, mean_eps: 0.653728
  547727/2000000: episode: 791, duration: 29.810s, episode steps: 611, steps per second:  20, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.022170, mae: 1.774230, mean_q: 2.166918, mean_eps: 0.653301
  548380/2000000: episode: 792, duration: 31.924s, episode steps: 653, steps per second:  20, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.299 [0.000, 5.000],  loss: 0.019390, mae: 1.779914, mean_q: 2.178446, mean_eps: 0.652900
  549113/2000000: episode: 793, duration: 35.816s, episode steps: 733, steps per second:  20, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.019331, mae: 1.767667, mean_q: 2.160810, mean_eps: 0.652461
  550067/2000000: episode: 794, duration: 47.355s, episode steps: 954, steps per second:  20, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.019609, mae: 1.777770, mean_q: 2.173253, mean_eps: 0.651926
  550614/2000000: episode: 795, duration: 26.645s, episode steps: 547, steps per second:  21, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.709 [0.000, 5.000],  loss: 0.018745, mae: 1.820960, mean_q: 2.230929, mean_eps: 0.651451
  551240/2000000: episode: 796, duration: 30.699s, episode steps: 626, steps per second:  20, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.296 [0.000, 5.000],  loss: 0.019661, mae: 1.814341, mean_q: 2.217860, mean_eps: 0.651080
  551751/2000000: episode: 797, duration: 25.040s, episode steps: 511, steps per second:  20, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.020853, mae: 1.830234, mean_q: 2.237666, mean_eps: 0.650720
  552537/2000000: episode: 798, duration: 38.668s, episode steps: 786, steps per second:  20, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.020319, mae: 1.812041, mean_q: 2.214686, mean_eps: 0.650309
  553528/2000000: episode: 799, duration: 48.726s, episode steps: 991, steps per second:  20, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.020008, mae: 1.810445, mean_q: 2.213655, mean_eps: 0.649746
  554419/2000000: episode: 800, duration: 43.720s, episode steps: 891, steps per second:  20, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.020358, mae: 1.812679, mean_q: 2.216417, mean_eps: 0.649151
  555130/2000000: episode: 801, duration: 34.985s, episode steps: 711, steps per second:  20, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.020149, mae: 1.825366, mean_q: 2.231747, mean_eps: 0.648643
  555774/2000000: episode: 802, duration: 31.760s, episode steps: 644, steps per second:  20, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.021262, mae: 1.801507, mean_q: 2.199404, mean_eps: 0.648214
  556791/2000000: episode: 803, duration: 50.026s, episode steps: 1017, steps per second:  20, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.021034, mae: 1.809701, mean_q: 2.211181, mean_eps: 0.647688
  557792/2000000: episode: 804, duration: 49.393s, episode steps: 1001, steps per second:  20, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.019927, mae: 1.829078, mean_q: 2.236584, mean_eps: 0.647050
  558689/2000000: episode: 805, duration: 44.394s, episode steps: 897, steps per second:  20, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.019897, mae: 1.808729, mean_q: 2.212626, mean_eps: 0.646448
  559829/2000000: episode: 806, duration: 56.576s, episode steps: 1140, steps per second:  20, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.021228, mae: 1.804923, mean_q: 2.207836, mean_eps: 0.645802
  560652/2000000: episode: 807, duration: 40.653s, episode steps: 823, steps per second:  20, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.019187, mae: 1.828746, mean_q: 2.235492, mean_eps: 0.645181
  561002/2000000: episode: 808, duration: 16.912s, episode steps: 350, steps per second:  21, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.017546, mae: 1.824011, mean_q: 2.229606, mean_eps: 0.644810
  561705/2000000: episode: 809, duration: 34.024s, episode steps: 703, steps per second:  21, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.020258, mae: 1.831354, mean_q: 2.235947, mean_eps: 0.644476
  562095/2000000: episode: 810, duration: 18.801s, episode steps: 390, steps per second:  21, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.019710, mae: 1.811105, mean_q: 2.212102, mean_eps: 0.644130
  562601/2000000: episode: 811, duration: 24.886s, episode steps: 506, steps per second:  20, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.287 [0.000, 5.000],  loss: 0.019894, mae: 1.824813, mean_q: 2.227979, mean_eps: 0.643846
  562980/2000000: episode: 812, duration: 18.770s, episode steps: 379, steps per second:  20, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.020230, mae: 1.827976, mean_q: 2.232286, mean_eps: 0.643566
  563328/2000000: episode: 813, duration: 17.573s, episode steps: 348, steps per second:  20, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.020552, mae: 1.802887, mean_q: 2.204176, mean_eps: 0.643337
  563982/2000000: episode: 814, duration: 32.364s, episode steps: 654, steps per second:  20, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.019860, mae: 1.831883, mean_q: 2.237589, mean_eps: 0.643019
  565136/2000000: episode: 815, duration: 56.808s, episode steps: 1154, steps per second:  20, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.020508, mae: 1.831173, mean_q: 2.235194, mean_eps: 0.642447
  565811/2000000: episode: 816, duration: 33.449s, episode steps: 675, steps per second:  20, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.020712, mae: 1.827393, mean_q: 2.229901, mean_eps: 0.641868
  566346/2000000: episode: 817, duration: 26.526s, episode steps: 535, steps per second:  20, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.022588, mae: 1.814941, mean_q: 2.216785, mean_eps: 0.641484
  566943/2000000: episode: 818, duration: 29.436s, episode steps: 597, steps per second:  20, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.707 [0.000, 5.000],  loss: 0.020944, mae: 1.836643, mean_q: 2.242089, mean_eps: 0.641125
  567456/2000000: episode: 819, duration: 24.843s, episode steps: 513, steps per second:  21, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.708 [0.000, 5.000],  loss: 0.020348, mae: 1.839669, mean_q: 2.247770, mean_eps: 0.640775
  568411/2000000: episode: 820, duration: 45.534s, episode steps: 955, steps per second:  21, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.020962, mae: 1.824733, mean_q: 2.228817, mean_eps: 0.640310
  569104/2000000: episode: 821, duration: 33.653s, episode steps: 693, steps per second:  21, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.020184, mae: 1.801897, mean_q: 2.200353, mean_eps: 0.639788
  569589/2000000: episode: 822, duration: 23.937s, episode steps: 485, steps per second:  20, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.020812, mae: 1.847805, mean_q: 2.254484, mean_eps: 0.639414
  570385/2000000: episode: 823, duration: 39.799s, episode steps: 796, steps per second:  20, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.018016, mae: 1.841996, mean_q: 2.249548, mean_eps: 0.639008
  571052/2000000: episode: 824, duration: 33.176s, episode steps: 667, steps per second:  20, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.018107, mae: 1.875658, mean_q: 2.293593, mean_eps: 0.638545
  572090/2000000: episode: 825, duration: 50.243s, episode steps: 1038, steps per second:  21, episode reward: 12.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.020025, mae: 1.877169, mean_q: 2.292475, mean_eps: 0.638006
  572602/2000000: episode: 826, duration: 24.953s, episode steps: 512, steps per second:  21, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.123 [0.000, 5.000],  loss: 0.019739, mae: 1.869423, mean_q: 2.281801, mean_eps: 0.637514
  573763/2000000: episode: 827, duration: 56.158s, episode steps: 1161, steps per second:  21, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 0.020241, mae: 1.864516, mean_q: 2.274666, mean_eps: 0.636985
  574251/2000000: episode: 828, duration: 23.471s, episode steps: 488, steps per second:  21, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.281 [0.000, 5.000],  loss: 0.022041, mae: 1.883298, mean_q: 2.297354, mean_eps: 0.636463
  574819/2000000: episode: 829, duration: 27.346s, episode steps: 568, steps per second:  21, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.020521, mae: 1.869546, mean_q: 2.284019, mean_eps: 0.636128
  575565/2000000: episode: 830, duration: 36.149s, episode steps: 746, steps per second:  21, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.021146, mae: 1.882715, mean_q: 2.299728, mean_eps: 0.635712
  575976/2000000: episode: 831, duration: 20.075s, episode steps: 411, steps per second:  20, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.019224, mae: 1.855576, mean_q: 2.264388, mean_eps: 0.635346
  576773/2000000: episode: 832, duration: 38.899s, episode steps: 797, steps per second:  20, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.263 [0.000, 5.000],  loss: 0.020523, mae: 1.848790, mean_q: 2.257109, mean_eps: 0.634963
  577108/2000000: episode: 833, duration: 16.445s, episode steps: 335, steps per second:  20, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.021042, mae: 1.872864, mean_q: 2.287015, mean_eps: 0.634605
  577495/2000000: episode: 834, duration: 19.075s, episode steps: 387, steps per second:  20, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.021346, mae: 1.859761, mean_q: 2.270605, mean_eps: 0.634377
  578496/2000000: episode: 835, duration: 49.088s, episode steps: 1001, steps per second:  20, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.019903, mae: 1.865685, mean_q: 2.276361, mean_eps: 0.633937
  579464/2000000: episode: 836, duration: 47.721s, episode steps: 968, steps per second:  20, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.018864, mae: 1.857147, mean_q: 2.265806, mean_eps: 0.633314
  580198/2000000: episode: 837, duration: 35.691s, episode steps: 734, steps per second:  21, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.019848, mae: 1.870436, mean_q: 2.281110, mean_eps: 0.632774
  581242/2000000: episode: 838, duration: 52.738s, episode steps: 1044, steps per second:  20, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.018766, mae: 1.873311, mean_q: 2.286600, mean_eps: 0.632211
  581866/2000000: episode: 839, duration: 31.025s, episode steps: 624, steps per second:  20, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.019985, mae: 1.858445, mean_q: 2.266602, mean_eps: 0.631682
  582212/2000000: episode: 840, duration: 17.600s, episode steps: 346, steps per second:  20, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.019603, mae: 1.880489, mean_q: 2.292699, mean_eps: 0.631376
  582817/2000000: episode: 841, duration: 30.711s, episode steps: 605, steps per second:  20, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.019850, mae: 1.846756, mean_q: 2.251388, mean_eps: 0.631074
  583226/2000000: episode: 842, duration: 20.412s, episode steps: 409, steps per second:  20, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.020941, mae: 1.881626, mean_q: 2.295590, mean_eps: 0.630753
  583770/2000000: episode: 843, duration: 27.079s, episode steps: 544, steps per second:  20, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.019062, mae: 1.868119, mean_q: 2.280413, mean_eps: 0.630451
  584419/2000000: episode: 844, duration: 32.586s, episode steps: 649, steps per second:  20, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.019534, mae: 1.857672, mean_q: 2.270123, mean_eps: 0.630074
  584985/2000000: episode: 845, duration: 28.473s, episode steps: 566, steps per second:  20, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.332 [0.000, 5.000],  loss: 0.020357, mae: 1.874259, mean_q: 2.284850, mean_eps: 0.629689
  585487/2000000: episode: 846, duration: 25.352s, episode steps: 502, steps per second:  20, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.020597, mae: 1.847603, mean_q: 2.254434, mean_eps: 0.629351
  586037/2000000: episode: 847, duration: 27.832s, episode steps: 550, steps per second:  20, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.021550, mae: 1.871258, mean_q: 2.282506, mean_eps: 0.629017
  586470/2000000: episode: 848, duration: 21.974s, episode steps: 433, steps per second:  20, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.020111, mae: 1.879926, mean_q: 2.293046, mean_eps: 0.628706
  587080/2000000: episode: 849, duration: 30.646s, episode steps: 610, steps per second:  20, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.019704, mae: 1.855604, mean_q: 2.263689, mean_eps: 0.628376
  587620/2000000: episode: 850, duration: 27.645s, episode steps: 540, steps per second:  20, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.020330, mae: 1.861773, mean_q: 2.271963, mean_eps: 0.628013
  588254/2000000: episode: 851, duration: 31.921s, episode steps: 634, steps per second:  20, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.020563, mae: 1.871412, mean_q: 2.281635, mean_eps: 0.627641
  588633/2000000: episode: 852, duration: 19.277s, episode steps: 379, steps per second:  20, episode reward:  9.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.022231, mae: 1.872958, mean_q: 2.284344, mean_eps: 0.627319
  589401/2000000: episode: 853, duration: 38.375s, episode steps: 768, steps per second:  20, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.020022, mae: 1.871802, mean_q: 2.283053, mean_eps: 0.626955
  590116/2000000: episode: 854, duration: 36.452s, episode steps: 715, steps per second:  20, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.697 [0.000, 5.000],  loss: 0.021349, mae: 1.857130, mean_q: 2.264507, mean_eps: 0.626487
  590756/2000000: episode: 855, duration: 32.456s, episode steps: 640, steps per second:  20, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.022097, mae: 1.905900, mean_q: 2.324758, mean_eps: 0.626058
  591569/2000000: episode: 856, duration: 41.353s, episode steps: 813, steps per second:  20, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.022762, mae: 1.898209, mean_q: 2.316333, mean_eps: 0.625597
  592205/2000000: episode: 857, duration: 32.061s, episode steps: 636, steps per second:  20, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.020994, mae: 1.896917, mean_q: 2.315826, mean_eps: 0.625138
  592964/2000000: episode: 858, duration: 38.065s, episode steps: 759, steps per second:  20, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.019726, mae: 1.882733, mean_q: 2.297122, mean_eps: 0.624697
  593648/2000000: episode: 859, duration: 34.838s, episode steps: 684, steps per second:  20, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.021330, mae: 1.892053, mean_q: 2.307540, mean_eps: 0.624241
  594427/2000000: episode: 860, duration: 39.430s, episode steps: 779, steps per second:  20, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.022207, mae: 1.894716, mean_q: 2.309687, mean_eps: 0.623777
  595127/2000000: episode: 861, duration: 34.943s, episode steps: 700, steps per second:  20, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.020665, mae: 1.889038, mean_q: 2.301496, mean_eps: 0.623309
  595650/2000000: episode: 862, duration: 26.751s, episode steps: 523, steps per second:  20, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.020325, mae: 1.890659, mean_q: 2.304525, mean_eps: 0.622921
  596885/2000000: episode: 863, duration: 61.983s, episode steps: 1235, steps per second:  20, episode reward: 18.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.021372, mae: 1.886280, mean_q: 2.299358, mean_eps: 0.622364
  597688/2000000: episode: 864, duration: 39.323s, episode steps: 803, steps per second:  20, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.770 [0.000, 5.000],  loss: 0.022002, mae: 1.881888, mean_q: 2.294930, mean_eps: 0.621719
  598326/2000000: episode: 865, duration: 31.266s, episode steps: 638, steps per second:  20, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.677 [0.000, 5.000],  loss: 0.020743, mae: 1.884955, mean_q: 2.299151, mean_eps: 0.621263
  598871/2000000: episode: 866, duration: 26.702s, episode steps: 545, steps per second:  20, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.022566, mae: 1.880398, mean_q: 2.292599, mean_eps: 0.620888
  600125/2000000: episode: 867, duration: 63.149s, episode steps: 1254, steps per second:  20, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.022427, mae: 1.888928, mean_q: 2.301128, mean_eps: 0.620318
  600635/2000000: episode: 868, duration: 25.863s, episode steps: 510, steps per second:  20, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.708 [0.000, 5.000],  loss: 0.019119, mae: 1.912760, mean_q: 2.332326, mean_eps: 0.619759
  601134/2000000: episode: 869, duration: 25.863s, episode steps: 499, steps per second:  19, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.020664, mae: 1.926334, mean_q: 2.347935, mean_eps: 0.619440
  602177/2000000: episode: 870, duration: 52.700s, episode steps: 1043, steps per second:  20, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.021519, mae: 1.917675, mean_q: 2.336786, mean_eps: 0.618951
  602859/2000000: episode: 871, duration: 33.370s, episode steps: 682, steps per second:  20, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.020773, mae: 1.912558, mean_q: 2.331737, mean_eps: 0.618405
  603229/2000000: episode: 872, duration: 18.224s, episode steps: 370, steps per second:  20, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.021927, mae: 1.910134, mean_q: 2.327075, mean_eps: 0.618072
  603982/2000000: episode: 873, duration: 37.182s, episode steps: 753, steps per second:  20, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.022250, mae: 1.910953, mean_q: 2.330015, mean_eps: 0.617716
  604702/2000000: episode: 874, duration: 36.024s, episode steps: 720, steps per second:  20, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.021828, mae: 1.913142, mean_q: 2.332364, mean_eps: 0.617250
  605201/2000000: episode: 875, duration: 25.176s, episode steps: 499, steps per second:  20, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.021744, mae: 1.911053, mean_q: 2.331225, mean_eps: 0.616864
  605853/2000000: episode: 876, duration: 32.930s, episode steps: 652, steps per second:  20, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.319 [0.000, 5.000],  loss: 0.021382, mae: 1.921337, mean_q: 2.341085, mean_eps: 0.616499
  606371/2000000: episode: 877, duration: 26.121s, episode steps: 518, steps per second:  20, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.021113, mae: 1.926604, mean_q: 2.348166, mean_eps: 0.616129
  607182/2000000: episode: 878, duration: 40.916s, episode steps: 811, steps per second:  20, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.020867, mae: 1.907455, mean_q: 2.324057, mean_eps: 0.615709
  607696/2000000: episode: 879, duration: 26.230s, episode steps: 514, steps per second:  20, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.020868, mae: 1.920304, mean_q: 2.340939, mean_eps: 0.615289
  608403/2000000: episode: 880, duration: 36.048s, episode steps: 707, steps per second:  20, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.021570, mae: 1.924397, mean_q: 2.345847, mean_eps: 0.614903
  609167/2000000: episode: 881, duration: 38.964s, episode steps: 764, steps per second:  20, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.021851, mae: 1.919082, mean_q: 2.340910, mean_eps: 0.614437
  609733/2000000: episode: 882, duration: 28.798s, episode steps: 566, steps per second:  20, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.022102, mae: 1.910652, mean_q: 2.325924, mean_eps: 0.614015
  610501/2000000: episode: 883, duration: 38.891s, episode steps: 768, steps per second:  20, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.658 [0.000, 5.000],  loss: 0.019705, mae: 1.968141, mean_q: 2.399324, mean_eps: 0.613592
  611328/2000000: episode: 884, duration: 42.349s, episode steps: 827, steps per second:  20, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.019548, mae: 1.966713, mean_q: 2.396812, mean_eps: 0.613088
  612120/2000000: episode: 885, duration: 39.916s, episode steps: 792, steps per second:  20, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.021642, mae: 1.967452, mean_q: 2.396732, mean_eps: 0.612576
  612913/2000000: episode: 886, duration: 40.973s, episode steps: 793, steps per second:  19, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.019902, mae: 1.952363, mean_q: 2.380555, mean_eps: 0.612073
  613548/2000000: episode: 887, duration: 32.269s, episode steps: 635, steps per second:  20, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.021406, mae: 1.967327, mean_q: 2.396915, mean_eps: 0.611621
  614205/2000000: episode: 888, duration: 33.973s, episode steps: 657, steps per second:  19, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.022843, mae: 1.965100, mean_q: 2.393060, mean_eps: 0.611212
  614682/2000000: episode: 889, duration: 24.638s, episode steps: 477, steps per second:  19, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.023079, mae: 1.971833, mean_q: 2.402997, mean_eps: 0.610852
  615201/2000000: episode: 890, duration: 25.701s, episode steps: 519, steps per second:  20, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.022092, mae: 1.964006, mean_q: 2.393638, mean_eps: 0.610537
  616021/2000000: episode: 891, duration: 40.330s, episode steps: 820, steps per second:  20, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.023599, mae: 1.956856, mean_q: 2.385498, mean_eps: 0.610112
  616990/2000000: episode: 892, duration: 48.008s, episode steps: 969, steps per second:  20, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.021072, mae: 1.956827, mean_q: 2.385473, mean_eps: 0.609546
  617408/2000000: episode: 893, duration: 21.078s, episode steps: 418, steps per second:  20, episode reward: 11.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.773 [0.000, 5.000],  loss: 0.021286, mae: 1.941759, mean_q: 2.365720, mean_eps: 0.609108
  617901/2000000: episode: 894, duration: 25.028s, episode steps: 493, steps per second:  20, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.021678, mae: 1.968233, mean_q: 2.397973, mean_eps: 0.608819
  618642/2000000: episode: 895, duration: 37.893s, episode steps: 741, steps per second:  20, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.020631, mae: 1.957366, mean_q: 2.384903, mean_eps: 0.608428
  619750/2000000: episode: 896, duration: 57.029s, episode steps: 1108, steps per second:  19, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.022000, mae: 1.964828, mean_q: 2.395106, mean_eps: 0.607843
  620345/2000000: episode: 897, duration: 30.510s, episode steps: 595, steps per second:  20, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.019446, mae: 1.988557, mean_q: 2.424915, mean_eps: 0.607303
  620913/2000000: episode: 898, duration: 29.530s, episode steps: 568, steps per second:  19, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.019637, mae: 1.999453, mean_q: 2.437971, mean_eps: 0.606934
  621581/2000000: episode: 899, duration: 34.229s, episode steps: 668, steps per second:  20, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.124 [0.000, 5.000],  loss: 0.022006, mae: 1.983142, mean_q: 2.416633, mean_eps: 0.606543
  622054/2000000: episode: 900, duration: 24.808s, episode steps: 473, steps per second:  19, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.672 [0.000, 5.000],  loss: 0.021095, mae: 1.985343, mean_q: 2.417361, mean_eps: 0.606182
  622911/2000000: episode: 901, duration: 44.062s, episode steps: 857, steps per second:  19, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.659 [0.000, 5.000],  loss: 0.022569, mae: 1.977880, mean_q: 2.409216, mean_eps: 0.605761
  623479/2000000: episode: 902, duration: 29.119s, episode steps: 568, steps per second:  20, episode reward: 14.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.022150, mae: 1.994732, mean_q: 2.431005, mean_eps: 0.605310
  624045/2000000: episode: 903, duration: 29.652s, episode steps: 566, steps per second:  19, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.307 [0.000, 5.000],  loss: 0.024282, mae: 1.973304, mean_q: 2.402867, mean_eps: 0.604951
  624648/2000000: episode: 904, duration: 31.367s, episode steps: 603, steps per second:  19, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.021094, mae: 1.980896, mean_q: 2.413961, mean_eps: 0.604581
  625413/2000000: episode: 905, duration: 39.450s, episode steps: 765, steps per second:  19, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.627 [0.000, 5.000],  loss: 0.023789, mae: 1.979175, mean_q: 2.411148, mean_eps: 0.604148
  626139/2000000: episode: 906, duration: 36.879s, episode steps: 726, steps per second:  20, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.025686, mae: 1.994297, mean_q: 2.427898, mean_eps: 0.603675
  626831/2000000: episode: 907, duration: 35.848s, episode steps: 692, steps per second:  19, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.021474, mae: 1.995293, mean_q: 2.430056, mean_eps: 0.603227
  627497/2000000: episode: 908, duration: 34.614s, episode steps: 666, steps per second:  19, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.021707, mae: 1.977541, mean_q: 2.409451, mean_eps: 0.602796
  627907/2000000: episode: 909, duration: 21.350s, episode steps: 410, steps per second:  19, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.693 [0.000, 5.000],  loss: 0.020617, mae: 1.999995, mean_q: 2.437109, mean_eps: 0.602455
  628261/2000000: episode: 910, duration: 18.588s, episode steps: 354, steps per second:  19, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.021463, mae: 2.003185, mean_q: 2.439755, mean_eps: 0.602213
  628943/2000000: episode: 911, duration: 35.531s, episode steps: 682, steps per second:  19, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.672 [0.000, 5.000],  loss: 0.021479, mae: 1.987908, mean_q: 2.422576, mean_eps: 0.601885
  629445/2000000: episode: 912, duration: 26.548s, episode steps: 502, steps per second:  19, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.024009, mae: 1.992475, mean_q: 2.426260, mean_eps: 0.601510
  630159/2000000: episode: 913, duration: 36.838s, episode steps: 714, steps per second:  19, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.023750, mae: 1.987964, mean_q: 2.421327, mean_eps: 0.601125
  631196/2000000: episode: 914, duration: 54.912s, episode steps: 1037, steps per second:  19, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.021179, mae: 2.047054, mean_q: 2.495360, mean_eps: 0.600572
  632066/2000000: episode: 915, duration: 45.047s, episode steps: 870, steps per second:  19, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.022998, mae: 2.046388, mean_q: 2.491557, mean_eps: 0.599968
  632802/2000000: episode: 916, duration: 37.288s, episode steps: 736, steps per second:  20, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.022503, mae: 2.041878, mean_q: 2.485047, mean_eps: 0.599458
  633994/2000000: episode: 917, duration: 59.704s, episode steps: 1192, steps per second:  20, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.022131, mae: 2.029232, mean_q: 2.470132, mean_eps: 0.598848
  634923/2000000: episode: 918, duration: 47.212s, episode steps: 929, steps per second:  20, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.022000, mae: 2.031303, mean_q: 2.472266, mean_eps: 0.598177
  635596/2000000: episode: 919, duration: 34.404s, episode steps: 673, steps per second:  20, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.351 [0.000, 5.000],  loss: 0.023684, mae: 2.035052, mean_q: 2.475954, mean_eps: 0.597670
  636186/2000000: episode: 920, duration: 30.688s, episode steps: 590, steps per second:  19, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.022333, mae: 2.010589, mean_q: 2.448398, mean_eps: 0.597270
  636890/2000000: episode: 921, duration: 36.508s, episode steps: 704, steps per second:  19, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.653 [0.000, 5.000],  loss: 0.022967, mae: 2.038098, mean_q: 2.479372, mean_eps: 0.596859
  637703/2000000: episode: 922, duration: 42.789s, episode steps: 813, steps per second:  19, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.023913, mae: 2.023613, mean_q: 2.462166, mean_eps: 0.596379
  638644/2000000: episode: 923, duration: 48.966s, episode steps: 941, steps per second:  19, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.022290, mae: 2.025655, mean_q: 2.464686, mean_eps: 0.595824
  639581/2000000: episode: 924, duration: 48.575s, episode steps: 937, steps per second:  19, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.023526, mae: 2.029886, mean_q: 2.470213, mean_eps: 0.595229
  640412/2000000: episode: 925, duration: 42.867s, episode steps: 831, steps per second:  19, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.020634, mae: 2.042109, mean_q: 2.486229, mean_eps: 0.594669
  641242/2000000: episode: 926, duration: 43.489s, episode steps: 830, steps per second:  19, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.019978, mae: 2.041738, mean_q: 2.484712, mean_eps: 0.594144
  642394/2000000: episode: 927, duration: 58.787s, episode steps: 1152, steps per second:  20, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.021975, mae: 2.047273, mean_q: 2.492109, mean_eps: 0.593515
  643069/2000000: episode: 928, duration: 34.261s, episode steps: 675, steps per second:  20, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.307 [0.000, 5.000],  loss: 0.021290, mae: 2.035091, mean_q: 2.475198, mean_eps: 0.592936
  643810/2000000: episode: 929, duration: 37.469s, episode steps: 741, steps per second:  20, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.021704, mae: 2.056257, mean_q: 2.500259, mean_eps: 0.592488
  644350/2000000: episode: 930, duration: 27.504s, episode steps: 540, steps per second:  20, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.021759, mae: 2.047338, mean_q: 2.490055, mean_eps: 0.592083
  645044/2000000: episode: 931, duration: 35.011s, episode steps: 694, steps per second:  20, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.022587, mae: 2.052317, mean_q: 2.495596, mean_eps: 0.591693
  645509/2000000: episode: 932, duration: 23.517s, episode steps: 465, steps per second:  20, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.021605, mae: 2.045408, mean_q: 2.488622, mean_eps: 0.591325
  646399/2000000: episode: 933, duration: 44.467s, episode steps: 890, steps per second:  20, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.022747, mae: 2.047556, mean_q: 2.490647, mean_eps: 0.590896
  646758/2000000: episode: 934, duration: 18.151s, episode steps: 359, steps per second:  20, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 3.003 [0.000, 5.000],  loss: 0.022213, mae: 2.035259, mean_q: 2.475049, mean_eps: 0.590501
  647752/2000000: episode: 935, duration: 50.233s, episode steps: 994, steps per second:  20, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.022257, mae: 2.047308, mean_q: 2.490164, mean_eps: 0.590072
  648556/2000000: episode: 936, duration: 40.569s, episode steps: 804, steps per second:  20, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.022255, mae: 2.036795, mean_q: 2.475157, mean_eps: 0.589504
  649185/2000000: episode: 937, duration: 32.479s, episode steps: 629, steps per second:  19, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.700 [0.000, 5.000],  loss: 0.021753, mae: 2.047578, mean_q: 2.489758, mean_eps: 0.589049
  650031/2000000: episode: 938, duration: 44.650s, episode steps: 846, steps per second:  19, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.021447, mae: 2.042831, mean_q: 2.482380, mean_eps: 0.588582
  650860/2000000: episode: 939, duration: 41.753s, episode steps: 829, steps per second:  20, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 0.020472, mae: 2.059926, mean_q: 2.505715, mean_eps: 0.588052
  651258/2000000: episode: 940, duration: 20.201s, episode steps: 398, steps per second:  20, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.022632, mae: 2.089123, mean_q: 2.539972, mean_eps: 0.587663
  651597/2000000: episode: 941, duration: 18.672s, episode steps: 339, steps per second:  18, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.026473, mae: 2.087589, mean_q: 2.539426, mean_eps: 0.587429
  652085/2000000: episode: 942, duration: 25.416s, episode steps: 488, steps per second:  19, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.256 [0.000, 5.000],  loss: 0.023704, mae: 2.054196, mean_q: 2.498130, mean_eps: 0.587167
  652791/2000000: episode: 943, duration: 36.231s, episode steps: 706, steps per second:  19, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.052 [0.000, 5.000],  loss: 0.020981, mae: 2.056544, mean_q: 2.501428, mean_eps: 0.586789
  653661/2000000: episode: 944, duration: 44.766s, episode steps: 870, steps per second:  19, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.025344, mae: 2.076227, mean_q: 2.526964, mean_eps: 0.586290
  654260/2000000: episode: 945, duration: 30.410s, episode steps: 599, steps per second:  20, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.023777, mae: 2.056567, mean_q: 2.505676, mean_eps: 0.585825
  655213/2000000: episode: 946, duration: 49.454s, episode steps: 953, steps per second:  19, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.022629, mae: 2.037263, mean_q: 2.478827, mean_eps: 0.585334
  655761/2000000: episode: 947, duration: 28.361s, episode steps: 548, steps per second:  19, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.022335, mae: 2.046736, mean_q: 2.489590, mean_eps: 0.584858
  656774/2000000: episode: 948, duration: 52.022s, episode steps: 1013, steps per second:  19, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.022139, mae: 2.059039, mean_q: 2.506698, mean_eps: 0.584364
  657527/2000000: episode: 949, duration: 38.550s, episode steps: 753, steps per second:  20, episode reward: 22.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.022699, mae: 2.042438, mean_q: 2.486390, mean_eps: 0.583805
  658178/2000000: episode: 950, duration: 33.650s, episode steps: 651, steps per second:  19, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.022556, mae: 2.049654, mean_q: 2.494513, mean_eps: 0.583360
  659220/2000000: episode: 951, duration: 53.269s, episode steps: 1042, steps per second:  20, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.023020, mae: 2.056669, mean_q: 2.502084, mean_eps: 0.582825
  659945/2000000: episode: 952, duration: 37.520s, episode steps: 725, steps per second:  19, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.023153, mae: 2.067351, mean_q: 2.515126, mean_eps: 0.582265
  660796/2000000: episode: 953, duration: 43.920s, episode steps: 851, steps per second:  19, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.020902, mae: 2.078055, mean_q: 2.529411, mean_eps: 0.581766
  661362/2000000: episode: 954, duration: 29.443s, episode steps: 566, steps per second:  19, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.020439, mae: 2.073110, mean_q: 2.524014, mean_eps: 0.581317
  662185/2000000: episode: 955, duration: 42.497s, episode steps: 823, steps per second:  19, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.022433, mae: 2.076136, mean_q: 2.525711, mean_eps: 0.580876
  662811/2000000: episode: 956, duration: 32.329s, episode steps: 626, steps per second:  19, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.022193, mae: 2.079968, mean_q: 2.533663, mean_eps: 0.580418
  663371/2000000: episode: 957, duration: 29.213s, episode steps: 560, steps per second:  19, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 0.022958, mae: 2.069552, mean_q: 2.517770, mean_eps: 0.580043
  664313/2000000: episode: 958, duration: 49.392s, episode steps: 942, steps per second:  19, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.022414, mae: 2.065832, mean_q: 2.512690, mean_eps: 0.579567
  664865/2000000: episode: 959, duration: 28.510s, episode steps: 552, steps per second:  19, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.024225, mae: 2.067337, mean_q: 2.515043, mean_eps: 0.579093
  665520/2000000: episode: 960, duration: 34.346s, episode steps: 655, steps per second:  19, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.021745, mae: 2.051754, mean_q: 2.495813, mean_eps: 0.578712
  666101/2000000: episode: 961, duration: 30.996s, episode steps: 581, steps per second:  19, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.023999, mae: 2.077058, mean_q: 2.527073, mean_eps: 0.578320
  666743/2000000: episode: 962, duration: 33.019s, episode steps: 642, steps per second:  19, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.023151, mae: 2.079946, mean_q: 2.528239, mean_eps: 0.577933
  667267/2000000: episode: 963, duration: 27.303s, episode steps: 524, steps per second:  19, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.021774, mae: 2.085409, mean_q: 2.536476, mean_eps: 0.577564
  668184/2000000: episode: 964, duration: 46.888s, episode steps: 917, steps per second:  20, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.022839, mae: 2.076905, mean_q: 2.524449, mean_eps: 0.577108
  668723/2000000: episode: 965, duration: 27.451s, episode steps: 539, steps per second:  20, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.258 [0.000, 5.000],  loss: 0.021830, mae: 2.071818, mean_q: 2.518966, mean_eps: 0.576647
  669395/2000000: episode: 966, duration: 34.143s, episode steps: 672, steps per second:  20, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.022754, mae: 2.069823, mean_q: 2.515962, mean_eps: 0.576263
  670023/2000000: episode: 967, duration: 32.064s, episode steps: 628, steps per second:  20, episode reward: 17.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.307 [0.000, 5.000],  loss: 0.021568, mae: 2.067302, mean_q: 2.512113, mean_eps: 0.575852
  670956/2000000: episode: 968, duration: 47.906s, episode steps: 933, steps per second:  19, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.020478, mae: 2.075275, mean_q: 2.524585, mean_eps: 0.575358
  671677/2000000: episode: 969, duration: 37.080s, episode steps: 721, steps per second:  19, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.671 [0.000, 5.000],  loss: 0.020545, mae: 2.097290, mean_q: 2.551682, mean_eps: 0.574833
  672586/2000000: episode: 970, duration: 46.790s, episode steps: 909, steps per second:  19, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.021975, mae: 2.076831, mean_q: 2.525753, mean_eps: 0.574316
  673329/2000000: episode: 971, duration: 38.604s, episode steps: 743, steps per second:  19, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.021823, mae: 2.086764, mean_q: 2.536848, mean_eps: 0.573793
  674185/2000000: episode: 972, duration: 43.510s, episode steps: 856, steps per second:  20, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.022642, mae: 2.086173, mean_q: 2.535346, mean_eps: 0.573287
  675038/2000000: episode: 973, duration: 43.387s, episode steps: 853, steps per second:  20, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.023225, mae: 2.100526, mean_q: 2.552543, mean_eps: 0.572746
  676070/2000000: episode: 974, duration: 52.313s, episode steps: 1032, steps per second:  20, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.244 [0.000, 5.000],  loss: 0.024158, mae: 2.081676, mean_q: 2.530638, mean_eps: 0.572149
  676730/2000000: episode: 975, duration: 33.658s, episode steps: 660, steps per second:  20, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.025235, mae: 2.095860, mean_q: 2.546506, mean_eps: 0.571613
  677701/2000000: episode: 976, duration: 49.474s, episode steps: 971, steps per second:  20, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.025073, mae: 2.084923, mean_q: 2.534231, mean_eps: 0.571097
  678720/2000000: episode: 977, duration: 51.848s, episode steps: 1019, steps per second:  20, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.022059, mae: 2.089379, mean_q: 2.540731, mean_eps: 0.570467
  679490/2000000: episode: 978, duration: 39.554s, episode steps: 770, steps per second:  19, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.024749, mae: 2.082771, mean_q: 2.533561, mean_eps: 0.569901
  680358/2000000: episode: 979, duration: 44.460s, episode steps: 868, steps per second:  20, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.021675, mae: 2.088734, mean_q: 2.540336, mean_eps: 0.569381
  681425/2000000: episode: 980, duration: 54.605s, episode steps: 1067, steps per second:  20, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.022505, mae: 2.105296, mean_q: 2.561371, mean_eps: 0.568768
  682075/2000000: episode: 981, duration: 33.113s, episode steps: 650, steps per second:  20, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.780 [0.000, 5.000],  loss: 0.021906, mae: 2.113641, mean_q: 2.569214, mean_eps: 0.568225
  682845/2000000: episode: 982, duration: 39.796s, episode steps: 770, steps per second:  19, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.191 [0.000, 5.000],  loss: 0.027400, mae: 2.115243, mean_q: 2.571872, mean_eps: 0.567775
  683479/2000000: episode: 983, duration: 32.452s, episode steps: 634, steps per second:  20, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.023993, mae: 2.112420, mean_q: 2.569149, mean_eps: 0.567331
  684414/2000000: episode: 984, duration: 48.310s, episode steps: 935, steps per second:  19, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.022220, mae: 2.108653, mean_q: 2.563934, mean_eps: 0.566834
  685242/2000000: episode: 985, duration: 42.820s, episode steps: 828, steps per second:  19, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.024146, mae: 2.129654, mean_q: 2.586106, mean_eps: 0.566276
  685738/2000000: episode: 986, duration: 25.581s, episode steps: 496, steps per second:  19, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.284 [0.000, 5.000],  loss: 0.026390, mae: 2.132847, mean_q: 2.591386, mean_eps: 0.565856
  686366/2000000: episode: 987, duration: 32.362s, episode steps: 628, steps per second:  19, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.831 [0.000, 5.000],  loss: 0.023224, mae: 2.118768, mean_q: 2.574786, mean_eps: 0.565500
  687023/2000000: episode: 988, duration: 33.771s, episode steps: 657, steps per second:  19, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.025770, mae: 2.128494, mean_q: 2.584617, mean_eps: 0.565094
  687490/2000000: episode: 989, duration: 24.098s, episode steps: 467, steps per second:  19, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.773 [0.000, 5.000],  loss: 0.023068, mae: 2.108658, mean_q: 2.562083, mean_eps: 0.564738
  688571/2000000: episode: 990, duration: 56.217s, episode steps: 1081, steps per second:  19, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.024039, mae: 2.111967, mean_q: 2.566721, mean_eps: 0.564248
  689104/2000000: episode: 991, duration: 27.833s, episode steps: 533, steps per second:  19, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.023435, mae: 2.107646, mean_q: 2.562561, mean_eps: 0.563737
  690115/2000000: episode: 992, duration: 53.088s, episode steps: 1011, steps per second:  19, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.291 [0.000, 5.000],  loss: 0.024482, mae: 2.118627, mean_q: 2.576782, mean_eps: 0.563248
  691074/2000000: episode: 993, duration: 50.214s, episode steps: 959, steps per second:  19, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.022318, mae: 2.172006, mean_q: 2.642664, mean_eps: 0.562624
  691637/2000000: episode: 994, duration: 29.566s, episode steps: 563, steps per second:  19, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.748 [0.000, 5.000],  loss: 0.024265, mae: 2.186076, mean_q: 2.658668, mean_eps: 0.562141
  692103/2000000: episode: 995, duration: 24.450s, episode steps: 466, steps per second:  19, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.022000, mae: 2.159397, mean_q: 2.627387, mean_eps: 0.561816
  692744/2000000: episode: 996, duration: 33.940s, episode steps: 641, steps per second:  19, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.693 [0.000, 5.000],  loss: 0.023149, mae: 2.170529, mean_q: 2.638697, mean_eps: 0.561466
  693127/2000000: episode: 997, duration: 20.478s, episode steps: 383, steps per second:  19, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.277 [0.000, 5.000],  loss: 0.024538, mae: 2.161503, mean_q: 2.628993, mean_eps: 0.561142
  694339/2000000: episode: 998, duration: 64.183s, episode steps: 1212, steps per second:  19, episode reward: 15.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.025100, mae: 2.158794, mean_q: 2.625814, mean_eps: 0.560636
  695145/2000000: episode: 999, duration: 42.285s, episode steps: 806, steps per second:  19, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.024323, mae: 2.165467, mean_q: 2.630065, mean_eps: 0.559997
  695700/2000000: episode: 1000, duration: 28.703s, episode steps: 555, steps per second:  19, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.690 [0.000, 5.000],  loss: 0.025909, mae: 2.156734, mean_q: 2.618574, mean_eps: 0.559566
  696379/2000000: episode: 1001, duration: 35.113s, episode steps: 679, steps per second:  19, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.029431, mae: 2.178359, mean_q: 2.647579, mean_eps: 0.559176
  697621/2000000: episode: 1002, duration: 64.466s, episode steps: 1242, steps per second:  19, episode reward: 20.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.026333, mae: 2.172906, mean_q: 2.639957, mean_eps: 0.558567
  698512/2000000: episode: 1003, duration: 45.926s, episode steps: 891, steps per second:  19, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.023335, mae: 2.164148, mean_q: 2.628564, mean_eps: 0.557892
  699177/2000000: episode: 1004, duration: 34.616s, episode steps: 665, steps per second:  19, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.024227, mae: 2.157060, mean_q: 2.618871, mean_eps: 0.557399
  699716/2000000: episode: 1005, duration: 28.011s, episode steps: 539, steps per second:  19, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.023045, mae: 2.156990, mean_q: 2.620764, mean_eps: 0.557018
  700649/2000000: episode: 1006, duration: 49.858s, episode steps: 933, steps per second:  19, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.023109, mae: 2.204103, mean_q: 2.678507, mean_eps: 0.556551
  701787/2000000: episode: 1007, duration: 59.050s, episode steps: 1138, steps per second:  19, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.022489, mae: 2.206263, mean_q: 2.680768, mean_eps: 0.555895
  702453/2000000: episode: 1008, duration: 35.065s, episode steps: 666, steps per second:  19, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.024515, mae: 2.202811, mean_q: 2.674529, mean_eps: 0.555324
  702969/2000000: episode: 1009, duration: 26.630s, episode steps: 516, steps per second:  19, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.024591, mae: 2.204647, mean_q: 2.678730, mean_eps: 0.554949
  703651/2000000: episode: 1010, duration: 35.259s, episode steps: 682, steps per second:  19, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.023664, mae: 2.198265, mean_q: 2.671565, mean_eps: 0.554570
  704041/2000000: episode: 1011, duration: 20.587s, episode steps: 390, steps per second:  19, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.182 [0.000, 5.000],  loss: 0.024244, mae: 2.191493, mean_q: 2.663058, mean_eps: 0.554231
  704658/2000000: episode: 1012, duration: 32.158s, episode steps: 617, steps per second:  19, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.024608, mae: 2.220866, mean_q: 2.701263, mean_eps: 0.553912
  705026/2000000: episode: 1013, duration: 19.352s, episode steps: 368, steps per second:  19, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.087 [0.000, 5.000],  loss: 0.033382, mae: 2.211085, mean_q: 2.686028, mean_eps: 0.553600
  706034/2000000: episode: 1014, duration: 53.781s, episode steps: 1008, steps per second:  19, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.024982, mae: 2.221802, mean_q: 2.699558, mean_eps: 0.553164
  706717/2000000: episode: 1015, duration: 36.491s, episode steps: 683, steps per second:  19, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.023668, mae: 2.205106, mean_q: 2.677303, mean_eps: 0.552629
  707926/2000000: episode: 1016, duration: 64.411s, episode steps: 1209, steps per second:  19, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.208 [0.000, 5.000],  loss: 0.025096, mae: 2.212700, mean_q: 2.687356, mean_eps: 0.552029
  709020/2000000: episode: 1017, duration: 59.135s, episode steps: 1094, steps per second:  18, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.023620, mae: 2.200090, mean_q: 2.670765, mean_eps: 0.551301
  709841/2000000: episode: 1018, duration: 44.345s, episode steps: 821, steps per second:  19, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.023159, mae: 2.200373, mean_q: 2.672110, mean_eps: 0.550694
  710548/2000000: episode: 1019, duration: 38.484s, episode steps: 707, steps per second:  18, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.024926, mae: 2.248844, mean_q: 2.732589, mean_eps: 0.550210
  711675/2000000: episode: 1020, duration: 60.025s, episode steps: 1127, steps per second:  19, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.266 [0.000, 5.000],  loss: 0.025154, mae: 2.245465, mean_q: 2.727174, mean_eps: 0.549630
  712686/2000000: episode: 1021, duration: 52.930s, episode steps: 1011, steps per second:  19, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.025315, mae: 2.223010, mean_q: 2.698471, mean_eps: 0.548953
  713101/2000000: episode: 1022, duration: 21.751s, episode steps: 415, steps per second:  19, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.275 [0.000, 5.000],  loss: 0.023660, mae: 2.231679, mean_q: 2.712012, mean_eps: 0.548500
  713583/2000000: episode: 1023, duration: 25.530s, episode steps: 482, steps per second:  19, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.023685, mae: 2.233476, mean_q: 2.712815, mean_eps: 0.548217
  714352/2000000: episode: 1024, duration: 40.107s, episode steps: 769, steps per second:  19, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.023372, mae: 2.237698, mean_q: 2.716027, mean_eps: 0.547822
  715047/2000000: episode: 1025, duration: 36.176s, episode steps: 695, steps per second:  19, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.186 [0.000, 5.000],  loss: 0.024510, mae: 2.239756, mean_q: 2.719926, mean_eps: 0.547358
  715389/2000000: episode: 1026, duration: 17.820s, episode steps: 342, steps per second:  19, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.026141, mae: 2.242112, mean_q: 2.719704, mean_eps: 0.547029
  715766/2000000: episode: 1027, duration: 19.557s, episode steps: 377, steps per second:  19, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.023150, mae: 2.240249, mean_q: 2.717741, mean_eps: 0.546801
  716276/2000000: episode: 1028, duration: 26.706s, episode steps: 510, steps per second:  19, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.022307, mae: 2.246397, mean_q: 2.727032, mean_eps: 0.546521
  716788/2000000: episode: 1029, duration: 26.861s, episode steps: 512, steps per second:  19, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.027198, mae: 2.255351, mean_q: 2.737684, mean_eps: 0.546198
  717307/2000000: episode: 1030, duration: 27.331s, episode steps: 519, steps per second:  19, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.281 [0.000, 5.000],  loss: 0.023672, mae: 2.231979, mean_q: 2.707112, mean_eps: 0.545871
  717854/2000000: episode: 1031, duration: 29.083s, episode steps: 547, steps per second:  19, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.025501, mae: 2.246201, mean_q: 2.724344, mean_eps: 0.545533
  718252/2000000: episode: 1032, duration: 21.296s, episode steps: 398, steps per second:  19, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: 0.027145, mae: 2.253255, mean_q: 2.732784, mean_eps: 0.545234
  718761/2000000: episode: 1033, duration: 27.040s, episode steps: 509, steps per second:  19, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.218 [0.000, 5.000],  loss: 0.026250, mae: 2.244649, mean_q: 2.723696, mean_eps: 0.544946
  719235/2000000: episode: 1034, duration: 25.214s, episode steps: 474, steps per second:  19, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.026484, mae: 2.241881, mean_q: 2.719473, mean_eps: 0.544635
  719867/2000000: episode: 1035, duration: 33.123s, episode steps: 632, steps per second:  19, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.191 [0.000, 5.000],  loss: 0.026275, mae: 2.236468, mean_q: 2.714571, mean_eps: 0.544285
  720648/2000000: episode: 1036, duration: 40.874s, episode steps: 781, steps per second:  19, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.022789, mae: 2.246523, mean_q: 2.730592, mean_eps: 0.543838
  721009/2000000: episode: 1037, duration: 20.769s, episode steps: 361, steps per second:  17, episode reward:  9.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.024319, mae: 2.275521, mean_q: 2.764268, mean_eps: 0.543476
  721756/2000000: episode: 1038, duration: 40.738s, episode steps: 747, steps per second:  18, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.024588, mae: 2.270556, mean_q: 2.758232, mean_eps: 0.543125
  722529/2000000: episode: 1039, duration: 42.136s, episode steps: 773, steps per second:  18, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.024034, mae: 2.295113, mean_q: 2.788768, mean_eps: 0.542643
  723224/2000000: episode: 1040, duration: 37.127s, episode steps: 695, steps per second:  19, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.060 [0.000, 5.000],  loss: 0.024434, mae: 2.259787, mean_q: 2.745509, mean_eps: 0.542179
  723867/2000000: episode: 1041, duration: 34.835s, episode steps: 643, steps per second:  18, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.023661, mae: 2.265911, mean_q: 2.752804, mean_eps: 0.541755
  724432/2000000: episode: 1042, duration: 30.384s, episode steps: 565, steps per second:  19, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.281 [0.000, 5.000],  loss: 0.029669, mae: 2.266814, mean_q: 2.755390, mean_eps: 0.541373
  725186/2000000: episode: 1043, duration: 40.838s, episode steps: 754, steps per second:  18, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.027330, mae: 2.275954, mean_q: 2.767104, mean_eps: 0.540955
  726133/2000000: episode: 1044, duration: 51.104s, episode steps: 947, steps per second:  19, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.273 [0.000, 5.000],  loss: 0.024463, mae: 2.260337, mean_q: 2.745127, mean_eps: 0.540415
  727117/2000000: episode: 1045, duration: 53.420s, episode steps: 984, steps per second:  18, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.029722, mae: 2.267744, mean_q: 2.752528, mean_eps: 0.539804
  727650/2000000: episode: 1046, duration: 28.592s, episode steps: 533, steps per second:  19, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.023988, mae: 2.274607, mean_q: 2.760623, mean_eps: 0.539323
  728155/2000000: episode: 1047, duration: 27.454s, episode steps: 505, steps per second:  18, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.024726, mae: 2.277539, mean_q: 2.763958, mean_eps: 0.538995
  728866/2000000: episode: 1048, duration: 38.390s, episode steps: 711, steps per second:  19, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.025907, mae: 2.254111, mean_q: 2.735501, mean_eps: 0.538610
  729623/2000000: episode: 1049, duration: 41.204s, episode steps: 757, steps per second:  18, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.030 [0.000, 5.000],  loss: 0.023485, mae: 2.276712, mean_q: 2.762291, mean_eps: 0.538145
  730564/2000000: episode: 1050, duration: 51.120s, episode steps: 941, steps per second:  18, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: 0.025022, mae: 2.291420, mean_q: 2.780793, mean_eps: 0.537608
  731012/2000000: episode: 1051, duration: 24.055s, episode steps: 448, steps per second:  19, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.024718, mae: 2.304739, mean_q: 2.798441, mean_eps: 0.537169
  731827/2000000: episode: 1052, duration: 43.506s, episode steps: 815, steps per second:  19, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.248 [0.000, 5.000],  loss: 0.023572, mae: 2.300995, mean_q: 2.794187, mean_eps: 0.536769
  732304/2000000: episode: 1053, duration: 25.938s, episode steps: 477, steps per second:  18, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 0.025345, mae: 2.309896, mean_q: 2.802418, mean_eps: 0.536359
  733257/2000000: episode: 1054, duration: 51.519s, episode steps: 953, steps per second:  18, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.024931, mae: 2.290194, mean_q: 2.780529, mean_eps: 0.535906
  733731/2000000: episode: 1055, duration: 25.860s, episode steps: 474, steps per second:  18, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.717 [0.000, 5.000],  loss: 0.025431, mae: 2.315876, mean_q: 2.813726, mean_eps: 0.535454
  734203/2000000: episode: 1056, duration: 25.867s, episode steps: 472, steps per second:  18, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.027864, mae: 2.296742, mean_q: 2.788470, mean_eps: 0.535155
  734602/2000000: episode: 1057, duration: 21.392s, episode steps: 399, steps per second:  19, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.692 [0.000, 5.000],  loss: 0.025633, mae: 2.285278, mean_q: 2.776046, mean_eps: 0.534879
  735360/2000000: episode: 1058, duration: 41.072s, episode steps: 758, steps per second:  18, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.228 [0.000, 5.000],  loss: 0.026805, mae: 2.300080, mean_q: 2.792603, mean_eps: 0.534513
  735887/2000000: episode: 1059, duration: 28.536s, episode steps: 527, steps per second:  18, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.763 [0.000, 5.000],  loss: 0.024500, mae: 2.274800, mean_q: 2.761354, mean_eps: 0.534106
  736266/2000000: episode: 1060, duration: 20.290s, episode steps: 379, steps per second:  19, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.024063, mae: 2.272102, mean_q: 2.759495, mean_eps: 0.533819
  736768/2000000: episode: 1061, duration: 26.638s, episode steps: 502, steps per second:  19, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.024507, mae: 2.306752, mean_q: 2.801782, mean_eps: 0.533540
  737524/2000000: episode: 1062, duration: 40.179s, episode steps: 756, steps per second:  19, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 0.025415, mae: 2.301200, mean_q: 2.793039, mean_eps: 0.533142
  738229/2000000: episode: 1063, duration: 37.765s, episode steps: 705, steps per second:  19, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.027322, mae: 2.295534, mean_q: 2.784987, mean_eps: 0.532679
  739201/2000000: episode: 1064, duration: 51.913s, episode steps: 972, steps per second:  19, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: 0.024494, mae: 2.293170, mean_q: 2.784947, mean_eps: 0.532147
  740257/2000000: episode: 1065, duration: 56.898s, episode steps: 1056, steps per second:  19, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.265 [0.000, 5.000],  loss: 0.024299, mae: 2.283006, mean_q: 2.773068, mean_eps: 0.531504
  740751/2000000: episode: 1066, duration: 27.083s, episode steps: 494, steps per second:  18, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.022866, mae: 2.302296, mean_q: 2.795743, mean_eps: 0.531014
  741297/2000000: episode: 1067, duration: 30.174s, episode steps: 546, steps per second:  18, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.025448, mae: 2.300144, mean_q: 2.794568, mean_eps: 0.530685
  741925/2000000: episode: 1068, duration: 34.175s, episode steps: 628, steps per second:  18, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.024418, mae: 2.275950, mean_q: 2.763809, mean_eps: 0.530312
  742558/2000000: episode: 1069, duration: 34.345s, episode steps: 633, steps per second:  18, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.027109, mae: 2.288042, mean_q: 2.780159, mean_eps: 0.529913
  743867/2000000: episode: 1070, duration: 71.161s, episode steps: 1309, steps per second:  18, episode reward: 29.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.257 [0.000, 5.000],  loss: 0.026197, mae: 2.284460, mean_q: 2.773360, mean_eps: 0.529299
  744673/2000000: episode: 1071, duration: 42.662s, episode steps: 806, steps per second:  19, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.028055, mae: 2.285424, mean_q: 2.774348, mean_eps: 0.528629
  745318/2000000: episode: 1072, duration: 34.460s, episode steps: 645, steps per second:  19, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.025272, mae: 2.295796, mean_q: 2.787007, mean_eps: 0.528169
  746282/2000000: episode: 1073, duration: 51.234s, episode steps: 964, steps per second:  19, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.299 [0.000, 5.000],  loss: 0.025286, mae: 2.296081, mean_q: 2.789439, mean_eps: 0.527660
  746903/2000000: episode: 1074, duration: 33.315s, episode steps: 621, steps per second:  19, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.662 [0.000, 5.000],  loss: 0.027078, mae: 2.273283, mean_q: 2.760834, mean_eps: 0.527158
  747879/2000000: episode: 1075, duration: 52.180s, episode steps: 976, steps per second:  19, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.026211, mae: 2.282609, mean_q: 2.772433, mean_eps: 0.526653
  748614/2000000: episode: 1076, duration: 39.272s, episode steps: 735, steps per second:  19, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.025861, mae: 2.298875, mean_q: 2.790152, mean_eps: 0.526111
  749473/2000000: episode: 1077, duration: 45.929s, episode steps: 859, steps per second:  19, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.025850, mae: 2.288235, mean_q: 2.777080, mean_eps: 0.525605
  750462/2000000: episode: 1078, duration: 54.862s, episode steps: 989, steps per second:  18, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.024274, mae: 2.320234, mean_q: 2.815094, mean_eps: 0.525020
  750935/2000000: episode: 1079, duration: 25.474s, episode steps: 473, steps per second:  19, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.023 [0.000, 5.000],  loss: 0.026613, mae: 2.353723, mean_q: 2.861797, mean_eps: 0.524558
  751512/2000000: episode: 1080, duration: 31.346s, episode steps: 577, steps per second:  18, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.024958, mae: 2.327104, mean_q: 2.823343, mean_eps: 0.524226
  752237/2000000: episode: 1081, duration: 39.350s, episode steps: 725, steps per second:  18, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.025747, mae: 2.355683, mean_q: 2.857588, mean_eps: 0.523813
  752955/2000000: episode: 1082, duration: 38.545s, episode steps: 718, steps per second:  19, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.188 [0.000, 5.000],  loss: 0.028845, mae: 2.339568, mean_q: 2.837096, mean_eps: 0.523356
  753622/2000000: episode: 1083, duration: 35.726s, episode steps: 667, steps per second:  19, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.026742, mae: 2.353462, mean_q: 2.856806, mean_eps: 0.522918
  754559/2000000: episode: 1084, duration: 51.081s, episode steps: 937, steps per second:  18, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.027168, mae: 2.361026, mean_q: 2.866732, mean_eps: 0.522410
  755110/2000000: episode: 1085, duration: 29.762s, episode steps: 551, steps per second:  19, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.027480, mae: 2.340217, mean_q: 2.838778, mean_eps: 0.521938
  755830/2000000: episode: 1086, duration: 39.022s, episode steps: 720, steps per second:  18, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.026436, mae: 2.359749, mean_q: 2.863227, mean_eps: 0.521536
  756427/2000000: episode: 1087, duration: 32.295s, episode steps: 597, steps per second:  18, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.206 [0.000, 5.000],  loss: 0.027991, mae: 2.344828, mean_q: 2.845963, mean_eps: 0.521119
  756875/2000000: episode: 1088, duration: 24.577s, episode steps: 448, steps per second:  18, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.026363, mae: 2.342463, mean_q: 2.841010, mean_eps: 0.520788
  757952/2000000: episode: 1089, duration: 58.217s, episode steps: 1077, steps per second:  18, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.026384, mae: 2.333447, mean_q: 2.831553, mean_eps: 0.520306
  758350/2000000: episode: 1090, duration: 21.725s, episode steps: 398, steps per second:  18, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.746 [0.000, 5.000],  loss: 0.026815, mae: 2.338391, mean_q: 2.835438, mean_eps: 0.519838
  758738/2000000: episode: 1091, duration: 20.872s, episode steps: 388, steps per second:  19, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.245 [0.000, 5.000],  loss: 0.027924, mae: 2.359189, mean_q: 2.863634, mean_eps: 0.519589
  759367/2000000: episode: 1092, duration: 33.799s, episode steps: 629, steps per second:  19, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.924 [0.000, 5.000],  loss: 0.026213, mae: 2.348421, mean_q: 2.846925, mean_eps: 0.519267
  759755/2000000: episode: 1093, duration: 20.920s, episode steps: 388, steps per second:  19, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.028836, mae: 2.382540, mean_q: 2.888961, mean_eps: 0.518945
  760186/2000000: episode: 1094, duration: 23.558s, episode steps: 431, steps per second:  18, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.026812, mae: 2.396625, mean_q: 2.908036, mean_eps: 0.518686
  761051/2000000: episode: 1095, duration: 46.538s, episode steps: 865, steps per second:  19, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.024669, mae: 2.405143, mean_q: 2.920449, mean_eps: 0.518275
  761419/2000000: episode: 1096, duration: 19.971s, episode steps: 368, steps per second:  18, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.832 [0.000, 5.000],  loss: 0.024568, mae: 2.409227, mean_q: 2.926899, mean_eps: 0.517885
  761918/2000000: episode: 1097, duration: 27.421s, episode steps: 499, steps per second:  18, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.216 [0.000, 5.000],  loss: 0.026332, mae: 2.418909, mean_q: 2.939046, mean_eps: 0.517610
  762572/2000000: episode: 1098, duration: 35.037s, episode steps: 654, steps per second:  19, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.030361, mae: 2.417056, mean_q: 2.934043, mean_eps: 0.517245
  763270/2000000: episode: 1099, duration: 37.842s, episode steps: 698, steps per second:  18, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.026749, mae: 2.438313, mean_q: 2.959553, mean_eps: 0.516817
  764114/2000000: episode: 1100, duration: 45.983s, episode steps: 844, steps per second:  18, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.028740, mae: 2.410910, mean_q: 2.925649, mean_eps: 0.516328
  764594/2000000: episode: 1101, duration: 26.008s, episode steps: 480, steps per second:  18, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.027046, mae: 2.424126, mean_q: 2.942420, mean_eps: 0.515909
  765577/2000000: episode: 1102, duration: 53.478s, episode steps: 983, steps per second:  18, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.026475, mae: 2.424573, mean_q: 2.942750, mean_eps: 0.515446
  766327/2000000: episode: 1103, duration: 40.818s, episode steps: 750, steps per second:  18, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.025585, mae: 2.414366, mean_q: 2.931276, mean_eps: 0.514897
  766722/2000000: episode: 1104, duration: 21.589s, episode steps: 395, steps per second:  18, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.099 [0.000, 5.000],  loss: 0.025930, mae: 2.396862, mean_q: 2.910140, mean_eps: 0.514535
  767100/2000000: episode: 1105, duration: 21.021s, episode steps: 378, steps per second:  18, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.028352, mae: 2.411078, mean_q: 2.927445, mean_eps: 0.514290
  767647/2000000: episode: 1106, duration: 30.069s, episode steps: 547, steps per second:  18, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.196 [0.000, 5.000],  loss: 0.029253, mae: 2.421283, mean_q: 2.938812, mean_eps: 0.513998
  768655/2000000: episode: 1107, duration: 54.701s, episode steps: 1008, steps per second:  18, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.028122, mae: 2.397627, mean_q: 2.909990, mean_eps: 0.513505
  769280/2000000: episode: 1108, duration: 35.220s, episode steps: 625, steps per second:  18, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.030562, mae: 2.422134, mean_q: 2.942519, mean_eps: 0.512988
  770021/2000000: episode: 1109, duration: 40.172s, episode steps: 741, steps per second:  18, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.028586, mae: 2.416845, mean_q: 2.934658, mean_eps: 0.512555
  770408/2000000: episode: 1110, duration: 21.138s, episode steps: 387, steps per second:  18, episode reward: 12.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.948 [0.000, 5.000],  loss: 0.022880, mae: 2.388370, mean_q: 2.902765, mean_eps: 0.512198
  771343/2000000: episode: 1111, duration: 51.482s, episode steps: 935, steps per second:  18, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.025138, mae: 2.392508, mean_q: 2.906232, mean_eps: 0.511780
  772188/2000000: episode: 1112, duration: 46.231s, episode steps: 845, steps per second:  18, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.024646, mae: 2.403646, mean_q: 2.918223, mean_eps: 0.511216
  772970/2000000: episode: 1113, duration: 43.554s, episode steps: 782, steps per second:  18, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.024748, mae: 2.393408, mean_q: 2.904666, mean_eps: 0.510701
  773918/2000000: episode: 1114, duration: 51.589s, episode steps: 948, steps per second:  18, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.026617, mae: 2.406382, mean_q: 2.922676, mean_eps: 0.510152
  774514/2000000: episode: 1115, duration: 32.297s, episode steps: 596, steps per second:  18, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.026082, mae: 2.398512, mean_q: 2.911580, mean_eps: 0.509663
  775195/2000000: episode: 1116, duration: 36.700s, episode steps: 681, steps per second:  19, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.291 [0.000, 5.000],  loss: 0.026315, mae: 2.394717, mean_q: 2.909292, mean_eps: 0.509259
  775686/2000000: episode: 1117, duration: 26.411s, episode steps: 491, steps per second:  19, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.825 [0.000, 5.000],  loss: 0.027386, mae: 2.378869, mean_q: 2.885846, mean_eps: 0.508888
  776696/2000000: episode: 1118, duration: 54.866s, episode steps: 1010, steps per second:  18, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.647 [0.000, 5.000],  loss: 0.025228, mae: 2.378429, mean_q: 2.887546, mean_eps: 0.508413
  777447/2000000: episode: 1119, duration: 40.914s, episode steps: 751, steps per second:  18, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.029184, mae: 2.386030, mean_q: 2.896652, mean_eps: 0.507856
  778154/2000000: episode: 1120, duration: 38.563s, episode steps: 707, steps per second:  18, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.351 [0.000, 5.000],  loss: 0.029099, mae: 2.397715, mean_q: 2.908794, mean_eps: 0.507393
  778685/2000000: episode: 1121, duration: 29.051s, episode steps: 531, steps per second:  18, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.028238, mae: 2.382543, mean_q: 2.892974, mean_eps: 0.507001
  779405/2000000: episode: 1122, duration: 39.143s, episode steps: 720, steps per second:  18, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.332 [0.000, 5.000],  loss: 0.026425, mae: 2.400851, mean_q: 2.911209, mean_eps: 0.506604
  779844/2000000: episode: 1123, duration: 24.098s, episode steps: 439, steps per second:  18, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.729 [0.000, 5.000],  loss: 0.025393, mae: 2.386562, mean_q: 2.895468, mean_eps: 0.506238
  780540/2000000: episode: 1124, duration: 37.845s, episode steps: 696, steps per second:  18, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.025686, mae: 2.413561, mean_q: 2.932052, mean_eps: 0.505880
  781352/2000000: episode: 1125, duration: 43.949s, episode steps: 812, steps per second:  18, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.024912, mae: 2.411318, mean_q: 2.927580, mean_eps: 0.505402
  782070/2000000: episode: 1126, duration: 38.897s, episode steps: 718, steps per second:  18, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.928 [0.000, 5.000],  loss: 0.027236, mae: 2.409814, mean_q: 2.926176, mean_eps: 0.504917
  782949/2000000: episode: 1127, duration: 47.559s, episode steps: 879, steps per second:  18, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.039897, mae: 2.398273, mean_q: 2.911109, mean_eps: 0.504410
  783952/2000000: episode: 1128, duration: 54.156s, episode steps: 1003, steps per second:  19, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.029475, mae: 2.393138, mean_q: 2.906611, mean_eps: 0.503815
  784616/2000000: episode: 1129, duration: 36.116s, episode steps: 664, steps per second:  18, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.029254, mae: 2.389217, mean_q: 2.899442, mean_eps: 0.503288
  785524/2000000: episode: 1130, duration: 49.456s, episode steps: 908, steps per second:  18, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.026476, mae: 2.411777, mean_q: 2.925780, mean_eps: 0.502790
  786080/2000000: episode: 1131, duration: 30.408s, episode steps: 556, steps per second:  18, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.284 [0.000, 5.000],  loss: 0.031921, mae: 2.408975, mean_q: 2.922977, mean_eps: 0.502327
  786725/2000000: episode: 1132, duration: 35.296s, episode steps: 645, steps per second:  18, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.026966, mae: 2.409618, mean_q: 2.923738, mean_eps: 0.501945
  787114/2000000: episode: 1133, duration: 21.889s, episode steps: 389, steps per second:  18, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.025119, mae: 2.377630, mean_q: 2.882283, mean_eps: 0.501617
  788316/2000000: episode: 1134, duration: 66.291s, episode steps: 1202, steps per second:  18, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.027532, mae: 2.416377, mean_q: 2.931370, mean_eps: 0.501114
  789006/2000000: episode: 1135, duration: 38.420s, episode steps: 690, steps per second:  18, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.027330, mae: 2.381260, mean_q: 2.890486, mean_eps: 0.500515
  789949/2000000: episode: 1136, duration: 52.041s, episode steps: 943, steps per second:  18, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.027720, mae: 2.399668, mean_q: 2.911372, mean_eps: 0.499997
  790587/2000000: episode: 1137, duration: 35.243s, episode steps: 638, steps per second:  18, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.024759, mae: 2.415495, mean_q: 2.931679, mean_eps: 0.499497
  791323/2000000: episode: 1138, duration: 40.173s, episode steps: 736, steps per second:  18, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.023585, mae: 2.389483, mean_q: 2.899139, mean_eps: 0.499062
  791927/2000000: episode: 1139, duration: 32.893s, episode steps: 604, steps per second:  18, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.026054, mae: 2.398407, mean_q: 2.908322, mean_eps: 0.498638
  792624/2000000: episode: 1140, duration: 38.012s, episode steps: 697, steps per second:  18, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.816 [0.000, 5.000],  loss: 0.026599, mae: 2.389792, mean_q: 2.899365, mean_eps: 0.498226
  793135/2000000: episode: 1141, duration: 27.991s, episode steps: 511, steps per second:  18, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.652 [0.000, 5.000],  loss: 0.026410, mae: 2.390169, mean_q: 2.897434, mean_eps: 0.497844
  793539/2000000: episode: 1142, duration: 22.120s, episode steps: 404, steps per second:  18, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.024622, mae: 2.412603, mean_q: 2.923389, mean_eps: 0.497554
  794196/2000000: episode: 1143, duration: 35.911s, episode steps: 657, steps per second:  18, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.714 [0.000, 5.000],  loss: 0.029289, mae: 2.390489, mean_q: 2.898374, mean_eps: 0.497218
  794692/2000000: episode: 1144, duration: 27.226s, episode steps: 496, steps per second:  18, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.028847, mae: 2.399864, mean_q: 2.911594, mean_eps: 0.496853
  795936/2000000: episode: 1145, duration: 67.944s, episode steps: 1244, steps per second:  18, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.027840, mae: 2.399256, mean_q: 2.909532, mean_eps: 0.496302
  796496/2000000: episode: 1146, duration: 30.800s, episode steps: 560, steps per second:  18, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.027027, mae: 2.403426, mean_q: 2.913117, mean_eps: 0.495731
  796964/2000000: episode: 1147, duration: 25.836s, episode steps: 468, steps per second:  18, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.797 [0.000, 5.000],  loss: 0.027194, mae: 2.383813, mean_q: 2.892435, mean_eps: 0.495406
  797764/2000000: episode: 1148, duration: 44.097s, episode steps: 800, steps per second:  18, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.027620, mae: 2.390159, mean_q: 2.898835, mean_eps: 0.495004
  798713/2000000: episode: 1149, duration: 52.132s, episode steps: 949, steps per second:  18, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.028686, mae: 2.384582, mean_q: 2.893801, mean_eps: 0.494449
  799695/2000000: episode: 1150, duration: 53.420s, episode steps: 982, steps per second:  18, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.028589, mae: 2.379206, mean_q: 2.888004, mean_eps: 0.493837
  800118/2000000: episode: 1151, duration: 24.311s, episode steps: 423, steps per second:  17, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.771 [0.000, 5.000],  loss: 0.024656, mae: 2.387943, mean_q: 2.896077, mean_eps: 0.493393
  801000/2000000: episode: 1152, duration: 48.178s, episode steps: 882, steps per second:  18, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.024506, mae: 2.433922, mean_q: 2.951681, mean_eps: 0.492980
  801740/2000000: episode: 1153, duration: 40.584s, episode steps: 740, steps per second:  18, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: 0.025392, mae: 2.439302, mean_q: 2.957080, mean_eps: 0.492467
  802513/2000000: episode: 1154, duration: 42.420s, episode steps: 773, steps per second:  18, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.027031, mae: 2.440139, mean_q: 2.958829, mean_eps: 0.491987
  803395/2000000: episode: 1155, duration: 48.336s, episode steps: 882, steps per second:  18, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.714 [0.000, 5.000],  loss: 0.027422, mae: 2.448114, mean_q: 2.967503, mean_eps: 0.491462
  804402/2000000: episode: 1156, duration: 55.567s, episode steps: 1007, steps per second:  18, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.026787, mae: 2.440794, mean_q: 2.960081, mean_eps: 0.490865
  805195/2000000: episode: 1157, duration: 43.493s, episode steps: 793, steps per second:  18, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.027465, mae: 2.412157, mean_q: 2.922186, mean_eps: 0.490295
  805720/2000000: episode: 1158, duration: 29.560s, episode steps: 525, steps per second:  18, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.027634, mae: 2.434802, mean_q: 2.950728, mean_eps: 0.489878
  806564/2000000: episode: 1159, duration: 46.208s, episode steps: 844, steps per second:  18, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.027752, mae: 2.439047, mean_q: 2.956533, mean_eps: 0.489445
  807051/2000000: episode: 1160, duration: 26.698s, episode steps: 487, steps per second:  18, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.205 [0.000, 5.000],  loss: 0.030675, mae: 2.443423, mean_q: 2.961411, mean_eps: 0.489023
  808061/2000000: episode: 1161, duration: 55.660s, episode steps: 1010, steps per second:  18, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.026509, mae: 2.433740, mean_q: 2.950476, mean_eps: 0.488548
  808697/2000000: episode: 1162, duration: 34.903s, episode steps: 636, steps per second:  18, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.025176, mae: 2.430037, mean_q: 2.946406, mean_eps: 0.488026
  809314/2000000: episode: 1163, duration: 33.953s, episode steps: 617, steps per second:  18, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.026697, mae: 2.426310, mean_q: 2.942511, mean_eps: 0.487630
  810365/2000000: episode: 1164, duration: 58.264s, episode steps: 1051, steps per second:  18, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.026706, mae: 2.437880, mean_q: 2.956636, mean_eps: 0.487101
  811356/2000000: episode: 1165, duration: 55.082s, episode steps: 991, steps per second:  18, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.024763, mae: 2.445376, mean_q: 2.966684, mean_eps: 0.486455
  812262/2000000: episode: 1166, duration: 50.137s, episode steps: 906, steps per second:  18, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.024016, mae: 2.439373, mean_q: 2.957200, mean_eps: 0.485855
  813174/2000000: episode: 1167, duration: 50.526s, episode steps: 912, steps per second:  18, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.028478, mae: 2.433494, mean_q: 2.949548, mean_eps: 0.485279
  813517/2000000: episode: 1168, duration: 19.060s, episode steps: 343, steps per second:  18, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.700 [0.000, 5.000],  loss: 0.027965, mae: 2.452845, mean_q: 2.971676, mean_eps: 0.484881
  814008/2000000: episode: 1169, duration: 27.298s, episode steps: 491, steps per second:  18, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.029769, mae: 2.460844, mean_q: 2.983515, mean_eps: 0.484617
  814554/2000000: episode: 1170, duration: 30.414s, episode steps: 546, steps per second:  18, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.267 [0.000, 5.000],  loss: 0.029035, mae: 2.450945, mean_q: 2.969970, mean_eps: 0.484289
  815565/2000000: episode: 1171, duration: 55.778s, episode steps: 1011, steps per second:  18, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.027001, mae: 2.452148, mean_q: 2.973017, mean_eps: 0.483795
  816529/2000000: episode: 1172, duration: 53.634s, episode steps: 964, steps per second:  18, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.217 [0.000, 5.000],  loss: 0.030631, mae: 2.445205, mean_q: 2.963751, mean_eps: 0.483170
  817026/2000000: episode: 1173, duration: 27.333s, episode steps: 497, steps per second:  18, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.829 [0.000, 5.000],  loss: 0.027666, mae: 2.432898, mean_q: 2.948020, mean_eps: 0.482707
  817683/2000000: episode: 1174, duration: 36.449s, episode steps: 657, steps per second:  18, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.026126, mae: 2.449133, mean_q: 2.969878, mean_eps: 0.482342
  818068/2000000: episode: 1175, duration: 21.373s, episode steps: 385, steps per second:  18, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.028325, mae: 2.416636, mean_q: 2.928859, mean_eps: 0.482013
  818900/2000000: episode: 1176, duration: 46.564s, episode steps: 832, steps per second:  18, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.025776, mae: 2.437892, mean_q: 2.953727, mean_eps: 0.481628
  819300/2000000: episode: 1177, duration: 22.294s, episode steps: 400, steps per second:  18, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.028593, mae: 2.470342, mean_q: 2.991857, mean_eps: 0.481238
  820456/2000000: episode: 1178, duration: 64.820s, episode steps: 1156, steps per second:  18, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.025060, mae: 2.432681, mean_q: 2.951555, mean_eps: 0.480745
  820939/2000000: episode: 1179, duration: 27.023s, episode steps: 483, steps per second:  18, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.025433, mae: 2.439903, mean_q: 2.960100, mean_eps: 0.480226
  821420/2000000: episode: 1180, duration: 27.012s, episode steps: 481, steps per second:  18, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.033548, mae: 2.437534, mean_q: 2.954571, mean_eps: 0.479921
  821780/2000000: episode: 1181, duration: 20.624s, episode steps: 360, steps per second:  17, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.028181, mae: 2.459451, mean_q: 2.981962, mean_eps: 0.479655
  822752/2000000: episode: 1182, duration: 53.863s, episode steps: 972, steps per second:  18, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.834 [0.000, 5.000],  loss: 0.027315, mae: 2.416847, mean_q: 2.930833, mean_eps: 0.479233
  823391/2000000: episode: 1183, duration: 35.436s, episode steps: 639, steps per second:  18, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.834 [0.000, 5.000],  loss: 0.027003, mae: 2.432742, mean_q: 2.948194, mean_eps: 0.478722
  823796/2000000: episode: 1184, duration: 22.657s, episode steps: 405, steps per second:  18, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.257 [0.000, 5.000],  loss: 0.026693, mae: 2.440108, mean_q: 2.960172, mean_eps: 0.478392
  824623/2000000: episode: 1185, duration: 45.922s, episode steps: 827, steps per second:  18, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.190 [0.000, 5.000],  loss: 0.028136, mae: 2.419026, mean_q: 2.932128, mean_eps: 0.478002
  825227/2000000: episode: 1186, duration: 33.386s, episode steps: 604, steps per second:  18, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.025741, mae: 2.421551, mean_q: 2.934887, mean_eps: 0.477548
  825874/2000000: episode: 1187, duration: 35.909s, episode steps: 647, steps per second:  18, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.026689, mae: 2.423230, mean_q: 2.935328, mean_eps: 0.477152
  826671/2000000: episode: 1188, duration: 44.519s, episode steps: 797, steps per second:  18, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.026682, mae: 2.425333, mean_q: 2.939857, mean_eps: 0.476694
  827825/2000000: episode: 1189, duration: 64.487s, episode steps: 1154, steps per second:  18, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.027340, mae: 2.439199, mean_q: 2.955373, mean_eps: 0.476076
  828980/2000000: episode: 1190, duration: 64.962s, episode steps: 1155, steps per second:  18, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.028521, mae: 2.436497, mean_q: 2.951198, mean_eps: 0.475345
  829359/2000000: episode: 1191, duration: 21.609s, episode steps: 379, steps per second:  18, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.027109, mae: 2.423680, mean_q: 2.936238, mean_eps: 0.474860
  830040/2000000: episode: 1192, duration: 38.422s, episode steps: 681, steps per second:  18, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.264 [0.000, 5.000],  loss: 0.027144, mae: 2.422776, mean_q: 2.934450, mean_eps: 0.474525
  830879/2000000: episode: 1193, duration: 47.403s, episode steps: 839, steps per second:  18, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.022826, mae: 2.435643, mean_q: 2.951660, mean_eps: 0.474043
  831458/2000000: episode: 1194, duration: 32.426s, episode steps: 579, steps per second:  18, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.025602, mae: 2.415878, mean_q: 2.925434, mean_eps: 0.473594
  832628/2000000: episode: 1195, duration: 65.785s, episode steps: 1170, steps per second:  18, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.024297, mae: 2.406254, mean_q: 2.914806, mean_eps: 0.473040
  833202/2000000: episode: 1196, duration: 32.168s, episode steps: 574, steps per second:  18, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.028886, mae: 2.425437, mean_q: 2.935794, mean_eps: 0.472488
  833702/2000000: episode: 1197, duration: 28.272s, episode steps: 500, steps per second:  18, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.023351, mae: 2.414156, mean_q: 2.925215, mean_eps: 0.472147
  834445/2000000: episode: 1198, duration: 41.827s, episode steps: 743, steps per second:  18, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.025122, mae: 2.411168, mean_q: 2.921379, mean_eps: 0.471753
  835319/2000000: episode: 1199, duration: 49.142s, episode steps: 874, steps per second:  18, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.024127, mae: 2.405284, mean_q: 2.913886, mean_eps: 0.471241
  836386/2000000: episode: 1200, duration: 60.429s, episode steps: 1067, steps per second:  18, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.027058, mae: 2.405362, mean_q: 2.914737, mean_eps: 0.470627
  837094/2000000: episode: 1201, duration: 40.132s, episode steps: 708, steps per second:  18, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.028486, mae: 2.415107, mean_q: 2.923998, mean_eps: 0.470065
  838062/2000000: episode: 1202, duration: 54.687s, episode steps: 968, steps per second:  18, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.779 [0.000, 5.000],  loss: 0.025843, mae: 2.398586, mean_q: 2.905258, mean_eps: 0.469534
  838964/2000000: episode: 1203, duration: 50.103s, episode steps: 902, steps per second:  18, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.025807, mae: 2.417840, mean_q: 2.928630, mean_eps: 0.468942
  839680/2000000: episode: 1204, duration: 40.032s, episode steps: 716, steps per second:  18, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.025783, mae: 2.405309, mean_q: 2.911439, mean_eps: 0.468431
  840311/2000000: episode: 1205, duration: 35.348s, episode steps: 631, steps per second:  18, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.271 [0.000, 5.000],  loss: 0.024054, mae: 2.424622, mean_q: 2.937887, mean_eps: 0.468004
  840891/2000000: episode: 1206, duration: 32.282s, episode steps: 580, steps per second:  18, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.266 [0.000, 5.000],  loss: 0.024921, mae: 2.446664, mean_q: 2.963481, mean_eps: 0.467620
  841522/2000000: episode: 1207, duration: 35.398s, episode steps: 631, steps per second:  18, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.023665, mae: 2.449337, mean_q: 2.967656, mean_eps: 0.467236
  842375/2000000: episode: 1208, duration: 47.698s, episode steps: 853, steps per second:  18, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.202 [0.000, 5.000],  loss: 0.024192, mae: 2.454801, mean_q: 2.971989, mean_eps: 0.466766
  843389/2000000: episode: 1209, duration: 56.781s, episode steps: 1014, steps per second:  18, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.026843, mae: 2.439660, mean_q: 2.953157, mean_eps: 0.466175
  844180/2000000: episode: 1210, duration: 44.639s, episode steps: 791, steps per second:  18, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.026031, mae: 2.435996, mean_q: 2.948295, mean_eps: 0.465603
  844843/2000000: episode: 1211, duration: 37.614s, episode steps: 663, steps per second:  18, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.026740, mae: 2.450899, mean_q: 2.968038, mean_eps: 0.465144
  845539/2000000: episode: 1212, duration: 39.262s, episode steps: 696, steps per second:  18, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.033382, mae: 2.444103, mean_q: 2.959502, mean_eps: 0.464713
  846586/2000000: episode: 1213, duration: 58.614s, episode steps: 1047, steps per second:  18, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.027461, mae: 2.440385, mean_q: 2.954652, mean_eps: 0.464161
  847417/2000000: episode: 1214, duration: 46.469s, episode steps: 831, steps per second:  18, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.729 [0.000, 5.000],  loss: 0.027630, mae: 2.438977, mean_q: 2.954533, mean_eps: 0.463565
  848675/2000000: episode: 1215, duration: 70.551s, episode steps: 1258, steps per second:  18, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.029369, mae: 2.433949, mean_q: 2.947544, mean_eps: 0.462904
  849254/2000000: episode: 1216, duration: 32.493s, episode steps: 579, steps per second:  18, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.306 [0.000, 5.000],  loss: 0.028422, mae: 2.440784, mean_q: 2.952748, mean_eps: 0.462323
  849996/2000000: episode: 1217, duration: 41.871s, episode steps: 742, steps per second:  18, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.024912, mae: 2.450877, mean_q: 2.968415, mean_eps: 0.461905
  850491/2000000: episode: 1218, duration: 29.114s, episode steps: 495, steps per second:  17, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.184 [0.000, 5.000],  loss: 0.023047, mae: 2.499753, mean_q: 3.028015, mean_eps: 0.461513
  851085/2000000: episode: 1219, duration: 33.930s, episode steps: 594, steps per second:  18, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.026213, mae: 2.533429, mean_q: 3.069475, mean_eps: 0.461168
  851877/2000000: episode: 1220, duration: 46.306s, episode steps: 792, steps per second:  17, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.024657, mae: 2.507634, mean_q: 3.037847, mean_eps: 0.460728
  852543/2000000: episode: 1221, duration: 37.447s, episode steps: 666, steps per second:  18, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.026314, mae: 2.527020, mean_q: 3.061138, mean_eps: 0.460267
  853653/2000000: episode: 1222, duration: 63.236s, episode steps: 1110, steps per second:  18, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.027021, mae: 2.511677, mean_q: 3.042232, mean_eps: 0.459705
  854558/2000000: episode: 1223, duration: 51.215s, episode steps: 905, steps per second:  18, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.027339, mae: 2.528118, mean_q: 3.061299, mean_eps: 0.459066
  855270/2000000: episode: 1224, duration: 40.231s, episode steps: 712, steps per second:  18, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.024766, mae: 2.501709, mean_q: 3.030603, mean_eps: 0.458554
  856390/2000000: episode: 1225, duration: 62.991s, episode steps: 1120, steps per second:  18, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.031823, mae: 2.517220, mean_q: 3.048994, mean_eps: 0.457974
  857075/2000000: episode: 1226, duration: 38.739s, episode steps: 685, steps per second:  18, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.033560, mae: 2.517225, mean_q: 3.048193, mean_eps: 0.457403
  858490/2000000: episode: 1227, duration: 79.915s, episode steps: 1415, steps per second:  18, episode reward: 30.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.029454, mae: 2.524425, mean_q: 3.058611, mean_eps: 0.456738
  859634/2000000: episode: 1228, duration: 64.280s, episode steps: 1144, steps per second:  18, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.313 [0.000, 5.000],  loss: 0.027436, mae: 2.513803, mean_q: 3.045295, mean_eps: 0.455927
  860650/2000000: episode: 1229, duration: 57.364s, episode steps: 1016, steps per second:  18, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.024647, mae: 2.542410, mean_q: 3.081740, mean_eps: 0.455243
  861150/2000000: episode: 1230, duration: 28.183s, episode steps: 500, steps per second:  18, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.732 [0.000, 5.000],  loss: 0.025363, mae: 2.538954, mean_q: 3.076625, mean_eps: 0.454763
  861875/2000000: episode: 1231, duration: 41.174s, episode steps: 725, steps per second:  18, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.690 [0.000, 5.000],  loss: 0.027137, mae: 2.546313, mean_q: 3.086897, mean_eps: 0.454376
  862527/2000000: episode: 1232, duration: 37.108s, episode steps: 652, steps per second:  18, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.026531, mae: 2.533933, mean_q: 3.070943, mean_eps: 0.453940
  863273/2000000: episode: 1233, duration: 42.795s, episode steps: 746, steps per second:  17, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.025309, mae: 2.527212, mean_q: 3.063130, mean_eps: 0.453497
  863902/2000000: episode: 1234, duration: 35.926s, episode steps: 629, steps per second:  18, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.134 [0.000, 5.000],  loss: 0.025615, mae: 2.527112, mean_q: 3.059651, mean_eps: 0.453061
  864827/2000000: episode: 1235, duration: 52.937s, episode steps: 925, steps per second:  17, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.025682, mae: 2.524096, mean_q: 3.057625, mean_eps: 0.452569
  865497/2000000: episode: 1236, duration: 38.375s, episode steps: 670, steps per second:  17, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.024281, mae: 2.537129, mean_q: 3.073016, mean_eps: 0.452064
  866047/2000000: episode: 1237, duration: 31.363s, episode steps: 550, steps per second:  18, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.220 [0.000, 5.000],  loss: 0.025787, mae: 2.538845, mean_q: 3.073846, mean_eps: 0.451678
  866547/2000000: episode: 1238, duration: 28.987s, episode steps: 500, steps per second:  17, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.740 [0.000, 5.000],  loss: 0.026383, mae: 2.522508, mean_q: 3.055446, mean_eps: 0.451346
  867365/2000000: episode: 1239, duration: 46.980s, episode steps: 818, steps per second:  17, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.029041, mae: 2.527656, mean_q: 3.062143, mean_eps: 0.450928
  868022/2000000: episode: 1240, duration: 38.276s, episode steps: 657, steps per second:  17, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.029244, mae: 2.525502, mean_q: 3.056303, mean_eps: 0.450460
  868785/2000000: episode: 1241, duration: 43.874s, episode steps: 763, steps per second:  17, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.030457, mae: 2.517559, mean_q: 3.047091, mean_eps: 0.450011
  869721/2000000: episode: 1242, duration: 54.065s, episode steps: 936, steps per second:  17, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.027967, mae: 2.542267, mean_q: 3.077908, mean_eps: 0.449472
  870380/2000000: episode: 1243, duration: 37.660s, episode steps: 659, steps per second:  17, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.031197, mae: 2.589342, mean_q: 3.137873, mean_eps: 0.448968
  871141/2000000: episode: 1244, duration: 43.487s, episode steps: 761, steps per second:  17, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.024635, mae: 2.590772, mean_q: 3.138778, mean_eps: 0.448519
  871984/2000000: episode: 1245, duration: 47.964s, episode steps: 843, steps per second:  18, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.749 [0.000, 5.000],  loss: 0.023521, mae: 2.587124, mean_q: 3.135575, mean_eps: 0.448011
  872886/2000000: episode: 1246, duration: 51.363s, episode steps: 902, steps per second:  18, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.025934, mae: 2.594694, mean_q: 3.140455, mean_eps: 0.447458
  873766/2000000: episode: 1247, duration: 50.212s, episode steps: 880, steps per second:  18, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.751 [0.000, 5.000],  loss: 0.029146, mae: 2.579746, mean_q: 3.125623, mean_eps: 0.446894
  875211/2000000: episode: 1248, duration: 82.658s, episode steps: 1445, steps per second:  17, episode reward: 29.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.028024, mae: 2.577452, mean_q: 3.119746, mean_eps: 0.446158
  875993/2000000: episode: 1249, duration: 45.191s, episode steps: 782, steps per second:  17, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.689 [0.000, 5.000],  loss: 0.026854, mae: 2.569710, mean_q: 3.109799, mean_eps: 0.445452
  876925/2000000: episode: 1250, duration: 54.134s, episode steps: 932, steps per second:  17, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.773 [0.000, 5.000],  loss: 0.026228, mae: 2.582794, mean_q: 3.127272, mean_eps: 0.444909
  877436/2000000: episode: 1251, duration: 29.953s, episode steps: 511, steps per second:  17, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.025457, mae: 2.580896, mean_q: 3.121183, mean_eps: 0.444453
  878232/2000000: episode: 1252, duration: 46.134s, episode steps: 796, steps per second:  17, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.300 [0.000, 5.000],  loss: 0.024901, mae: 2.557883, mean_q: 3.094850, mean_eps: 0.444040
  878836/2000000: episode: 1253, duration: 35.126s, episode steps: 604, steps per second:  17, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.027337, mae: 2.555463, mean_q: 3.091516, mean_eps: 0.443596
  879528/2000000: episode: 1254, duration: 40.352s, episode steps: 692, steps per second:  17, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.028389, mae: 2.562741, mean_q: 3.098642, mean_eps: 0.443186
  880234/2000000: episode: 1255, duration: 41.479s, episode steps: 706, steps per second:  17, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.802 [0.000, 5.000],  loss: 0.028018, mae: 2.599055, mean_q: 3.146626, mean_eps: 0.442743
  880811/2000000: episode: 1256, duration: 33.293s, episode steps: 577, steps per second:  17, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.025945, mae: 2.608948, mean_q: 3.158376, mean_eps: 0.442336
  881670/2000000: episode: 1257, duration: 50.728s, episode steps: 859, steps per second:  17, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.026199, mae: 2.602050, mean_q: 3.151617, mean_eps: 0.441881
  882306/2000000: episode: 1258, duration: 36.994s, episode steps: 636, steps per second:  17, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.026436, mae: 2.601198, mean_q: 3.150055, mean_eps: 0.441408
  883393/2000000: episode: 1259, duration: 63.696s, episode steps: 1087, steps per second:  17, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.026236, mae: 2.598135, mean_q: 3.146632, mean_eps: 0.440862
  884075/2000000: episode: 1260, duration: 39.290s, episode steps: 682, steps per second:  17, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.025743, mae: 2.586002, mean_q: 3.129205, mean_eps: 0.440302
  884818/2000000: episode: 1261, duration: 43.592s, episode steps: 743, steps per second:  17, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.025271, mae: 2.598004, mean_q: 3.146727, mean_eps: 0.439851
  885778/2000000: episode: 1262, duration: 55.773s, episode steps: 960, steps per second:  17, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.267 [0.000, 5.000],  loss: 0.028140, mae: 2.580211, mean_q: 3.123925, mean_eps: 0.439311
  886480/2000000: episode: 1263, duration: 40.207s, episode steps: 702, steps per second:  17, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.027746, mae: 2.616842, mean_q: 3.169051, mean_eps: 0.438786
  886838/2000000: episode: 1264, duration: 20.610s, episode steps: 358, steps per second:  17, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.026421, mae: 2.606535, mean_q: 3.152884, mean_eps: 0.438450
  887505/2000000: episode: 1265, duration: 38.347s, episode steps: 667, steps per second:  17, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.025741, mae: 2.608576, mean_q: 3.156463, mean_eps: 0.438124
  888445/2000000: episode: 1266, duration: 53.926s, episode steps: 940, steps per second:  17, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.028109, mae: 2.600054, mean_q: 3.146296, mean_eps: 0.437615
  889223/2000000: episode: 1267, duration: 44.855s, episode steps: 778, steps per second:  17, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.206 [0.000, 5.000],  loss: 0.028709, mae: 2.587939, mean_q: 3.130700, mean_eps: 0.437072
  890023/2000000: episode: 1268, duration: 46.578s, episode steps: 800, steps per second:  17, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.027338, mae: 2.602000, mean_q: 3.149072, mean_eps: 0.436573
  890945/2000000: episode: 1269, duration: 53.707s, episode steps: 922, steps per second:  17, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.025005, mae: 2.624732, mean_q: 3.178761, mean_eps: 0.436027
  891675/2000000: episode: 1270, duration: 42.514s, episode steps: 730, steps per second:  17, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.026228, mae: 2.656782, mean_q: 3.217533, mean_eps: 0.435504
  892464/2000000: episode: 1271, duration: 46.364s, episode steps: 789, steps per second:  17, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.233 [0.000, 5.000],  loss: 0.027295, mae: 2.633501, mean_q: 3.186402, mean_eps: 0.435024
  893618/2000000: episode: 1272, duration: 68.092s, episode steps: 1154, steps per second:  17, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.809 [0.000, 5.000],  loss: 0.028188, mae: 2.611120, mean_q: 3.160595, mean_eps: 0.434408
  894645/2000000: episode: 1273, duration: 61.141s, episode steps: 1027, steps per second:  17, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.026026, mae: 2.632079, mean_q: 3.183069, mean_eps: 0.433716
  895578/2000000: episode: 1274, duration: 54.928s, episode steps: 933, steps per second:  17, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 0.028959, mae: 2.622503, mean_q: 3.172436, mean_eps: 0.433096
  896461/2000000: episode: 1275, duration: 52.064s, episode steps: 883, steps per second:  17, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.027894, mae: 2.614228, mean_q: 3.162052, mean_eps: 0.432521
  897184/2000000: episode: 1276, duration: 42.353s, episode steps: 723, steps per second:  17, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.028476, mae: 2.624457, mean_q: 3.176389, mean_eps: 0.432013
  897889/2000000: episode: 1277, duration: 41.779s, episode steps: 705, steps per second:  17, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.753 [0.000, 5.000],  loss: 0.026646, mae: 2.628024, mean_q: 3.182000, mean_eps: 0.431561
  898511/2000000: episode: 1278, duration: 36.472s, episode steps: 622, steps per second:  17, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.855 [0.000, 5.000],  loss: 0.028180, mae: 2.614120, mean_q: 3.161992, mean_eps: 0.431140
  899231/2000000: episode: 1279, duration: 42.395s, episode steps: 720, steps per second:  17, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.026417, mae: 2.604680, mean_q: 3.150705, mean_eps: 0.430716
  899693/2000000: episode: 1280, duration: 27.573s, episode steps: 462, steps per second:  17, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.031035, mae: 2.619730, mean_q: 3.166289, mean_eps: 0.430341
  900402/2000000: episode: 1281, duration: 42.987s, episode steps: 709, steps per second:  16, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.726 [0.000, 5.000],  loss: 0.026737, mae: 2.619387, mean_q: 3.170154, mean_eps: 0.429970
  901050/2000000: episode: 1282, duration: 38.308s, episode steps: 648, steps per second:  17, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.026487, mae: 2.639493, mean_q: 3.192229, mean_eps: 0.429540
  901743/2000000: episode: 1283, duration: 40.059s, episode steps: 693, steps per second:  17, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.026728, mae: 2.616349, mean_q: 3.162657, mean_eps: 0.429116
  902637/2000000: episode: 1284, duration: 51.858s, episode steps: 894, steps per second:  17, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.029575, mae: 2.637306, mean_q: 3.187501, mean_eps: 0.428613
  903309/2000000: episode: 1285, duration: 38.924s, episode steps: 672, steps per second:  17, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.028221, mae: 2.611737, mean_q: 3.154632, mean_eps: 0.428116
  903832/2000000: episode: 1286, duration: 30.578s, episode steps: 523, steps per second:  17, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.172 [0.000, 5.000],  loss: 0.026025, mae: 2.629807, mean_q: 3.178197, mean_eps: 0.427739
  904883/2000000: episode: 1287, duration: 61.364s, episode steps: 1051, steps per second:  17, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.027515, mae: 2.631018, mean_q: 3.179543, mean_eps: 0.427241
  906266/2000000: episode: 1288, duration: 81.404s, episode steps: 1383, steps per second:  17, episode reward: 26.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.792 [0.000, 5.000],  loss: 0.028682, mae: 2.624885, mean_q: 3.173034, mean_eps: 0.426470
  906992/2000000: episode: 1289, duration: 42.734s, episode steps: 726, steps per second:  17, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.157 [0.000, 5.000],  loss: 0.028928, mae: 2.630909, mean_q: 3.178512, mean_eps: 0.425802
  907761/2000000: episode: 1290, duration: 44.940s, episode steps: 769, steps per second:  17, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.636 [0.000, 5.000],  loss: 0.029117, mae: 2.633759, mean_q: 3.184675, mean_eps: 0.425329
  908570/2000000: episode: 1291, duration: 46.939s, episode steps: 809, steps per second:  17, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.027728, mae: 2.632880, mean_q: 3.183085, mean_eps: 0.424828
  909259/2000000: episode: 1292, duration: 40.158s, episode steps: 689, steps per second:  17, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.028037, mae: 2.622525, mean_q: 3.170865, mean_eps: 0.424354
  910075/2000000: episode: 1293, duration: 47.357s, episode steps: 816, steps per second:  17, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: 0.027633, mae: 2.622275, mean_q: 3.170358, mean_eps: 0.423878
  910886/2000000: episode: 1294, duration: 47.086s, episode steps: 811, steps per second:  17, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.026664, mae: 2.664740, mean_q: 3.222949, mean_eps: 0.423363
  911830/2000000: episode: 1295, duration: 54.889s, episode steps: 944, steps per second:  17, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.027183, mae: 2.671169, mean_q: 3.229066, mean_eps: 0.422807
  913156/2000000: episode: 1296, duration: 77.496s, episode steps: 1326, steps per second:  17, episode reward: 21.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.028284, mae: 2.678717, mean_q: 3.238011, mean_eps: 0.422088
  913842/2000000: episode: 1297, duration: 40.106s, episode steps: 686, steps per second:  17, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.027571, mae: 2.662228, mean_q: 3.218774, mean_eps: 0.421451
  914907/2000000: episode: 1298, duration: 64.009s, episode steps: 1065, steps per second:  17, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.028583, mae: 2.673143, mean_q: 3.230918, mean_eps: 0.420896
  915366/2000000: episode: 1299, duration: 27.230s, episode steps: 459, steps per second:  17, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.028598, mae: 2.663867, mean_q: 3.218509, mean_eps: 0.420414
  916314/2000000: episode: 1300, duration: 56.359s, episode steps: 948, steps per second:  17, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.030614, mae: 2.681498, mean_q: 3.241502, mean_eps: 0.419968
  917050/2000000: episode: 1301, duration: 42.862s, episode steps: 736, steps per second:  17, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.033474, mae: 2.679653, mean_q: 3.238530, mean_eps: 0.419435
  917749/2000000: episode: 1302, duration: 40.589s, episode steps: 699, steps per second:  17, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.031687, mae: 2.655494, mean_q: 3.211788, mean_eps: 0.418980
  918325/2000000: episode: 1303, duration: 33.623s, episode steps: 576, steps per second:  17, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.027895, mae: 2.660671, mean_q: 3.214982, mean_eps: 0.418576
  919015/2000000: episode: 1304, duration: 39.912s, episode steps: 690, steps per second:  17, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.723 [0.000, 5.000],  loss: 0.029896, mae: 2.673055, mean_q: 3.229396, mean_eps: 0.418176
  919974/2000000: episode: 1305, duration: 55.797s, episode steps: 959, steps per second:  17, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.029239, mae: 2.679645, mean_q: 3.239495, mean_eps: 0.417654
  920728/2000000: episode: 1306, duration: 44.181s, episode steps: 754, steps per second:  17, episode reward: 21.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.024137, mae: 2.714810, mean_q: 3.285690, mean_eps: 0.417112
  921801/2000000: episode: 1307, duration: 62.573s, episode steps: 1073, steps per second:  17, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.027288, mae: 2.722806, mean_q: 3.293704, mean_eps: 0.416533
  922749/2000000: episode: 1308, duration: 55.027s, episode steps: 948, steps per second:  17, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.031524, mae: 2.723446, mean_q: 3.291924, mean_eps: 0.415892
  923571/2000000: episode: 1309, duration: 47.735s, episode steps: 822, steps per second:  17, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.029930, mae: 2.721600, mean_q: 3.290884, mean_eps: 0.415332
  924232/2000000: episode: 1310, duration: 38.908s, episode steps: 661, steps per second:  17, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.025828, mae: 2.711610, mean_q: 3.279303, mean_eps: 0.414863
  925015/2000000: episode: 1311, duration: 45.748s, episode steps: 783, steps per second:  17, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.114 [0.000, 5.000],  loss: 0.028428, mae: 2.713672, mean_q: 3.278737, mean_eps: 0.414406
  925642/2000000: episode: 1312, duration: 36.849s, episode steps: 627, steps per second:  17, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.174 [0.000, 5.000],  loss: 0.027362, mae: 2.692689, mean_q: 3.254500, mean_eps: 0.413959
  926441/2000000: episode: 1313, duration: 46.826s, episode steps: 799, steps per second:  17, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.028532, mae: 2.710090, mean_q: 3.276611, mean_eps: 0.413507
  927168/2000000: episode: 1314, duration: 42.626s, episode steps: 727, steps per second:  17, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.029743, mae: 2.718927, mean_q: 3.286338, mean_eps: 0.413024
  927926/2000000: episode: 1315, duration: 44.670s, episode steps: 758, steps per second:  17, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.708 [0.000, 5.000],  loss: 0.028981, mae: 2.729414, mean_q: 3.301106, mean_eps: 0.412554
  928861/2000000: episode: 1316, duration: 54.793s, episode steps: 935, steps per second:  17, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.030354, mae: 2.722342, mean_q: 3.288468, mean_eps: 0.412017
  929352/2000000: episode: 1317, duration: 29.005s, episode steps: 491, steps per second:  17, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.334 [0.000, 5.000],  loss: 0.029675, mae: 2.730331, mean_q: 3.301557, mean_eps: 0.411566
  930331/2000000: episode: 1318, duration: 58.238s, episode steps: 979, steps per second:  17, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.032002, mae: 2.714676, mean_q: 3.283242, mean_eps: 0.411101
  931092/2000000: episode: 1319, duration: 45.561s, episode steps: 761, steps per second:  17, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.027498, mae: 2.745022, mean_q: 3.320680, mean_eps: 0.410550
  931474/2000000: episode: 1320, duration: 23.315s, episode steps: 382, steps per second:  16, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.025413, mae: 2.743061, mean_q: 3.313336, mean_eps: 0.410188
  932088/2000000: episode: 1321, duration: 36.628s, episode steps: 614, steps per second:  17, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.026953, mae: 2.734358, mean_q: 3.304823, mean_eps: 0.409873
  932638/2000000: episode: 1322, duration: 32.534s, episode steps: 550, steps per second:  17, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.711 [0.000, 5.000],  loss: 0.029797, mae: 2.741665, mean_q: 3.312699, mean_eps: 0.409504
  933767/2000000: episode: 1323, duration: 66.254s, episode steps: 1129, steps per second:  17, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.030194, mae: 2.736057, mean_q: 3.305346, mean_eps: 0.408972
  934686/2000000: episode: 1324, duration: 54.033s, episode steps: 919, steps per second:  17, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.822 [0.000, 5.000],  loss: 0.032896, mae: 2.726627, mean_q: 3.296048, mean_eps: 0.408324
  935380/2000000: episode: 1325, duration: 40.953s, episode steps: 694, steps per second:  17, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.133 [0.000, 5.000],  loss: 0.037390, mae: 2.742762, mean_q: 3.311188, mean_eps: 0.407813
  936137/2000000: episode: 1326, duration: 44.753s, episode steps: 757, steps per second:  17, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.044930, mae: 2.742443, mean_q: 3.313505, mean_eps: 0.407353
  936597/2000000: episode: 1327, duration: 27.130s, episode steps: 460, steps per second:  17, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.924 [0.000, 5.000],  loss: 0.033163, mae: 2.734429, mean_q: 3.301625, mean_eps: 0.406967
  937386/2000000: episode: 1328, duration: 46.497s, episode steps: 789, steps per second:  17, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.706 [0.000, 5.000],  loss: 0.031064, mae: 2.736923, mean_q: 3.307379, mean_eps: 0.406572
  938089/2000000: episode: 1329, duration: 41.432s, episode steps: 703, steps per second:  17, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.716 [0.000, 5.000],  loss: 0.028126, mae: 2.726411, mean_q: 3.294164, mean_eps: 0.406099
  938995/2000000: episode: 1330, duration: 53.595s, episode steps: 906, steps per second:  17, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.841 [0.000, 5.000],  loss: 0.026197, mae: 2.741736, mean_q: 3.310836, mean_eps: 0.405590
  940243/2000000: episode: 1331, duration: 73.958s, episode steps: 1248, steps per second:  17, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.266 [0.000, 5.000],  loss: 0.029720, mae: 2.732209, mean_q: 3.301225, mean_eps: 0.404909
  941203/2000000: episode: 1332, duration: 57.147s, episode steps: 960, steps per second:  17, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.222 [0.000, 5.000],  loss: 0.027905, mae: 2.746442, mean_q: 3.319422, mean_eps: 0.404209
  942086/2000000: episode: 1333, duration: 53.137s, episode steps: 883, steps per second:  17, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.034644, mae: 2.738985, mean_q: 3.309164, mean_eps: 0.403625
  942898/2000000: episode: 1334, duration: 48.823s, episode steps: 812, steps per second:  17, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.028487, mae: 2.740947, mean_q: 3.309852, mean_eps: 0.403088
  943466/2000000: episode: 1335, duration: 34.338s, episode steps: 568, steps per second:  17, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.879 [0.000, 5.000],  loss: 0.028303, mae: 2.756714, mean_q: 3.330264, mean_eps: 0.402651
  944476/2000000: episode: 1336, duration: 60.417s, episode steps: 1010, steps per second:  17, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.189 [0.000, 5.000],  loss: 0.036455, mae: 2.735493, mean_q: 3.304329, mean_eps: 0.402152
  945465/2000000: episode: 1337, duration: 59.543s, episode steps: 989, steps per second:  17, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.033716, mae: 2.742786, mean_q: 3.310273, mean_eps: 0.401519
  946088/2000000: episode: 1338, duration: 38.048s, episode steps: 623, steps per second:  16, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.034054, mae: 2.746131, mean_q: 3.316533, mean_eps: 0.401009
  946848/2000000: episode: 1339, duration: 46.033s, episode steps: 760, steps per second:  17, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.033081, mae: 2.757019, mean_q: 3.329201, mean_eps: 0.400572
  948098/2000000: episode: 1340, duration: 74.352s, episode steps: 1250, steps per second:  17, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.031203, mae: 2.735060, mean_q: 3.300014, mean_eps: 0.399934
  949071/2000000: episode: 1341, duration: 57.990s, episode steps: 973, steps per second:  17, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.027576, mae: 2.715161, mean_q: 3.279549, mean_eps: 0.399230
  950124/2000000: episode: 1342, duration: 63.912s, episode steps: 1053, steps per second:  16, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.691 [0.000, 5.000],  loss: 0.026898, mae: 2.747908, mean_q: 3.318636, mean_eps: 0.398589
  950844/2000000: episode: 1343, duration: 43.711s, episode steps: 720, steps per second:  16, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.026546, mae: 2.734450, mean_q: 3.302159, mean_eps: 0.398028
  951537/2000000: episode: 1344, duration: 41.455s, episode steps: 693, steps per second:  17, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.028361, mae: 2.753353, mean_q: 3.326021, mean_eps: 0.397580
  952148/2000000: episode: 1345, duration: 36.718s, episode steps: 611, steps per second:  17, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.727 [0.000, 5.000],  loss: 0.026360, mae: 2.754262, mean_q: 3.326404, mean_eps: 0.397167
  952828/2000000: episode: 1346, duration: 41.129s, episode steps: 680, steps per second:  17, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.026757, mae: 2.731089, mean_q: 3.298969, mean_eps: 0.396759
  953587/2000000: episode: 1347, duration: 45.515s, episode steps: 759, steps per second:  17, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.769 [0.000, 5.000],  loss: 0.027015, mae: 2.752321, mean_q: 3.326303, mean_eps: 0.396303
  954641/2000000: episode: 1348, duration: 63.175s, episode steps: 1054, steps per second:  17, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.026012, mae: 2.749969, mean_q: 3.321525, mean_eps: 0.395728
  955797/2000000: episode: 1349, duration: 69.270s, episode steps: 1156, steps per second:  17, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.026751, mae: 2.738338, mean_q: 3.308293, mean_eps: 0.395027
  956280/2000000: episode: 1350, duration: 28.834s, episode steps: 483, steps per second:  17, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.027926, mae: 2.760758, mean_q: 3.334110, mean_eps: 0.394509
  957026/2000000: episode: 1351, duration: 44.725s, episode steps: 746, steps per second:  17, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.027535, mae: 2.753801, mean_q: 3.326396, mean_eps: 0.394120
  957656/2000000: episode: 1352, duration: 38.098s, episode steps: 630, steps per second:  17, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.941 [0.000, 5.000],  loss: 0.027966, mae: 2.729939, mean_q: 3.299742, mean_eps: 0.393685
  958548/2000000: episode: 1353, duration: 54.173s, episode steps: 892, steps per second:  16, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.693 [0.000, 5.000],  loss: 0.026719, mae: 2.731852, mean_q: 3.302562, mean_eps: 0.393203
  959099/2000000: episode: 1354, duration: 33.706s, episode steps: 551, steps per second:  16, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.027538, mae: 2.730370, mean_q: 3.298145, mean_eps: 0.392746
  959475/2000000: episode: 1355, duration: 23.196s, episode steps: 376, steps per second:  16, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.306 [0.000, 5.000],  loss: 0.028425, mae: 2.733946, mean_q: 3.301188, mean_eps: 0.392452
  960325/2000000: episode: 1356, duration: 51.864s, episode steps: 850, steps per second:  16, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.027572, mae: 2.741552, mean_q: 3.311776, mean_eps: 0.392063
  960967/2000000: episode: 1357, duration: 39.163s, episode steps: 642, steps per second:  16, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.025054, mae: 2.737800, mean_q: 3.307680, mean_eps: 0.391591
  961612/2000000: episode: 1358, duration: 39.450s, episode steps: 645, steps per second:  16, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.836 [0.000, 5.000],  loss: 0.026452, mae: 2.751594, mean_q: 3.323911, mean_eps: 0.391184
  962259/2000000: episode: 1359, duration: 39.316s, episode steps: 647, steps per second:  16, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.026295, mae: 2.731746, mean_q: 3.299688, mean_eps: 0.390775
  962639/2000000: episode: 1360, duration: 22.737s, episode steps: 380, steps per second:  17, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.832 [0.000, 5.000],  loss: 0.028016, mae: 2.762564, mean_q: 3.336661, mean_eps: 0.390450
  963581/2000000: episode: 1361, duration: 56.426s, episode steps: 942, steps per second:  17, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.680 [0.000, 5.000],  loss: 0.027254, mae: 2.738680, mean_q: 3.306492, mean_eps: 0.390030
  964119/2000000: episode: 1362, duration: 32.170s, episode steps: 538, steps per second:  17, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.028432, mae: 2.732094, mean_q: 3.295368, mean_eps: 0.389562
  964657/2000000: episode: 1363, duration: 32.547s, episode steps: 538, steps per second:  17, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.861 [0.000, 5.000],  loss: 0.029432, mae: 2.742939, mean_q: 3.314393, mean_eps: 0.389221
  965475/2000000: episode: 1364, duration: 48.994s, episode steps: 818, steps per second:  17, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.026353, mae: 2.743932, mean_q: 3.312631, mean_eps: 0.388792
  966192/2000000: episode: 1365, duration: 43.339s, episode steps: 717, steps per second:  17, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.028346, mae: 2.757130, mean_q: 3.326390, mean_eps: 0.388306
  966681/2000000: episode: 1366, duration: 30.593s, episode steps: 489, steps per second:  16, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.030036, mae: 2.721749, mean_q: 3.288469, mean_eps: 0.387924
  967380/2000000: episode: 1367, duration: 42.301s, episode steps: 699, steps per second:  17, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.263 [0.000, 5.000],  loss: 0.027983, mae: 2.730233, mean_q: 3.295375, mean_eps: 0.387548
  968118/2000000: episode: 1368, duration: 45.110s, episode steps: 738, steps per second:  16, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.030234, mae: 2.747639, mean_q: 3.316068, mean_eps: 0.387093
  968795/2000000: episode: 1369, duration: 40.625s, episode steps: 677, steps per second:  17, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.038163, mae: 2.751143, mean_q: 3.322533, mean_eps: 0.386645
  969485/2000000: episode: 1370, duration: 41.637s, episode steps: 690, steps per second:  17, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.051 [0.000, 5.000],  loss: 0.033876, mae: 2.767191, mean_q: 3.339714, mean_eps: 0.386211
  970432/2000000: episode: 1371, duration: 57.068s, episode steps: 947, steps per second:  17, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.172 [0.000, 5.000],  loss: 0.026992, mae: 2.739807, mean_q: 3.310589, mean_eps: 0.385693
  970808/2000000: episode: 1372, duration: 22.923s, episode steps: 376, steps per second:  16, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.024241, mae: 2.777065, mean_q: 3.355122, mean_eps: 0.385275
  971894/2000000: episode: 1373, duration: 65.581s, episode steps: 1086, steps per second:  17, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.029162, mae: 2.759679, mean_q: 3.334574, mean_eps: 0.384812
  972820/2000000: episode: 1374, duration: 56.028s, episode steps: 926, steps per second:  17, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.249 [0.000, 5.000],  loss: 0.026876, mae: 2.778161, mean_q: 3.355939, mean_eps: 0.384175
  973200/2000000: episode: 1375, duration: 23.034s, episode steps: 380, steps per second:  16, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.161 [0.000, 5.000],  loss: 0.024968, mae: 2.761904, mean_q: 3.334944, mean_eps: 0.383762
  973687/2000000: episode: 1376, duration: 29.594s, episode steps: 487, steps per second:  16, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.277 [0.000, 5.000],  loss: 0.026754, mae: 2.756354, mean_q: 3.327427, mean_eps: 0.383487
  974345/2000000: episode: 1377, duration: 40.907s, episode steps: 658, steps per second:  16, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.266 [0.000, 5.000],  loss: 0.025132, mae: 2.756093, mean_q: 3.326971, mean_eps: 0.383123
  974871/2000000: episode: 1378, duration: 32.065s, episode steps: 526, steps per second:  16, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.137 [0.000, 5.000],  loss: 0.026153, mae: 2.765512, mean_q: 3.337876, mean_eps: 0.382748
  975778/2000000: episode: 1379, duration: 55.434s, episode steps: 907, steps per second:  16, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.734 [0.000, 5.000],  loss: 0.026687, mae: 2.755377, mean_q: 3.326440, mean_eps: 0.382295
  976574/2000000: episode: 1380, duration: 48.978s, episode steps: 796, steps per second:  16, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.035159, mae: 2.745207, mean_q: 3.314103, mean_eps: 0.381755
  977342/2000000: episode: 1381, duration: 46.605s, episode steps: 768, steps per second:  16, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.646 [0.000, 5.000],  loss: 0.026623, mae: 2.766653, mean_q: 3.339079, mean_eps: 0.381260
  977896/2000000: episode: 1382, duration: 33.563s, episode steps: 554, steps per second:  17, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.024758, mae: 2.765470, mean_q: 3.338730, mean_eps: 0.380842
  978673/2000000: episode: 1383, duration: 47.244s, episode steps: 777, steps per second:  16, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.210 [0.000, 5.000],  loss: 0.028305, mae: 2.738358, mean_q: 3.306503, mean_eps: 0.380420
  979700/2000000: episode: 1384, duration: 62.480s, episode steps: 1027, steps per second:  16, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.025534, mae: 2.761263, mean_q: 3.333174, mean_eps: 0.379849
  980400/2000000: episode: 1385, duration: 42.486s, episode steps: 700, steps per second:  16, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.777 [0.000, 5.000],  loss: 0.024550, mae: 2.758662, mean_q: 3.332475, mean_eps: 0.379303
  981093/2000000: episode: 1386, duration: 42.025s, episode steps: 693, steps per second:  16, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.024977, mae: 2.759620, mean_q: 3.334291, mean_eps: 0.378861
  981651/2000000: episode: 1387, duration: 33.961s, episode steps: 558, steps per second:  16, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.030081, mae: 2.776738, mean_q: 3.354318, mean_eps: 0.378464
  982266/2000000: episode: 1388, duration: 37.611s, episode steps: 615, steps per second:  16, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.026755, mae: 2.747512, mean_q: 3.320001, mean_eps: 0.378093
  982897/2000000: episode: 1389, duration: 38.380s, episode steps: 631, steps per second:  16, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.168 [0.000, 5.000],  loss: 0.027901, mae: 2.753129, mean_q: 3.322835, mean_eps: 0.377698
  983453/2000000: episode: 1390, duration: 33.987s, episode steps: 556, steps per second:  16, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.757 [0.000, 5.000],  loss: 0.026438, mae: 2.767663, mean_q: 3.341375, mean_eps: 0.377322
  984038/2000000: episode: 1391, duration: 35.557s, episode steps: 585, steps per second:  16, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.026781, mae: 2.752444, mean_q: 3.325726, mean_eps: 0.376961
  984908/2000000: episode: 1392, duration: 53.198s, episode steps: 870, steps per second:  16, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: 0.028473, mae: 2.759389, mean_q: 3.332638, mean_eps: 0.376501
  985539/2000000: episode: 1393, duration: 38.414s, episode steps: 631, steps per second:  16, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.658 [0.000, 5.000],  loss: 0.026473, mae: 2.750182, mean_q: 3.322604, mean_eps: 0.376026
  986184/2000000: episode: 1394, duration: 39.617s, episode steps: 645, steps per second:  16, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.025912, mae: 2.743089, mean_q: 3.313007, mean_eps: 0.375622
  987004/2000000: episode: 1395, duration: 50.179s, episode steps: 820, steps per second:  16, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.026192, mae: 2.757270, mean_q: 3.328918, mean_eps: 0.375158
  987801/2000000: episode: 1396, duration: 48.885s, episode steps: 797, steps per second:  16, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 0.029315, mae: 2.770920, mean_q: 3.344560, mean_eps: 0.374645
  989113/2000000: episode: 1397, duration: 80.776s, episode steps: 1312, steps per second:  16, episode reward: 21.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.033290, mae: 2.766796, mean_q: 3.339609, mean_eps: 0.373977
  989826/2000000: episode: 1398, duration: 43.560s, episode steps: 713, steps per second:  16, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.177 [0.000, 5.000],  loss: 0.026246, mae: 2.733652, mean_q: 3.299482, mean_eps: 0.373336
  990375/2000000: episode: 1399, duration: 33.816s, episode steps: 549, steps per second:  16, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.025643, mae: 2.781012, mean_q: 3.360271, mean_eps: 0.372937
  991368/2000000: episode: 1400, duration: 61.349s, episode steps: 993, steps per second:  16, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.025371, mae: 2.790841, mean_q: 3.371908, mean_eps: 0.372449
  991742/2000000: episode: 1401, duration: 23.215s, episode steps: 374, steps per second:  16, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.025048, mae: 2.770086, mean_q: 3.344792, mean_eps: 0.372016
  992769/2000000: episode: 1402, duration: 62.331s, episode steps: 1027, steps per second:  16, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.299 [0.000, 5.000],  loss: 0.024766, mae: 2.782288, mean_q: 3.359424, mean_eps: 0.371571
  993425/2000000: episode: 1403, duration: 39.938s, episode steps: 656, steps per second:  16, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.024830, mae: 2.793504, mean_q: 3.371017, mean_eps: 0.371038
  993994/2000000: episode: 1404, duration: 34.762s, episode steps: 569, steps per second:  16, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.029408, mae: 2.801606, mean_q: 3.382240, mean_eps: 0.370650
  994617/2000000: episode: 1405, duration: 38.006s, episode steps: 623, steps per second:  16, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.217 [0.000, 5.000],  loss: 0.028143, mae: 2.789333, mean_q: 3.366754, mean_eps: 0.370273
  995291/2000000: episode: 1406, duration: 40.878s, episode steps: 674, steps per second:  16, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.693 [0.000, 5.000],  loss: 0.028548, mae: 2.803176, mean_q: 3.385492, mean_eps: 0.369862
  996360/2000000: episode: 1407, duration: 66.047s, episode steps: 1069, steps per second:  16, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.028602, mae: 2.778066, mean_q: 3.354280, mean_eps: 0.369311
  997090/2000000: episode: 1408, duration: 45.327s, episode steps: 730, steps per second:  16, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.204 [0.000, 5.000],  loss: 0.028441, mae: 2.776896, mean_q: 3.351864, mean_eps: 0.368741
  998180/2000000: episode: 1409, duration: 66.978s, episode steps: 1090, steps per second:  16, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.925 [0.000, 5.000],  loss: 0.031694, mae: 2.783671, mean_q: 3.361122, mean_eps: 0.368165
  999161/2000000: episode: 1410, duration: 60.601s, episode steps: 981, steps per second:  16, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.026603, mae: 2.771618, mean_q: 3.346412, mean_eps: 0.367509
  999522/2000000: episode: 1411, duration: 22.317s, episode steps: 361, steps per second:  16, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.028765, mae: 2.760137, mean_q: 3.330187, mean_eps: 0.367083
 1000474/2000000: episode: 1412, duration: 59.957s, episode steps: 952, steps per second:  16, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.026780, mae: 2.769451, mean_q: 3.346578, mean_eps: 0.366668
 1000986/2000000: episode: 1413, duration: 31.615s, episode steps: 512, steps per second:  16, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.026842, mae: 2.774868, mean_q: 3.351629, mean_eps: 0.366204
 1001499/2000000: episode: 1414, duration: 31.309s, episode steps: 513, steps per second:  16, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.918 [0.000, 5.000],  loss: 0.027253, mae: 2.765038, mean_q: 3.338287, mean_eps: 0.365880
 1002126/2000000: episode: 1415, duration: 39.991s, episode steps: 627, steps per second:  16, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.028175, mae: 2.754460, mean_q: 3.326077, mean_eps: 0.365519
 1003020/2000000: episode: 1416, duration: 54.879s, episode steps: 894, steps per second:  16, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.029948, mae: 2.763356, mean_q: 3.338689, mean_eps: 0.365038
 1003861/2000000: episode: 1417, duration: 52.091s, episode steps: 841, steps per second:  16, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.027669, mae: 2.762362, mean_q: 3.335296, mean_eps: 0.364488
 1004594/2000000: episode: 1418, duration: 45.069s, episode steps: 733, steps per second:  16, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.029807, mae: 2.761033, mean_q: 3.331958, mean_eps: 0.363989
 1004974/2000000: episode: 1419, duration: 23.704s, episode steps: 380, steps per second:  16, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.905 [0.000, 5.000],  loss: 0.027720, mae: 2.761126, mean_q: 3.333670, mean_eps: 0.363637
 1005980/2000000: episode: 1420, duration: 62.370s, episode steps: 1006, steps per second:  16, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.728 [0.000, 5.000],  loss: 0.028296, mae: 2.772892, mean_q: 3.347142, mean_eps: 0.363199
 1006873/2000000: episode: 1421, duration: 55.184s, episode steps: 893, steps per second:  16, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.029587, mae: 2.766558, mean_q: 3.341028, mean_eps: 0.362597
 1007683/2000000: episode: 1422, duration: 49.208s, episode steps: 810, steps per second:  16, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.028637, mae: 2.768918, mean_q: 3.343873, mean_eps: 0.362057
 1008143/2000000: episode: 1423, duration: 28.034s, episode steps: 460, steps per second:  16, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.027019, mae: 2.744453, mean_q: 3.315175, mean_eps: 0.361656
 1008610/2000000: episode: 1424, duration: 28.615s, episode steps: 467, steps per second:  16, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.054 [0.000, 5.000],  loss: 0.027875, mae: 2.740435, mean_q: 3.308483, mean_eps: 0.361362
 1009768/2000000: episode: 1425, duration: 70.340s, episode steps: 1158, steps per second:  16, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.026745, mae: 2.753025, mean_q: 3.324118, mean_eps: 0.360848
 1010684/2000000: episode: 1426, duration: 55.880s, episode steps: 916, steps per second:  16, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.026114, mae: 2.772428, mean_q: 3.349371, mean_eps: 0.360191
 1011525/2000000: episode: 1427, duration: 51.460s, episode steps: 841, steps per second:  16, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.028592, mae: 2.776849, mean_q: 3.351942, mean_eps: 0.359634
 1012402/2000000: episode: 1428, duration: 56.396s, episode steps: 877, steps per second:  16, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.748 [0.000, 5.000],  loss: 0.024777, mae: 2.761773, mean_q: 3.336176, mean_eps: 0.359089
 1013076/2000000: episode: 1429, duration: 46.257s, episode steps: 674, steps per second:  15, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.027084, mae: 2.755591, mean_q: 3.325718, mean_eps: 0.358599
 1013444/2000000: episode: 1430, duration: 23.037s, episode steps: 368, steps per second:  16, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.929 [0.000, 5.000],  loss: 0.027454, mae: 2.749733, mean_q: 3.318500, mean_eps: 0.358270
 1014255/2000000: episode: 1431, duration: 49.988s, episode steps: 811, steps per second:  16, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.025507, mae: 2.756083, mean_q: 3.329184, mean_eps: 0.357896
 1015188/2000000: episode: 1432, duration: 57.406s, episode steps: 933, steps per second:  16, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.029457, mae: 2.748705, mean_q: 3.317711, mean_eps: 0.357344
 1015859/2000000: episode: 1433, duration: 41.115s, episode steps: 671, steps per second:  16, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.647 [0.000, 5.000],  loss: 0.028426, mae: 2.762103, mean_q: 3.334535, mean_eps: 0.356836
 1016376/2000000: episode: 1434, duration: 32.090s, episode steps: 517, steps per second:  16, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.909 [0.000, 5.000],  loss: 0.025592, mae: 2.750312, mean_q: 3.318492, mean_eps: 0.356460
 1017546/2000000: episode: 1435, duration: 71.660s, episode steps: 1170, steps per second:  16, episode reward: 18.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.217 [0.000, 5.000],  loss: 0.026682, mae: 2.768114, mean_q: 3.341439, mean_eps: 0.355925
 1018378/2000000: episode: 1436, duration: 50.793s, episode steps: 832, steps per second:  16, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.027166, mae: 2.764866, mean_q: 3.335901, mean_eps: 0.355291
 1018926/2000000: episode: 1437, duration: 33.458s, episode steps: 548, steps per second:  16, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.042 [0.000, 5.000],  loss: 0.045209, mae: 2.733871, mean_q: 3.299081, mean_eps: 0.354854
 1019276/2000000: episode: 1438, duration: 21.542s, episode steps: 350, steps per second:  16, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.031 [0.000, 5.000],  loss: 0.029088, mae: 2.740803, mean_q: 3.307401, mean_eps: 0.354570
 1020290/2000000: episode: 1439, duration: 62.199s, episode steps: 1014, steps per second:  16, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.024 [0.000, 5.000],  loss: 0.027972, mae: 2.747453, mean_q: 3.317474, mean_eps: 0.354138
 1020783/2000000: episode: 1440, duration: 30.319s, episode steps: 493, steps per second:  16, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.181 [0.000, 5.000],  loss: 0.024758, mae: 2.769577, mean_q: 3.342948, mean_eps: 0.353661
 1021897/2000000: episode: 1441, duration: 67.950s, episode steps: 1114, steps per second:  16, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.724 [0.000, 5.000],  loss: 0.025554, mae: 2.765254, mean_q: 3.336163, mean_eps: 0.353151
 1022593/2000000: episode: 1442, duration: 42.537s, episode steps: 696, steps per second:  16, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.859 [0.000, 5.000],  loss: 0.025414, mae: 2.792213, mean_q: 3.367357, mean_eps: 0.352578
 1023395/2000000: episode: 1443, duration: 48.987s, episode steps: 802, steps per second:  16, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.026154, mae: 2.777955, mean_q: 3.351276, mean_eps: 0.352104
 1024073/2000000: episode: 1444, duration: 41.166s, episode steps: 678, steps per second:  16, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.069 [0.000, 5.000],  loss: 0.025124, mae: 2.783060, mean_q: 3.358073, mean_eps: 0.351635
 1025246/2000000: episode: 1445, duration: 71.118s, episode steps: 1173, steps per second:  16, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.026587, mae: 2.768868, mean_q: 3.341480, mean_eps: 0.351049
 1025916/2000000: episode: 1446, duration: 41.553s, episode steps: 670, steps per second:  16, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.804 [0.000, 5.000],  loss: 0.025547, mae: 2.747029, mean_q: 3.312120, mean_eps: 0.350466
 1026935/2000000: episode: 1447, duration: 62.548s, episode steps: 1019, steps per second:  16, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.027244, mae: 2.756147, mean_q: 3.325176, mean_eps: 0.349931
 1027745/2000000: episode: 1448, duration: 49.420s, episode steps: 810, steps per second:  16, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.204 [0.000, 5.000],  loss: 0.031510, mae: 2.762572, mean_q: 3.334118, mean_eps: 0.349351
 1028414/2000000: episode: 1449, duration: 40.987s, episode steps: 669, steps per second:  16, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.026957, mae: 2.759312, mean_q: 3.327721, mean_eps: 0.348883
 1028954/2000000: episode: 1450, duration: 32.811s, episode steps: 540, steps per second:  16, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.027283, mae: 2.758981, mean_q: 3.325993, mean_eps: 0.348500
 1029478/2000000: episode: 1451, duration: 32.028s, episode steps: 524, steps per second:  16, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.027436, mae: 2.758482, mean_q: 3.328332, mean_eps: 0.348163
 1030155/2000000: episode: 1452, duration: 41.380s, episode steps: 677, steps per second:  16, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.727 [0.000, 5.000],  loss: 0.027103, mae: 2.770892, mean_q: 3.346278, mean_eps: 0.347783
 1030947/2000000: episode: 1453, duration: 48.363s, episode steps: 792, steps per second:  16, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.024045, mae: 2.776704, mean_q: 3.351293, mean_eps: 0.347318
 1031780/2000000: episode: 1454, duration: 50.945s, episode steps: 833, steps per second:  16, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.643 [0.000, 5.000],  loss: 0.026329, mae: 2.768445, mean_q: 3.342301, mean_eps: 0.346804
 1032640/2000000: episode: 1455, duration: 52.849s, episode steps: 860, steps per second:  16, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.024864, mae: 2.770995, mean_q: 3.342685, mean_eps: 0.346268
 1033219/2000000: episode: 1456, duration: 36.709s, episode steps: 579, steps per second:  16, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.173 [0.000, 5.000],  loss: 0.027820, mae: 2.765673, mean_q: 3.338312, mean_eps: 0.345812
 1033740/2000000: episode: 1457, duration: 32.096s, episode steps: 521, steps per second:  16, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.288 [0.000, 5.000],  loss: 0.028630, mae: 2.769195, mean_q: 3.342081, mean_eps: 0.345464
 1034768/2000000: episode: 1458, duration: 63.926s, episode steps: 1028, steps per second:  16, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.027233, mae: 2.761844, mean_q: 3.331514, mean_eps: 0.344974
 1035721/2000000: episode: 1459, duration: 58.712s, episode steps: 953, steps per second:  16, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.694 [0.000, 5.000],  loss: 0.026583, mae: 2.761269, mean_q: 3.332750, mean_eps: 0.344345
 1036515/2000000: episode: 1460, duration: 48.499s, episode steps: 794, steps per second:  16, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.028086, mae: 2.767722, mean_q: 3.338611, mean_eps: 0.343792
 1037062/2000000: episode: 1461, duration: 33.535s, episode steps: 547, steps per second:  16, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.973 [0.000, 5.000],  loss: 0.026347, mae: 2.764898, mean_q: 3.334918, mean_eps: 0.343368
 1038164/2000000: episode: 1462, duration: 67.272s, episode steps: 1102, steps per second:  16, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.152 [0.000, 5.000],  loss: 0.026406, mae: 2.758153, mean_q: 3.328635, mean_eps: 0.342846
 1039150/2000000: episode: 1463, duration: 60.577s, episode steps: 986, steps per second:  16, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.677 [0.000, 5.000],  loss: 0.027802, mae: 2.769006, mean_q: 3.339396, mean_eps: 0.342185
 1040047/2000000: episode: 1464, duration: 54.697s, episode steps: 897, steps per second:  16, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.027524, mae: 2.757524, mean_q: 3.326853, mean_eps: 0.341588
 1040705/2000000: episode: 1465, duration: 40.360s, episode steps: 658, steps per second:  16, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.024043, mae: 2.755530, mean_q: 3.326441, mean_eps: 0.341095
 1041449/2000000: episode: 1466, duration: 45.197s, episode steps: 744, steps per second:  16, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.024368, mae: 2.749461, mean_q: 3.318145, mean_eps: 0.340651
 1042279/2000000: episode: 1467, duration: 50.854s, episode steps: 830, steps per second:  16, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 0.024971, mae: 2.745437, mean_q: 3.313198, mean_eps: 0.340153
 1043527/2000000: episode: 1468, duration: 76.306s, episode steps: 1248, steps per second:  16, episode reward: 25.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.026921, mae: 2.750599, mean_q: 3.320445, mean_eps: 0.339495
 1044379/2000000: episode: 1469, duration: 52.133s, episode steps: 852, steps per second:  16, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.142 [0.000, 5.000],  loss: 0.026394, mae: 2.723559, mean_q: 3.284266, mean_eps: 0.338830
 1045144/2000000: episode: 1470, duration: 46.896s, episode steps: 765, steps per second:  16, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.854 [0.000, 5.000],  loss: 0.027316, mae: 2.743945, mean_q: 3.311114, mean_eps: 0.338319
 1045842/2000000: episode: 1471, duration: 42.895s, episode steps: 698, steps per second:  16, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.026023, mae: 2.763256, mean_q: 3.334924, mean_eps: 0.337855
 1046435/2000000: episode: 1472, duration: 35.857s, episode steps: 593, steps per second:  17, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.143 [0.000, 5.000],  loss: 0.033478, mae: 2.744950, mean_q: 3.310013, mean_eps: 0.337446
 1046804/2000000: episode: 1473, duration: 22.797s, episode steps: 369, steps per second:  16, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.089 [0.000, 5.000],  loss: 0.028052, mae: 2.742574, mean_q: 3.307299, mean_eps: 0.337142
 1048097/2000000: episode: 1474, duration: 79.266s, episode steps: 1293, steps per second:  16, episode reward: 28.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.653 [0.000, 5.000],  loss: 0.026752, mae: 2.752300, mean_q: 3.320106, mean_eps: 0.336615
 1048874/2000000: episode: 1475, duration: 47.386s, episode steps: 777, steps per second:  16, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.726 [0.000, 5.000],  loss: 0.025173, mae: 2.751151, mean_q: 3.319372, mean_eps: 0.335959
 1049679/2000000: episode: 1476, duration: 49.238s, episode steps: 805, steps per second:  16, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.027186, mae: 2.732410, mean_q: 3.299578, mean_eps: 0.335459
 1050083/2000000: episode: 1477, duration: 25.618s, episode steps: 404, steps per second:  16, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.028920, mae: 2.746319, mean_q: 3.313664, mean_eps: 0.335076
 1051207/2000000: episode: 1478, duration: 68.691s, episode steps: 1124, steps per second:  16, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.024927, mae: 2.749224, mean_q: 3.318631, mean_eps: 0.334592
 1051857/2000000: episode: 1479, duration: 40.501s, episode steps: 650, steps per second:  16, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.027026, mae: 2.735223, mean_q: 3.301736, mean_eps: 0.334030
 1052347/2000000: episode: 1480, duration: 29.647s, episode steps: 490, steps per second:  17, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.025161, mae: 2.756298, mean_q: 3.329771, mean_eps: 0.333669
 1053285/2000000: episode: 1481, duration: 57.207s, episode steps: 938, steps per second:  16, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.026438, mae: 2.758359, mean_q: 3.328217, mean_eps: 0.333217
 1054434/2000000: episode: 1482, duration: 69.845s, episode steps: 1149, steps per second:  16, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.026275, mae: 2.759593, mean_q: 3.330038, mean_eps: 0.332555
 1055111/2000000: episode: 1483, duration: 41.397s, episode steps: 677, steps per second:  16, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.025218, mae: 2.757212, mean_q: 3.324997, mean_eps: 0.331978
 1056031/2000000: episode: 1484, duration: 56.182s, episode steps: 920, steps per second:  16, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.024930, mae: 2.747642, mean_q: 3.314569, mean_eps: 0.331472
 1056554/2000000: episode: 1485, duration: 31.890s, episode steps: 523, steps per second:  16, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.233 [0.000, 5.000],  loss: 0.025514, mae: 2.760823, mean_q: 3.329467, mean_eps: 0.331015
 1057359/2000000: episode: 1486, duration: 49.148s, episode steps: 805, steps per second:  16, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.319 [0.000, 5.000],  loss: 0.025723, mae: 2.749185, mean_q: 3.313479, mean_eps: 0.330595
 1058275/2000000: episode: 1487, duration: 55.727s, episode steps: 916, steps per second:  16, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.019 [0.000, 5.000],  loss: 0.024717, mae: 2.752175, mean_q: 3.320703, mean_eps: 0.330050
 1059814/2000000: episode: 1488, duration: 94.104s, episode steps: 1539, steps per second:  16, episode reward: 36.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.026755, mae: 2.736551, mean_q: 3.300332, mean_eps: 0.329272
 1060571/2000000: episode: 1489, duration: 46.283s, episode steps: 757, steps per second:  16, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.025686, mae: 2.756363, mean_q: 3.325974, mean_eps: 0.328545
 1061419/2000000: episode: 1490, duration: 51.491s, episode steps: 848, steps per second:  16, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.025656, mae: 2.723728, mean_q: 3.286307, mean_eps: 0.328037
 1062469/2000000: episode: 1491, duration: 64.399s, episode steps: 1050, steps per second:  16, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.778 [0.000, 5.000],  loss: 0.027725, mae: 2.728017, mean_q: 3.292790, mean_eps: 0.327435
 1063524/2000000: episode: 1492, duration: 64.854s, episode steps: 1055, steps per second:  16, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.747 [0.000, 5.000],  loss: 0.028758, mae: 2.735858, mean_q: 3.300769, mean_eps: 0.326769
 1064027/2000000: episode: 1493, duration: 31.133s, episode steps: 503, steps per second:  16, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.027088, mae: 2.706688, mean_q: 3.262960, mean_eps: 0.326276
 1064981/2000000: episode: 1494, duration: 58.827s, episode steps: 954, steps per second:  16, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.026526, mae: 2.737242, mean_q: 3.300415, mean_eps: 0.325814
 1065808/2000000: episode: 1495, duration: 50.750s, episode steps: 827, steps per second:  16, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: 0.028347, mae: 2.734538, mean_q: 3.297334, mean_eps: 0.325250
 1066463/2000000: episode: 1496, duration: 39.825s, episode steps: 655, steps per second:  16, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.025253, mae: 2.735236, mean_q: 3.298619, mean_eps: 0.324782
 1067065/2000000: episode: 1497, duration: 36.918s, episode steps: 602, steps per second:  16, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.028256, mae: 2.747166, mean_q: 3.313866, mean_eps: 0.324383
 1067889/2000000: episode: 1498, duration: 50.288s, episode steps: 824, steps per second:  16, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.027423, mae: 2.730284, mean_q: 3.292961, mean_eps: 0.323931
 1068584/2000000: episode: 1499, duration: 42.380s, episode steps: 695, steps per second:  16, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.029553, mae: 2.740542, mean_q: 3.306550, mean_eps: 0.323451
 1069619/2000000: episode: 1500, duration: 63.763s, episode steps: 1035, steps per second:  16, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.027374, mae: 2.742056, mean_q: 3.305748, mean_eps: 0.322903
 1070317/2000000: episode: 1501, duration: 43.021s, episode steps: 698, steps per second:  16, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.025384, mae: 2.753847, mean_q: 3.322192, mean_eps: 0.322354
 1071523/2000000: episode: 1502, duration: 73.761s, episode steps: 1206, steps per second:  16, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.024896, mae: 2.773592, mean_q: 3.348035, mean_eps: 0.321751
 1072234/2000000: episode: 1503, duration: 43.952s, episode steps: 711, steps per second:  16, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.029175, mae: 2.756593, mean_q: 3.327270, mean_eps: 0.321144
 1072952/2000000: episode: 1504, duration: 44.114s, episode steps: 718, steps per second:  16, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.026077, mae: 2.759253, mean_q: 3.330461, mean_eps: 0.320692
 1074054/2000000: episode: 1505, duration: 67.882s, episode steps: 1102, steps per second:  16, episode reward: 12.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.715 [0.000, 5.000],  loss: 0.026759, mae: 2.754764, mean_q: 3.323971, mean_eps: 0.320115
 1075256/2000000: episode: 1506, duration: 73.873s, episode steps: 1202, steps per second:  16, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.026231, mae: 2.745691, mean_q: 3.311899, mean_eps: 0.319386
 1075652/2000000: episode: 1507, duration: 24.632s, episode steps: 396, steps per second:  16, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.854 [0.000, 5.000],  loss: 0.029201, mae: 2.735744, mean_q: 3.299538, mean_eps: 0.318880
 1076513/2000000: episode: 1508, duration: 52.742s, episode steps: 861, steps per second:  16, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.026769, mae: 2.756806, mean_q: 3.325770, mean_eps: 0.318481
 1077374/2000000: episode: 1509, duration: 52.724s, episode steps: 861, steps per second:  16, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.256 [0.000, 5.000],  loss: 0.026189, mae: 2.761874, mean_q: 3.331604, mean_eps: 0.317935
 1078176/2000000: episode: 1510, duration: 49.728s, episode steps: 802, steps per second:  16, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.031585, mae: 2.746736, mean_q: 3.313934, mean_eps: 0.317410
 1078758/2000000: episode: 1511, duration: 36.100s, episode steps: 582, steps per second:  16, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.026520, mae: 2.734616, mean_q: 3.304343, mean_eps: 0.316972
 1079505/2000000: episode: 1512, duration: 46.392s, episode steps: 747, steps per second:  16, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.137 [0.000, 5.000],  loss: 0.028959, mae: 2.741686, mean_q: 3.309256, mean_eps: 0.316550
 1080596/2000000: episode: 1513, duration: 66.802s, episode steps: 1091, steps per second:  16, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.026384, mae: 2.761051, mean_q: 3.331981, mean_eps: 0.315968
 1080982/2000000: episode: 1514, duration: 23.713s, episode steps: 386, steps per second:  16, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.635 [0.000, 5.000],  loss: 0.022797, mae: 2.773073, mean_q: 3.345602, mean_eps: 0.315501
 1081372/2000000: episode: 1515, duration: 23.939s, episode steps: 390, steps per second:  16, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.027747, mae: 2.786881, mean_q: 3.362108, mean_eps: 0.315255
 1081884/2000000: episode: 1516, duration: 31.502s, episode steps: 512, steps per second:  16, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.252 [0.000, 5.000],  loss: 0.025713, mae: 2.816215, mean_q: 3.399494, mean_eps: 0.314970
 1082555/2000000: episode: 1517, duration: 41.205s, episode steps: 671, steps per second:  16, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.981 [0.000, 5.000],  loss: 0.028472, mae: 2.770150, mean_q: 3.340231, mean_eps: 0.314595
 1083431/2000000: episode: 1518, duration: 53.771s, episode steps: 876, steps per second:  16, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.028123, mae: 2.773139, mean_q: 3.344870, mean_eps: 0.314105
 1084523/2000000: episode: 1519, duration: 67.277s, episode steps: 1092, steps per second:  16, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.026946, mae: 2.751702, mean_q: 3.320673, mean_eps: 0.313482
 1085166/2000000: episode: 1520, duration: 39.308s, episode steps: 643, steps per second:  16, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 0.031756, mae: 2.766698, mean_q: 3.339537, mean_eps: 0.312932
 1086604/2000000: episode: 1521, duration: 87.783s, episode steps: 1438, steps per second:  16, episode reward: 18.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.028145, mae: 2.766562, mean_q: 3.335526, mean_eps: 0.312273
 1087057/2000000: episode: 1522, duration: 28.045s, episode steps: 453, steps per second:  16, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.249 [0.000, 5.000],  loss: 0.029097, mae: 2.732800, mean_q: 3.295578, mean_eps: 0.311674
 1088134/2000000: episode: 1523, duration: 66.034s, episode steps: 1077, steps per second:  16, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.032744, mae: 2.769438, mean_q: 3.340142, mean_eps: 0.311189
 1088581/2000000: episode: 1524, duration: 27.473s, episode steps: 447, steps per second:  16, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.029805, mae: 2.772216, mean_q: 3.342164, mean_eps: 0.310707
 1089504/2000000: episode: 1525, duration: 56.423s, episode steps: 923, steps per second:  16, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.179 [0.000, 5.000],  loss: 0.027080, mae: 2.760644, mean_q: 3.330832, mean_eps: 0.310273
 1090621/2000000: episode: 1526, duration: 68.301s, episode steps: 1117, steps per second:  16, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.228 [0.000, 5.000],  loss: 0.026162, mae: 2.782871, mean_q: 3.356849, mean_eps: 0.309627
 1091336/2000000: episode: 1527, duration: 43.738s, episode steps: 715, steps per second:  16, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.026560, mae: 2.814208, mean_q: 3.394183, mean_eps: 0.309047
 1092010/2000000: episode: 1528, duration: 43.795s, episode steps: 674, steps per second:  15, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.779 [0.000, 5.000],  loss: 0.025599, mae: 2.768654, mean_q: 3.342845, mean_eps: 0.308608
 1092634/2000000: episode: 1529, duration: 40.054s, episode steps: 624, steps per second:  16, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.202 [0.000, 5.000],  loss: 0.024021, mae: 2.783969, mean_q: 3.359080, mean_eps: 0.308196
 1092994/2000000: episode: 1530, duration: 22.392s, episode steps: 360, steps per second:  16, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.139 [0.000, 5.000],  loss: 0.025762, mae: 2.774731, mean_q: 3.347557, mean_eps: 0.307884
 1093643/2000000: episode: 1531, duration: 40.144s, episode steps: 649, steps per second:  16, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.097 [0.000, 5.000],  loss: 0.026873, mae: 2.805194, mean_q: 3.383735, mean_eps: 0.307565
 1094642/2000000: episode: 1532, duration: 61.499s, episode steps: 999, steps per second:  16, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.222 [0.000, 5.000],  loss: 0.027535, mae: 2.795913, mean_q: 3.371313, mean_eps: 0.307043
 1095833/2000000: episode: 1533, duration: 72.795s, episode steps: 1191, steps per second:  16, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.666 [0.000, 5.000],  loss: 0.028558, mae: 2.796818, mean_q: 3.373652, mean_eps: 0.306349
 1096746/2000000: episode: 1534, duration: 56.150s, episode steps: 913, steps per second:  16, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.027673, mae: 2.817906, mean_q: 3.397505, mean_eps: 0.305683
 1097323/2000000: episode: 1535, duration: 35.041s, episode steps: 577, steps per second:  16, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.026761, mae: 2.789633, mean_q: 3.364794, mean_eps: 0.305212
 1097694/2000000: episode: 1536, duration: 22.937s, episode steps: 371, steps per second:  16, episode reward:  9.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.205 [0.000, 5.000],  loss: 0.025727, mae: 2.810807, mean_q: 3.389161, mean_eps: 0.304912
 1098539/2000000: episode: 1537, duration: 51.574s, episode steps: 845, steps per second:  16, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.123 [0.000, 5.000],  loss: 0.025482, mae: 2.766004, mean_q: 3.335037, mean_eps: 0.304527
 1099700/2000000: episode: 1538, duration: 70.816s, episode steps: 1161, steps per second:  16, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.244 [0.000, 5.000],  loss: 0.027625, mae: 2.782769, mean_q: 3.355482, mean_eps: 0.303892
 1100944/2000000: episode: 1539, duration: 77.447s, episode steps: 1244, steps per second:  16, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.157 [0.000, 5.000],  loss: 0.026051, mae: 2.806997, mean_q: 3.386486, mean_eps: 0.303131
 1101636/2000000: episode: 1540, duration: 42.578s, episode steps: 692, steps per second:  16, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.025554, mae: 2.812442, mean_q: 3.393955, mean_eps: 0.302518
 1102313/2000000: episode: 1541, duration: 41.611s, episode steps: 677, steps per second:  16, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.842 [0.000, 5.000],  loss: 0.029913, mae: 2.775850, mean_q: 3.347999, mean_eps: 0.302083
 1103213/2000000: episode: 1542, duration: 54.830s, episode steps: 900, steps per second:  16, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.756 [0.000, 5.000],  loss: 0.029750, mae: 2.804760, mean_q: 3.382073, mean_eps: 0.301583
 1104201/2000000: episode: 1543, duration: 60.227s, episode steps: 988, steps per second:  16, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.026156, mae: 2.804463, mean_q: 3.382305, mean_eps: 0.300985
 1104782/2000000: episode: 1544, duration: 35.455s, episode steps: 581, steps per second:  16, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.025602, mae: 2.784660, mean_q: 3.357441, mean_eps: 0.300488
 1105538/2000000: episode: 1545, duration: 46.179s, episode steps: 756, steps per second:  16, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.025421, mae: 2.803023, mean_q: 3.381325, mean_eps: 0.300065
 1106216/2000000: episode: 1546, duration: 41.397s, episode steps: 678, steps per second:  16, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.028 [0.000, 5.000],  loss: 0.029268, mae: 2.796673, mean_q: 3.370484, mean_eps: 0.299612
 1106829/2000000: episode: 1547, duration: 37.570s, episode steps: 613, steps per second:  16, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.028623, mae: 2.811282, mean_q: 3.388563, mean_eps: 0.299203
 1107676/2000000: episode: 1548, duration: 51.730s, episode steps: 847, steps per second:  16, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.028018, mae: 2.803203, mean_q: 3.379376, mean_eps: 0.298740
 1108646/2000000: episode: 1549, duration: 59.277s, episode steps: 970, steps per second:  16, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.641 [0.000, 5.000],  loss: 0.025264, mae: 2.797436, mean_q: 3.373644, mean_eps: 0.298165
 1109565/2000000: episode: 1550, duration: 56.372s, episode steps: 919, steps per second:  16, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.025331, mae: 2.802211, mean_q: 3.381313, mean_eps: 0.297566
 1110437/2000000: episode: 1551, duration: 53.627s, episode steps: 872, steps per second:  16, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.023823, mae: 2.810071, mean_q: 3.391165, mean_eps: 0.296999
 1111509/2000000: episode: 1552, duration: 65.251s, episode steps: 1072, steps per second:  16, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.025393, mae: 2.777174, mean_q: 3.351006, mean_eps: 0.296383
 1112359/2000000: episode: 1553, duration: 51.785s, episode steps: 850, steps per second:  16, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.185 [0.000, 5.000],  loss: 0.024186, mae: 2.786500, mean_q: 3.361624, mean_eps: 0.295775
 1112852/2000000: episode: 1554, duration: 30.456s, episode steps: 493, steps per second:  16, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.025531, mae: 2.812857, mean_q: 3.392963, mean_eps: 0.295351
 1113675/2000000: episode: 1555, duration: 50.477s, episode steps: 823, steps per second:  16, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.028140, mae: 2.779278, mean_q: 3.353988, mean_eps: 0.294934
 1114467/2000000: episode: 1556, duration: 48.669s, episode steps: 792, steps per second:  16, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.269 [0.000, 5.000],  loss: 0.027259, mae: 2.766257, mean_q: 3.335175, mean_eps: 0.294422
 1115276/2000000: episode: 1557, duration: 49.523s, episode steps: 809, steps per second:  16, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.024888, mae: 2.794856, mean_q: 3.372279, mean_eps: 0.293916
 1116576/2000000: episode: 1558, duration: 80.005s, episode steps: 1300, steps per second:  16, episode reward: 27.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.182 [0.000, 5.000],  loss: 0.025022, mae: 2.782871, mean_q: 3.357052, mean_eps: 0.293248
 1117452/2000000: episode: 1559, duration: 53.821s, episode steps: 876, steps per second:  16, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.312 [0.000, 5.000],  loss: 0.025448, mae: 2.781634, mean_q: 3.355767, mean_eps: 0.292559
 1118346/2000000: episode: 1560, duration: 54.894s, episode steps: 894, steps per second:  16, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.296 [0.000, 5.000],  loss: 0.024338, mae: 2.780550, mean_q: 3.354906, mean_eps: 0.291998
 1119342/2000000: episode: 1561, duration: 60.823s, episode steps: 996, steps per second:  16, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.843 [0.000, 5.000],  loss: 0.024933, mae: 2.781042, mean_q: 3.353379, mean_eps: 0.291399
 1120367/2000000: episode: 1562, duration: 62.667s, episode steps: 1025, steps per second:  16, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.712 [0.000, 5.000],  loss: 0.025499, mae: 2.790929, mean_q: 3.365701, mean_eps: 0.290759
 1121468/2000000: episode: 1563, duration: 67.956s, episode steps: 1101, steps per second:  16, episode reward: 30.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.026948, mae: 2.806559, mean_q: 3.385529, mean_eps: 0.290087
 1122428/2000000: episode: 1564, duration: 59.673s, episode steps: 960, steps per second:  16, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.025702, mae: 2.796818, mean_q: 3.372296, mean_eps: 0.289434
 1123421/2000000: episode: 1565, duration: 61.448s, episode steps: 993, steps per second:  16, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.027137, mae: 2.802795, mean_q: 3.380206, mean_eps: 0.288815
 1124414/2000000: episode: 1566, duration: 61.024s, episode steps: 993, steps per second:  16, episode reward: 30.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.642 [0.000, 5.000],  loss: 0.026649, mae: 2.793314, mean_q: 3.367955, mean_eps: 0.288185
 1125524/2000000: episode: 1567, duration: 68.012s, episode steps: 1110, steps per second:  16, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.025057, mae: 2.818074, mean_q: 3.398817, mean_eps: 0.287520
 1126015/2000000: episode: 1568, duration: 29.911s, episode steps: 491, steps per second:  16, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.776 [0.000, 5.000],  loss: 0.026377, mae: 2.797840, mean_q: 3.371980, mean_eps: 0.287014
 1126861/2000000: episode: 1569, duration: 51.779s, episode steps: 846, steps per second:  16, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.265 [0.000, 5.000],  loss: 0.026156, mae: 2.793216, mean_q: 3.366867, mean_eps: 0.286589
 1127567/2000000: episode: 1570, duration: 43.314s, episode steps: 706, steps per second:  16, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.326 [0.000, 5.000],  loss: 0.026491, mae: 2.780254, mean_q: 3.351076, mean_eps: 0.286098
 1128241/2000000: episode: 1571, duration: 41.377s, episode steps: 674, steps per second:  16, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: 0.026608, mae: 2.796678, mean_q: 3.372114, mean_eps: 0.285661
 1129166/2000000: episode: 1572, duration: 56.090s, episode steps: 925, steps per second:  16, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.054 [0.000, 5.000],  loss: 0.027700, mae: 2.799058, mean_q: 3.374135, mean_eps: 0.285154
 1129957/2000000: episode: 1573, duration: 48.478s, episode steps: 791, steps per second:  16, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.026172, mae: 2.787046, mean_q: 3.361206, mean_eps: 0.284611
 1130620/2000000: episode: 1574, duration: 40.651s, episode steps: 663, steps per second:  16, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.023655, mae: 2.779225, mean_q: 3.352739, mean_eps: 0.284151
 1131120/2000000: episode: 1575, duration: 30.857s, episode steps: 500, steps per second:  16, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.022616, mae: 2.748457, mean_q: 3.315307, mean_eps: 0.283784
 1131983/2000000: episode: 1576, duration: 52.953s, episode steps: 863, steps per second:  16, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.023221, mae: 2.734656, mean_q: 3.297968, mean_eps: 0.283352
 1132626/2000000: episode: 1577, duration: 39.449s, episode steps: 643, steps per second:  16, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.888 [0.000, 5.000],  loss: 0.025135, mae: 2.762051, mean_q: 3.332554, mean_eps: 0.282874
 1133269/2000000: episode: 1578, duration: 39.578s, episode steps: 643, steps per second:  16, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.026990, mae: 2.754393, mean_q: 3.325875, mean_eps: 0.282466
 1133899/2000000: episode: 1579, duration: 38.624s, episode steps: 630, steps per second:  16, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.233 [0.000, 5.000],  loss: 0.027800, mae: 2.745123, mean_q: 3.308589, mean_eps: 0.282063
 1134603/2000000: episode: 1580, duration: 43.103s, episode steps: 704, steps per second:  16, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.027985, mae: 2.750289, mean_q: 3.316291, mean_eps: 0.281642
 1135548/2000000: episode: 1581, duration: 58.035s, episode steps: 945, steps per second:  16, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.674 [0.000, 5.000],  loss: 0.024346, mae: 2.752187, mean_q: 3.319591, mean_eps: 0.281120
 1135900/2000000: episode: 1582, duration: 21.969s, episode steps: 352, steps per second:  16, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.024899, mae: 2.733606, mean_q: 3.294254, mean_eps: 0.280709
 1136556/2000000: episode: 1583, duration: 40.433s, episode steps: 656, steps per second:  16, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.026056, mae: 2.753047, mean_q: 3.318329, mean_eps: 0.280390
 1137382/2000000: episode: 1584, duration: 50.695s, episode steps: 826, steps per second:  16, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.026101, mae: 2.738242, mean_q: 3.300951, mean_eps: 0.279920
 1138156/2000000: episode: 1585, duration: 47.614s, episode steps: 774, steps per second:  16, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.961 [0.000, 5.000],  loss: 0.023625, mae: 2.744884, mean_q: 3.312715, mean_eps: 0.279414
 1138979/2000000: episode: 1586, duration: 50.946s, episode steps: 823, steps per second:  16, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.875 [0.000, 5.000],  loss: 0.024965, mae: 2.756596, mean_q: 3.322686, mean_eps: 0.278908
 1139406/2000000: episode: 1587, duration: 26.054s, episode steps: 427, steps per second:  16, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.892 [0.000, 5.000],  loss: 0.026116, mae: 2.749200, mean_q: 3.313431, mean_eps: 0.278512
 1140447/2000000: episode: 1588, duration: 63.384s, episode steps: 1041, steps per second:  16, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.654 [0.000, 5.000],  loss: 0.024376, mae: 2.751819, mean_q: 3.318781, mean_eps: 0.278047
 1141259/2000000: episode: 1589, duration: 49.977s, episode steps: 812, steps per second:  16, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.022980, mae: 2.780297, mean_q: 3.354099, mean_eps: 0.277460
 1142231/2000000: episode: 1590, duration: 59.533s, episode steps: 972, steps per second:  16, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.215 [0.000, 5.000],  loss: 0.023458, mae: 2.755925, mean_q: 3.324120, mean_eps: 0.276895
 1142714/2000000: episode: 1591, duration: 29.775s, episode steps: 483, steps per second:  16, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.027071, mae: 2.758526, mean_q: 3.325872, mean_eps: 0.276434
 1143323/2000000: episode: 1592, duration: 37.539s, episode steps: 609, steps per second:  16, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.718 [0.000, 5.000],  loss: 0.023197, mae: 2.766566, mean_q: 3.335625, mean_eps: 0.276089
 1144427/2000000: episode: 1593, duration: 67.844s, episode steps: 1104, steps per second:  16, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.923 [0.000, 5.000],  loss: 0.025202, mae: 2.752196, mean_q: 3.319364, mean_eps: 0.275546
 1145080/2000000: episode: 1594, duration: 40.215s, episode steps: 653, steps per second:  16, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.994 [0.000, 5.000],  loss: 0.025572, mae: 2.749567, mean_q: 3.315303, mean_eps: 0.274990
 1146004/2000000: episode: 1595, duration: 56.476s, episode steps: 924, steps per second:  16, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.026672, mae: 2.773902, mean_q: 3.346947, mean_eps: 0.274491
 1146942/2000000: episode: 1596, duration: 57.686s, episode steps: 938, steps per second:  16, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.017 [0.000, 5.000],  loss: 0.027200, mae: 2.777704, mean_q: 3.349505, mean_eps: 0.273901
 1147876/2000000: episode: 1597, duration: 57.340s, episode steps: 934, steps per second:  16, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.901 [0.000, 5.000],  loss: 0.025329, mae: 2.777796, mean_q: 3.346711, mean_eps: 0.273308
 1148902/2000000: episode: 1598, duration: 62.877s, episode steps: 1026, steps per second:  16, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.227 [0.000, 5.000],  loss: 0.025727, mae: 2.763188, mean_q: 3.332828, mean_eps: 0.272688
 1149723/2000000: episode: 1599, duration: 50.067s, episode steps: 821, steps per second:  16, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.024125, mae: 2.742068, mean_q: 3.308966, mean_eps: 0.272102
 1150247/2000000: episode: 1600, duration: 34.192s, episode steps: 524, steps per second:  15, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.025310, mae: 2.796332, mean_q: 3.374503, mean_eps: 0.271677
 1151297/2000000: episode: 1601, duration: 64.982s, episode steps: 1050, steps per second:  16, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.791 [0.000, 5.000],  loss: 0.022336, mae: 2.803152, mean_q: 3.385531, mean_eps: 0.271178
 1152167/2000000: episode: 1602, duration: 53.490s, episode steps: 870, steps per second:  16, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.271 [0.000, 5.000],  loss: 0.024781, mae: 2.787166, mean_q: 3.364114, mean_eps: 0.270570
 1152847/2000000: episode: 1603, duration: 41.941s, episode steps: 680, steps per second:  16, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.026504, mae: 2.799136, mean_q: 3.375915, mean_eps: 0.270080
 1153972/2000000: episode: 1604, duration: 70.462s, episode steps: 1125, steps per second:  16, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.222 [0.000, 5.000],  loss: 0.026279, mae: 2.794426, mean_q: 3.369730, mean_eps: 0.269508
 1154843/2000000: episode: 1605, duration: 53.691s, episode steps: 871, steps per second:  16, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.179 [0.000, 5.000],  loss: 0.025410, mae: 2.774035, mean_q: 3.345254, mean_eps: 0.268876
 1155629/2000000: episode: 1606, duration: 48.045s, episode steps: 786, steps per second:  16, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.025022, mae: 2.775164, mean_q: 3.343738, mean_eps: 0.268351
 1156277/2000000: episode: 1607, duration: 39.696s, episode steps: 648, steps per second:  16, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.961 [0.000, 5.000],  loss: 0.023555, mae: 2.780183, mean_q: 3.349344, mean_eps: 0.267896
 1157293/2000000: episode: 1608, duration: 62.185s, episode steps: 1016, steps per second:  16, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.025778, mae: 2.779636, mean_q: 3.351101, mean_eps: 0.267369
 1158080/2000000: episode: 1609, duration: 48.230s, episode steps: 787, steps per second:  16, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.026706, mae: 2.789708, mean_q: 3.364283, mean_eps: 0.266799
 1158570/2000000: episode: 1610, duration: 30.127s, episode steps: 490, steps per second:  16, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.959 [0.000, 5.000],  loss: 0.025058, mae: 2.784397, mean_q: 3.355933, mean_eps: 0.266395
 1159068/2000000: episode: 1611, duration: 30.819s, episode steps: 498, steps per second:  16, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.697 [0.000, 5.000],  loss: 0.025784, mae: 2.784543, mean_q: 3.355899, mean_eps: 0.266082
 1159462/2000000: episode: 1612, duration: 24.300s, episode steps: 394, steps per second:  16, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.025488, mae: 2.772525, mean_q: 3.342826, mean_eps: 0.265799
 1160024/2000000: episode: 1613, duration: 34.584s, episode steps: 562, steps per second:  16, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.024364, mae: 2.790846, mean_q: 3.363911, mean_eps: 0.265497
 1160422/2000000: episode: 1614, duration: 24.588s, episode steps: 398, steps per second:  16, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: 0.021088, mae: 2.800684, mean_q: 3.380232, mean_eps: 0.265193
 1161117/2000000: episode: 1615, duration: 42.650s, episode steps: 695, steps per second:  16, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.306 [0.000, 5.000],  loss: 0.022400, mae: 2.795326, mean_q: 3.371468, mean_eps: 0.264846
 1161816/2000000: episode: 1616, duration: 43.284s, episode steps: 699, steps per second:  16, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.024067, mae: 2.782816, mean_q: 3.356845, mean_eps: 0.264405
 1162952/2000000: episode: 1617, duration: 70.135s, episode steps: 1136, steps per second:  16, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.024648, mae: 2.811583, mean_q: 3.389342, mean_eps: 0.263825
 1163899/2000000: episode: 1618, duration: 57.964s, episode steps: 947, steps per second:  16, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.024992, mae: 2.793018, mean_q: 3.366707, mean_eps: 0.263165
 1164446/2000000: episode: 1619, duration: 33.737s, episode steps: 547, steps per second:  16, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.907 [0.000, 5.000],  loss: 0.023506, mae: 2.806590, mean_q: 3.383786, mean_eps: 0.262691
 1165364/2000000: episode: 1620, duration: 56.217s, episode steps: 918, steps per second:  16, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.025895, mae: 2.804124, mean_q: 3.380177, mean_eps: 0.262227
 1166024/2000000: episode: 1621, duration: 40.666s, episode steps: 660, steps per second:  16, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.114 [0.000, 5.000],  loss: 0.027655, mae: 2.808679, mean_q: 3.384803, mean_eps: 0.261728
 1167062/2000000: episode: 1622, duration: 63.902s, episode steps: 1038, steps per second:  16, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.026801, mae: 2.796922, mean_q: 3.370708, mean_eps: 0.261190
 1167709/2000000: episode: 1623, duration: 39.936s, episode steps: 647, steps per second:  16, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.651 [0.000, 5.000],  loss: 0.023051, mae: 2.808383, mean_q: 3.386543, mean_eps: 0.260656
 1168673/2000000: episode: 1624, duration: 59.074s, episode steps: 964, steps per second:  16, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.025536, mae: 2.789715, mean_q: 3.362613, mean_eps: 0.260145
 1169460/2000000: episode: 1625, duration: 48.288s, episode steps: 787, steps per second:  16, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.089 [0.000, 5.000],  loss: 0.023628, mae: 2.783600, mean_q: 3.354805, mean_eps: 0.259592
 1170308/2000000: episode: 1626, duration: 51.850s, episode steps: 848, steps per second:  16, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.923 [0.000, 5.000],  loss: 0.024330, mae: 2.789422, mean_q: 3.364663, mean_eps: 0.259075
 1171208/2000000: episode: 1627, duration: 55.150s, episode steps: 900, steps per second:  16, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.022619, mae: 2.811325, mean_q: 3.391537, mean_eps: 0.258521
 1171890/2000000: episode: 1628, duration: 41.701s, episode steps: 682, steps per second:  16, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.024652, mae: 2.781879, mean_q: 3.355601, mean_eps: 0.258020
 1172521/2000000: episode: 1629, duration: 38.500s, episode steps: 631, steps per second:  16, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.182 [0.000, 5.000],  loss: 0.025194, mae: 2.796760, mean_q: 3.372004, mean_eps: 0.257603
 1173553/2000000: episode: 1630, duration: 63.093s, episode steps: 1032, steps per second:  16, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.024168, mae: 2.793402, mean_q: 3.367816, mean_eps: 0.257076
 1174244/2000000: episode: 1631, duration: 42.574s, episode steps: 691, steps per second:  16, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.039 [0.000, 5.000],  loss: 0.025036, mae: 2.781224, mean_q: 3.353575, mean_eps: 0.256531
 1174877/2000000: episode: 1632, duration: 38.906s, episode steps: 633, steps per second:  16, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.024124, mae: 2.794582, mean_q: 3.368977, mean_eps: 0.256112
 1175514/2000000: episode: 1633, duration: 38.853s, episode steps: 637, steps per second:  16, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.025057, mae: 2.782642, mean_q: 3.353081, mean_eps: 0.255709
 1176823/2000000: episode: 1634, duration: 80.068s, episode steps: 1309, steps per second:  16, episode reward: 26.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.157 [0.000, 5.000],  loss: 0.024102, mae: 2.787708, mean_q: 3.360810, mean_eps: 0.255094
 1177208/2000000: episode: 1635, duration: 23.933s, episode steps: 385, steps per second:  16, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 3.096 [0.000, 5.000],  loss: 0.023299, mae: 2.796162, mean_q: 3.366527, mean_eps: 0.254558
 1177841/2000000: episode: 1636, duration: 39.595s, episode steps: 633, steps per second:  16, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.210 [0.000, 5.000],  loss: 0.026606, mae: 2.803011, mean_q: 3.377331, mean_eps: 0.254235
 1178473/2000000: episode: 1637, duration: 38.960s, episode steps: 632, steps per second:  16, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.026184, mae: 2.780886, mean_q: 3.352022, mean_eps: 0.253833
 1179509/2000000: episode: 1638, duration: 63.378s, episode steps: 1036, steps per second:  16, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.027246, mae: 2.782775, mean_q: 3.353395, mean_eps: 0.253305
 1180339/2000000: episode: 1639, duration: 50.934s, episode steps: 830, steps per second:  16, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.869 [0.000, 5.000],  loss: 0.023410, mae: 2.788361, mean_q: 3.360528, mean_eps: 0.252715
 1180888/2000000: episode: 1640, duration: 34.188s, episode steps: 549, steps per second:  16, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.089 [0.000, 5.000],  loss: 0.022611, mae: 2.793920, mean_q: 3.371387, mean_eps: 0.252279
 1181655/2000000: episode: 1641, duration: 47.592s, episode steps: 767, steps per second:  16, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.023243, mae: 2.803953, mean_q: 3.381763, mean_eps: 0.251862
 1182608/2000000: episode: 1642, duration: 58.726s, episode steps: 953, steps per second:  16, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.022976, mae: 2.780768, mean_q: 3.350517, mean_eps: 0.251318
 1183222/2000000: episode: 1643, duration: 37.769s, episode steps: 614, steps per second:  16, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.770 [0.000, 5.000],  loss: 0.025016, mae: 2.809640, mean_q: 3.385642, mean_eps: 0.250821
 1183694/2000000: episode: 1644, duration: 28.776s, episode steps: 472, steps per second:  16, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.026182, mae: 2.795828, mean_q: 3.368441, mean_eps: 0.250477
 1184748/2000000: episode: 1645, duration: 64.517s, episode steps: 1054, steps per second:  16, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.026304, mae: 2.799249, mean_q: 3.372978, mean_eps: 0.249994
 1185444/2000000: episode: 1646, duration: 42.730s, episode steps: 696, steps per second:  16, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.003 [0.000, 5.000],  loss: 0.024889, mae: 2.786661, mean_q: 3.359739, mean_eps: 0.249440
 1185977/2000000: episode: 1647, duration: 33.055s, episode steps: 533, steps per second:  16, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.024843, mae: 2.814965, mean_q: 3.392186, mean_eps: 0.249050
 1186483/2000000: episode: 1648, duration: 30.713s, episode steps: 506, steps per second:  16, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.024435, mae: 2.794434, mean_q: 3.369340, mean_eps: 0.248721
 1187656/2000000: episode: 1649, duration: 72.275s, episode steps: 1173, steps per second:  16, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.023995, mae: 2.800897, mean_q: 3.375736, mean_eps: 0.248190
 1188271/2000000: episode: 1650, duration: 37.734s, episode steps: 615, steps per second:  16, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.025951, mae: 2.795786, mean_q: 3.369790, mean_eps: 0.247624
 1189407/2000000: episode: 1651, duration: 69.213s, episode steps: 1136, steps per second:  16, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.027608, mae: 2.797395, mean_q: 3.370491, mean_eps: 0.247069
 1189897/2000000: episode: 1652, duration: 30.240s, episode steps: 490, steps per second:  16, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.023593, mae: 2.785229, mean_q: 3.355827, mean_eps: 0.246554
 1190772/2000000: episode: 1653, duration: 53.403s, episode steps: 875, steps per second:  16, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.887 [0.000, 5.000],  loss: 0.023049, mae: 2.773467, mean_q: 3.346266, mean_eps: 0.246122
 1191908/2000000: episode: 1654, duration: 69.529s, episode steps: 1136, steps per second:  16, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.022494, mae: 2.775138, mean_q: 3.345157, mean_eps: 0.245486
 1192452/2000000: episode: 1655, duration: 33.415s, episode steps: 544, steps per second:  16, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.846 [0.000, 5.000],  loss: 0.023803, mae: 2.780837, mean_q: 3.352791, mean_eps: 0.244954
 1193295/2000000: episode: 1656, duration: 51.499s, episode steps: 843, steps per second:  16, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.913 [0.000, 5.000],  loss: 0.024202, mae: 2.776269, mean_q: 3.345199, mean_eps: 0.244514
 1194831/2000000: episode: 1657, duration: 93.898s, episode steps: 1536, steps per second:  16, episode reward: 34.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.296 [0.000, 5.000],  loss: 0.023873, mae: 2.778013, mean_q: 3.348871, mean_eps: 0.243761
 1195677/2000000: episode: 1658, duration: 52.150s, episode steps: 846, steps per second:  16, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.023253, mae: 2.776696, mean_q: 3.346897, mean_eps: 0.243006
 1196348/2000000: episode: 1659, duration: 40.763s, episode steps: 671, steps per second:  16, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.023678, mae: 2.775621, mean_q: 3.346481, mean_eps: 0.242526
 1197369/2000000: episode: 1660, duration: 63.142s, episode steps: 1021, steps per second:  16, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.024413, mae: 2.774851, mean_q: 3.342845, mean_eps: 0.241990
 1198157/2000000: episode: 1661, duration: 48.049s, episode steps: 788, steps per second:  16, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.207 [0.000, 5.000],  loss: 0.024140, mae: 2.761556, mean_q: 3.327437, mean_eps: 0.241416
 1199080/2000000: episode: 1662, duration: 56.317s, episode steps: 923, steps per second:  16, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.151 [0.000, 5.000],  loss: 0.024430, mae: 2.774851, mean_q: 3.345740, mean_eps: 0.240875
 1200006/2000000: episode: 1663, duration: 57.723s, episode steps: 926, steps per second:  16, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.222 [0.000, 5.000],  loss: 0.024032, mae: 2.778702, mean_q: 3.348536, mean_eps: 0.240290
 1200675/2000000: episode: 1664, duration: 40.905s, episode steps: 669, steps per second:  16, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.943 [0.000, 5.000],  loss: 0.021300, mae: 2.786069, mean_q: 3.360495, mean_eps: 0.239785
 1201254/2000000: episode: 1665, duration: 35.466s, episode steps: 579, steps per second:  16, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.174 [0.000, 5.000],  loss: 0.021548, mae: 2.787820, mean_q: 3.363503, mean_eps: 0.239389
 1201887/2000000: episode: 1666, duration: 38.929s, episode steps: 633, steps per second:  16, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.267 [0.000, 5.000],  loss: 0.024362, mae: 2.782865, mean_q: 3.355096, mean_eps: 0.239006
 1202383/2000000: episode: 1667, duration: 30.378s, episode steps: 496, steps per second:  16, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.929 [0.000, 5.000],  loss: 0.023410, mae: 2.760002, mean_q: 3.326811, mean_eps: 0.238648
 1203110/2000000: episode: 1668, duration: 44.255s, episode steps: 727, steps per second:  16, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.025694, mae: 2.766407, mean_q: 3.335642, mean_eps: 0.238261
 1203702/2000000: episode: 1669, duration: 36.871s, episode steps: 592, steps per second:  16, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.062 [0.000, 5.000],  loss: 0.027141, mae: 2.764926, mean_q: 3.331662, mean_eps: 0.237843
 1204347/2000000: episode: 1670, duration: 39.958s, episode steps: 645, steps per second:  16, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.024395, mae: 2.771762, mean_q: 3.340341, mean_eps: 0.237451
 1204884/2000000: episode: 1671, duration: 32.773s, episode steps: 537, steps per second:  16, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.980 [0.000, 5.000],  loss: 0.025032, mae: 2.770472, mean_q: 3.339870, mean_eps: 0.237078
 1205509/2000000: episode: 1672, duration: 38.504s, episode steps: 625, steps per second:  16, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.023626, mae: 2.777832, mean_q: 3.346759, mean_eps: 0.236709
 1206416/2000000: episode: 1673, duration: 55.558s, episode steps: 907, steps per second:  16, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.149 [0.000, 5.000],  loss: 0.025076, mae: 2.771640, mean_q: 3.339293, mean_eps: 0.236224
 1207023/2000000: episode: 1674, duration: 37.236s, episode steps: 607, steps per second:  16, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.023782, mae: 2.750299, mean_q: 3.312811, mean_eps: 0.235745
 1207981/2000000: episode: 1675, duration: 58.591s, episode steps: 958, steps per second:  16, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.261 [0.000, 5.000],  loss: 0.023319, mae: 2.775074, mean_q: 3.344763, mean_eps: 0.235249
 1209007/2000000: episode: 1676, duration: 62.817s, episode steps: 1026, steps per second:  16, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.024740, mae: 2.776469, mean_q: 3.345339, mean_eps: 0.234620
 1209516/2000000: episode: 1677, duration: 31.506s, episode steps: 509, steps per second:  16, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.743 [0.000, 5.000],  loss: 0.023427, mae: 2.765520, mean_q: 3.332014, mean_eps: 0.234135
 1210261/2000000: episode: 1678, duration: 46.513s, episode steps: 745, steps per second:  16, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.309 [0.000, 5.000],  loss: 0.023256, mae: 2.775606, mean_q: 3.343668, mean_eps: 0.233738
 1211135/2000000: episode: 1679, duration: 53.516s, episode steps: 874, steps per second:  16, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.696 [0.000, 5.000],  loss: 0.022476, mae: 2.785513, mean_q: 3.359792, mean_eps: 0.233225
 1211517/2000000: episode: 1680, duration: 23.721s, episode steps: 382, steps per second:  16, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.024025, mae: 2.790858, mean_q: 3.364845, mean_eps: 0.232827
 1211974/2000000: episode: 1681, duration: 28.306s, episode steps: 457, steps per second:  16, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.208 [0.000, 5.000],  loss: 0.020993, mae: 2.791414, mean_q: 3.363304, mean_eps: 0.232561
 1212880/2000000: episode: 1682, duration: 55.609s, episode steps: 906, steps per second:  16, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.947 [0.000, 5.000],  loss: 0.024752, mae: 2.799732, mean_q: 3.374129, mean_eps: 0.232130
 1213276/2000000: episode: 1683, duration: 24.443s, episode steps: 396, steps per second:  16, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.970 [0.000, 5.000],  loss: 0.025068, mae: 2.804123, mean_q: 3.379705, mean_eps: 0.231719
 1214236/2000000: episode: 1684, duration: 59.020s, episode steps: 960, steps per second:  16, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.024939, mae: 2.801888, mean_q: 3.376899, mean_eps: 0.231289
 1214730/2000000: episode: 1685, duration: 30.323s, episode steps: 494, steps per second:  16, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.027093, mae: 2.778620, mean_q: 3.346339, mean_eps: 0.230828
 1215955/2000000: episode: 1686, duration: 75.267s, episode steps: 1225, steps per second:  16, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.650 [0.000, 5.000],  loss: 0.024482, mae: 2.791892, mean_q: 3.365414, mean_eps: 0.230283
 1216738/2000000: episode: 1687, duration: 48.269s, episode steps: 783, steps per second:  16, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.022406, mae: 2.796285, mean_q: 3.369791, mean_eps: 0.229648
 1217426/2000000: episode: 1688, duration: 42.417s, episode steps: 688, steps per second:  16, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.062 [0.000, 5.000],  loss: 0.024267, mae: 2.789070, mean_q: 3.359060, mean_eps: 0.229181
 1218026/2000000: episode: 1689, duration: 36.809s, episode steps: 600, steps per second:  16, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.722 [0.000, 5.000],  loss: 0.024698, mae: 2.782882, mean_q: 3.352531, mean_eps: 0.228774
 1219152/2000000: episode: 1690, duration: 69.366s, episode steps: 1126, steps per second:  16, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.179 [0.000, 5.000],  loss: 0.024716, mae: 2.791129, mean_q: 3.363936, mean_eps: 0.228228
 1219930/2000000: episode: 1691, duration: 48.041s, episode steps: 778, steps per second:  16, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.915 [0.000, 5.000],  loss: 0.025030, mae: 2.787992, mean_q: 3.359275, mean_eps: 0.227625
 1220495/2000000: episode: 1692, duration: 34.947s, episode steps: 565, steps per second:  16, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.020134, mae: 2.816752, mean_q: 3.397416, mean_eps: 0.227199
 1220942/2000000: episode: 1693, duration: 27.873s, episode steps: 447, steps per second:  16, episode reward: 11.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.881 [0.000, 5.000],  loss: 0.021389, mae: 2.803472, mean_q: 3.379613, mean_eps: 0.226879
 1221536/2000000: episode: 1694, duration: 36.731s, episode steps: 594, steps per second:  16, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.021274, mae: 2.773485, mean_q: 3.344663, mean_eps: 0.226549
 1222659/2000000: episode: 1695, duration: 68.839s, episode steps: 1123, steps per second:  16, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.022267, mae: 2.791637, mean_q: 3.365303, mean_eps: 0.226006
 1223333/2000000: episode: 1696, duration: 41.725s, episode steps: 674, steps per second:  16, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.024444, mae: 2.796737, mean_q: 3.370564, mean_eps: 0.225436
 1224028/2000000: episode: 1697, duration: 42.864s, episode steps: 695, steps per second:  16, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.025268, mae: 2.795497, mean_q: 3.371184, mean_eps: 0.225003
 1224582/2000000: episode: 1698, duration: 34.101s, episode steps: 554, steps per second:  16, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.025921, mae: 2.787089, mean_q: 3.359374, mean_eps: 0.224607
 1225698/2000000: episode: 1699, duration: 68.494s, episode steps: 1116, steps per second:  16, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.085 [0.000, 5.000],  loss: 0.023665, mae: 2.769159, mean_q: 3.335653, mean_eps: 0.224078
 1226763/2000000: episode: 1700, duration: 65.464s, episode steps: 1065, steps per second:  16, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.918 [0.000, 5.000],  loss: 0.024127, mae: 2.789162, mean_q: 3.359402, mean_eps: 0.223388
 1227494/2000000: episode: 1701, duration: 44.494s, episode steps: 731, steps per second:  16, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.023502, mae: 2.804546, mean_q: 3.376967, mean_eps: 0.222819
 1228388/2000000: episode: 1702, duration: 54.549s, episode steps: 894, steps per second:  16, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.659 [0.000, 5.000],  loss: 0.024360, mae: 2.784636, mean_q: 3.354117, mean_eps: 0.222305
 1229205/2000000: episode: 1703, duration: 50.462s, episode steps: 817, steps per second:  16, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.207 [0.000, 5.000],  loss: 0.023533, mae: 2.796826, mean_q: 3.369134, mean_eps: 0.221763
 1230255/2000000: episode: 1704, duration: 63.883s, episode steps: 1050, steps per second:  16, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.052 [0.000, 5.000],  loss: 0.022775, mae: 2.783928, mean_q: 3.355288, mean_eps: 0.221171
 1230788/2000000: episode: 1705, duration: 32.942s, episode steps: 533, steps per second:  16, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.700 [0.000, 5.000],  loss: 0.020067, mae: 2.812979, mean_q: 3.392332, mean_eps: 0.220671
 1232104/2000000: episode: 1706, duration: 80.509s, episode steps: 1316, steps per second:  16, episode reward: 25.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.021483, mae: 2.807484, mean_q: 3.383704, mean_eps: 0.220085
 1233335/2000000: episode: 1707, duration: 75.005s, episode steps: 1231, steps per second:  16, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.176 [0.000, 5.000],  loss: 0.024015, mae: 2.795309, mean_q: 3.369008, mean_eps: 0.219279
 1234747/2000000: episode: 1708, duration: 86.143s, episode steps: 1412, steps per second:  16, episode reward: 27.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.024018, mae: 2.815038, mean_q: 3.391288, mean_eps: 0.218441
 1235703/2000000: episode: 1709, duration: 58.532s, episode steps: 956, steps per second:  16, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.023427, mae: 2.803055, mean_q: 3.376890, mean_eps: 0.217691
 1236664/2000000: episode: 1710, duration: 59.267s, episode steps: 961, steps per second:  16, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.190 [0.000, 5.000],  loss: 0.023527, mae: 2.805594, mean_q: 3.382141, mean_eps: 0.217085
 1237641/2000000: episode: 1711, duration: 59.889s, episode steps: 977, steps per second:  16, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.888 [0.000, 5.000],  loss: 0.025982, mae: 2.809120, mean_q: 3.385667, mean_eps: 0.216470
 1238357/2000000: episode: 1712, duration: 43.624s, episode steps: 716, steps per second:  16, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.011 [0.000, 5.000],  loss: 0.023844, mae: 2.778250, mean_q: 3.347673, mean_eps: 0.215933
 1238755/2000000: episode: 1713, duration: 24.297s, episode steps: 398, steps per second:  16, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.025739, mae: 2.806916, mean_q: 3.381139, mean_eps: 0.215581
 1239502/2000000: episode: 1714, duration: 45.775s, episode steps: 747, steps per second:  16, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.317 [0.000, 5.000],  loss: 0.023363, mae: 2.802551, mean_q: 3.373350, mean_eps: 0.215219
 1240496/2000000: episode: 1715, duration: 61.790s, episode steps: 994, steps per second:  16, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.023045, mae: 2.790367, mean_q: 3.364147, mean_eps: 0.214668
 1240889/2000000: episode: 1716, duration: 24.447s, episode steps: 393, steps per second:  16, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.654 [0.000, 5.000],  loss: 0.019491, mae: 2.791234, mean_q: 3.365569, mean_eps: 0.214228
 1241660/2000000: episode: 1717, duration: 47.664s, episode steps: 771, steps per second:  16, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.136 [0.000, 5.000],  loss: 0.023037, mae: 2.817214, mean_q: 3.397345, mean_eps: 0.213860
 1242450/2000000: episode: 1718, duration: 48.266s, episode steps: 790, steps per second:  16, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.022443, mae: 2.805340, mean_q: 3.380406, mean_eps: 0.213366
 1243325/2000000: episode: 1719, duration: 53.305s, episode steps: 875, steps per second:  16, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.181 [0.000, 5.000],  loss: 0.022888, mae: 2.802419, mean_q: 3.374375, mean_eps: 0.212838
 1244428/2000000: episode: 1720, duration: 67.542s, episode steps: 1103, steps per second:  16, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.023380, mae: 2.794943, mean_q: 3.366473, mean_eps: 0.212212
 1244803/2000000: episode: 1721, duration: 23.148s, episode steps: 375, steps per second:  16, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.192 [0.000, 5.000],  loss: 0.024663, mae: 2.813034, mean_q: 3.389333, mean_eps: 0.211744
 1245206/2000000: episode: 1722, duration: 24.682s, episode steps: 403, steps per second:  16, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.963 [0.000, 5.000],  loss: 0.022844, mae: 2.803885, mean_q: 3.377067, mean_eps: 0.211497
 1245699/2000000: episode: 1723, duration: 30.175s, episode steps: 493, steps per second:  16, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.024793, mae: 2.819468, mean_q: 3.397038, mean_eps: 0.211214
 1246371/2000000: episode: 1724, duration: 41.172s, episode steps: 672, steps per second:  16, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.722 [0.000, 5.000],  loss: 0.022612, mae: 2.804300, mean_q: 3.378608, mean_eps: 0.210845
 1246925/2000000: episode: 1725, duration: 34.106s, episode steps: 554, steps per second:  16, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.021483, mae: 2.807084, mean_q: 3.379488, mean_eps: 0.210456
 1248156/2000000: episode: 1726, duration: 75.428s, episode steps: 1231, steps per second:  16, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.023220, mae: 2.817740, mean_q: 3.394619, mean_eps: 0.209891
 1248957/2000000: episode: 1727, duration: 49.245s, episode steps: 801, steps per second:  16, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.824 [0.000, 5.000],  loss: 0.025206, mae: 2.800477, mean_q: 3.372728, mean_eps: 0.209248
 1249650/2000000: episode: 1728, duration: 42.446s, episode steps: 693, steps per second:  16, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.024813, mae: 2.809224, mean_q: 3.382566, mean_eps: 0.208774
 1251196/2000000: episode: 1729, duration: 95.817s, episode steps: 1546, steps per second:  16, episode reward: 30.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.281 [0.000, 5.000],  loss: 0.021855, mae: 2.811246, mean_q: 3.388059, mean_eps: 0.208066
 1251674/2000000: episode: 1730, duration: 29.344s, episode steps: 478, steps per second:  16, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.167 [0.000, 5.000],  loss: 0.021641, mae: 2.804711, mean_q: 3.379866, mean_eps: 0.207425
 1252305/2000000: episode: 1731, duration: 38.928s, episode steps: 631, steps per second:  16, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.758 [0.000, 5.000],  loss: 0.020574, mae: 2.790625, mean_q: 3.360727, mean_eps: 0.207073
 1252990/2000000: episode: 1732, duration: 41.931s, episode steps: 685, steps per second:  16, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.721 [0.000, 5.000],  loss: 0.020794, mae: 2.785441, mean_q: 3.354856, mean_eps: 0.206656
 1253716/2000000: episode: 1733, duration: 44.738s, episode steps: 726, steps per second:  16, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.317 [0.000, 5.000],  loss: 0.023002, mae: 2.806162, mean_q: 3.382209, mean_eps: 0.206210
 1254496/2000000: episode: 1734, duration: 47.795s, episode steps: 780, steps per second:  16, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.022616, mae: 2.802966, mean_q: 3.375440, mean_eps: 0.205734
 1255472/2000000: episode: 1735, duration: 59.839s, episode steps: 976, steps per second:  16, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.023840, mae: 2.805548, mean_q: 3.378739, mean_eps: 0.205178
 1256219/2000000: episode: 1736, duration: 45.963s, episode steps: 747, steps per second:  16, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.244 [0.000, 5.000],  loss: 0.022321, mae: 2.804375, mean_q: 3.378374, mean_eps: 0.204632
 1256925/2000000: episode: 1737, duration: 43.170s, episode steps: 706, steps per second:  16, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.024499, mae: 2.813397, mean_q: 3.388139, mean_eps: 0.204171
 1257686/2000000: episode: 1738, duration: 46.492s, episode steps: 761, steps per second:  16, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.942 [0.000, 5.000],  loss: 0.025410, mae: 2.790261, mean_q: 3.363773, mean_eps: 0.203706
 1258319/2000000: episode: 1739, duration: 38.705s, episode steps: 633, steps per second:  16, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.679 [0.000, 5.000],  loss: 0.022862, mae: 2.801700, mean_q: 3.374141, mean_eps: 0.203265
 1259240/2000000: episode: 1740, duration: 56.049s, episode steps: 921, steps per second:  16, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.898 [0.000, 5.000],  loss: 0.023610, mae: 2.794175, mean_q: 3.367051, mean_eps: 0.202774
 1259905/2000000: episode: 1741, duration: 41.117s, episode steps: 665, steps per second:  16, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.155 [0.000, 5.000],  loss: 0.022397, mae: 2.807672, mean_q: 3.380391, mean_eps: 0.202271
 1260839/2000000: episode: 1742, duration: 57.557s, episode steps: 934, steps per second:  16, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.021194, mae: 2.802741, mean_q: 3.377762, mean_eps: 0.201764
 1261207/2000000: episode: 1743, duration: 22.875s, episode steps: 368, steps per second:  16, episode reward:  9.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.984 [0.000, 5.000],  loss: 0.023279, mae: 2.779092, mean_q: 3.350951, mean_eps: 0.201353
 1261870/2000000: episode: 1744, duration: 40.503s, episode steps: 663, steps per second:  16, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.189 [0.000, 5.000],  loss: 0.022641, mae: 2.784344, mean_q: 3.353941, mean_eps: 0.201026
 1262387/2000000: episode: 1745, duration: 31.683s, episode steps: 517, steps per second:  16, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.785 [0.000, 5.000],  loss: 0.021525, mae: 2.797334, mean_q: 3.371194, mean_eps: 0.200652
 1263256/2000000: episode: 1746, duration: 53.527s, episode steps: 869, steps per second:  16, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.022748, mae: 2.797519, mean_q: 3.370673, mean_eps: 0.200214
 1263927/2000000: episode: 1747, duration: 41.029s, episode steps: 671, steps per second:  16, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.021288, mae: 2.787514, mean_q: 3.357669, mean_eps: 0.199726
 1264935/2000000: episode: 1748, duration: 61.713s, episode steps: 1008, steps per second:  16, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.023642, mae: 2.795232, mean_q: 3.368246, mean_eps: 0.199194
 1265614/2000000: episode: 1749, duration: 41.659s, episode steps: 679, steps per second:  16, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.729 [0.000, 5.000],  loss: 0.024410, mae: 2.798993, mean_q: 3.371080, mean_eps: 0.198660
 1266016/2000000: episode: 1750, duration: 24.649s, episode steps: 402, steps per second:  16, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.403 [0.000, 5.000],  loss: 0.022329, mae: 2.803488, mean_q: 3.376886, mean_eps: 0.198318
 1266649/2000000: episode: 1751, duration: 38.772s, episode steps: 633, steps per second:  16, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.062 [0.000, 5.000],  loss: 0.021336, mae: 2.805716, mean_q: 3.379378, mean_eps: 0.197990
 1267254/2000000: episode: 1752, duration: 36.884s, episode steps: 605, steps per second:  16, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.022051, mae: 2.793105, mean_q: 3.364431, mean_eps: 0.197597
 1268186/2000000: episode: 1753, duration: 59.308s, episode steps: 932, steps per second:  16, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.832 [0.000, 5.000],  loss: 0.022131, mae: 2.788086, mean_q: 3.358636, mean_eps: 0.197111
 1268689/2000000: episode: 1754, duration: 31.082s, episode steps: 503, steps per second:  16, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.025487, mae: 2.813429, mean_q: 3.389674, mean_eps: 0.196656
 1269570/2000000: episode: 1755, duration: 54.467s, episode steps: 881, steps per second:  16, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.024003, mae: 2.788378, mean_q: 3.357471, mean_eps: 0.196218
 1270345/2000000: episode: 1756, duration: 47.950s, episode steps: 775, steps per second:  16, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.023247, mae: 2.776539, mean_q: 3.345753, mean_eps: 0.195693
 1271053/2000000: episode: 1757, duration: 43.602s, episode steps: 708, steps per second:  16, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.309 [0.000, 5.000],  loss: 0.020894, mae: 2.775723, mean_q: 3.346029, mean_eps: 0.195223
 1271429/2000000: episode: 1758, duration: 22.880s, episode steps: 376, steps per second:  16, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.745 [0.000, 5.000],  loss: 0.020182, mae: 2.744675, mean_q: 3.310534, mean_eps: 0.194880
 1272057/2000000: episode: 1759, duration: 38.425s, episode steps: 628, steps per second:  16, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.710 [0.000, 5.000],  loss: 0.021271, mae: 2.784138, mean_q: 3.355389, mean_eps: 0.194562
 1272993/2000000: episode: 1760, duration: 57.015s, episode steps: 936, steps per second:  16, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.023377, mae: 2.769474, mean_q: 3.336079, mean_eps: 0.194067
 1273604/2000000: episode: 1761, duration: 37.444s, episode steps: 611, steps per second:  16, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.802 [0.000, 5.000],  loss: 0.023514, mae: 2.767741, mean_q: 3.332651, mean_eps: 0.193578
 1274767/2000000: episode: 1762, duration: 70.999s, episode steps: 1163, steps per second:  16, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.183 [0.000, 5.000],  loss: 0.023191, mae: 2.767323, mean_q: 3.332217, mean_eps: 0.193017
 1275918/2000000: episode: 1763, duration: 70.553s, episode steps: 1151, steps per second:  16, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.652 [0.000, 5.000],  loss: 0.021881, mae: 2.768453, mean_q: 3.333781, mean_eps: 0.192283
 1276916/2000000: episode: 1764, duration: 61.441s, episode steps: 998, steps per second:  16, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.091 [0.000, 5.000],  loss: 0.023421, mae: 2.769718, mean_q: 3.335840, mean_eps: 0.191603
 1277913/2000000: episode: 1765, duration: 61.485s, episode steps: 997, steps per second:  16, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.022680, mae: 2.766347, mean_q: 3.333771, mean_eps: 0.190971
 1279194/2000000: episode: 1766, duration: 78.181s, episode steps: 1281, steps per second:  16, episode reward: 25.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.022408, mae: 2.767576, mean_q: 3.334147, mean_eps: 0.190249
 1279858/2000000: episode: 1767, duration: 40.762s, episode steps: 664, steps per second:  16, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.158 [0.000, 5.000],  loss: 0.023421, mae: 2.781821, mean_q: 3.351662, mean_eps: 0.189634
 1280352/2000000: episode: 1768, duration: 30.384s, episode steps: 494, steps per second:  16, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.053 [0.000, 5.000],  loss: 0.018192, mae: 2.773564, mean_q: 3.347481, mean_eps: 0.189267
 1281405/2000000: episode: 1769, duration: 64.772s, episode steps: 1053, steps per second:  16, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.021413, mae: 2.790047, mean_q: 3.363140, mean_eps: 0.188777
 1282033/2000000: episode: 1770, duration: 38.572s, episode steps: 628, steps per second:  16, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.228 [0.000, 5.000],  loss: 0.022201, mae: 2.800172, mean_q: 3.374777, mean_eps: 0.188244
 1283178/2000000: episode: 1771, duration: 70.045s, episode steps: 1145, steps per second:  16, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.992 [0.000, 5.000],  loss: 0.023727, mae: 2.781331, mean_q: 3.352170, mean_eps: 0.187683
 1283568/2000000: episode: 1772, duration: 24.094s, episode steps: 390, steps per second:  16, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.021372, mae: 2.765597, mean_q: 3.331225, mean_eps: 0.187198
 1284278/2000000: episode: 1773, duration: 43.749s, episode steps: 710, steps per second:  16, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.022903, mae: 2.778897, mean_q: 3.348742, mean_eps: 0.186849
 1284838/2000000: episode: 1774, duration: 34.528s, episode steps: 560, steps per second:  16, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.022410, mae: 2.778722, mean_q: 3.345339, mean_eps: 0.186447
 1285552/2000000: episode: 1775, duration: 44.317s, episode steps: 714, steps per second:  16, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.758 [0.000, 5.000],  loss: 0.023224, mae: 2.785832, mean_q: 3.356589, mean_eps: 0.186044
 1286406/2000000: episode: 1776, duration: 52.403s, episode steps: 854, steps per second:  16, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.830 [0.000, 5.000],  loss: 0.021274, mae: 2.781371, mean_q: 3.350650, mean_eps: 0.185547
 1287386/2000000: episode: 1777, duration: 59.973s, episode steps: 980, steps per second:  16, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.022492, mae: 2.781657, mean_q: 3.352165, mean_eps: 0.184966
 1288103/2000000: episode: 1778, duration: 43.741s, episode steps: 717, steps per second:  16, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.105 [0.000, 5.000],  loss: 0.024599, mae: 2.780678, mean_q: 3.349717, mean_eps: 0.184429
 1288712/2000000: episode: 1779, duration: 37.233s, episode steps: 609, steps per second:  16, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.852 [0.000, 5.000],  loss: 0.024477, mae: 2.771193, mean_q: 3.338443, mean_eps: 0.184010
 1289952/2000000: episode: 1780, duration: 76.188s, episode steps: 1240, steps per second:  16, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.023325, mae: 2.774092, mean_q: 3.339827, mean_eps: 0.183424
 1291014/2000000: episode: 1781, duration: 64.753s, episode steps: 1062, steps per second:  16, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.020652, mae: 2.788428, mean_q: 3.359916, mean_eps: 0.182695
 1292149/2000000: episode: 1782, duration: 69.275s, episode steps: 1135, steps per second:  16, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.020686, mae: 2.790398, mean_q: 3.363747, mean_eps: 0.181998
 1293455/2000000: episode: 1783, duration: 79.428s, episode steps: 1306, steps per second:  16, episode reward: 22.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.064 [0.000, 5.000],  loss: 0.021922, mae: 2.790421, mean_q: 3.362265, mean_eps: 0.181225
 1293797/2000000: episode: 1784, duration: 21.002s, episode steps: 342, steps per second:  16, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 3.158 [0.000, 5.000],  loss: 0.022003, mae: 2.779005, mean_q: 3.350771, mean_eps: 0.180704
 1294889/2000000: episode: 1785, duration: 66.465s, episode steps: 1092, steps per second:  16, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.021155, mae: 2.797262, mean_q: 3.371700, mean_eps: 0.180249
 1295794/2000000: episode: 1786, duration: 54.959s, episode steps: 905, steps per second:  16, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.273 [0.000, 5.000],  loss: 0.021816, mae: 2.779424, mean_q: 3.348690, mean_eps: 0.179617
 1296590/2000000: episode: 1787, duration: 48.509s, episode steps: 796, steps per second:  16, episode reward: 25.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.741 [0.000, 5.000],  loss: 0.021034, mae: 2.772619, mean_q: 3.339400, mean_eps: 0.179078
 1297284/2000000: episode: 1788, duration: 42.650s, episode steps: 694, steps per second:  16, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.022753, mae: 2.774766, mean_q: 3.342582, mean_eps: 0.178607
 1297829/2000000: episode: 1789, duration: 33.759s, episode steps: 545, steps per second:  16, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.991 [0.000, 5.000],  loss: 0.022449, mae: 2.774711, mean_q: 3.345333, mean_eps: 0.178215
 1298523/2000000: episode: 1790, duration: 42.825s, episode steps: 694, steps per second:  16, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.021631, mae: 2.778603, mean_q: 3.350102, mean_eps: 0.177822
 1299499/2000000: episode: 1791, duration: 59.919s, episode steps: 976, steps per second:  16, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.791 [0.000, 5.000],  loss: 0.022287, mae: 2.776178, mean_q: 3.343037, mean_eps: 0.177294
 1300463/2000000: episode: 1792, duration: 60.345s, episode steps: 964, steps per second:  16, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.020457, mae: 2.772797, mean_q: 3.341427, mean_eps: 0.176679
 1301124/2000000: episode: 1793, duration: 40.279s, episode steps: 661, steps per second:  16, episode reward: 20.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.946 [0.000, 5.000],  loss: 0.021515, mae: 2.766107, mean_q: 3.331441, mean_eps: 0.176165
 1301577/2000000: episode: 1794, duration: 27.975s, episode steps: 453, steps per second:  16, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.020835, mae: 2.771410, mean_q: 3.336646, mean_eps: 0.175812
 1302590/2000000: episode: 1795, duration: 61.419s, episode steps: 1013, steps per second:  16, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.155 [0.000, 5.000],  loss: 0.022440, mae: 2.767059, mean_q: 3.333494, mean_eps: 0.175347
 1303296/2000000: episode: 1796, duration: 43.206s, episode steps: 706, steps per second:  16, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.023796, mae: 2.776438, mean_q: 3.344949, mean_eps: 0.174803
 1304885/2000000: episode: 1797, duration: 97.110s, episode steps: 1589, steps per second:  16, episode reward: 31.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.023119, mae: 2.761435, mean_q: 3.328102, mean_eps: 0.174076
 1305975/2000000: episode: 1798, duration: 66.726s, episode steps: 1090, steps per second:  16, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.699 [0.000, 5.000],  loss: 0.022516, mae: 2.747127, mean_q: 3.311359, mean_eps: 0.173228
 1306806/2000000: episode: 1799, duration: 50.728s, episode steps: 831, steps per second:  16, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.813 [0.000, 5.000],  loss: 0.021784, mae: 2.758758, mean_q: 3.323714, mean_eps: 0.172620
 1308018/2000000: episode: 1800, duration: 73.952s, episode steps: 1212, steps per second:  16, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.505 [0.000, 5.000],  loss: 0.023289, mae: 2.769160, mean_q: 3.336532, mean_eps: 0.171972
 1308646/2000000: episode: 1801, duration: 38.580s, episode steps: 628, steps per second:  16, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.689 [0.000, 5.000],  loss: 0.021181, mae: 2.769707, mean_q: 3.337828, mean_eps: 0.171390
 1309021/2000000: episode: 1802, duration: 22.908s, episode steps: 375, steps per second:  16, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.020632, mae: 2.756038, mean_q: 3.320257, mean_eps: 0.171072
 1309437/2000000: episode: 1803, duration: 25.631s, episode steps: 416, steps per second:  16, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.021152, mae: 2.756741, mean_q: 3.320810, mean_eps: 0.170821
 1310011/2000000: episode: 1804, duration: 35.258s, episode steps: 574, steps per second:  16, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.111 [0.000, 5.000],  loss: 0.023272, mae: 2.744117, mean_q: 3.305951, mean_eps: 0.170508
 1310578/2000000: episode: 1805, duration: 34.817s, episode steps: 567, steps per second:  16, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.019282, mae: 2.773648, mean_q: 3.344944, mean_eps: 0.170147
 1311120/2000000: episode: 1806, duration: 33.164s, episode steps: 542, steps per second:  16, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.022754, mae: 2.740058, mean_q: 3.300281, mean_eps: 0.169796
 1312332/2000000: episode: 1807, duration: 74.559s, episode steps: 1212, steps per second:  16, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.022104, mae: 2.757929, mean_q: 3.324095, mean_eps: 0.169241
 1313233/2000000: episode: 1808, duration: 55.393s, episode steps: 901, steps per second:  16, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.021056, mae: 2.742535, mean_q: 3.302957, mean_eps: 0.168571
 1314367/2000000: episode: 1809, duration: 69.347s, episode steps: 1134, steps per second:  16, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.021416, mae: 2.755191, mean_q: 3.321086, mean_eps: 0.167927
 1315057/2000000: episode: 1810, duration: 42.515s, episode steps: 690, steps per second:  16, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.075 [0.000, 5.000],  loss: 0.022185, mae: 2.762413, mean_q: 3.327993, mean_eps: 0.167349
 1316141/2000000: episode: 1811, duration: 66.297s, episode steps: 1084, steps per second:  16, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.851 [0.000, 5.000],  loss: 0.023215, mae: 2.744601, mean_q: 3.306910, mean_eps: 0.166787
 1316663/2000000: episode: 1812, duration: 31.760s, episode steps: 522, steps per second:  16, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.172 [0.000, 5.000],  loss: 0.021007, mae: 2.760944, mean_q: 3.327010, mean_eps: 0.166279
 1317187/2000000: episode: 1813, duration: 32.231s, episode steps: 524, steps per second:  16, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.771 [0.000, 5.000],  loss: 0.023083, mae: 2.755463, mean_q: 3.319022, mean_eps: 0.165948
 1318432/2000000: episode: 1814, duration: 75.819s, episode steps: 1245, steps per second:  16, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.022600, mae: 2.757929, mean_q: 3.321943, mean_eps: 0.165388
 1318905/2000000: episode: 1815, duration: 29.284s, episode steps: 473, steps per second:  16, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.123 [0.000, 5.000],  loss: 0.022365, mae: 2.757111, mean_q: 3.325345, mean_eps: 0.164844
 1319970/2000000: episode: 1816, duration: 65.694s, episode steps: 1065, steps per second:  16, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: 0.023392, mae: 2.751978, mean_q: 3.315484, mean_eps: 0.164356
 1320474/2000000: episode: 1817, duration: 30.894s, episode steps: 504, steps per second:  16, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.019706, mae: 2.737075, mean_q: 3.299916, mean_eps: 0.163859
 1321523/2000000: episode: 1818, duration: 63.852s, episode steps: 1049, steps per second:  16, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.020877, mae: 2.732428, mean_q: 3.293334, mean_eps: 0.163368
 1322702/2000000: episode: 1819, duration: 72.087s, episode steps: 1179, steps per second:  16, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.714 [0.000, 5.000],  loss: 0.022536, mae: 2.721897, mean_q: 3.278664, mean_eps: 0.162662
 1323360/2000000: episode: 1820, duration: 40.550s, episode steps: 658, steps per second:  16, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.790 [0.000, 5.000],  loss: 0.022111, mae: 2.736318, mean_q: 3.298017, mean_eps: 0.162081
 1324178/2000000: episode: 1821, duration: 50.915s, episode steps: 818, steps per second:  16, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.022152, mae: 2.742027, mean_q: 3.305784, mean_eps: 0.161614
 1324728/2000000: episode: 1822, duration: 33.858s, episode steps: 550, steps per second:  16, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.021887, mae: 2.730281, mean_q: 3.288884, mean_eps: 0.161180
 1325540/2000000: episode: 1823, duration: 49.853s, episode steps: 812, steps per second:  16, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.161 [0.000, 5.000],  loss: 0.021411, mae: 2.715707, mean_q: 3.271352, mean_eps: 0.160750
 1326174/2000000: episode: 1824, duration: 40.249s, episode steps: 634, steps per second:  16, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.312 [0.000, 5.000],  loss: 0.022190, mae: 2.738507, mean_q: 3.297828, mean_eps: 0.160291
 1326798/2000000: episode: 1825, duration: 38.690s, episode steps: 624, steps per second:  16, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.679 [0.000, 5.000],  loss: 0.020434, mae: 2.734518, mean_q: 3.293788, mean_eps: 0.159892
 1327667/2000000: episode: 1826, duration: 53.864s, episode steps: 869, steps per second:  16, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.323 [0.000, 5.000],  loss: 0.022501, mae: 2.722533, mean_q: 3.276636, mean_eps: 0.159420
 1328429/2000000: episode: 1827, duration: 46.733s, episode steps: 762, steps per second:  16, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.022974, mae: 2.715720, mean_q: 3.270711, mean_eps: 0.158903
 1329397/2000000: episode: 1828, duration: 59.228s, episode steps: 968, steps per second:  16, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.021701, mae: 2.718120, mean_q: 3.274193, mean_eps: 0.158354
 1330074/2000000: episode: 1829, duration: 41.723s, episode steps: 677, steps per second:  16, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.023693, mae: 2.724502, mean_q: 3.279806, mean_eps: 0.157834
 1330888/2000000: episode: 1830, duration: 49.892s, episode steps: 814, steps per second:  16, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.084 [0.000, 5.000],  loss: 0.019138, mae: 2.761497, mean_q: 3.325469, mean_eps: 0.157363
 1332015/2000000: episode: 1831, duration: 68.917s, episode steps: 1127, steps per second:  16, episode reward: 15.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.019491, mae: 2.762379, mean_q: 3.327584, mean_eps: 0.156748
 1332660/2000000: episode: 1832, duration: 39.632s, episode steps: 645, steps per second:  16, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.020259, mae: 2.738081, mean_q: 3.297057, mean_eps: 0.156187
 1333282/2000000: episode: 1833, duration: 38.254s, episode steps: 622, steps per second:  16, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.770 [0.000, 5.000],  loss: 0.020670, mae: 2.754703, mean_q: 3.317548, mean_eps: 0.155786
 1334656/2000000: episode: 1834, duration: 84.009s, episode steps: 1374, steps per second:  16, episode reward: 18.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.020556, mae: 2.750210, mean_q: 3.311878, mean_eps: 0.155154
 1335156/2000000: episode: 1835, duration: 30.870s, episode steps: 500, steps per second:  16, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.326 [0.000, 5.000],  loss: 0.020125, mae: 2.753535, mean_q: 3.315795, mean_eps: 0.154561
 1335661/2000000: episode: 1836, duration: 31.067s, episode steps: 505, steps per second:  16, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.907 [0.000, 5.000],  loss: 0.022228, mae: 2.747516, mean_q: 3.304893, mean_eps: 0.154242
 1336409/2000000: episode: 1837, duration: 45.757s, episode steps: 748, steps per second:  16, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.197 [0.000, 5.000],  loss: 0.019809, mae: 2.764543, mean_q: 3.327470, mean_eps: 0.153844
 1337510/2000000: episode: 1838, duration: 67.317s, episode steps: 1101, steps per second:  16, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.884 [0.000, 5.000],  loss: 0.020387, mae: 2.756019, mean_q: 3.315582, mean_eps: 0.153259
 1338159/2000000: episode: 1839, duration: 39.618s, episode steps: 649, steps per second:  16, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.263 [0.000, 5.000],  loss: 0.023085, mae: 2.740338, mean_q: 3.298857, mean_eps: 0.152705
 1338801/2000000: episode: 1840, duration: 39.514s, episode steps: 642, steps per second:  16, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.021964, mae: 2.750507, mean_q: 3.310229, mean_eps: 0.152296
 1339966/2000000: episode: 1841, duration: 71.198s, episode steps: 1165, steps per second:  16, episode reward: 31.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.319 [0.000, 5.000],  loss: 0.021295, mae: 2.738163, mean_q: 3.296185, mean_eps: 0.151723
 1340760/2000000: episode: 1842, duration: 48.804s, episode steps: 794, steps per second:  16, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.676 [0.000, 5.000],  loss: 0.017037, mae: 2.757574, mean_q: 3.323929, mean_eps: 0.151104
 1341259/2000000: episode: 1843, duration: 30.543s, episode steps: 499, steps per second:  16, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.323 [0.000, 5.000],  loss: 0.019789, mae: 2.749210, mean_q: 3.310600, mean_eps: 0.150695
 1341731/2000000: episode: 1844, duration: 28.913s, episode steps: 472, steps per second:  16, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.019416, mae: 2.727402, mean_q: 3.287600, mean_eps: 0.150387
 1342894/2000000: episode: 1845, duration: 71.306s, episode steps: 1163, steps per second:  16, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.020744, mae: 2.760228, mean_q: 3.323680, mean_eps: 0.149869
 1343835/2000000: episode: 1846, duration: 57.965s, episode steps: 941, steps per second:  16, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.794 [0.000, 5.000],  loss: 0.021531, mae: 2.752466, mean_q: 3.313646, mean_eps: 0.149203
 1344311/2000000: episode: 1847, duration: 29.448s, episode steps: 476, steps per second:  16, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 3.002 [0.000, 5.000],  loss: 0.021434, mae: 2.763697, mean_q: 3.326467, mean_eps: 0.148754
 1344903/2000000: episode: 1848, duration: 36.126s, episode steps: 592, steps per second:  16, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.775 [0.000, 5.000],  loss: 0.021142, mae: 2.753519, mean_q: 3.315184, mean_eps: 0.148416
 1345587/2000000: episode: 1849, duration: 41.722s, episode steps: 684, steps per second:  16, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: 0.020489, mae: 2.756323, mean_q: 3.318825, mean_eps: 0.148012
 1346143/2000000: episode: 1850, duration: 34.035s, episode steps: 556, steps per second:  16, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.736 [0.000, 5.000],  loss: 0.020913, mae: 2.743098, mean_q: 3.302185, mean_eps: 0.147619
 1346518/2000000: episode: 1851, duration: 22.919s, episode steps: 375, steps per second:  16, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.020416, mae: 2.746916, mean_q: 3.308086, mean_eps: 0.147324
 1347024/2000000: episode: 1852, duration: 30.850s, episode steps: 506, steps per second:  16, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.021762, mae: 2.760494, mean_q: 3.323307, mean_eps: 0.147046
 1347628/2000000: episode: 1853, duration: 37.430s, episode steps: 604, steps per second:  16, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.020835, mae: 2.750927, mean_q: 3.313916, mean_eps: 0.146695
 1348146/2000000: episode: 1854, duration: 31.748s, episode steps: 518, steps per second:  16, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.018151, mae: 2.751684, mean_q: 3.313657, mean_eps: 0.146339
 1348825/2000000: episode: 1855, duration: 41.775s, episode steps: 679, steps per second:  16, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.023268, mae: 2.740551, mean_q: 3.299227, mean_eps: 0.145959
 1349239/2000000: episode: 1856, duration: 25.359s, episode steps: 414, steps per second:  16, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.020433, mae: 2.752609, mean_q: 3.316003, mean_eps: 0.145613
 1350343/2000000: episode: 1857, duration: 68.478s, episode steps: 1104, steps per second:  16, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.163 [0.000, 5.000],  loss: 0.021041, mae: 2.745320, mean_q: 3.306137, mean_eps: 0.145133
 1351303/2000000: episode: 1858, duration: 58.644s, episode steps: 960, steps per second:  16, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.125 [0.000, 5.000],  loss: 0.018494, mae: 2.754760, mean_q: 3.317205, mean_eps: 0.144479
 1351870/2000000: episode: 1859, duration: 34.414s, episode steps: 567, steps per second:  16, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.136 [0.000, 5.000],  loss: 0.019387, mae: 2.743190, mean_q: 3.302745, mean_eps: 0.143996
 1352860/2000000: episode: 1860, duration: 60.723s, episode steps: 990, steps per second:  16, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.021538, mae: 2.760466, mean_q: 3.326144, mean_eps: 0.143503
 1353899/2000000: episode: 1861, duration: 63.442s, episode steps: 1039, steps per second:  16, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.020650, mae: 2.748118, mean_q: 3.308530, mean_eps: 0.142861
 1354550/2000000: episode: 1862, duration: 40.017s, episode steps: 651, steps per second:  16, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.020729, mae: 2.740481, mean_q: 3.297316, mean_eps: 0.142325
 1355399/2000000: episode: 1863, duration: 52.102s, episode steps: 849, steps per second:  16, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.978 [0.000, 5.000],  loss: 0.023050, mae: 2.737022, mean_q: 3.292928, mean_eps: 0.141850
 1356014/2000000: episode: 1864, duration: 37.592s, episode steps: 615, steps per second:  16, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.820 [0.000, 5.000],  loss: 0.021175, mae: 2.732180, mean_q: 3.289262, mean_eps: 0.141386
 1356663/2000000: episode: 1865, duration: 39.809s, episode steps: 649, steps per second:  16, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.020497, mae: 2.756292, mean_q: 3.319848, mean_eps: 0.140986
 1357051/2000000: episode: 1866, duration: 24.177s, episode steps: 388, steps per second:  16, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.021180, mae: 2.755862, mean_q: 3.319275, mean_eps: 0.140658
 1357653/2000000: episode: 1867, duration: 36.865s, episode steps: 602, steps per second:  16, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.021077, mae: 2.768265, mean_q: 3.334472, mean_eps: 0.140344
 1358375/2000000: episode: 1868, duration: 44.412s, episode steps: 722, steps per second:  16, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.054 [0.000, 5.000],  loss: 0.020783, mae: 2.733179, mean_q: 3.292700, mean_eps: 0.139924
 1359089/2000000: episode: 1869, duration: 44.107s, episode steps: 714, steps per second:  16, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.020540, mae: 2.738059, mean_q: 3.297639, mean_eps: 0.139470
 1359763/2000000: episode: 1870, duration: 41.062s, episode steps: 674, steps per second:  16, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.156 [0.000, 5.000],  loss: 0.021194, mae: 2.750407, mean_q: 3.312338, mean_eps: 0.139030
 1360422/2000000: episode: 1871, duration: 40.464s, episode steps: 659, steps per second:  16, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.020628, mae: 2.766669, mean_q: 3.333213, mean_eps: 0.138608
 1361321/2000000: episode: 1872, duration: 55.052s, episode steps: 899, steps per second:  16, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.082 [0.000, 5.000],  loss: 0.020874, mae: 2.750427, mean_q: 3.313838, mean_eps: 0.138114
 1362127/2000000: episode: 1873, duration: 49.366s, episode steps: 806, steps per second:  16, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.018798, mae: 2.739936, mean_q: 3.300256, mean_eps: 0.137575
 1362987/2000000: episode: 1874, duration: 52.322s, episode steps: 860, steps per second:  16, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.020690, mae: 2.740948, mean_q: 3.299211, mean_eps: 0.137048
 1364682/2000000: episode: 1875, duration: 103.484s, episode steps: 1695, steps per second:  16, episode reward: 32.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.678 [0.000, 5.000],  loss: 0.023001, mae: 2.759502, mean_q: 3.321851, mean_eps: 0.136238
 1365802/2000000: episode: 1876, duration: 68.508s, episode steps: 1120, steps per second:  16, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.204 [0.000, 5.000],  loss: 0.020593, mae: 2.740626, mean_q: 3.301028, mean_eps: 0.135347
 1366266/2000000: episode: 1877, duration: 28.384s, episode steps: 464, steps per second:  16, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.784 [0.000, 5.000],  loss: 0.022013, mae: 2.753322, mean_q: 3.318528, mean_eps: 0.134845
 1367214/2000000: episode: 1878, duration: 57.935s, episode steps: 948, steps per second:  16, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.021578, mae: 2.740455, mean_q: 3.302523, mean_eps: 0.134398
 1367983/2000000: episode: 1879, duration: 46.899s, episode steps: 769, steps per second:  16, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.901 [0.000, 5.000],  loss: 0.019612, mae: 2.748256, mean_q: 3.311061, mean_eps: 0.133855
 1368962/2000000: episode: 1880, duration: 59.942s, episode steps: 979, steps per second:  16, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.219 [0.000, 5.000],  loss: 0.021343, mae: 2.751897, mean_q: 3.313306, mean_eps: 0.133301
 1369630/2000000: episode: 1881, duration: 40.590s, episode steps: 668, steps per second:  16, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.022081, mae: 2.740495, mean_q: 3.298259, mean_eps: 0.132779
 1370418/2000000: episode: 1882, duration: 48.199s, episode steps: 788, steps per second:  16, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.877 [0.000, 5.000],  loss: 0.018839, mae: 2.755273, mean_q: 3.317936, mean_eps: 0.132318
 1371613/2000000: episode: 1883, duration: 73.149s, episode steps: 1195, steps per second:  16, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.200 [0.000, 5.000],  loss: 0.018433, mae: 2.745625, mean_q: 3.308438, mean_eps: 0.131690
 1372314/2000000: episode: 1884, duration: 43.012s, episode steps: 701, steps per second:  16, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.728 [0.000, 5.000],  loss: 0.019839, mae: 2.742939, mean_q: 3.305025, mean_eps: 0.131089
 1372722/2000000: episode: 1885, duration: 25.214s, episode steps: 408, steps per second:  16, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.306 [0.000, 5.000],  loss: 0.020685, mae: 2.745034, mean_q: 3.305998, mean_eps: 0.130739
 1373202/2000000: episode: 1886, duration: 29.751s, episode steps: 480, steps per second:  16, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 0.018574, mae: 2.747367, mean_q: 3.312645, mean_eps: 0.130457
 1373672/2000000: episode: 1887, duration: 28.741s, episode steps: 470, steps per second:  16, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.019671, mae: 2.749765, mean_q: 3.311733, mean_eps: 0.130157
 1374949/2000000: episode: 1888, duration: 78.773s, episode steps: 1277, steps per second:  16, episode reward: 26.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.033 [0.000, 5.000],  loss: 0.019901, mae: 2.743294, mean_q: 3.303040, mean_eps: 0.129604
 1375832/2000000: episode: 1889, duration: 53.854s, episode steps: 883, steps per second:  16, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.130 [0.000, 5.000],  loss: 0.020257, mae: 2.740967, mean_q: 3.298950, mean_eps: 0.128920
 1376764/2000000: episode: 1890, duration: 57.202s, episode steps: 932, steps per second:  16, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.983 [0.000, 5.000],  loss: 0.019210, mae: 2.738759, mean_q: 3.296078, mean_eps: 0.128346
 1377682/2000000: episode: 1891, duration: 56.809s, episode steps: 918, steps per second:  16, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.020772, mae: 2.737290, mean_q: 3.294997, mean_eps: 0.127759
 1378445/2000000: episode: 1892, duration: 46.762s, episode steps: 763, steps per second:  16, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.738 [0.000, 5.000],  loss: 0.021348, mae: 2.744790, mean_q: 3.301219, mean_eps: 0.127226
 1379820/2000000: episode: 1893, duration: 83.821s, episode steps: 1375, steps per second:  16, episode reward: 26.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.020769, mae: 2.750483, mean_q: 3.309744, mean_eps: 0.126550
 1380573/2000000: episode: 1894, duration: 46.582s, episode steps: 753, steps per second:  16, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.020550, mae: 2.768889, mean_q: 3.335361, mean_eps: 0.125876
 1381660/2000000: episode: 1895, duration: 66.226s, episode steps: 1087, steps per second:  16, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.832 [0.000, 5.000],  loss: 0.020996, mae: 2.776840, mean_q: 3.344932, mean_eps: 0.125293
 1382825/2000000: episode: 1896, duration: 71.551s, episode steps: 1165, steps per second:  16, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.020163, mae: 2.771324, mean_q: 3.338274, mean_eps: 0.124580
 1383811/2000000: episode: 1897, duration: 60.305s, episode steps: 986, steps per second:  16, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.086 [0.000, 5.000],  loss: 0.021032, mae: 2.764047, mean_q: 3.329144, mean_eps: 0.123899
 1384745/2000000: episode: 1898, duration: 59.069s, episode steps: 934, steps per second:  16, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.020636, mae: 2.764674, mean_q: 3.328868, mean_eps: 0.123291
 1385388/2000000: episode: 1899, duration: 39.627s, episode steps: 643, steps per second:  16, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.020127, mae: 2.766288, mean_q: 3.329423, mean_eps: 0.122792
 1386344/2000000: episode: 1900, duration: 59.791s, episode steps: 956, steps per second:  16, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.019694, mae: 2.751888, mean_q: 3.313392, mean_eps: 0.122286
 1387107/2000000: episode: 1901, duration: 46.874s, episode steps: 763, steps per second:  16, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.810 [0.000, 5.000],  loss: 0.021356, mae: 2.751848, mean_q: 3.310701, mean_eps: 0.121741
 1387834/2000000: episode: 1902, duration: 44.701s, episode steps: 727, steps per second:  16, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.021721, mae: 2.761166, mean_q: 3.323575, mean_eps: 0.121269
 1388970/2000000: episode: 1903, duration: 69.801s, episode steps: 1136, steps per second:  16, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.020318, mae: 2.755947, mean_q: 3.316725, mean_eps: 0.120679
 1389885/2000000: episode: 1904, duration: 56.016s, episode steps: 915, steps per second:  16, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.020881, mae: 2.758661, mean_q: 3.319259, mean_eps: 0.120029
 1390462/2000000: episode: 1905, duration: 35.189s, episode steps: 577, steps per second:  16, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.789 [0.000, 5.000],  loss: 0.018026, mae: 2.782570, mean_q: 3.353461, mean_eps: 0.119556
 1390971/2000000: episode: 1906, duration: 30.940s, episode steps: 509, steps per second:  16, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.721 [0.000, 5.000],  loss: 0.018124, mae: 2.783943, mean_q: 3.351780, mean_eps: 0.119213
 1391483/2000000: episode: 1907, duration: 31.111s, episode steps: 512, steps per second:  16, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.963 [0.000, 5.000],  loss: 0.017800, mae: 2.788386, mean_q: 3.356436, mean_eps: 0.118890
 1392533/2000000: episode: 1908, duration: 64.201s, episode steps: 1050, steps per second:  16, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.021467, mae: 2.778351, mean_q: 3.344387, mean_eps: 0.118395
 1393496/2000000: episode: 1909, duration: 58.824s, episode steps: 963, steps per second:  16, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.063 [0.000, 5.000],  loss: 0.021145, mae: 2.790357, mean_q: 3.359285, mean_eps: 0.117758
 1394063/2000000: episode: 1910, duration: 34.767s, episode steps: 567, steps per second:  16, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.051 [0.000, 5.000],  loss: 0.020879, mae: 2.778793, mean_q: 3.343636, mean_eps: 0.117274
 1395046/2000000: episode: 1911, duration: 60.114s, episode steps: 983, steps per second:  16, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.019807, mae: 2.792193, mean_q: 3.361886, mean_eps: 0.116782
 1395585/2000000: episode: 1912, duration: 32.998s, episode steps: 539, steps per second:  16, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.019912, mae: 2.798918, mean_q: 3.367631, mean_eps: 0.116300
 1396537/2000000: episode: 1913, duration: 57.922s, episode steps: 952, steps per second:  16, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.922 [0.000, 5.000],  loss: 0.021288, mae: 2.789725, mean_q: 3.358301, mean_eps: 0.115827
 1397079/2000000: episode: 1914, duration: 32.974s, episode steps: 542, steps per second:  16, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.212 [0.000, 5.000],  loss: 0.022704, mae: 2.801516, mean_q: 3.373147, mean_eps: 0.115355
 1398003/2000000: episode: 1915, duration: 56.279s, episode steps: 924, steps per second:  16, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.845 [0.000, 5.000],  loss: 0.021562, mae: 2.774140, mean_q: 3.340999, mean_eps: 0.114891
 1399064/2000000: episode: 1916, duration: 65.120s, episode steps: 1061, steps per second:  16, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.021608, mae: 2.775748, mean_q: 3.341790, mean_eps: 0.114263
 1400145/2000000: episode: 1917, duration: 67.555s, episode steps: 1081, steps per second:  16, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.020962, mae: 2.782607, mean_q: 3.350217, mean_eps: 0.113584
 1400887/2000000: episode: 1918, duration: 45.263s, episode steps: 742, steps per second:  16, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.235 [0.000, 5.000],  loss: 0.021127, mae: 2.754832, mean_q: 3.318010, mean_eps: 0.113007
 1401780/2000000: episode: 1919, duration: 54.930s, episode steps: 893, steps per second:  16, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.999 [0.000, 5.000],  loss: 0.019715, mae: 2.760091, mean_q: 3.323246, mean_eps: 0.112490
 1402701/2000000: episode: 1920, duration: 56.446s, episode steps: 921, steps per second:  16, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.068 [0.000, 5.000],  loss: 0.020591, mae: 2.764324, mean_q: 3.327260, mean_eps: 0.111915
 1403769/2000000: episode: 1921, duration: 65.627s, episode steps: 1068, steps per second:  16, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.020663, mae: 2.751450, mean_q: 3.312874, mean_eps: 0.111284
 1404867/2000000: episode: 1922, duration: 67.056s, episode steps: 1098, steps per second:  16, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.835 [0.000, 5.000],  loss: 0.021960, mae: 2.750278, mean_q: 3.312991, mean_eps: 0.110599
 1405991/2000000: episode: 1923, duration: 68.365s, episode steps: 1124, steps per second:  16, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.069 [0.000, 5.000],  loss: 0.020722, mae: 2.763294, mean_q: 3.326797, mean_eps: 0.109896
 1406661/2000000: episode: 1924, duration: 40.912s, episode steps: 670, steps per second:  16, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.793 [0.000, 5.000],  loss: 0.020251, mae: 2.753703, mean_q: 3.318651, mean_eps: 0.109327
 1407458/2000000: episode: 1925, duration: 48.542s, episode steps: 797, steps per second:  16, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.023455, mae: 2.751730, mean_q: 3.315879, mean_eps: 0.108862
 1408391/2000000: episode: 1926, duration: 57.048s, episode steps: 933, steps per second:  16, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.281 [0.000, 5.000],  loss: 0.021904, mae: 2.756456, mean_q: 3.318286, mean_eps: 0.108315
 1409099/2000000: episode: 1927, duration: 43.310s, episode steps: 708, steps per second:  16, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.839 [0.000, 5.000],  loss: 0.022572, mae: 2.767115, mean_q: 3.330787, mean_eps: 0.107795
 1410143/2000000: episode: 1928, duration: 63.680s, episode steps: 1044, steps per second:  16, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.020502, mae: 2.753321, mean_q: 3.314550, mean_eps: 0.107241
 1410638/2000000: episode: 1929, duration: 30.122s, episode steps: 495, steps per second:  16, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.826 [0.000, 5.000],  loss: 0.017299, mae: 2.778731, mean_q: 3.349050, mean_eps: 0.106753
 1411767/2000000: episode: 1930, duration: 69.081s, episode steps: 1129, steps per second:  16, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.318 [0.000, 5.000],  loss: 0.019734, mae: 2.777846, mean_q: 3.344896, mean_eps: 0.106239
 1412481/2000000: episode: 1931, duration: 43.899s, episode steps: 714, steps per second:  16, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.697 [0.000, 5.000],  loss: 0.022957, mae: 2.779750, mean_q: 3.348185, mean_eps: 0.105655
 1412917/2000000: episode: 1932, duration: 26.757s, episode steps: 436, steps per second:  16, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.131 [0.000, 5.000],  loss: 0.020779, mae: 2.755049, mean_q: 3.317957, mean_eps: 0.105290
 1413459/2000000: episode: 1933, duration: 33.120s, episode steps: 542, steps per second:  16, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.055 [0.000, 5.000],  loss: 0.023716, mae: 2.777050, mean_q: 3.342904, mean_eps: 0.104981
 1414574/2000000: episode: 1934, duration: 68.295s, episode steps: 1115, steps per second:  16, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.021660, mae: 2.760191, mean_q: 3.325861, mean_eps: 0.104457
 1415330/2000000: episode: 1935, duration: 46.337s, episode steps: 756, steps per second:  16, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.262 [0.000, 5.000],  loss: 0.020016, mae: 2.765511, mean_q: 3.331561, mean_eps: 0.103864
 1416168/2000000: episode: 1936, duration: 51.901s, episode steps: 838, steps per second:  16, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.681 [0.000, 5.000],  loss: 0.020770, mae: 2.765081, mean_q: 3.330069, mean_eps: 0.103360
 1416913/2000000: episode: 1937, duration: 46.027s, episode steps: 745, steps per second:  16, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.138 [0.000, 5.000],  loss: 0.021354, mae: 2.775553, mean_q: 3.341352, mean_eps: 0.102858
 1417850/2000000: episode: 1938, duration: 57.370s, episode steps: 937, steps per second:  16, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.020557, mae: 2.762871, mean_q: 3.325658, mean_eps: 0.102325
 1418501/2000000: episode: 1939, duration: 40.145s, episode steps: 651, steps per second:  16, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: 0.020363, mae: 2.769591, mean_q: 3.334966, mean_eps: 0.101822
 1419395/2000000: episode: 1940, duration: 54.680s, episode steps: 894, steps per second:  16, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.021466, mae: 2.768014, mean_q: 3.331653, mean_eps: 0.101333
 1420243/2000000: episode: 1941, duration: 51.375s, episode steps: 848, steps per second:  17, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.705 [0.000, 5.000],  loss: 0.019420, mae: 2.751742, mean_q: 3.314824, mean_eps: 0.100782
 1421044/2000000: episode: 1942, duration: 49.608s, episode steps: 801, steps per second:  16, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.019916, mae: 2.744630, mean_q: 3.305219, mean_eps: 0.100260
 1421686/2000000: episode: 1943, duration: 39.393s, episode steps: 642, steps per second:  16, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.271 [0.000, 5.000],  loss: 0.020668, mae: 2.767610, mean_q: 3.332764, mean_eps: 0.099803
 1422701/2000000: episode: 1944, duration: 62.163s, episode steps: 1015, steps per second:  16, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.018844, mae: 2.771834, mean_q: 3.337493, mean_eps: 0.099277
 1423300/2000000: episode: 1945, duration: 36.566s, episode steps: 599, steps per second:  16, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.234 [0.000, 5.000],  loss: 0.019069, mae: 2.762393, mean_q: 3.328704, mean_eps: 0.098767
 1424200/2000000: episode: 1946, duration: 55.551s, episode steps: 900, steps per second:  16, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 0.020297, mae: 2.750182, mean_q: 3.310352, mean_eps: 0.098293
 1424628/2000000: episode: 1947, duration: 26.361s, episode steps: 428, steps per second:  16, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.019961, mae: 2.750332, mean_q: 3.312187, mean_eps: 0.097872
 1425402/2000000: episode: 1948, duration: 47.518s, episode steps: 774, steps per second:  16, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.683 [0.000, 5.000],  loss: 0.021745, mae: 2.764977, mean_q: 3.330562, mean_eps: 0.097491
 1426205/2000000: episode: 1949, duration: 49.173s, episode steps: 803, steps per second:  16, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.826 [0.000, 5.000],  loss: 0.020137, mae: 2.756679, mean_q: 3.319433, mean_eps: 0.096991
 1426819/2000000: episode: 1950, duration: 37.407s, episode steps: 614, steps per second:  16, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.176 [0.000, 5.000],  loss: 0.019622, mae: 2.750110, mean_q: 3.310265, mean_eps: 0.096542
 1427219/2000000: episode: 1951, duration: 24.374s, episode steps: 400, steps per second:  16, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.925 [0.000, 5.000],  loss: 0.022945, mae: 2.756647, mean_q: 3.319084, mean_eps: 0.096222
 1427616/2000000: episode: 1952, duration: 24.561s, episode steps: 397, steps per second:  16, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.917 [0.000, 5.000],  loss: 0.020989, mae: 2.762213, mean_q: 3.325415, mean_eps: 0.095970
 1428281/2000000: episode: 1953, duration: 40.726s, episode steps: 665, steps per second:  16, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.144 [0.000, 5.000],  loss: 0.020081, mae: 2.750091, mean_q: 3.310969, mean_eps: 0.095633
 1429264/2000000: episode: 1954, duration: 60.316s, episode steps: 983, steps per second:  16, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.020537, mae: 2.749341, mean_q: 3.310427, mean_eps: 0.095111
 1429886/2000000: episode: 1955, duration: 38.295s, episode steps: 622, steps per second:  16, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.011 [0.000, 5.000],  loss: 0.021976, mae: 2.751488, mean_q: 3.311646, mean_eps: 0.094603
 1430793/2000000: episode: 1956, duration: 55.379s, episode steps: 907, steps per second:  16, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.213 [0.000, 5.000],  loss: 0.018100, mae: 2.761797, mean_q: 3.325646, mean_eps: 0.094118
 1431912/2000000: episode: 1957, duration: 68.623s, episode steps: 1119, steps per second:  16, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.906 [0.000, 5.000],  loss: 0.019707, mae: 2.741893, mean_q: 3.300217, mean_eps: 0.093477
 1432862/2000000: episode: 1958, duration: 58.557s, episode steps: 950, steps per second:  16, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.318 [0.000, 5.000],  loss: 0.021187, mae: 2.732714, mean_q: 3.290102, mean_eps: 0.092822
 1434035/2000000: episode: 1959, duration: 71.345s, episode steps: 1173, steps per second:  16, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.930 [0.000, 5.000],  loss: 0.021002, mae: 2.735796, mean_q: 3.292664, mean_eps: 0.092150
 1435154/2000000: episode: 1960, duration: 68.238s, episode steps: 1119, steps per second:  16, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.153 [0.000, 5.000],  loss: 0.020812, mae: 2.745074, mean_q: 3.302594, mean_eps: 0.091424
 1435732/2000000: episode: 1961, duration: 35.489s, episode steps: 578, steps per second:  16, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.021760, mae: 2.765687, mean_q: 3.328816, mean_eps: 0.090887
 1436294/2000000: episode: 1962, duration: 34.753s, episode steps: 562, steps per second:  16, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.267 [0.000, 5.000],  loss: 0.021291, mae: 2.741650, mean_q: 3.299082, mean_eps: 0.090526
 1436686/2000000: episode: 1963, duration: 25.412s, episode steps: 392, steps per second:  15, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.921 [0.000, 5.000],  loss: 0.022685, mae: 2.735380, mean_q: 3.293320, mean_eps: 0.090223
 1437258/2000000: episode: 1964, duration: 35.037s, episode steps: 572, steps per second:  16, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.021950, mae: 2.733150, mean_q: 3.289125, mean_eps: 0.089918
 1437960/2000000: episode: 1965, duration: 43.034s, episode steps: 702, steps per second:  16, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.281 [0.000, 5.000],  loss: 0.022508, mae: 2.751119, mean_q: 3.312933, mean_eps: 0.089515
 1438494/2000000: episode: 1966, duration: 32.585s, episode steps: 534, steps per second:  16, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.734 [0.000, 5.000],  loss: 0.020867, mae: 2.717189, mean_q: 3.272195, mean_eps: 0.089124
 1439111/2000000: episode: 1967, duration: 37.499s, episode steps: 617, steps per second:  16, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.020481, mae: 2.746830, mean_q: 3.306654, mean_eps: 0.088759
 1439812/2000000: episode: 1968, duration: 43.031s, episode steps: 701, steps per second:  16, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.039 [0.000, 5.000],  loss: 0.022431, mae: 2.739387, mean_q: 3.295892, mean_eps: 0.088342
 1440509/2000000: episode: 1969, duration: 42.796s, episode steps: 697, steps per second:  16, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.018561, mae: 2.735642, mean_q: 3.292456, mean_eps: 0.087899
 1441439/2000000: episode: 1970, duration: 56.530s, episode steps: 930, steps per second:  16, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.074 [0.000, 5.000],  loss: 0.018477, mae: 2.712168, mean_q: 3.264214, mean_eps: 0.087383
 1442044/2000000: episode: 1971, duration: 36.994s, episode steps: 605, steps per second:  16, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.109 [0.000, 5.000],  loss: 0.016550, mae: 2.715775, mean_q: 3.269888, mean_eps: 0.086898
 1443060/2000000: episode: 1972, duration: 63.387s, episode steps: 1016, steps per second:  16, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.202 [0.000, 5.000],  loss: 0.018298, mae: 2.721829, mean_q: 3.276319, mean_eps: 0.086385
 1444077/2000000: episode: 1973, duration: 62.711s, episode steps: 1017, steps per second:  16, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.157 [0.000, 5.000],  loss: 0.018836, mae: 2.713945, mean_q: 3.266947, mean_eps: 0.085740
 1444892/2000000: episode: 1974, duration: 50.510s, episode steps: 815, steps per second:  16, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.018746, mae: 2.714097, mean_q: 3.266328, mean_eps: 0.085160
 1445780/2000000: episode: 1975, duration: 54.555s, episode steps: 888, steps per second:  16, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.291 [0.000, 5.000],  loss: 0.020655, mae: 2.709045, mean_q: 3.260388, mean_eps: 0.084622
 1446333/2000000: episode: 1976, duration: 34.196s, episode steps: 553, steps per second:  16, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.195 [0.000, 5.000],  loss: 0.020590, mae: 2.700303, mean_q: 3.253308, mean_eps: 0.084165
 1447419/2000000: episode: 1977, duration: 66.691s, episode steps: 1086, steps per second:  16, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.019338, mae: 2.708521, mean_q: 3.260296, mean_eps: 0.083645
 1447835/2000000: episode: 1978, duration: 26.589s, episode steps: 416, steps per second:  16, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.606 [0.000, 5.000],  loss: 0.021387, mae: 2.710011, mean_q: 3.260438, mean_eps: 0.083170
 1448824/2000000: episode: 1979, duration: 60.740s, episode steps: 989, steps per second:  16, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.159 [0.000, 5.000],  loss: 0.020746, mae: 2.718047, mean_q: 3.268711, mean_eps: 0.082726
 1449296/2000000: episode: 1980, duration: 29.426s, episode steps: 472, steps per second:  16, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.831 [0.000, 5.000],  loss: 0.022149, mae: 2.716936, mean_q: 3.268933, mean_eps: 0.082263
 1449892/2000000: episode: 1981, duration: 36.793s, episode steps: 596, steps per second:  16, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.019947, mae: 2.697311, mean_q: 3.244231, mean_eps: 0.081925
 1450513/2000000: episode: 1982, duration: 39.606s, episode steps: 621, steps per second:  16, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.942 [0.000, 5.000],  loss: 0.017531, mae: 2.698115, mean_q: 3.248914, mean_eps: 0.081539
 1451127/2000000: episode: 1983, duration: 37.773s, episode steps: 614, steps per second:  16, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.720 [0.000, 5.000],  loss: 0.017995, mae: 2.699145, mean_q: 3.249461, mean_eps: 0.081147
 1451799/2000000: episode: 1984, duration: 40.968s, episode steps: 672, steps per second:  16, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.016 [0.000, 5.000],  loss: 0.020221, mae: 2.696703, mean_q: 3.245398, mean_eps: 0.080741
 1452753/2000000: episode: 1985, duration: 58.637s, episode steps: 954, steps per second:  16, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.019149, mae: 2.700012, mean_q: 3.248346, mean_eps: 0.080225
 1453459/2000000: episode: 1986, duration: 43.097s, episode steps: 706, steps per second:  16, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.999 [0.000, 5.000],  loss: 0.019837, mae: 2.685298, mean_q: 3.231363, mean_eps: 0.079700
 1454153/2000000: episode: 1987, duration: 42.644s, episode steps: 694, steps per second:  16, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.275 [0.000, 5.000],  loss: 0.020390, mae: 2.688813, mean_q: 3.240084, mean_eps: 0.079256
 1455547/2000000: episode: 1988, duration: 85.021s, episode steps: 1394, steps per second:  16, episode reward: 33.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.762 [0.000, 5.000],  loss: 0.019720, mae: 2.685584, mean_q: 3.232616, mean_eps: 0.078595
 1456454/2000000: episode: 1989, duration: 55.834s, episode steps: 907, steps per second:  16, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.950 [0.000, 5.000],  loss: 0.019062, mae: 2.682405, mean_q: 3.226901, mean_eps: 0.077867
 1457177/2000000: episode: 1990, duration: 44.463s, episode steps: 723, steps per second:  16, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.020226, mae: 2.691834, mean_q: 3.239016, mean_eps: 0.077350
 1457832/2000000: episode: 1991, duration: 40.404s, episode steps: 655, steps per second:  16, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.940 [0.000, 5.000],  loss: 0.021102, mae: 2.691336, mean_q: 3.239858, mean_eps: 0.076914
 1458540/2000000: episode: 1992, duration: 44.014s, episode steps: 708, steps per second:  16, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.019526, mae: 2.676841, mean_q: 3.219672, mean_eps: 0.076483
 1459787/2000000: episode: 1993, duration: 76.368s, episode steps: 1247, steps per second:  16, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.251 [0.000, 5.000],  loss: 0.020985, mae: 2.706333, mean_q: 3.256064, mean_eps: 0.075864
 1460412/2000000: episode: 1994, duration: 38.338s, episode steps: 625, steps per second:  16, episode reward: 17.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.016336, mae: 2.654332, mean_q: 3.195020, mean_eps: 0.075271
 1461156/2000000: episode: 1995, duration: 46.137s, episode steps: 744, steps per second:  16, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.980 [0.000, 5.000],  loss: 0.018612, mae: 2.667113, mean_q: 3.211037, mean_eps: 0.074838
 1461780/2000000: episode: 1996, duration: 38.229s, episode steps: 624, steps per second:  16, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.074 [0.000, 5.000],  loss: 0.017506, mae: 2.647859, mean_q: 3.186188, mean_eps: 0.074405
 1462545/2000000: episode: 1997, duration: 47.154s, episode steps: 765, steps per second:  16, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.192 [0.000, 5.000],  loss: 0.018990, mae: 2.669664, mean_q: 3.213581, mean_eps: 0.073964
 1463059/2000000: episode: 1998, duration: 31.537s, episode steps: 514, steps per second:  16, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.017318, mae: 2.662166, mean_q: 3.204551, mean_eps: 0.073559
 1463584/2000000: episode: 1999, duration: 32.166s, episode steps: 525, steps per second:  16, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.018963, mae: 2.667450, mean_q: 3.208542, mean_eps: 0.073231
 1464522/2000000: episode: 2000, duration: 57.728s, episode steps: 938, steps per second:  16, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.247 [0.000, 5.000],  loss: 0.019380, mae: 2.665777, mean_q: 3.206302, mean_eps: 0.072767
 1465679/2000000: episode: 2001, duration: 70.877s, episode steps: 1157, steps per second:  16, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.019664, mae: 2.662771, mean_q: 3.204193, mean_eps: 0.072103
 1466648/2000000: episode: 2002, duration: 59.355s, episode steps: 969, steps per second:  16, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.117 [0.000, 5.000],  loss: 0.021824, mae: 2.676803, mean_q: 3.220551, mean_eps: 0.071431
 1467343/2000000: episode: 2003, duration: 42.572s, episode steps: 695, steps per second:  16, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.839 [0.000, 5.000],  loss: 0.018446, mae: 2.653891, mean_q: 3.193602, mean_eps: 0.070904
 1468064/2000000: episode: 2004, duration: 44.224s, episode steps: 721, steps per second:  16, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.015 [0.000, 5.000],  loss: 0.021241, mae: 2.674394, mean_q: 3.219796, mean_eps: 0.070455
 1469203/2000000: episode: 2005, duration: 69.614s, episode steps: 1139, steps per second:  16, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.178 [0.000, 5.000],  loss: 0.020064, mae: 2.665481, mean_q: 3.207779, mean_eps: 0.069866
 1470154/2000000: episode: 2006, duration: 58.332s, episode steps: 951, steps per second:  16, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.019189, mae: 2.667131, mean_q: 3.211045, mean_eps: 0.069204
 1470780/2000000: episode: 2007, duration: 38.486s, episode steps: 626, steps per second:  16, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.040 [0.000, 5.000],  loss: 0.018133, mae: 2.675745, mean_q: 3.221665, mean_eps: 0.068705
 1472550/2000000: episode: 2008, duration: 108.325s, episode steps: 1770, steps per second:  16, episode reward: 24.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.019671, mae: 2.686613, mean_q: 3.234980, mean_eps: 0.067946
 1473234/2000000: episode: 2009, duration: 41.857s, episode steps: 684, steps per second:  16, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.800 [0.000, 5.000],  loss: 0.018492, mae: 2.665354, mean_q: 3.210576, mean_eps: 0.067168
 1474619/2000000: episode: 2010, duration: 84.970s, episode steps: 1385, steps per second:  16, episode reward: 27.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.100 [0.000, 5.000],  loss: 0.019430, mae: 2.662967, mean_q: 3.206430, mean_eps: 0.066514
 1475937/2000000: episode: 2011, duration: 84.737s, episode steps: 1318, steps per second:  16, episode reward: 18.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.019155, mae: 2.669775, mean_q: 3.213855, mean_eps: 0.065657
 1477013/2000000: episode: 2012, duration: 67.064s, episode steps: 1076, steps per second:  16, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.806 [0.000, 5.000],  loss: 0.018642, mae: 2.668936, mean_q: 3.211634, mean_eps: 0.064899
 1477748/2000000: episode: 2013, duration: 46.166s, episode steps: 735, steps per second:  16, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.706 [0.000, 5.000],  loss: 0.019470, mae: 2.647535, mean_q: 3.186133, mean_eps: 0.064326
 1478417/2000000: episode: 2014, duration: 41.825s, episode steps: 669, steps per second:  16, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.019988, mae: 2.653723, mean_q: 3.195202, mean_eps: 0.063881
 1479229/2000000: episode: 2015, duration: 51.670s, episode steps: 812, steps per second:  16, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.020783, mae: 2.671494, mean_q: 3.215234, mean_eps: 0.063411
 1480045/2000000: episode: 2016, duration: 51.071s, episode steps: 816, steps per second:  16, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.672 [0.000, 5.000],  loss: 0.020857, mae: 2.673718, mean_q: 3.219155, mean_eps: 0.062896
 1480940/2000000: episode: 2017, duration: 56.360s, episode steps: 895, steps per second:  16, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.989 [0.000, 5.000],  loss: 0.017464, mae: 2.647257, mean_q: 3.188419, mean_eps: 0.062355
 1481899/2000000: episode: 2018, duration: 60.003s, episode steps: 959, steps per second:  16, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.018082, mae: 2.627810, mean_q: 3.163300, mean_eps: 0.061769
 1482321/2000000: episode: 2019, duration: 26.399s, episode steps: 422, steps per second:  16, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.040 [0.000, 5.000],  loss: 0.018355, mae: 2.656256, mean_q: 3.199058, mean_eps: 0.061330
 1483403/2000000: episode: 2020, duration: 67.259s, episode steps: 1082, steps per second:  16, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.206 [0.000, 5.000],  loss: 0.018599, mae: 2.638242, mean_q: 3.175577, mean_eps: 0.060854
 1484401/2000000: episode: 2021, duration: 61.655s, episode steps: 998, steps per second:  16, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.997 [0.000, 5.000],  loss: 0.019456, mae: 2.647421, mean_q: 3.186428, mean_eps: 0.060195
 1484949/2000000: episode: 2022, duration: 33.692s, episode steps: 548, steps per second:  16, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.697 [0.000, 5.000],  loss: 0.019986, mae: 2.642745, mean_q: 3.183666, mean_eps: 0.059705
 1485836/2000000: episode: 2023, duration: 54.438s, episode steps: 887, steps per second:  16, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.019358, mae: 2.650954, mean_q: 3.192419, mean_eps: 0.059252
 1486497/2000000: episode: 2024, duration: 40.909s, episode steps: 661, steps per second:  16, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.693 [0.000, 5.000],  loss: 0.018244, mae: 2.632130, mean_q: 3.167485, mean_eps: 0.058762
 1487314/2000000: episode: 2025, duration: 50.034s, episode steps: 817, steps per second:  16, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.022 [0.000, 5.000],  loss: 0.018052, mae: 2.640917, mean_q: 3.178117, mean_eps: 0.058293
 1487833/2000000: episode: 2026, duration: 31.775s, episode steps: 519, steps per second:  16, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.135 [0.000, 5.000],  loss: 0.020743, mae: 2.658746, mean_q: 3.202950, mean_eps: 0.057870
 1488287/2000000: episode: 2027, duration: 27.984s, episode steps: 454, steps per second:  16, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.802 [0.000, 5.000],  loss: 0.020602, mae: 2.638656, mean_q: 3.174168, mean_eps: 0.057562
 1489151/2000000: episode: 2028, duration: 53.371s, episode steps: 864, steps per second:  16, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.019165, mae: 2.647577, mean_q: 3.185530, mean_eps: 0.057145
 1490315/2000000: episode: 2029, duration: 72.246s, episode steps: 1164, steps per second:  16, episode reward: 11.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.017511, mae: 2.642903, mean_q: 3.181566, mean_eps: 0.056503
 1491040/2000000: episode: 2030, duration: 45.842s, episode steps: 725, steps per second:  16, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.538 [0.000, 5.000],  loss: 0.018332, mae: 2.640601, mean_q: 3.179805, mean_eps: 0.055905
 1491720/2000000: episode: 2031, duration: 43.524s, episode steps: 680, steps per second:  16, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.193 [0.000, 5.000],  loss: 0.018381, mae: 2.628473, mean_q: 3.163843, mean_eps: 0.055461
 1492694/2000000: episode: 2032, duration: 52.229s, episode steps: 974, steps per second:  19, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.222 [0.000, 5.000],  loss: 0.018857, mae: 2.644286, mean_q: 3.183892, mean_eps: 0.054936
 1493911/2000000: episode: 2033, duration: 64.996s, episode steps: 1217, steps per second:  19, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.019597, mae: 2.631508, mean_q: 3.167730, mean_eps: 0.054242
 1494453/2000000: episode: 2034, duration: 29.395s, episode steps: 542, steps per second:  18, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.288 [0.000, 5.000],  loss: 0.019981, mae: 2.614282, mean_q: 3.146399, mean_eps: 0.053685
 1495234/2000000: episode: 2035, duration: 42.056s, episode steps: 781, steps per second:  19, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.909 [0.000, 5.000],  loss: 0.019581, mae: 2.635200, mean_q: 3.173984, mean_eps: 0.053265
 1496202/2000000: episode: 2036, duration: 51.951s, episode steps: 968, steps per second:  19, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.116 [0.000, 5.000],  loss: 0.020289, mae: 2.629497, mean_q: 3.165100, mean_eps: 0.052712
 1497468/2000000: episode: 2037, duration: 69.077s, episode steps: 1266, steps per second:  18, episode reward: 28.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.185 [0.000, 5.000],  loss: 0.018413, mae: 2.638743, mean_q: 3.176392, mean_eps: 0.052005
 1498150/2000000: episode: 2038, duration: 36.628s, episode steps: 682, steps per second:  19, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: 0.018797, mae: 2.635034, mean_q: 3.169927, mean_eps: 0.051388
 1499140/2000000: episode: 2039, duration: 57.541s, episode steps: 990, steps per second:  17, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.019511, mae: 2.634077, mean_q: 3.171904, mean_eps: 0.050859
 1500600/2000000: episode: 2040, duration: 79.979s, episode steps: 1460, steps per second:  18, episode reward: 33.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.018287, mae: 2.631019, mean_q: 3.167686, mean_eps: 0.050161
 1501550/2000000: episode: 2041, duration: 51.130s, episode steps: 950, steps per second:  19, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.017214, mae: 2.636299, mean_q: 3.176044, mean_eps: 0.050000
 1502235/2000000: episode: 2042, duration: 38.355s, episode steps: 685, steps per second:  18, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.250 [0.000, 5.000],  loss: 0.018875, mae: 2.632588, mean_q: 3.169381, mean_eps: 0.050000
 1503557/2000000: episode: 2043, duration: 71.091s, episode steps: 1322, steps per second:  19, episode reward: 23.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.249 [0.000, 5.000],  loss: 0.018897, mae: 2.633342, mean_q: 3.170135, mean_eps: 0.050000
 1504233/2000000: episode: 2044, duration: 36.412s, episode steps: 676, steps per second:  19, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.017602, mae: 2.626305, mean_q: 3.162206, mean_eps: 0.050000
 1504851/2000000: episode: 2045, duration: 33.380s, episode steps: 618, steps per second:  19, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.888 [0.000, 5.000],  loss: 0.018574, mae: 2.617608, mean_q: 3.152913, mean_eps: 0.050000
 1505766/2000000: episode: 2046, duration: 48.915s, episode steps: 915, steps per second:  19, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.133 [0.000, 5.000],  loss: 0.019122, mae: 2.628945, mean_q: 3.167117, mean_eps: 0.050000
 1506500/2000000: episode: 2047, duration: 39.505s, episode steps: 734, steps per second:  19, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.018624, mae: 2.630198, mean_q: 3.167551, mean_eps: 0.050000
 1507032/2000000: episode: 2048, duration: 28.817s, episode steps: 532, steps per second:  18, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 3.331 [0.000, 5.000],  loss: 0.019966, mae: 2.652951, mean_q: 3.193473, mean_eps: 0.050000
 1507400/2000000: episode: 2049, duration: 19.948s, episode steps: 368, steps per second:  18, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 3.011 [0.000, 5.000],  loss: 0.017154, mae: 2.634785, mean_q: 3.170832, mean_eps: 0.050000
 1508114/2000000: episode: 2050, duration: 38.505s, episode steps: 714, steps per second:  19, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.034 [0.000, 5.000],  loss: 0.018417, mae: 2.635495, mean_q: 3.172083, mean_eps: 0.050000
 1508660/2000000: episode: 2051, duration: 29.304s, episode steps: 546, steps per second:  19, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.749 [0.000, 5.000],  loss: 0.017920, mae: 2.616780, mean_q: 3.148169, mean_eps: 0.050000
 1509108/2000000: episode: 2052, duration: 24.439s, episode steps: 448, steps per second:  18, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.019037, mae: 2.635733, mean_q: 3.172774, mean_eps: 0.050000
 1509932/2000000: episode: 2053, duration: 44.850s, episode steps: 824, steps per second:  18, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.101 [0.000, 5.000],  loss: 0.018740, mae: 2.635265, mean_q: 3.170978, mean_eps: 0.050000
 1510612/2000000: episode: 2054, duration: 36.749s, episode steps: 680, steps per second:  19, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.437 [0.000, 5.000],  loss: 0.016134, mae: 2.626115, mean_q: 3.162831, mean_eps: 0.050000
 1511535/2000000: episode: 2055, duration: 49.638s, episode steps: 923, steps per second:  19, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.706 [0.000, 5.000],  loss: 0.018255, mae: 2.631997, mean_q: 3.168692, mean_eps: 0.050000
 1512653/2000000: episode: 2056, duration: 60.017s, episode steps: 1118, steps per second:  19, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.685 [0.000, 5.000],  loss: 0.019329, mae: 2.619364, mean_q: 3.155551, mean_eps: 0.050000
 1513515/2000000: episode: 2057, duration: 46.444s, episode steps: 862, steps per second:  19, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.181 [0.000, 5.000],  loss: 0.020760, mae: 2.627912, mean_q: 3.166373, mean_eps: 0.050000
 1514143/2000000: episode: 2058, duration: 33.695s, episode steps: 628, steps per second:  19, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.306 [0.000, 5.000],  loss: 0.017122, mae: 2.627924, mean_q: 3.163035, mean_eps: 0.050000
 1515016/2000000: episode: 2059, duration: 46.801s, episode steps: 873, steps per second:  19, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.018995, mae: 2.620005, mean_q: 3.152563, mean_eps: 0.050000
 1515674/2000000: episode: 2060, duration: 35.260s, episode steps: 658, steps per second:  19, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.669 [0.000, 5.000],  loss: 0.018804, mae: 2.632762, mean_q: 3.167683, mean_eps: 0.050000
 1516520/2000000: episode: 2061, duration: 45.170s, episode steps: 846, steps per second:  19, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.866 [0.000, 5.000],  loss: 0.019196, mae: 2.616290, mean_q: 3.148621, mean_eps: 0.050000
 1517426/2000000: episode: 2062, duration: 48.830s, episode steps: 906, steps per second:  19, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.639 [0.000, 5.000],  loss: 0.017524, mae: 2.623430, mean_q: 3.158104, mean_eps: 0.050000
 1518496/2000000: episode: 2063, duration: 57.182s, episode steps: 1070, steps per second:  19, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.786 [0.000, 5.000],  loss: 0.019756, mae: 2.624225, mean_q: 3.158255, mean_eps: 0.050000
 1518851/2000000: episode: 2064, duration: 19.223s, episode steps: 355, steps per second:  18, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.017313, mae: 2.618581, mean_q: 3.154520, mean_eps: 0.050000
 1519559/2000000: episode: 2065, duration: 38.351s, episode steps: 708, steps per second:  18, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.788 [0.000, 5.000],  loss: 0.018080, mae: 2.642007, mean_q: 3.180597, mean_eps: 0.050000
 1520487/2000000: episode: 2066, duration: 50.467s, episode steps: 928, steps per second:  18, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.752 [0.000, 5.000],  loss: 0.017110, mae: 2.623792, mean_q: 3.159654, mean_eps: 0.050000
 1521274/2000000: episode: 2067, duration: 42.621s, episode steps: 787, steps per second:  18, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.017755, mae: 2.621328, mean_q: 3.155411, mean_eps: 0.050000
 1521753/2000000: episode: 2068, duration: 25.771s, episode steps: 479, steps per second:  19, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.770 [0.000, 5.000],  loss: 0.016189, mae: 2.636192, mean_q: 3.173772, mean_eps: 0.050000
 1522750/2000000: episode: 2069, duration: 53.367s, episode steps: 997, steps per second:  19, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.019104, mae: 2.616422, mean_q: 3.150019, mean_eps: 0.050000
 1523459/2000000: episode: 2070, duration: 37.783s, episode steps: 709, steps per second:  19, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.752 [0.000, 5.000],  loss: 0.017556, mae: 2.620083, mean_q: 3.155358, mean_eps: 0.050000
 1524344/2000000: episode: 2071, duration: 48.061s, episode steps: 885, steps per second:  18, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.236 [0.000, 5.000],  loss: 0.017931, mae: 2.626823, mean_q: 3.162530, mean_eps: 0.050000
 1525036/2000000: episode: 2072, duration: 37.220s, episode steps: 692, steps per second:  19, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.019 [0.000, 5.000],  loss: 0.018135, mae: 2.627616, mean_q: 3.161248, mean_eps: 0.050000
 1525846/2000000: episode: 2073, duration: 43.338s, episode steps: 810, steps per second:  19, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.018588, mae: 2.628940, mean_q: 3.163436, mean_eps: 0.050000
 1527354/2000000: episode: 2074, duration: 80.796s, episode steps: 1508, steps per second:  19, episode reward: 31.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.018113, mae: 2.621087, mean_q: 3.155359, mean_eps: 0.050000
 1528490/2000000: episode: 2075, duration: 60.735s, episode steps: 1136, steps per second:  19, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.017893, mae: 2.622896, mean_q: 3.157209, mean_eps: 0.050000
 1529556/2000000: episode: 2076, duration: 57.561s, episode steps: 1066, steps per second:  19, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.019252, mae: 2.631186, mean_q: 3.168169, mean_eps: 0.050000
 1530391/2000000: episode: 2077, duration: 44.739s, episode steps: 835, steps per second:  19, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.074 [0.000, 5.000],  loss: 0.017455, mae: 2.623614, mean_q: 3.158787, mean_eps: 0.050000
 1531260/2000000: episode: 2078, duration: 46.960s, episode steps: 869, steps per second:  19, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.017605, mae: 2.641427, mean_q: 3.180499, mean_eps: 0.050000
 1532576/2000000: episode: 2079, duration: 71.065s, episode steps: 1316, steps per second:  19, episode reward: 21.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.287 [0.000, 5.000],  loss: 0.016288, mae: 2.621171, mean_q: 3.156586, mean_eps: 0.050000
 1533508/2000000: episode: 2080, duration: 50.268s, episode steps: 932, steps per second:  19, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.871 [0.000, 5.000],  loss: 0.017951, mae: 2.624417, mean_q: 3.159965, mean_eps: 0.050000
 1534412/2000000: episode: 2081, duration: 48.659s, episode steps: 904, steps per second:  19, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.017830, mae: 2.620284, mean_q: 3.153570, mean_eps: 0.050000
 1535678/2000000: episode: 2082, duration: 67.623s, episode steps: 1266, steps per second:  19, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.018196, mae: 2.625341, mean_q: 3.158421, mean_eps: 0.050000
 1536340/2000000: episode: 2083, duration: 35.478s, episode steps: 662, steps per second:  19, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.018223, mae: 2.636381, mean_q: 3.173788, mean_eps: 0.050000
 1537198/2000000: episode: 2084, duration: 46.044s, episode steps: 858, steps per second:  19, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.772 [0.000, 5.000],  loss: 0.018805, mae: 2.623425, mean_q: 3.158353, mean_eps: 0.050000
 1538047/2000000: episode: 2085, duration: 45.676s, episode steps: 849, steps per second:  19, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.018441, mae: 2.616533, mean_q: 3.148937, mean_eps: 0.050000
 1538725/2000000: episode: 2086, duration: 36.677s, episode steps: 678, steps per second:  18, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.881 [0.000, 5.000],  loss: 0.018515, mae: 2.627122, mean_q: 3.163506, mean_eps: 0.050000
 1539257/2000000: episode: 2087, duration: 28.617s, episode steps: 532, steps per second:  19, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.974 [0.000, 5.000],  loss: 0.017072, mae: 2.629575, mean_q: 3.163542, mean_eps: 0.050000
 1540128/2000000: episode: 2088, duration: 46.667s, episode steps: 871, steps per second:  19, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.247 [0.000, 5.000],  loss: 0.019175, mae: 2.617802, mean_q: 3.151951, mean_eps: 0.050000
 1540712/2000000: episode: 2089, duration: 31.377s, episode steps: 584, steps per second:  19, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.779 [0.000, 5.000],  loss: 0.016237, mae: 2.637360, mean_q: 3.174747, mean_eps: 0.050000
 1541250/2000000: episode: 2090, duration: 29.010s, episode steps: 538, steps per second:  19, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.985 [0.000, 5.000],  loss: 0.018130, mae: 2.626058, mean_q: 3.160441, mean_eps: 0.050000
 1542736/2000000: episode: 2091, duration: 79.522s, episode steps: 1486, steps per second:  19, episode reward: 30.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.018066, mae: 2.612815, mean_q: 3.146585, mean_eps: 0.050000
 1543391/2000000: episode: 2092, duration: 35.182s, episode steps: 655, steps per second:  19, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.957 [0.000, 5.000],  loss: 0.017734, mae: 2.617749, mean_q: 3.151080, mean_eps: 0.050000
 1544700/2000000: episode: 2093, duration: 70.180s, episode steps: 1309, steps per second:  19, episode reward: 27.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.018456, mae: 2.632494, mean_q: 3.167736, mean_eps: 0.050000
 1545815/2000000: episode: 2094, duration: 59.581s, episode steps: 1115, steps per second:  19, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.926 [0.000, 5.000],  loss: 0.019154, mae: 2.622835, mean_q: 3.156605, mean_eps: 0.050000
 1546205/2000000: episode: 2095, duration: 20.999s, episode steps: 390, steps per second:  19, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.226 [0.000, 5.000],  loss: 0.019036, mae: 2.631716, mean_q: 3.166723, mean_eps: 0.050000
 1547108/2000000: episode: 2096, duration: 48.512s, episode steps: 903, steps per second:  19, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.017980, mae: 2.619794, mean_q: 3.151900, mean_eps: 0.050000
 1547833/2000000: episode: 2097, duration: 39.082s, episode steps: 725, steps per second:  19, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.870 [0.000, 5.000],  loss: 0.017624, mae: 2.626956, mean_q: 3.159964, mean_eps: 0.050000
 1549024/2000000: episode: 2098, duration: 63.804s, episode steps: 1191, steps per second:  19, episode reward: 14.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.257 [0.000, 5.000],  loss: 0.018464, mae: 2.624503, mean_q: 3.157333, mean_eps: 0.050000
 1549758/2000000: episode: 2099, duration: 39.509s, episode steps: 734, steps per second:  19, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.020167, mae: 2.616027, mean_q: 3.146262, mean_eps: 0.050000
 1551052/2000000: episode: 2100, duration: 70.818s, episode steps: 1294, steps per second:  18, episode reward: 16.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.016861, mae: 2.611620, mean_q: 3.143094, mean_eps: 0.050000
 1551986/2000000: episode: 2101, duration: 49.879s, episode steps: 934, steps per second:  19, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.983 [0.000, 5.000],  loss: 0.016813, mae: 2.610564, mean_q: 3.143355, mean_eps: 0.050000
 1552654/2000000: episode: 2102, duration: 35.683s, episode steps: 668, steps per second:  19, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.757 [0.000, 5.000],  loss: 0.016900, mae: 2.608844, mean_q: 3.141688, mean_eps: 0.050000
 1553458/2000000: episode: 2103, duration: 43.407s, episode steps: 804, steps per second:  19, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.858 [0.000, 5.000],  loss: 0.018467, mae: 2.623165, mean_q: 3.156656, mean_eps: 0.050000
 1554151/2000000: episode: 2104, duration: 36.992s, episode steps: 693, steps per second:  19, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.118 [0.000, 5.000],  loss: 0.018980, mae: 2.619744, mean_q: 3.153620, mean_eps: 0.050000
 1555287/2000000: episode: 2105, duration: 60.609s, episode steps: 1136, steps per second:  19, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.261 [0.000, 5.000],  loss: 0.018984, mae: 2.612415, mean_q: 3.144083, mean_eps: 0.050000
 1555702/2000000: episode: 2106, duration: 22.451s, episode steps: 415, steps per second:  18, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 3.005 [0.000, 5.000],  loss: 0.018390, mae: 2.628445, mean_q: 3.163632, mean_eps: 0.050000
 1556330/2000000: episode: 2107, duration: 33.810s, episode steps: 628, steps per second:  19, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.017653, mae: 2.610826, mean_q: 3.143613, mean_eps: 0.050000
 1557557/2000000: episode: 2108, duration: 65.874s, episode steps: 1227, steps per second:  19, episode reward: 20.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.084 [0.000, 5.000],  loss: 0.018667, mae: 2.623532, mean_q: 3.158167, mean_eps: 0.050000
 1558172/2000000: episode: 2109, duration: 33.158s, episode steps: 615, steps per second:  19, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.019266, mae: 2.618742, mean_q: 3.150735, mean_eps: 0.050000
 1559225/2000000: episode: 2110, duration: 56.552s, episode steps: 1053, steps per second:  19, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.324 [0.000, 5.000],  loss: 0.018902, mae: 2.615003, mean_q: 3.146493, mean_eps: 0.050000
 1559834/2000000: episode: 2111, duration: 32.675s, episode steps: 609, steps per second:  19, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.775 [0.000, 5.000],  loss: 0.018604, mae: 2.607388, mean_q: 3.137152, mean_eps: 0.050000
 1560688/2000000: episode: 2112, duration: 45.679s, episode steps: 854, steps per second:  19, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.973 [0.000, 5.000],  loss: 0.016167, mae: 2.617946, mean_q: 3.153119, mean_eps: 0.050000
 1561296/2000000: episode: 2113, duration: 33.060s, episode steps: 608, steps per second:  18, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 3.225 [0.000, 5.000],  loss: 0.017468, mae: 2.619421, mean_q: 3.155363, mean_eps: 0.050000
 1562193/2000000: episode: 2114, duration: 48.332s, episode steps: 897, steps per second:  19, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.017596, mae: 2.621582, mean_q: 3.157080, mean_eps: 0.050000
 1563050/2000000: episode: 2115, duration: 45.730s, episode steps: 857, steps per second:  19, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.209 [0.000, 5.000],  loss: 0.019295, mae: 2.615017, mean_q: 3.147504, mean_eps: 0.050000
 1563748/2000000: episode: 2116, duration: 37.442s, episode steps: 698, steps per second:  19, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.262 [0.000, 5.000],  loss: 0.019821, mae: 2.627105, mean_q: 3.162792, mean_eps: 0.050000
 1564759/2000000: episode: 2117, duration: 54.203s, episode steps: 1011, steps per second:  19, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.300 [0.000, 5.000],  loss: 0.017443, mae: 2.613512, mean_q: 3.145034, mean_eps: 0.050000
 1565405/2000000: episode: 2118, duration: 34.692s, episode steps: 646, steps per second:  19, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.659 [0.000, 5.000],  loss: 0.017662, mae: 2.612525, mean_q: 3.145981, mean_eps: 0.050000
 1566290/2000000: episode: 2119, duration: 47.700s, episode steps: 885, steps per second:  19, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.721 [0.000, 5.000],  loss: 0.017553, mae: 2.606926, mean_q: 3.137729, mean_eps: 0.050000
 1567307/2000000: episode: 2120, duration: 54.051s, episode steps: 1017, steps per second:  19, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.019231, mae: 2.621465, mean_q: 3.155740, mean_eps: 0.050000
 1567948/2000000: episode: 2121, duration: 34.429s, episode steps: 641, steps per second:  19, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.018093, mae: 2.615313, mean_q: 3.148542, mean_eps: 0.050000
 1568585/2000000: episode: 2122, duration: 34.235s, episode steps: 637, steps per second:  19, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.016855, mae: 2.615441, mean_q: 3.148081, mean_eps: 0.050000
 1569695/2000000: episode: 2123, duration: 59.336s, episode steps: 1110, steps per second:  19, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.155 [0.000, 5.000],  loss: 0.018153, mae: 2.610526, mean_q: 3.142834, mean_eps: 0.050000
 1570691/2000000: episode: 2124, duration: 55.491s, episode steps: 996, steps per second:  18, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.016122, mae: 2.608280, mean_q: 3.138743, mean_eps: 0.050000
 1571651/2000000: episode: 2125, duration: 52.376s, episode steps: 960, steps per second:  18, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.016446, mae: 2.599880, mean_q: 3.130598, mean_eps: 0.050000
 1572454/2000000: episode: 2126, duration: 43.208s, episode steps: 803, steps per second:  19, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.204 [0.000, 5.000],  loss: 0.016727, mae: 2.599790, mean_q: 3.128568, mean_eps: 0.050000
 1573298/2000000: episode: 2127, duration: 45.510s, episode steps: 844, steps per second:  19, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.296 [0.000, 5.000],  loss: 0.016583, mae: 2.600448, mean_q: 3.128570, mean_eps: 0.050000
 1574356/2000000: episode: 2128, duration: 56.649s, episode steps: 1058, steps per second:  19, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.067 [0.000, 5.000],  loss: 0.017269, mae: 2.597236, mean_q: 3.124501, mean_eps: 0.050000
 1575170/2000000: episode: 2129, duration: 43.571s, episode steps: 814, steps per second:  19, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.955 [0.000, 5.000],  loss: 0.017215, mae: 2.600355, mean_q: 3.130978, mean_eps: 0.050000
 1575869/2000000: episode: 2130, duration: 37.657s, episode steps: 699, steps per second:  19, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.017140, mae: 2.591633, mean_q: 3.118162, mean_eps: 0.050000
 1576771/2000000: episode: 2131, duration: 48.281s, episode steps: 902, steps per second:  19, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.071 [0.000, 5.000],  loss: 0.018081, mae: 2.608637, mean_q: 3.139280, mean_eps: 0.050000
 1577818/2000000: episode: 2132, duration: 56.328s, episode steps: 1047, steps per second:  19, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.260 [0.000, 5.000],  loss: 0.017564, mae: 2.605899, mean_q: 3.135319, mean_eps: 0.050000
 1578842/2000000: episode: 2133, duration: 54.837s, episode steps: 1024, steps per second:  19, episode reward: 30.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.646 [0.000, 5.000],  loss: 0.019444, mae: 2.601769, mean_q: 3.128932, mean_eps: 0.050000
 1579765/2000000: episode: 2134, duration: 49.437s, episode steps: 923, steps per second:  19, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.018496, mae: 2.604105, mean_q: 3.133023, mean_eps: 0.050000
 1580422/2000000: episode: 2135, duration: 35.118s, episode steps: 657, steps per second:  19, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.201 [0.000, 5.000],  loss: 0.016493, mae: 2.581370, mean_q: 3.107638, mean_eps: 0.050000
 1581624/2000000: episode: 2136, duration: 64.701s, episode steps: 1202, steps per second:  19, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.016770, mae: 2.582789, mean_q: 3.108741, mean_eps: 0.050000
 1582873/2000000: episode: 2137, duration: 66.787s, episode steps: 1249, steps per second:  19, episode reward: 30.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.699 [0.000, 5.000],  loss: 0.016747, mae: 2.581346, mean_q: 3.106564, mean_eps: 0.050000
 1583567/2000000: episode: 2138, duration: 37.065s, episode steps: 694, steps per second:  19, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.016279, mae: 2.575272, mean_q: 3.097450, mean_eps: 0.050000
 1584218/2000000: episode: 2139, duration: 34.938s, episode steps: 651, steps per second:  19, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.071 [0.000, 5.000],  loss: 0.017879, mae: 2.584420, mean_q: 3.108543, mean_eps: 0.050000
 1585199/2000000: episode: 2140, duration: 52.745s, episode steps: 981, steps per second:  19, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.950 [0.000, 5.000],  loss: 0.017049, mae: 2.581586, mean_q: 3.106902, mean_eps: 0.050000
 1586774/2000000: episode: 2141, duration: 84.677s, episode steps: 1575, steps per second:  19, episode reward: 25.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.016522, mae: 2.582660, mean_q: 3.107953, mean_eps: 0.050000
 1587409/2000000: episode: 2142, duration: 34.112s, episode steps: 635, steps per second:  19, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.099 [0.000, 5.000],  loss: 0.017783, mae: 2.561950, mean_q: 3.081975, mean_eps: 0.050000
 1588209/2000000: episode: 2143, duration: 43.068s, episode steps: 800, steps per second:  19, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.016688, mae: 2.586628, mean_q: 3.112531, mean_eps: 0.050000
 1589089/2000000: episode: 2144, duration: 47.106s, episode steps: 880, steps per second:  19, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.017505, mae: 2.573968, mean_q: 3.097694, mean_eps: 0.050000
 1589746/2000000: episode: 2145, duration: 35.027s, episode steps: 657, steps per second:  19, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.661 [0.000, 5.000],  loss: 0.017491, mae: 2.576929, mean_q: 3.100850, mean_eps: 0.050000
 1590173/2000000: episode: 2146, duration: 23.066s, episode steps: 427, steps per second:  19, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.015680, mae: 2.564603, mean_q: 3.087694, mean_eps: 0.050000
 1591289/2000000: episode: 2147, duration: 59.934s, episode steps: 1116, steps per second:  19, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.015712, mae: 2.558625, mean_q: 3.081856, mean_eps: 0.050000
 1592417/2000000: episode: 2148, duration: 60.555s, episode steps: 1128, steps per second:  19, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.015575, mae: 2.549558, mean_q: 3.069776, mean_eps: 0.050000
 1593070/2000000: episode: 2149, duration: 34.997s, episode steps: 653, steps per second:  19, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.016533, mae: 2.542201, mean_q: 3.058313, mean_eps: 0.050000
 1593606/2000000: episode: 2150, duration: 28.750s, episode steps: 536, steps per second:  19, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.782 [0.000, 5.000],  loss: 0.017011, mae: 2.553236, mean_q: 3.071668, mean_eps: 0.050000
 1594643/2000000: episode: 2151, duration: 55.341s, episode steps: 1037, steps per second:  19, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.016587, mae: 2.544331, mean_q: 3.062583, mean_eps: 0.050000
 1595162/2000000: episode: 2152, duration: 27.883s, episode steps: 519, steps per second:  19, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.784 [0.000, 5.000],  loss: 0.017601, mae: 2.566182, mean_q: 3.089485, mean_eps: 0.050000
 1595929/2000000: episode: 2153, duration: 41.508s, episode steps: 767, steps per second:  18, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.016926, mae: 2.571492, mean_q: 3.093807, mean_eps: 0.050000
 1597001/2000000: episode: 2154, duration: 57.359s, episode steps: 1072, steps per second:  19, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.306 [0.000, 5.000],  loss: 0.017488, mae: 2.543593, mean_q: 3.062876, mean_eps: 0.050000
 1597841/2000000: episode: 2155, duration: 45.254s, episode steps: 840, steps per second:  19, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.017374, mae: 2.557542, mean_q: 3.079111, mean_eps: 0.050000
 1598514/2000000: episode: 2156, duration: 36.138s, episode steps: 673, steps per second:  19, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.744 [0.000, 5.000],  loss: 0.018046, mae: 2.541095, mean_q: 3.056937, mean_eps: 0.050000
 1599188/2000000: episode: 2157, duration: 36.366s, episode steps: 674, steps per second:  19, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.682 [0.000, 5.000],  loss: 0.016941, mae: 2.542921, mean_q: 3.060501, mean_eps: 0.050000
 1599816/2000000: episode: 2158, duration: 34.555s, episode steps: 628, steps per second:  18, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.016814, mae: 2.538725, mean_q: 3.054844, mean_eps: 0.050000
 1600463/2000000: episode: 2159, duration: 36.403s, episode steps: 647, steps per second:  18, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.719 [0.000, 5.000],  loss: 0.015797, mae: 2.559181, mean_q: 3.081383, mean_eps: 0.050000
 1600856/2000000: episode: 2160, duration: 21.113s, episode steps: 393, steps per second:  19, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.771 [0.000, 5.000],  loss: 0.015959, mae: 2.574857, mean_q: 3.099674, mean_eps: 0.050000
 1601763/2000000: episode: 2161, duration: 48.472s, episode steps: 907, steps per second:  19, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.022 [0.000, 5.000],  loss: 0.016945, mae: 2.563925, mean_q: 3.085250, mean_eps: 0.050000
 1602178/2000000: episode: 2162, duration: 22.234s, episode steps: 415, steps per second:  19, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.016228, mae: 2.572860, mean_q: 3.098357, mean_eps: 0.050000
 1602671/2000000: episode: 2163, duration: 26.482s, episode steps: 493, steps per second:  19, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.947 [0.000, 5.000],  loss: 0.017312, mae: 2.571064, mean_q: 3.094748, mean_eps: 0.050000
 1603452/2000000: episode: 2164, duration: 42.313s, episode steps: 781, steps per second:  18, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.209 [0.000, 5.000],  loss: 0.017092, mae: 2.576900, mean_q: 3.101414, mean_eps: 0.050000
 1604601/2000000: episode: 2165, duration: 61.845s, episode steps: 1149, steps per second:  19, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.189 [0.000, 5.000],  loss: 0.018051, mae: 2.568695, mean_q: 3.092301, mean_eps: 0.050000
 1605089/2000000: episode: 2166, duration: 26.367s, episode steps: 488, steps per second:  19, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.756 [0.000, 5.000],  loss: 0.018084, mae: 2.545723, mean_q: 3.061384, mean_eps: 0.050000
 1605654/2000000: episode: 2167, duration: 30.535s, episode steps: 565, steps per second:  19, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.358 [0.000, 5.000],  loss: 0.017452, mae: 2.563837, mean_q: 3.087970, mean_eps: 0.050000
 1606635/2000000: episode: 2168, duration: 52.584s, episode steps: 981, steps per second:  19, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.797 [0.000, 5.000],  loss: 0.016300, mae: 2.552950, mean_q: 3.072927, mean_eps: 0.050000
 1607488/2000000: episode: 2169, duration: 46.243s, episode steps: 853, steps per second:  18, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.017027, mae: 2.566387, mean_q: 3.088109, mean_eps: 0.050000
 1608053/2000000: episode: 2170, duration: 30.585s, episode steps: 565, steps per second:  18, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.867 [0.000, 5.000],  loss: 0.017659, mae: 2.563621, mean_q: 3.085028, mean_eps: 0.050000
 1608751/2000000: episode: 2171, duration: 37.179s, episode steps: 698, steps per second:  19, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.211 [0.000, 5.000],  loss: 0.016039, mae: 2.559302, mean_q: 3.081024, mean_eps: 0.050000
 1609323/2000000: episode: 2172, duration: 30.816s, episode steps: 572, steps per second:  19, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.056 [0.000, 5.000],  loss: 0.017786, mae: 2.559384, mean_q: 3.079024, mean_eps: 0.050000
 1610733/2000000: episode: 2173, duration: 75.388s, episode steps: 1410, steps per second:  19, episode reward: 19.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.206 [0.000, 5.000],  loss: 0.016940, mae: 2.565344, mean_q: 3.090007, mean_eps: 0.050000
 1611662/2000000: episode: 2174, duration: 49.727s, episode steps: 929, steps per second:  19, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.635 [0.000, 5.000],  loss: 0.016275, mae: 2.565049, mean_q: 3.089213, mean_eps: 0.050000
 1612975/2000000: episode: 2175, duration: 70.539s, episode steps: 1313, steps per second:  19, episode reward: 18.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.043 [0.000, 5.000],  loss: 0.016161, mae: 2.564952, mean_q: 3.086738, mean_eps: 0.050000
 1613672/2000000: episode: 2176, duration: 37.717s, episode steps: 697, steps per second:  18, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.017643, mae: 2.578690, mean_q: 3.101753, mean_eps: 0.050000
 1614151/2000000: episode: 2177, duration: 25.854s, episode steps: 479, steps per second:  19, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.960 [0.000, 5.000],  loss: 0.016061, mae: 2.558018, mean_q: 3.081270, mean_eps: 0.050000
 1614879/2000000: episode: 2178, duration: 39.384s, episode steps: 728, steps per second:  18, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.949 [0.000, 5.000],  loss: 0.016891, mae: 2.563069, mean_q: 3.087190, mean_eps: 0.050000
 1615733/2000000: episode: 2179, duration: 46.127s, episode steps: 854, steps per second:  19, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.141 [0.000, 5.000],  loss: 0.016391, mae: 2.559417, mean_q: 3.080740, mean_eps: 0.050000
 1616241/2000000: episode: 2180, duration: 27.236s, episode steps: 508, steps per second:  19, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.780 [0.000, 5.000],  loss: 0.017657, mae: 2.567499, mean_q: 3.091506, mean_eps: 0.050000
 1616889/2000000: episode: 2181, duration: 34.795s, episode steps: 648, steps per second:  19, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.753 [0.000, 5.000],  loss: 0.017657, mae: 2.570204, mean_q: 3.093659, mean_eps: 0.050000
 1617530/2000000: episode: 2182, duration: 34.202s, episode steps: 641, steps per second:  19, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.017561, mae: 2.559770, mean_q: 3.083104, mean_eps: 0.050000
 1619261/2000000: episode: 2183, duration: 92.423s, episode steps: 1731, steps per second:  19, episode reward: 29.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.002 [0.000, 5.000],  loss: 0.018021, mae: 2.553924, mean_q: 3.075898, mean_eps: 0.050000
 1620050/2000000: episode: 2184, duration: 42.154s, episode steps: 789, steps per second:  19, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.194 [0.000, 5.000],  loss: 0.017718, mae: 2.570659, mean_q: 3.093930, mean_eps: 0.050000
 1620666/2000000: episode: 2185, duration: 33.090s, episode steps: 616, steps per second:  19, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.313 [0.000, 5.000],  loss: 0.015901, mae: 2.557151, mean_q: 3.079750, mean_eps: 0.050000
 1621375/2000000: episode: 2186, duration: 37.791s, episode steps: 709, steps per second:  19, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.014737, mae: 2.552158, mean_q: 3.075311, mean_eps: 0.050000
 1621824/2000000: episode: 2187, duration: 24.386s, episode steps: 449, steps per second:  18, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.017089, mae: 2.560165, mean_q: 3.081464, mean_eps: 0.050000
 1622504/2000000: episode: 2188, duration: 36.699s, episode steps: 680, steps per second:  19, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.244 [0.000, 5.000],  loss: 0.018234, mae: 2.544223, mean_q: 3.063264, mean_eps: 0.050000
 1624149/2000000: episode: 2189, duration: 88.024s, episode steps: 1645, steps per second:  19, episode reward: 27.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.967 [0.000, 5.000],  loss: 0.017180, mae: 2.554868, mean_q: 3.076094, mean_eps: 0.050000
 1625112/2000000: episode: 2190, duration: 51.716s, episode steps: 963, steps per second:  19, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.646 [0.000, 5.000],  loss: 0.016786, mae: 2.544683, mean_q: 3.063333, mean_eps: 0.050000
 1625520/2000000: episode: 2191, duration: 22.036s, episode steps: 408, steps per second:  19, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.863 [0.000, 5.000],  loss: 0.016998, mae: 2.559738, mean_q: 3.081926, mean_eps: 0.050000
 1625980/2000000: episode: 2192, duration: 24.953s, episode steps: 460, steps per second:  18, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.857 [0.000, 5.000],  loss: 0.016493, mae: 2.551835, mean_q: 3.071195, mean_eps: 0.050000
 1626973/2000000: episode: 2193, duration: 53.195s, episode steps: 993, steps per second:  19, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.016426, mae: 2.545610, mean_q: 3.063175, mean_eps: 0.050000
 1627844/2000000: episode: 2194, duration: 46.350s, episode steps: 871, steps per second:  19, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.078 [0.000, 5.000],  loss: 0.016740, mae: 2.544134, mean_q: 3.062606, mean_eps: 0.050000
 1629018/2000000: episode: 2195, duration: 63.027s, episode steps: 1174, steps per second:  19, episode reward: 18.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.678 [0.000, 5.000],  loss: 0.016717, mae: 2.544160, mean_q: 3.063313, mean_eps: 0.050000
 1630114/2000000: episode: 2196, duration: 58.995s, episode steps: 1096, steps per second:  19, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.017285, mae: 2.545438, mean_q: 3.065813, mean_eps: 0.050000
 1630724/2000000: episode: 2197, duration: 32.905s, episode steps: 610, steps per second:  19, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 3.210 [0.000, 5.000],  loss: 0.014085, mae: 2.564430, mean_q: 3.088582, mean_eps: 0.050000
 1631155/2000000: episode: 2198, duration: 23.024s, episode steps: 431, steps per second:  19, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.114 [0.000, 5.000],  loss: 0.016525, mae: 2.564520, mean_q: 3.088400, mean_eps: 0.050000
 1632207/2000000: episode: 2199, duration: 56.380s, episode steps: 1052, steps per second:  19, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.308 [0.000, 5.000],  loss: 0.016461, mae: 2.557057, mean_q: 3.077486, mean_eps: 0.050000
 1632747/2000000: episode: 2200, duration: 28.851s, episode steps: 540, steps per second:  19, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.628 [0.000, 5.000],  loss: 0.016691, mae: 2.559685, mean_q: 3.081208, mean_eps: 0.050000
 1633918/2000000: episode: 2201, duration: 62.968s, episode steps: 1171, steps per second:  19, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.016680, mae: 2.563906, mean_q: 3.085024, mean_eps: 0.050000
 1635077/2000000: episode: 2202, duration: 61.630s, episode steps: 1159, steps per second:  19, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.016426, mae: 2.564565, mean_q: 3.087959, mean_eps: 0.050000
 1635457/2000000: episode: 2203, duration: 20.368s, episode steps: 380, steps per second:  19, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.932 [0.000, 5.000],  loss: 0.017752, mae: 2.562538, mean_q: 3.086874, mean_eps: 0.050000
 1636284/2000000: episode: 2204, duration: 44.240s, episode steps: 827, steps per second:  19, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.016421, mae: 2.554783, mean_q: 3.074417, mean_eps: 0.050000
 1637245/2000000: episode: 2205, duration: 53.883s, episode steps: 961, steps per second:  18, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.763 [0.000, 5.000],  loss: 0.016537, mae: 2.552676, mean_q: 3.073679, mean_eps: 0.050000
 1638110/2000000: episode: 2206, duration: 46.634s, episode steps: 865, steps per second:  19, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.746 [0.000, 5.000],  loss: 0.016694, mae: 2.561056, mean_q: 3.084253, mean_eps: 0.050000
 1639250/2000000: episode: 2207, duration: 61.442s, episode steps: 1140, steps per second:  19, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.867 [0.000, 5.000],  loss: 0.016803, mae: 2.546938, mean_q: 3.065664, mean_eps: 0.050000
 1639822/2000000: episode: 2208, duration: 30.796s, episode steps: 572, steps per second:  19, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.762 [0.000, 5.000],  loss: 0.017686, mae: 2.544718, mean_q: 3.062738, mean_eps: 0.050000
 1640723/2000000: episode: 2209, duration: 48.289s, episode steps: 901, steps per second:  19, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.841 [0.000, 5.000],  loss: 0.014483, mae: 2.560416, mean_q: 3.084664, mean_eps: 0.050000
 1641714/2000000: episode: 2210, duration: 53.321s, episode steps: 991, steps per second:  19, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.211 [0.000, 5.000],  loss: 0.015461, mae: 2.562662, mean_q: 3.086001, mean_eps: 0.050000
 1642713/2000000: episode: 2211, duration: 54.497s, episode steps: 999, steps per second:  18, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.016881, mae: 2.553525, mean_q: 3.072557, mean_eps: 0.050000
 1643347/2000000: episode: 2212, duration: 34.033s, episode steps: 634, steps per second:  19, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 3.139 [0.000, 5.000],  loss: 0.016216, mae: 2.559212, mean_q: 3.080695, mean_eps: 0.050000
 1644008/2000000: episode: 2213, duration: 35.596s, episode steps: 661, steps per second:  19, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.016079, mae: 2.552117, mean_q: 3.072683, mean_eps: 0.050000
 1644790/2000000: episode: 2214, duration: 41.849s, episode steps: 782, steps per second:  19, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.334 [0.000, 5.000],  loss: 0.015923, mae: 2.570312, mean_q: 3.094239, mean_eps: 0.050000
 1645480/2000000: episode: 2215, duration: 37.296s, episode steps: 690, steps per second:  19, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.796 [0.000, 5.000],  loss: 0.016434, mae: 2.554913, mean_q: 3.074997, mean_eps: 0.050000
 1646335/2000000: episode: 2216, duration: 45.774s, episode steps: 855, steps per second:  19, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.018146, mae: 2.551954, mean_q: 3.072234, mean_eps: 0.050000
 1647516/2000000: episode: 2217, duration: 63.343s, episode steps: 1181, steps per second:  19, episode reward: 14.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.016690, mae: 2.556535, mean_q: 3.077618, mean_eps: 0.050000
 1648033/2000000: episode: 2218, duration: 27.736s, episode steps: 517, steps per second:  19, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.979 [0.000, 5.000],  loss: 0.017078, mae: 2.559342, mean_q: 3.082202, mean_eps: 0.050000
 1648873/2000000: episode: 2219, duration: 44.931s, episode steps: 840, steps per second:  19, episode reward:  6.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.016046, mae: 2.567035, mean_q: 3.090183, mean_eps: 0.050000
 1649524/2000000: episode: 2220, duration: 34.901s, episode steps: 651, steps per second:  19, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.040 [0.000, 5.000],  loss: 0.017077, mae: 2.571849, mean_q: 3.096369, mean_eps: 0.050000
 1650147/2000000: episode: 2221, duration: 34.612s, episode steps: 623, steps per second:  18, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.016093, mae: 2.569381, mean_q: 3.094697, mean_eps: 0.050000
 1651369/2000000: episode: 2222, duration: 65.228s, episode steps: 1222, steps per second:  19, episode reward: 20.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.014809, mae: 2.595278, mean_q: 3.124985, mean_eps: 0.050000
 1652144/2000000: episode: 2223, duration: 41.322s, episode steps: 775, steps per second:  19, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.017113, mae: 2.595839, mean_q: 3.126356, mean_eps: 0.050000
 1652847/2000000: episode: 2224, duration: 37.552s, episode steps: 703, steps per second:  19, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.070 [0.000, 5.000],  loss: 0.015638, mae: 2.584449, mean_q: 3.113235, mean_eps: 0.050000
 1653846/2000000: episode: 2225, duration: 53.726s, episode steps: 999, steps per second:  19, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.864 [0.000, 5.000],  loss: 0.016445, mae: 2.589339, mean_q: 3.117230, mean_eps: 0.050000
 1654540/2000000: episode: 2226, duration: 37.437s, episode steps: 694, steps per second:  19, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.015949, mae: 2.592566, mean_q: 3.120410, mean_eps: 0.050000
 1655423/2000000: episode: 2227, duration: 47.330s, episode steps: 883, steps per second:  19, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.016236, mae: 2.592439, mean_q: 3.121767, mean_eps: 0.050000
 1656116/2000000: episode: 2228, duration: 37.149s, episode steps: 693, steps per second:  19, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.017255, mae: 2.584795, mean_q: 3.112242, mean_eps: 0.050000
 1656592/2000000: episode: 2229, duration: 25.587s, episode steps: 476, steps per second:  19, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.016610, mae: 2.592394, mean_q: 3.120884, mean_eps: 0.050000
 1657208/2000000: episode: 2230, duration: 33.154s, episode steps: 616, steps per second:  19, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.017962, mae: 2.600314, mean_q: 3.128189, mean_eps: 0.050000
 1657907/2000000: episode: 2231, duration: 37.664s, episode steps: 699, steps per second:  19, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 3.057 [0.000, 5.000],  loss: 0.015560, mae: 2.602930, mean_q: 3.133354, mean_eps: 0.050000
 1658849/2000000: episode: 2232, duration: 50.265s, episode steps: 942, steps per second:  19, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.899 [0.000, 5.000],  loss: 0.016872, mae: 2.586939, mean_q: 3.114914, mean_eps: 0.050000
 1659220/2000000: episode: 2233, duration: 19.784s, episode steps: 371, steps per second:  19, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.017023, mae: 2.585948, mean_q: 3.115121, mean_eps: 0.050000
 1659965/2000000: episode: 2234, duration: 39.997s, episode steps: 745, steps per second:  19, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.728 [0.000, 5.000],  loss: 0.016606, mae: 2.585524, mean_q: 3.112421, mean_eps: 0.050000
 1660950/2000000: episode: 2235, duration: 52.648s, episode steps: 985, steps per second:  19, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.015936, mae: 2.583767, mean_q: 3.111890, mean_eps: 0.050000
 1661576/2000000: episode: 2236, duration: 33.682s, episode steps: 626, steps per second:  19, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.016204, mae: 2.577456, mean_q: 3.105465, mean_eps: 0.050000
 1662905/2000000: episode: 2237, duration: 71.151s, episode steps: 1329, steps per second:  19, episode reward: 24.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.266 [0.000, 5.000],  loss: 0.016279, mae: 2.590557, mean_q: 3.118850, mean_eps: 0.050000
 1663830/2000000: episode: 2238, duration: 49.479s, episode steps: 925, steps per second:  19, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.016915, mae: 2.582287, mean_q: 3.109760, mean_eps: 0.050000
 1664633/2000000: episode: 2239, duration: 42.796s, episode steps: 803, steps per second:  19, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.782 [0.000, 5.000],  loss: 0.016907, mae: 2.572537, mean_q: 3.096883, mean_eps: 0.050000
 1665531/2000000: episode: 2240, duration: 47.988s, episode steps: 898, steps per second:  19, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.981 [0.000, 5.000],  loss: 0.018019, mae: 2.599061, mean_q: 3.128391, mean_eps: 0.050000
 1666224/2000000: episode: 2241, duration: 37.296s, episode steps: 693, steps per second:  19, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.167 [0.000, 5.000],  loss: 0.016220, mae: 2.580187, mean_q: 3.106188, mean_eps: 0.050000
 1666956/2000000: episode: 2242, duration: 39.994s, episode steps: 732, steps per second:  18, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.964 [0.000, 5.000],  loss: 0.016199, mae: 2.577034, mean_q: 3.103934, mean_eps: 0.050000
 1667605/2000000: episode: 2243, duration: 34.712s, episode steps: 649, steps per second:  19, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.048 [0.000, 5.000],  loss: 0.015875, mae: 2.567243, mean_q: 3.090187, mean_eps: 0.050000
 1668558/2000000: episode: 2244, duration: 50.433s, episode steps: 953, steps per second:  19, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.764 [0.000, 5.000],  loss: 0.017779, mae: 2.589179, mean_q: 3.117505, mean_eps: 0.050000
 1670015/2000000: episode: 2245, duration: 77.403s, episode steps: 1457, steps per second:  19, episode reward: 23.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.263 [0.000, 5.000],  loss: 0.015682, mae: 2.581817, mean_q: 3.107851, mean_eps: 0.050000
 1670691/2000000: episode: 2246, duration: 36.030s, episode steps: 676, steps per second:  19, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.312 [0.000, 5.000],  loss: 0.015641, mae: 2.591233, mean_q: 3.121106, mean_eps: 0.050000
 1671499/2000000: episode: 2247, duration: 43.071s, episode steps: 808, steps per second:  19, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.016850, mae: 2.606602, mean_q: 3.138033, mean_eps: 0.050000
 1672210/2000000: episode: 2248, duration: 38.160s, episode steps: 711, steps per second:  19, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.887 [0.000, 5.000],  loss: 0.015483, mae: 2.590426, mean_q: 3.118668, mean_eps: 0.050000
 1673179/2000000: episode: 2249, duration: 51.733s, episode steps: 969, steps per second:  19, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.996 [0.000, 5.000],  loss: 0.017113, mae: 2.590328, mean_q: 3.117482, mean_eps: 0.050000
 1674176/2000000: episode: 2250, duration: 53.292s, episode steps: 997, steps per second:  19, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.016817, mae: 2.587465, mean_q: 3.113658, mean_eps: 0.050000
 1674822/2000000: episode: 2251, duration: 34.556s, episode steps: 646, steps per second:  19, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.907 [0.000, 5.000],  loss: 0.016914, mae: 2.588347, mean_q: 3.115675, mean_eps: 0.050000
 1675665/2000000: episode: 2252, duration: 44.894s, episode steps: 843, steps per second:  19, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.249 [0.000, 5.000],  loss: 0.017258, mae: 2.587570, mean_q: 3.114294, mean_eps: 0.050000
 1676006/2000000: episode: 2253, duration: 18.159s, episode steps: 341, steps per second:  19, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.988 [0.000, 5.000],  loss: 0.016494, mae: 2.575317, mean_q: 3.098009, mean_eps: 0.050000
 1677162/2000000: episode: 2254, duration: 61.169s, episode steps: 1156, steps per second:  19, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.245 [0.000, 5.000],  loss: 0.015909, mae: 2.591328, mean_q: 3.120233, mean_eps: 0.050000
 1678293/2000000: episode: 2255, duration: 60.420s, episode steps: 1131, steps per second:  19, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.165 [0.000, 5.000],  loss: 0.016586, mae: 2.579853, mean_q: 3.106273, mean_eps: 0.050000
 1679057/2000000: episode: 2256, duration: 40.735s, episode steps: 764, steps per second:  19, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.949 [0.000, 5.000],  loss: 0.017298, mae: 2.597563, mean_q: 3.128023, mean_eps: 0.050000
 1680253/2000000: episode: 2257, duration: 63.828s, episode steps: 1196, steps per second:  19, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.779 [0.000, 5.000],  loss: 0.017832, mae: 2.590155, mean_q: 3.119819, mean_eps: 0.050000
 1681403/2000000: episode: 2258, duration: 62.745s, episode steps: 1150, steps per second:  18, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.014485, mae: 2.580872, mean_q: 3.108408, mean_eps: 0.050000
 1681938/2000000: episode: 2259, duration: 33.181s, episode steps: 535, steps per second:  16, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.686 [0.000, 5.000],  loss: 0.017256, mae: 2.579700, mean_q: 3.106215, mean_eps: 0.050000
 1682360/2000000: episode: 2260, duration: 26.370s, episode steps: 422, steps per second:  16, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 3.374 [0.000, 5.000],  loss: 0.016566, mae: 2.575749, mean_q: 3.098543, mean_eps: 0.050000
 1683412/2000000: episode: 2261, duration: 64.869s, episode steps: 1052, steps per second:  16, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.015860, mae: 2.566205, mean_q: 3.088890, mean_eps: 0.050000
 1684693/2000000: episode: 2262, duration: 78.386s, episode steps: 1281, steps per second:  16, episode reward: 28.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.017829, mae: 2.585533, mean_q: 3.112219, mean_eps: 0.050000
 1685446/2000000: episode: 2263, duration: 46.075s, episode steps: 753, steps per second:  16, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.016271, mae: 2.583019, mean_q: 3.109558, mean_eps: 0.050000
 1686287/2000000: episode: 2264, duration: 51.098s, episode steps: 841, steps per second:  16, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.007 [0.000, 5.000],  loss: 0.017155, mae: 2.583887, mean_q: 3.110115, mean_eps: 0.050000
 1687227/2000000: episode: 2265, duration: 57.228s, episode steps: 940, steps per second:  16, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.015951, mae: 2.572909, mean_q: 3.098398, mean_eps: 0.050000
 1687742/2000000: episode: 2266, duration: 31.239s, episode steps: 515, steps per second:  16, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.489 [0.000, 5.000],  loss: 0.016823, mae: 2.570972, mean_q: 3.094547, mean_eps: 0.050000
 1689214/2000000: episode: 2267, duration: 90.355s, episode steps: 1472, steps per second:  16, episode reward: 16.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.016353, mae: 2.581076, mean_q: 3.106090, mean_eps: 0.050000
 1689839/2000000: episode: 2268, duration: 38.055s, episode steps: 625, steps per second:  16, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.731 [0.000, 5.000],  loss: 0.017479, mae: 2.564777, mean_q: 3.085131, mean_eps: 0.050000
 1690723/2000000: episode: 2269, duration: 53.771s, episode steps: 884, steps per second:  16, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.016845, mae: 2.560016, mean_q: 3.081164, mean_eps: 0.050000
 1691976/2000000: episode: 2270, duration: 76.323s, episode steps: 1253, steps per second:  16, episode reward: 20.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.917 [0.000, 5.000],  loss: 0.014793, mae: 2.563844, mean_q: 3.086364, mean_eps: 0.050000
 1692572/2000000: episode: 2271, duration: 36.539s, episode steps: 596, steps per second:  16, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: 0.016431, mae: 2.534277, mean_q: 3.052577, mean_eps: 0.050000
 1693750/2000000: episode: 2272, duration: 71.616s, episode steps: 1178, steps per second:  16, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.079 [0.000, 5.000],  loss: 0.016927, mae: 2.563944, mean_q: 3.087725, mean_eps: 0.050000
 1694343/2000000: episode: 2273, duration: 36.188s, episode steps: 593, steps per second:  16, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.069 [0.000, 5.000],  loss: 0.017035, mae: 2.554352, mean_q: 3.074197, mean_eps: 0.050000
 1694937/2000000: episode: 2274, duration: 36.674s, episode steps: 594, steps per second:  16, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.016289, mae: 2.553864, mean_q: 3.075740, mean_eps: 0.050000
 1696114/2000000: episode: 2275, duration: 71.444s, episode steps: 1177, steps per second:  16, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.712 [0.000, 5.000],  loss: 0.016228, mae: 2.560809, mean_q: 3.083224, mean_eps: 0.050000
 1697001/2000000: episode: 2276, duration: 53.834s, episode steps: 887, steps per second:  16, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.752 [0.000, 5.000],  loss: 0.015787, mae: 2.559438, mean_q: 3.082274, mean_eps: 0.050000
 1698122/2000000: episode: 2277, duration: 67.846s, episode steps: 1121, steps per second:  17, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.016102, mae: 2.567661, mean_q: 3.091476, mean_eps: 0.050000
 1698599/2000000: episode: 2278, duration: 28.863s, episode steps: 477, steps per second:  17, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.014813, mae: 2.548450, mean_q: 3.067200, mean_eps: 0.050000
 1699557/2000000: episode: 2279, duration: 58.216s, episode steps: 958, steps per second:  16, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.016726, mae: 2.563356, mean_q: 3.084768, mean_eps: 0.050000
 1700117/2000000: episode: 2280, duration: 35.238s, episode steps: 560, steps per second:  16, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.016723, mae: 2.580561, mean_q: 3.106339, mean_eps: 0.050000
 1701271/2000000: episode: 2281, duration: 71.264s, episode steps: 1154, steps per second:  16, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.730 [0.000, 5.000],  loss: 0.014975, mae: 2.550590, mean_q: 3.071511, mean_eps: 0.050000
 1701973/2000000: episode: 2282, duration: 43.306s, episode steps: 702, steps per second:  16, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.872 [0.000, 5.000],  loss: 0.014906, mae: 2.554531, mean_q: 3.075151, mean_eps: 0.050000
 1702492/2000000: episode: 2283, duration: 31.834s, episode steps: 519, steps per second:  16, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.016970, mae: 2.560767, mean_q: 3.084415, mean_eps: 0.050000
 1703462/2000000: episode: 2284, duration: 59.689s, episode steps: 970, steps per second:  16, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.257 [0.000, 5.000],  loss: 0.016179, mae: 2.549317, mean_q: 3.068790, mean_eps: 0.050000
 1704055/2000000: episode: 2285, duration: 36.108s, episode steps: 593, steps per second:  16, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 3.305 [0.000, 5.000],  loss: 0.016166, mae: 2.550862, mean_q: 3.069040, mean_eps: 0.050000
 1704737/2000000: episode: 2286, duration: 41.509s, episode steps: 682, steps per second:  16, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.016964, mae: 2.569783, mean_q: 3.092727, mean_eps: 0.050000
 1705239/2000000: episode: 2287, duration: 30.323s, episode steps: 502, steps per second:  17, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.833 [0.000, 5.000],  loss: 0.016009, mae: 2.556922, mean_q: 3.078576, mean_eps: 0.050000
 1705873/2000000: episode: 2288, duration: 38.729s, episode steps: 634, steps per second:  16, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.740 [0.000, 5.000],  loss: 0.017361, mae: 2.544854, mean_q: 3.063423, mean_eps: 0.050000
 1706540/2000000: episode: 2289, duration: 40.764s, episode steps: 667, steps per second:  16, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.017198, mae: 2.547404, mean_q: 3.067266, mean_eps: 0.050000
 1707022/2000000: episode: 2290, duration: 29.532s, episode steps: 482, steps per second:  16, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.272 [0.000, 5.000],  loss: 0.018004, mae: 2.542510, mean_q: 3.063788, mean_eps: 0.050000
 1707558/2000000: episode: 2291, duration: 32.475s, episode steps: 536, steps per second:  17, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.252 [0.000, 5.000],  loss: 0.016542, mae: 2.545504, mean_q: 3.061634, mean_eps: 0.050000
 1708210/2000000: episode: 2292, duration: 39.862s, episode steps: 652, steps per second:  16, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.184 [0.000, 5.000],  loss: 0.017318, mae: 2.559143, mean_q: 3.080638, mean_eps: 0.050000
 1708862/2000000: episode: 2293, duration: 39.594s, episode steps: 652, steps per second:  16, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.696 [0.000, 5.000],  loss: 0.016898, mae: 2.532730, mean_q: 3.050924, mean_eps: 0.050000
 1709995/2000000: episode: 2294, duration: 68.491s, episode steps: 1133, steps per second:  17, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.111 [0.000, 5.000],  loss: 0.016764, mae: 2.545109, mean_q: 3.064452, mean_eps: 0.050000
 1710496/2000000: episode: 2295, duration: 30.552s, episode steps: 501, steps per second:  16, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.802 [0.000, 5.000],  loss: 0.014675, mae: 2.548441, mean_q: 3.068496, mean_eps: 0.050000
 1711219/2000000: episode: 2296, duration: 43.969s, episode steps: 723, steps per second:  16, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.015654, mae: 2.534341, mean_q: 3.050778, mean_eps: 0.050000
 1712493/2000000: episode: 2297, duration: 77.525s, episode steps: 1274, steps per second:  16, episode reward: 18.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.015495, mae: 2.540803, mean_q: 3.059015, mean_eps: 0.050000
 1713373/2000000: episode: 2298, duration: 53.485s, episode steps: 880, steps per second:  16, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.015630, mae: 2.544434, mean_q: 3.063510, mean_eps: 0.050000
 1714325/2000000: episode: 2299, duration: 57.848s, episode steps: 952, steps per second:  16, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.199 [0.000, 5.000],  loss: 0.017440, mae: 2.550550, mean_q: 3.068549, mean_eps: 0.050000
 1715159/2000000: episode: 2300, duration: 50.733s, episode steps: 834, steps per second:  16, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.704 [0.000, 5.000],  loss: 0.016001, mae: 2.538912, mean_q: 3.057560, mean_eps: 0.050000
 1715762/2000000: episode: 2301, duration: 36.879s, episode steps: 603, steps per second:  16, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.016024, mae: 2.528404, mean_q: 3.045192, mean_eps: 0.050000
 1716294/2000000: episode: 2302, duration: 32.512s, episode steps: 532, steps per second:  16, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.284 [0.000, 5.000],  loss: 0.014986, mae: 2.541266, mean_q: 3.061318, mean_eps: 0.050000
 1717284/2000000: episode: 2303, duration: 60.269s, episode steps: 990, steps per second:  16, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.016839, mae: 2.534559, mean_q: 3.050065, mean_eps: 0.050000
 1717793/2000000: episode: 2304, duration: 31.154s, episode steps: 509, steps per second:  16, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.017999, mae: 2.546538, mean_q: 3.066569, mean_eps: 0.050000
 1718711/2000000: episode: 2305, duration: 55.875s, episode steps: 918, steps per second:  16, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.781 [0.000, 5.000],  loss: 0.016645, mae: 2.538298, mean_q: 3.054246, mean_eps: 0.050000
 1719211/2000000: episode: 2306, duration: 30.572s, episode steps: 500, steps per second:  16, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.824 [0.000, 5.000],  loss: 0.018321, mae: 2.552881, mean_q: 3.070431, mean_eps: 0.050000
 1720331/2000000: episode: 2307, duration: 67.833s, episode steps: 1120, steps per second:  17, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.188 [0.000, 5.000],  loss: 0.016619, mae: 2.539009, mean_q: 3.055688, mean_eps: 0.050000
 1721487/2000000: episode: 2308, duration: 70.232s, episode steps: 1156, steps per second:  16, episode reward: 12.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.228 [0.000, 5.000],  loss: 0.014990, mae: 2.549033, mean_q: 3.069713, mean_eps: 0.050000
 1722290/2000000: episode: 2309, duration: 48.548s, episode steps: 803, steps per second:  17, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.015277, mae: 2.537556, mean_q: 3.054341, mean_eps: 0.050000
 1723097/2000000: episode: 2310, duration: 49.085s, episode steps: 807, steps per second:  16, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.015739, mae: 2.537225, mean_q: 3.054991, mean_eps: 0.050000
 1724081/2000000: episode: 2311, duration: 59.781s, episode steps: 984, steps per second:  16, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.138 [0.000, 5.000],  loss: 0.017612, mae: 2.549917, mean_q: 3.068222, mean_eps: 0.050000
 1724930/2000000: episode: 2312, duration: 51.496s, episode steps: 849, steps per second:  16, episode reward:  7.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.015877, mae: 2.550168, mean_q: 3.069315, mean_eps: 0.050000
 1725885/2000000: episode: 2313, duration: 57.987s, episode steps: 955, steps per second:  16, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.016901, mae: 2.551790, mean_q: 3.072768, mean_eps: 0.050000
 1726781/2000000: episode: 2314, duration: 54.513s, episode steps: 896, steps per second:  16, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.887 [0.000, 5.000],  loss: 0.017603, mae: 2.547671, mean_q: 3.067690, mean_eps: 0.050000
 1727444/2000000: episode: 2315, duration: 40.275s, episode steps: 663, steps per second:  16, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.824 [0.000, 5.000],  loss: 0.016640, mae: 2.532720, mean_q: 3.047996, mean_eps: 0.050000
 1728699/2000000: episode: 2316, duration: 76.496s, episode steps: 1255, steps per second:  16, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.016923, mae: 2.544087, mean_q: 3.061372, mean_eps: 0.050000
 1729816/2000000: episode: 2317, duration: 67.938s, episode steps: 1117, steps per second:  16, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.017053, mae: 2.543891, mean_q: 3.061772, mean_eps: 0.050000
 1730824/2000000: episode: 2318, duration: 61.515s, episode steps: 1008, steps per second:  16, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.014750, mae: 2.549241, mean_q: 3.069978, mean_eps: 0.050000
 1732088/2000000: episode: 2319, duration: 77.432s, episode steps: 1264, steps per second:  16, episode reward: 22.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.015088, mae: 2.549057, mean_q: 3.069942, mean_eps: 0.050000
 1733173/2000000: episode: 2320, duration: 66.623s, episode steps: 1085, steps per second:  16, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.655 [0.000, 5.000],  loss: 0.016385, mae: 2.547552, mean_q: 3.066239, mean_eps: 0.050000
 1734124/2000000: episode: 2321, duration: 57.749s, episode steps: 951, steps per second:  16, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.932 [0.000, 5.000],  loss: 0.015627, mae: 2.529536, mean_q: 3.045739, mean_eps: 0.050000
 1734836/2000000: episode: 2322, duration: 43.450s, episode steps: 712, steps per second:  16, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.805 [0.000, 5.000],  loss: 0.016451, mae: 2.530933, mean_q: 3.047407, mean_eps: 0.050000
 1735738/2000000: episode: 2323, duration: 55.035s, episode steps: 902, steps per second:  16, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.016311, mae: 2.539134, mean_q: 3.056212, mean_eps: 0.050000
 1736705/2000000: episode: 2324, duration: 58.753s, episode steps: 967, steps per second:  16, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.930 [0.000, 5.000],  loss: 0.017806, mae: 2.534842, mean_q: 3.052643, mean_eps: 0.050000
 1737188/2000000: episode: 2325, duration: 29.399s, episode steps: 483, steps per second:  16, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.799 [0.000, 5.000],  loss: 0.016225, mae: 2.546181, mean_q: 3.067040, mean_eps: 0.050000
 1737670/2000000: episode: 2326, duration: 29.497s, episode steps: 482, steps per second:  16, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.981 [0.000, 5.000],  loss: 0.016718, mae: 2.525154, mean_q: 3.039801, mean_eps: 0.050000
 1738585/2000000: episode: 2327, duration: 55.462s, episode steps: 915, steps per second:  16, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.895 [0.000, 5.000],  loss: 0.016064, mae: 2.525653, mean_q: 3.040055, mean_eps: 0.050000
 1739924/2000000: episode: 2328, duration: 81.449s, episode steps: 1339, steps per second:  16, episode reward: 32.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.015691, mae: 2.529324, mean_q: 3.044326, mean_eps: 0.050000
 1740818/2000000: episode: 2329, duration: 54.642s, episode steps: 894, steps per second:  16, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.266 [0.000, 5.000],  loss: 0.014146, mae: 2.557013, mean_q: 3.079402, mean_eps: 0.050000
 1741439/2000000: episode: 2330, duration: 37.712s, episode steps: 621, steps per second:  16, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.942 [0.000, 5.000],  loss: 0.016058, mae: 2.543245, mean_q: 3.061907, mean_eps: 0.050000
 1742089/2000000: episode: 2331, duration: 39.544s, episode steps: 650, steps per second:  16, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.015428, mae: 2.553458, mean_q: 3.074738, mean_eps: 0.050000
 1742762/2000000: episode: 2332, duration: 40.907s, episode steps: 673, steps per second:  16, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.015782, mae: 2.563773, mean_q: 3.085960, mean_eps: 0.050000
 1743521/2000000: episode: 2333, duration: 46.214s, episode steps: 759, steps per second:  16, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.015963, mae: 2.544017, mean_q: 3.062648, mean_eps: 0.050000
 1744051/2000000: episode: 2334, duration: 32.086s, episode steps: 530, steps per second:  17, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.015383, mae: 2.555588, mean_q: 3.075795, mean_eps: 0.050000
 1745286/2000000: episode: 2335, duration: 75.062s, episode steps: 1235, steps per second:  16, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.016697, mae: 2.539890, mean_q: 3.058328, mean_eps: 0.050000
 1746242/2000000: episode: 2336, duration: 58.320s, episode steps: 956, steps per second:  16, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.017238, mae: 2.548232, mean_q: 3.067761, mean_eps: 0.050000
 1746884/2000000: episode: 2337, duration: 39.106s, episode steps: 642, steps per second:  16, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.801 [0.000, 5.000],  loss: 0.017027, mae: 2.559794, mean_q: 3.083556, mean_eps: 0.050000
 1747717/2000000: episode: 2338, duration: 51.438s, episode steps: 833, steps per second:  16, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.264 [0.000, 5.000],  loss: 0.015468, mae: 2.547710, mean_q: 3.065508, mean_eps: 0.050000
 1748604/2000000: episode: 2339, duration: 53.994s, episode steps: 887, steps per second:  16, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.015600, mae: 2.545366, mean_q: 3.065530, mean_eps: 0.050000
 1749636/2000000: episode: 2340, duration: 63.091s, episode steps: 1032, steps per second:  16, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.823 [0.000, 5.000],  loss: 0.016467, mae: 2.549600, mean_q: 3.069276, mean_eps: 0.050000
 1750756/2000000: episode: 2341, duration: 69.345s, episode steps: 1120, steps per second:  16, episode reward: 13.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.071 [0.000, 5.000],  loss: 0.015537, mae: 2.540492, mean_q: 3.062628, mean_eps: 0.050000
 1751524/2000000: episode: 2342, duration: 47.142s, episode steps: 768, steps per second:  16, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.671 [0.000, 5.000],  loss: 0.015267, mae: 2.544810, mean_q: 3.064108, mean_eps: 0.050000
 1752770/2000000: episode: 2343, duration: 75.908s, episode steps: 1246, steps per second:  16, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.799 [0.000, 5.000],  loss: 0.015541, mae: 2.543970, mean_q: 3.064342, mean_eps: 0.050000
 1753614/2000000: episode: 2344, duration: 51.736s, episode steps: 844, steps per second:  16, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.873 [0.000, 5.000],  loss: 0.016599, mae: 2.532241, mean_q: 3.049248, mean_eps: 0.050000
 1754973/2000000: episode: 2345, duration: 82.751s, episode steps: 1359, steps per second:  16, episode reward: 25.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.016019, mae: 2.542236, mean_q: 3.061930, mean_eps: 0.050000
 1756180/2000000: episode: 2346, duration: 73.458s, episode steps: 1207, steps per second:  16, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.949 [0.000, 5.000],  loss: 0.016481, mae: 2.539551, mean_q: 3.057514, mean_eps: 0.050000
 1756666/2000000: episode: 2347, duration: 29.657s, episode steps: 486, steps per second:  16, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.858 [0.000, 5.000],  loss: 0.016419, mae: 2.538018, mean_q: 3.055435, mean_eps: 0.050000
 1757021/2000000: episode: 2348, duration: 21.738s, episode steps: 355, steps per second:  16, episode reward:  7.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.183 [0.000, 5.000],  loss: 0.017161, mae: 2.529888, mean_q: 3.045361, mean_eps: 0.050000
 1757715/2000000: episode: 2349, duration: 42.145s, episode steps: 694, steps per second:  16, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.015821, mae: 2.557001, mean_q: 3.080734, mean_eps: 0.050000
 1758190/2000000: episode: 2350, duration: 29.363s, episode steps: 475, steps per second:  16, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.316 [0.000, 5.000],  loss: 0.017620, mae: 2.543613, mean_q: 3.062373, mean_eps: 0.050000
 1759214/2000000: episode: 2351, duration: 62.134s, episode steps: 1024, steps per second:  16, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.016758, mae: 2.537580, mean_q: 3.055851, mean_eps: 0.050000
 1760390/2000000: episode: 2352, duration: 73.074s, episode steps: 1176, steps per second:  16, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.679 [0.000, 5.000],  loss: 0.015859, mae: 2.527413, mean_q: 3.043362, mean_eps: 0.050000
 1761209/2000000: episode: 2353, duration: 50.378s, episode steps: 819, steps per second:  16, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.015979, mae: 2.554345, mean_q: 3.077354, mean_eps: 0.050000
 1761679/2000000: episode: 2354, duration: 28.790s, episode steps: 470, steps per second:  16, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: 0.016835, mae: 2.557354, mean_q: 3.079103, mean_eps: 0.050000
 1762288/2000000: episode: 2355, duration: 37.791s, episode steps: 609, steps per second:  16, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.885 [0.000, 5.000],  loss: 0.015755, mae: 2.530686, mean_q: 3.048142, mean_eps: 0.050000
 1763036/2000000: episode: 2356, duration: 46.179s, episode steps: 748, steps per second:  16, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.694 [0.000, 5.000],  loss: 0.016969, mae: 2.553850, mean_q: 3.074331, mean_eps: 0.050000
 1763686/2000000: episode: 2357, duration: 39.867s, episode steps: 650, steps per second:  16, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.017587, mae: 2.534109, mean_q: 3.054384, mean_eps: 0.050000
 1764164/2000000: episode: 2358, duration: 29.395s, episode steps: 478, steps per second:  16, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.109 [0.000, 5.000],  loss: 0.018265, mae: 2.553569, mean_q: 3.074891, mean_eps: 0.050000
 1765148/2000000: episode: 2359, duration: 60.437s, episode steps: 984, steps per second:  16, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.017194, mae: 2.545833, mean_q: 3.063969, mean_eps: 0.050000
 1765537/2000000: episode: 2360, duration: 23.928s, episode steps: 389, steps per second:  16, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.154 [0.000, 5.000],  loss: 0.017230, mae: 2.544082, mean_q: 3.062115, mean_eps: 0.050000
 1766627/2000000: episode: 2361, duration: 66.437s, episode steps: 1090, steps per second:  16, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.015541, mae: 2.536375, mean_q: 3.054383, mean_eps: 0.050000
 1767367/2000000: episode: 2362, duration: 45.438s, episode steps: 740, steps per second:  16, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.301 [0.000, 5.000],  loss: 0.016941, mae: 2.544198, mean_q: 3.061980, mean_eps: 0.050000
 1767945/2000000: episode: 2363, duration: 35.400s, episode steps: 578, steps per second:  16, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.109 [0.000, 5.000],  loss: 0.016826, mae: 2.551887, mean_q: 3.072080, mean_eps: 0.050000
 1768964/2000000: episode: 2364, duration: 65.030s, episode steps: 1019, steps per second:  16, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.742 [0.000, 5.000],  loss: 0.017397, mae: 2.537206, mean_q: 3.054676, mean_eps: 0.050000
 1769873/2000000: episode: 2365, duration: 55.969s, episode steps: 909, steps per second:  16, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.088 [0.000, 5.000],  loss: 0.018615, mae: 2.552064, mean_q: 3.073474, mean_eps: 0.050000
 1770974/2000000: episode: 2366, duration: 67.305s, episode steps: 1101, steps per second:  16, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.015694, mae: 2.538144, mean_q: 3.056749, mean_eps: 0.050000
 1771645/2000000: episode: 2367, duration: 41.261s, episode steps: 671, steps per second:  16, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.014584, mae: 2.538319, mean_q: 3.056646, mean_eps: 0.050000
 1772688/2000000: episode: 2368, duration: 64.080s, episode steps: 1043, steps per second:  16, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.007 [0.000, 5.000],  loss: 0.015992, mae: 2.539989, mean_q: 3.058605, mean_eps: 0.050000
 1773515/2000000: episode: 2369, duration: 50.287s, episode steps: 827, steps per second:  16, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.254 [0.000, 5.000],  loss: 0.016306, mae: 2.542934, mean_q: 3.062821, mean_eps: 0.050000
 1774445/2000000: episode: 2370, duration: 56.843s, episode steps: 930, steps per second:  16, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.914 [0.000, 5.000],  loss: 0.015539, mae: 2.544963, mean_q: 3.063132, mean_eps: 0.050000
 1775336/2000000: episode: 2371, duration: 54.656s, episode steps: 891, steps per second:  16, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.945 [0.000, 5.000],  loss: 0.015812, mae: 2.539045, mean_q: 3.055052, mean_eps: 0.050000
 1775959/2000000: episode: 2372, duration: 38.198s, episode steps: 623, steps per second:  16, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.016556, mae: 2.548546, mean_q: 3.067449, mean_eps: 0.050000
 1776768/2000000: episode: 2373, duration: 49.938s, episode steps: 809, steps per second:  16, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.017040, mae: 2.533217, mean_q: 3.049134, mean_eps: 0.050000
 1777576/2000000: episode: 2374, duration: 50.041s, episode steps: 808, steps per second:  16, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.613 [0.000, 5.000],  loss: 0.015805, mae: 2.542684, mean_q: 3.061401, mean_eps: 0.050000
 1778347/2000000: episode: 2375, duration: 47.013s, episode steps: 771, steps per second:  16, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.184 [0.000, 5.000],  loss: 0.018166, mae: 2.539495, mean_q: 3.056255, mean_eps: 0.050000
 1778999/2000000: episode: 2376, duration: 40.769s, episode steps: 652, steps per second:  16, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.980 [0.000, 5.000],  loss: 0.016749, mae: 2.534472, mean_q: 3.050552, mean_eps: 0.050000
 1779511/2000000: episode: 2377, duration: 31.375s, episode steps: 512, steps per second:  16, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.852 [0.000, 5.000],  loss: 0.016154, mae: 2.537924, mean_q: 3.055034, mean_eps: 0.050000
 1780094/2000000: episode: 2378, duration: 35.554s, episode steps: 583, steps per second:  16, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.016640, mae: 2.554613, mean_q: 3.074547, mean_eps: 0.050000
 1780679/2000000: episode: 2379, duration: 35.823s, episode steps: 585, steps per second:  16, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.944 [0.000, 5.000],  loss: 0.015705, mae: 2.561167, mean_q: 3.083374, mean_eps: 0.050000
 1781386/2000000: episode: 2380, duration: 43.400s, episode steps: 707, steps per second:  16, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.015364, mae: 2.553274, mean_q: 3.074468, mean_eps: 0.050000
 1782453/2000000: episode: 2381, duration: 65.083s, episode steps: 1067, steps per second:  16, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.724 [0.000, 5.000],  loss: 0.015408, mae: 2.542440, mean_q: 3.060846, mean_eps: 0.050000
 1783116/2000000: episode: 2382, duration: 40.405s, episode steps: 663, steps per second:  16, episode reward: 19.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.273 [0.000, 5.000],  loss: 0.016374, mae: 2.539124, mean_q: 3.057314, mean_eps: 0.050000
 1783956/2000000: episode: 2383, duration: 51.367s, episode steps: 840, steps per second:  16, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.051 [0.000, 5.000],  loss: 0.015958, mae: 2.545625, mean_q: 3.063536, mean_eps: 0.050000
 1784446/2000000: episode: 2384, duration: 30.156s, episode steps: 490, steps per second:  16, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.016265, mae: 2.546847, mean_q: 3.064385, mean_eps: 0.050000
 1785145/2000000: episode: 2385, duration: 42.470s, episode steps: 699, steps per second:  16, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.017128, mae: 2.545485, mean_q: 3.062475, mean_eps: 0.050000
 1785846/2000000: episode: 2386, duration: 42.550s, episode steps: 701, steps per second:  16, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.009 [0.000, 5.000],  loss: 0.016879, mae: 2.556191, mean_q: 3.075162, mean_eps: 0.050000
 1786564/2000000: episode: 2387, duration: 44.235s, episode steps: 718, steps per second:  16, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.930 [0.000, 5.000],  loss: 0.016850, mae: 2.538897, mean_q: 3.055619, mean_eps: 0.050000
 1787084/2000000: episode: 2388, duration: 32.049s, episode steps: 520, steps per second:  16, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.908 [0.000, 5.000],  loss: 0.016719, mae: 2.536981, mean_q: 3.052992, mean_eps: 0.050000
 1788281/2000000: episode: 2389, duration: 73.116s, episode steps: 1197, steps per second:  16, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.017379, mae: 2.545783, mean_q: 3.064587, mean_eps: 0.050000
 1789704/2000000: episode: 2390, duration: 87.251s, episode steps: 1423, steps per second:  16, episode reward: 20.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.050 [0.000, 5.000],  loss: 0.016996, mae: 2.535120, mean_q: 3.051269, mean_eps: 0.050000
 1790103/2000000: episode: 2391, duration: 24.603s, episode steps: 399, steps per second:  16, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.170 [0.000, 5.000],  loss: 0.014776, mae: 2.545659, mean_q: 3.066105, mean_eps: 0.050000
 1790522/2000000: episode: 2392, duration: 25.670s, episode steps: 419, steps per second:  16, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 3.446 [0.000, 5.000],  loss: 0.015735, mae: 2.555102, mean_q: 3.079177, mean_eps: 0.050000
 1791191/2000000: episode: 2393, duration: 41.247s, episode steps: 669, steps per second:  16, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.093 [0.000, 5.000],  loss: 0.016228, mae: 2.555983, mean_q: 3.076868, mean_eps: 0.050000
 1792306/2000000: episode: 2394, duration: 68.463s, episode steps: 1115, steps per second:  16, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.286 [0.000, 5.000],  loss: 0.014860, mae: 2.538783, mean_q: 3.056647, mean_eps: 0.050000
 1793320/2000000: episode: 2395, duration: 61.961s, episode steps: 1014, steps per second:  16, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: 0.016163, mae: 2.545441, mean_q: 3.062692, mean_eps: 0.050000
 1794177/2000000: episode: 2396, duration: 52.183s, episode steps: 857, steps per second:  16, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.084 [0.000, 5.000],  loss: 0.016566, mae: 2.546656, mean_q: 3.064646, mean_eps: 0.050000
 1794808/2000000: episode: 2397, duration: 38.501s, episode steps: 631, steps per second:  16, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.510 [0.000, 5.000],  loss: 0.016002, mae: 2.547912, mean_q: 3.066516, mean_eps: 0.050000
 1795196/2000000: episode: 2398, duration: 23.878s, episode steps: 388, steps per second:  16, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.799 [0.000, 5.000],  loss: 0.017314, mae: 2.553637, mean_q: 3.073975, mean_eps: 0.050000
 1796327/2000000: episode: 2399, duration: 68.995s, episode steps: 1131, steps per second:  16, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.015926, mae: 2.548557, mean_q: 3.067543, mean_eps: 0.050000
 1796691/2000000: episode: 2400, duration: 22.152s, episode steps: 364, steps per second:  16, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.018532, mae: 2.530437, mean_q: 3.043772, mean_eps: 0.050000
 1797355/2000000: episode: 2401, duration: 40.690s, episode steps: 664, steps per second:  16, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.309 [0.000, 5.000],  loss: 0.017867, mae: 2.543629, mean_q: 3.061020, mean_eps: 0.050000
 1797871/2000000: episode: 2402, duration: 31.573s, episode steps: 516, steps per second:  16, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.058 [0.000, 5.000],  loss: 0.017905, mae: 2.548235, mean_q: 3.067019, mean_eps: 0.050000
 1798415/2000000: episode: 2403, duration: 33.377s, episode steps: 544, steps per second:  16, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.994 [0.000, 5.000],  loss: 0.016056, mae: 2.544634, mean_q: 3.063040, mean_eps: 0.050000
 1799063/2000000: episode: 2404, duration: 39.586s, episode steps: 648, steps per second:  16, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.981 [0.000, 5.000],  loss: 0.018064, mae: 2.541882, mean_q: 3.057721, mean_eps: 0.050000
 1799688/2000000: episode: 2405, duration: 38.196s, episode steps: 625, steps per second:  16, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.874 [0.000, 5.000],  loss: 0.017949, mae: 2.547635, mean_q: 3.067133, mean_eps: 0.050000
 1800364/2000000: episode: 2406, duration: 43.164s, episode steps: 676, steps per second:  16, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.781 [0.000, 5.000],  loss: 0.015619, mae: 2.567719, mean_q: 3.091366, mean_eps: 0.050000
 1801081/2000000: episode: 2407, duration: 44.235s, episode steps: 717, steps per second:  16, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.015415, mae: 2.567658, mean_q: 3.093381, mean_eps: 0.050000
 1801470/2000000: episode: 2408, duration: 23.857s, episode steps: 389, steps per second:  16, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.059 [0.000, 5.000],  loss: 0.014467, mae: 2.569884, mean_q: 3.093631, mean_eps: 0.050000
 1802335/2000000: episode: 2409, duration: 52.902s, episode steps: 865, steps per second:  16, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.016580, mae: 2.576115, mean_q: 3.100459, mean_eps: 0.050000
 1804329/2000000: episode: 2410, duration: 121.589s, episode steps: 1994, steps per second:  16, episode reward: 41.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.049 [0.000, 5.000],  loss: 0.017212, mae: 2.571518, mean_q: 3.096600, mean_eps: 0.050000
 1805421/2000000: episode: 2411, duration: 66.798s, episode steps: 1092, steps per second:  16, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.312 [0.000, 5.000],  loss: 0.016399, mae: 2.562616, mean_q: 3.084431, mean_eps: 0.050000
 1806042/2000000: episode: 2412, duration: 38.123s, episode steps: 621, steps per second:  16, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.731 [0.000, 5.000],  loss: 0.016923, mae: 2.575541, mean_q: 3.101261, mean_eps: 0.050000
 1806659/2000000: episode: 2413, duration: 37.722s, episode steps: 617, steps per second:  16, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.878 [0.000, 5.000],  loss: 0.017923, mae: 2.565545, mean_q: 3.088049, mean_eps: 0.050000
 1807055/2000000: episode: 2414, duration: 24.376s, episode steps: 396, steps per second:  16, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.048 [0.000, 5.000],  loss: 0.018247, mae: 2.567917, mean_q: 3.091193, mean_eps: 0.050000
 1807546/2000000: episode: 2415, duration: 29.982s, episode steps: 491, steps per second:  16, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.015452, mae: 2.570787, mean_q: 3.097871, mean_eps: 0.050000
 1808476/2000000: episode: 2416, duration: 57.055s, episode steps: 930, steps per second:  16, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.260 [0.000, 5.000],  loss: 0.017328, mae: 2.563853, mean_q: 3.087727, mean_eps: 0.050000
 1809275/2000000: episode: 2417, duration: 48.433s, episode steps: 799, steps per second:  16, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.016347, mae: 2.568837, mean_q: 3.094499, mean_eps: 0.050000
 1810149/2000000: episode: 2418, duration: 53.681s, episode steps: 874, steps per second:  16, episode reward:  9.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.015671, mae: 2.559866, mean_q: 3.082240, mean_eps: 0.050000
 1811282/2000000: episode: 2419, duration: 68.639s, episode steps: 1133, steps per second:  17, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.015649, mae: 2.551082, mean_q: 3.072469, mean_eps: 0.050000
 1812052/2000000: episode: 2420, duration: 47.303s, episode steps: 770, steps per second:  16, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.015605, mae: 2.558399, mean_q: 3.080608, mean_eps: 0.050000
 1812566/2000000: episode: 2421, duration: 31.377s, episode steps: 514, steps per second:  16, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 3.082 [0.000, 5.000],  loss: 0.015271, mae: 2.556322, mean_q: 3.078548, mean_eps: 0.050000
 1813769/2000000: episode: 2422, duration: 73.950s, episode steps: 1203, steps per second:  16, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.016319, mae: 2.554055, mean_q: 3.074759, mean_eps: 0.050000
 1814579/2000000: episode: 2423, duration: 49.374s, episode steps: 810, steps per second:  16, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.167 [0.000, 5.000],  loss: 0.014803, mae: 2.551110, mean_q: 3.072066, mean_eps: 0.050000
 1815066/2000000: episode: 2424, duration: 29.992s, episode steps: 487, steps per second:  16, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.924 [0.000, 5.000],  loss: 0.015534, mae: 2.545692, mean_q: 3.064408, mean_eps: 0.050000
 1815701/2000000: episode: 2425, duration: 38.599s, episode steps: 635, steps per second:  16, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.740 [0.000, 5.000],  loss: 0.016637, mae: 2.538995, mean_q: 3.054252, mean_eps: 0.050000
 1816611/2000000: episode: 2426, duration: 55.406s, episode steps: 910, steps per second:  16, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.031 [0.000, 5.000],  loss: 0.017288, mae: 2.554382, mean_q: 3.073542, mean_eps: 0.050000
 1817254/2000000: episode: 2427, duration: 39.534s, episode steps: 643, steps per second:  16, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.900 [0.000, 5.000],  loss: 0.017021, mae: 2.550581, mean_q: 3.070843, mean_eps: 0.050000
 1817716/2000000: episode: 2428, duration: 28.380s, episode steps: 462, steps per second:  16, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.294 [0.000, 5.000],  loss: 0.015494, mae: 2.559729, mean_q: 3.081822, mean_eps: 0.050000
 1818090/2000000: episode: 2429, duration: 22.864s, episode steps: 374, steps per second:  16, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.211 [0.000, 5.000],  loss: 0.015006, mae: 2.547912, mean_q: 3.067041, mean_eps: 0.050000
 1818941/2000000: episode: 2430, duration: 53.216s, episode steps: 851, steps per second:  16, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.754 [0.000, 5.000],  loss: 0.016982, mae: 2.544902, mean_q: 3.063638, mean_eps: 0.050000
 1819843/2000000: episode: 2431, duration: 56.009s, episode steps: 902, steps per second:  16, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.016444, mae: 2.544530, mean_q: 3.062634, mean_eps: 0.050000
 1820793/2000000: episode: 2432, duration: 58.899s, episode steps: 950, steps per second:  16, episode reward: 10.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.188 [0.000, 5.000],  loss: 0.014682, mae: 2.573058, mean_q: 3.099059, mean_eps: 0.050000
 1821154/2000000: episode: 2433, duration: 21.909s, episode steps: 361, steps per second:  16, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.252 [0.000, 5.000],  loss: 0.016588, mae: 2.544829, mean_q: 3.064275, mean_eps: 0.050000
 1821790/2000000: episode: 2434, duration: 39.176s, episode steps: 636, steps per second:  16, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.016085, mae: 2.560890, mean_q: 3.082503, mean_eps: 0.050000
 1822747/2000000: episode: 2435, duration: 58.369s, episode steps: 957, steps per second:  16, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.957 [0.000, 5.000],  loss: 0.016809, mae: 2.559968, mean_q: 3.082807, mean_eps: 0.050000
 1823130/2000000: episode: 2436, duration: 23.286s, episode steps: 383, steps per second:  16, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.016420, mae: 2.559528, mean_q: 3.081638, mean_eps: 0.050000
 1824099/2000000: episode: 2437, duration: 59.160s, episode steps: 969, steps per second:  16, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.016931, mae: 2.558774, mean_q: 3.080032, mean_eps: 0.050000
 1824730/2000000: episode: 2438, duration: 38.604s, episode steps: 631, steps per second:  16, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.772 [0.000, 5.000],  loss: 0.016109, mae: 2.553641, mean_q: 3.076788, mean_eps: 0.050000
 1825179/2000000: episode: 2439, duration: 27.428s, episode steps: 449, steps per second:  16, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.022 [0.000, 5.000],  loss: 0.016609, mae: 2.552139, mean_q: 3.073660, mean_eps: 0.050000
 1825718/2000000: episode: 2440, duration: 32.933s, episode steps: 539, steps per second:  16, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.016292, mae: 2.547604, mean_q: 3.066961, mean_eps: 0.050000
 1826686/2000000: episode: 2441, duration: 59.298s, episode steps: 968, steps per second:  16, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.017916, mae: 2.565767, mean_q: 3.087120, mean_eps: 0.050000
 1827906/2000000: episode: 2442, duration: 74.603s, episode steps: 1220, steps per second:  16, episode reward: 18.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.982 [0.000, 5.000],  loss: 0.016519, mae: 2.559643, mean_q: 3.080140, mean_eps: 0.050000
 1828722/2000000: episode: 2443, duration: 49.748s, episode steps: 816, steps per second:  16, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.760 [0.000, 5.000],  loss: 0.017312, mae: 2.549411, mean_q: 3.067965, mean_eps: 0.050000
 1829878/2000000: episode: 2444, duration: 70.623s, episode steps: 1156, steps per second:  16, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.885 [0.000, 5.000],  loss: 0.017743, mae: 2.541424, mean_q: 3.058386, mean_eps: 0.050000
 1830469/2000000: episode: 2445, duration: 36.178s, episode steps: 591, steps per second:  16, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.014585, mae: 2.556023, mean_q: 3.077108, mean_eps: 0.050000
 1831027/2000000: episode: 2446, duration: 34.089s, episode steps: 558, steps per second:  16, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.048 [0.000, 5.000],  loss: 0.015615, mae: 2.549201, mean_q: 3.068229, mean_eps: 0.050000
 1831871/2000000: episode: 2447, duration: 51.472s, episode steps: 844, steps per second:  16, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.858 [0.000, 5.000],  loss: 0.016243, mae: 2.542673, mean_q: 3.060708, mean_eps: 0.050000
 1833002/2000000: episode: 2448, duration: 69.083s, episode steps: 1131, steps per second:  16, episode reward: 35.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.122 [0.000, 5.000],  loss: 0.015753, mae: 2.548389, mean_q: 3.067602, mean_eps: 0.050000
 1833977/2000000: episode: 2449, duration: 59.518s, episode steps: 975, steps per second:  16, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.046 [0.000, 5.000],  loss: 0.017564, mae: 2.548469, mean_q: 3.067624, mean_eps: 0.050000
 1834823/2000000: episode: 2450, duration: 51.996s, episode steps: 846, steps per second:  16, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.016692, mae: 2.534180, mean_q: 3.049192, mean_eps: 0.050000
 1835276/2000000: episode: 2451, duration: 27.799s, episode steps: 453, steps per second:  16, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.196 [0.000, 5.000],  loss: 0.014759, mae: 2.544583, mean_q: 3.061862, mean_eps: 0.050000
 1836321/2000000: episode: 2452, duration: 64.133s, episode steps: 1045, steps per second:  16, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.987 [0.000, 5.000],  loss: 0.017193, mae: 2.536277, mean_q: 3.051530, mean_eps: 0.050000
 1837257/2000000: episode: 2453, duration: 57.201s, episode steps: 936, steps per second:  16, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.015328, mae: 2.544518, mean_q: 3.062366, mean_eps: 0.050000
 1838338/2000000: episode: 2454, duration: 65.601s, episode steps: 1081, steps per second:  16, episode reward: 12.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.172 [0.000, 5.000],  loss: 0.015970, mae: 2.552320, mean_q: 3.072204, mean_eps: 0.050000
 1838808/2000000: episode: 2455, duration: 28.857s, episode steps: 470, steps per second:  16, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.745 [0.000, 5.000],  loss: 0.016336, mae: 2.543625, mean_q: 3.062928, mean_eps: 0.050000
 1839708/2000000: episode: 2456, duration: 55.298s, episode steps: 900, steps per second:  16, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.153 [0.000, 5.000],  loss: 0.018359, mae: 2.538190, mean_q: 3.054416, mean_eps: 0.050000
 1840695/2000000: episode: 2457, duration: 60.238s, episode steps: 987, steps per second:  16, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.989 [0.000, 5.000],  loss: 0.014820, mae: 2.525882, mean_q: 3.043063, mean_eps: 0.050000
 1841235/2000000: episode: 2458, duration: 32.929s, episode steps: 540, steps per second:  16, episode reward: 15.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.015275, mae: 2.517472, mean_q: 3.032647, mean_eps: 0.050000
 1842202/2000000: episode: 2459, duration: 59.132s, episode steps: 967, steps per second:  16, episode reward: 12.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 3.164 [0.000, 5.000],  loss: 0.015443, mae: 2.519666, mean_q: 3.034806, mean_eps: 0.050000
 1843124/2000000: episode: 2460, duration: 56.321s, episode steps: 922, steps per second:  16, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.117 [0.000, 5.000],  loss: 0.015606, mae: 2.516575, mean_q: 3.029976, mean_eps: 0.050000
 1843566/2000000: episode: 2461, duration: 27.215s, episode steps: 442, steps per second:  16, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.163 [0.000, 5.000],  loss: 0.014852, mae: 2.528176, mean_q: 3.044439, mean_eps: 0.050000
 1844517/2000000: episode: 2462, duration: 57.940s, episode steps: 951, steps per second:  16, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.761 [0.000, 5.000],  loss: 0.016335, mae: 2.519235, mean_q: 3.033103, mean_eps: 0.050000
 1845313/2000000: episode: 2463, duration: 48.707s, episode steps: 796, steps per second:  16, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.016098, mae: 2.510867, mean_q: 3.021701, mean_eps: 0.050000
 1846567/2000000: episode: 2464, duration: 76.446s, episode steps: 1254, steps per second:  16, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.016888, mae: 2.518636, mean_q: 3.030995, mean_eps: 0.050000
 1847853/2000000: episode: 2465, duration: 78.558s, episode steps: 1286, steps per second:  16, episode reward: 17.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.787 [0.000, 5.000],  loss: 0.015982, mae: 2.515263, mean_q: 3.027008, mean_eps: 0.050000
 1848507/2000000: episode: 2466, duration: 39.907s, episode steps: 654, steps per second:  16, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.016202, mae: 2.522901, mean_q: 3.033462, mean_eps: 0.050000
 1849176/2000000: episode: 2467, duration: 41.193s, episode steps: 669, steps per second:  16, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.015930, mae: 2.521771, mean_q: 3.035258, mean_eps: 0.050000
 1849888/2000000: episode: 2468, duration: 43.929s, episode steps: 712, steps per second:  16, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.015097, mae: 2.505260, mean_q: 3.014492, mean_eps: 0.050000
 1850905/2000000: episode: 2469, duration: 63.322s, episode steps: 1017, steps per second:  16, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.016116, mae: 2.518243, mean_q: 3.031483, mean_eps: 0.050000
 1851737/2000000: episode: 2470, duration: 50.953s, episode steps: 832, steps per second:  16, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.015830, mae: 2.517376, mean_q: 3.029478, mean_eps: 0.050000
 1852115/2000000: episode: 2471, duration: 23.019s, episode steps: 378, steps per second:  16, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.014023, mae: 2.510501, mean_q: 3.023806, mean_eps: 0.050000
 1852765/2000000: episode: 2472, duration: 39.680s, episode steps: 650, steps per second:  16, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.246 [0.000, 5.000],  loss: 0.015949, mae: 2.526698, mean_q: 3.041525, mean_eps: 0.050000
 1853621/2000000: episode: 2473, duration: 52.121s, episode steps: 856, steps per second:  16, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.636 [0.000, 5.000],  loss: 0.015133, mae: 2.505868, mean_q: 3.015957, mean_eps: 0.050000
 1854428/2000000: episode: 2474, duration: 49.313s, episode steps: 807, steps per second:  16, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.988 [0.000, 5.000],  loss: 0.016469, mae: 2.514828, mean_q: 3.026395, mean_eps: 0.050000
 1855109/2000000: episode: 2475, duration: 41.775s, episode steps: 681, steps per second:  16, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.233 [0.000, 5.000],  loss: 0.016989, mae: 2.526550, mean_q: 3.039543, mean_eps: 0.050000
 1855503/2000000: episode: 2476, duration: 24.124s, episode steps: 394, steps per second:  16, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.995 [0.000, 5.000],  loss: 0.016165, mae: 2.537682, mean_q: 3.054373, mean_eps: 0.050000
 1856587/2000000: episode: 2477, duration: 66.081s, episode steps: 1084, steps per second:  16, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.016721, mae: 2.516252, mean_q: 3.029923, mean_eps: 0.050000
 1857272/2000000: episode: 2478, duration: 42.124s, episode steps: 685, steps per second:  16, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.845 [0.000, 5.000],  loss: 0.018243, mae: 2.513996, mean_q: 3.027010, mean_eps: 0.050000
 1858532/2000000: episode: 2479, duration: 76.857s, episode steps: 1260, steps per second:  16, episode reward: 30.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.165 [0.000, 5.000],  loss: 0.017043, mae: 2.517334, mean_q: 3.030008, mean_eps: 0.050000
 1859392/2000000: episode: 2480, duration: 52.746s, episode steps: 860, steps per second:  16, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.309 [0.000, 5.000],  loss: 0.016436, mae: 2.506165, mean_q: 3.017532, mean_eps: 0.050000
 1860042/2000000: episode: 2481, duration: 39.730s, episode steps: 650, steps per second:  16, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.016604, mae: 2.511299, mean_q: 3.023977, mean_eps: 0.050000
 1860945/2000000: episode: 2482, duration: 55.057s, episode steps: 903, steps per second:  16, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: 0.015251, mae: 2.522531, mean_q: 3.037287, mean_eps: 0.050000
 1861592/2000000: episode: 2483, duration: 39.402s, episode steps: 647, steps per second:  16, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.056 [0.000, 5.000],  loss: 0.016118, mae: 2.506267, mean_q: 3.017924, mean_eps: 0.050000
 1862209/2000000: episode: 2484, duration: 37.756s, episode steps: 617, steps per second:  16, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.848 [0.000, 5.000],  loss: 0.016343, mae: 2.522172, mean_q: 3.036135, mean_eps: 0.050000
 1863106/2000000: episode: 2485, duration: 54.826s, episode steps: 897, steps per second:  16, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.017606, mae: 2.513434, mean_q: 3.025521, mean_eps: 0.050000
 1863715/2000000: episode: 2486, duration: 36.852s, episode steps: 609, steps per second:  17, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.044 [0.000, 5.000],  loss: 0.015566, mae: 2.517759, mean_q: 3.030009, mean_eps: 0.050000
 1864348/2000000: episode: 2487, duration: 38.880s, episode steps: 633, steps per second:  16, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.967 [0.000, 5.000],  loss: 0.016196, mae: 2.516761, mean_q: 3.029426, mean_eps: 0.050000
 1865429/2000000: episode: 2488, duration: 66.305s, episode steps: 1081, steps per second:  16, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.648 [0.000, 5.000],  loss: 0.017013, mae: 2.518432, mean_q: 3.033088, mean_eps: 0.050000
 1866151/2000000: episode: 2489, duration: 44.006s, episode steps: 722, steps per second:  16, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.792 [0.000, 5.000],  loss: 0.015451, mae: 2.513219, mean_q: 3.024310, mean_eps: 0.050000
 1867078/2000000: episode: 2490, duration: 56.884s, episode steps: 927, steps per second:  16, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.015574, mae: 2.516457, mean_q: 3.030562, mean_eps: 0.050000
 1867861/2000000: episode: 2491, duration: 47.790s, episode steps: 783, steps per second:  16, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.016795, mae: 2.506833, mean_q: 3.018119, mean_eps: 0.050000
 1868363/2000000: episode: 2492, duration: 30.578s, episode steps: 502, steps per second:  16, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.108 [0.000, 5.000],  loss: 0.016410, mae: 2.509916, mean_q: 3.021373, mean_eps: 0.050000
 1869444/2000000: episode: 2493, duration: 66.023s, episode steps: 1081, steps per second:  16, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.016154, mae: 2.504887, mean_q: 3.015303, mean_eps: 0.050000
 1870231/2000000: episode: 2494, duration: 47.960s, episode steps: 787, steps per second:  16, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.826 [0.000, 5.000],  loss: 0.015362, mae: 2.507820, mean_q: 3.019143, mean_eps: 0.050000
 1871539/2000000: episode: 2495, duration: 80.294s, episode steps: 1308, steps per second:  16, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.195 [0.000, 5.000],  loss: 0.015084, mae: 2.494410, mean_q: 3.002942, mean_eps: 0.050000
 1872548/2000000: episode: 2496, duration: 61.807s, episode steps: 1009, steps per second:  16, episode reward: 12.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.015334, mae: 2.488483, mean_q: 2.995827, mean_eps: 0.050000
 1873161/2000000: episode: 2497, duration: 37.865s, episode steps: 613, steps per second:  16, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.703 [0.000, 5.000],  loss: 0.016117, mae: 2.501717, mean_q: 3.013837, mean_eps: 0.050000
 1874290/2000000: episode: 2498, duration: 68.571s, episode steps: 1129, steps per second:  16, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.065 [0.000, 5.000],  loss: 0.016988, mae: 2.496420, mean_q: 3.006049, mean_eps: 0.050000
 1875191/2000000: episode: 2499, duration: 54.916s, episode steps: 901, steps per second:  16, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.673 [0.000, 5.000],  loss: 0.016732, mae: 2.493774, mean_q: 3.000096, mean_eps: 0.050000
 1876781/2000000: episode: 2500, duration: 99.112s, episode steps: 1590, steps per second:  16, episode reward: 36.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.136 [0.000, 5.000],  loss: 0.016556, mae: 2.491581, mean_q: 2.999053, mean_eps: 0.050000
 1877274/2000000: episode: 2501, duration: 30.189s, episode steps: 493, steps per second:  16, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.817 [0.000, 5.000],  loss: 0.017205, mae: 2.498804, mean_q: 3.008525, mean_eps: 0.050000
 1878188/2000000: episode: 2502, duration: 56.328s, episode steps: 914, steps per second:  16, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.074 [0.000, 5.000],  loss: 0.015197, mae: 2.485659, mean_q: 2.992122, mean_eps: 0.050000
 1879237/2000000: episode: 2503, duration: 64.807s, episode steps: 1049, steps per second:  16, episode reward: 10.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.016800, mae: 2.489148, mean_q: 2.996379, mean_eps: 0.050000
 1880288/2000000: episode: 2504, duration: 64.556s, episode steps: 1051, steps per second:  16, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.245 [0.000, 5.000],  loss: 0.016372, mae: 2.497839, mean_q: 3.008271, mean_eps: 0.050000
 1880885/2000000: episode: 2505, duration: 36.656s, episode steps: 597, steps per second:  16, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.015626, mae: 2.494146, mean_q: 3.003521, mean_eps: 0.050000
 1881502/2000000: episode: 2506, duration: 37.773s, episode steps: 617, steps per second:  16, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.840 [0.000, 5.000],  loss: 0.015835, mae: 2.478158, mean_q: 2.984750, mean_eps: 0.050000
 1881867/2000000: episode: 2507, duration: 22.301s, episode steps: 365, steps per second:  16, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 3.096 [0.000, 5.000],  loss: 0.015782, mae: 2.484040, mean_q: 2.991594, mean_eps: 0.050000
 1882461/2000000: episode: 2508, duration: 36.415s, episode steps: 594, steps per second:  16, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 0.017042, mae: 2.486766, mean_q: 2.994519, mean_eps: 0.050000
 1883198/2000000: episode: 2509, duration: 44.846s, episode steps: 737, steps per second:  16, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.305 [0.000, 5.000],  loss: 0.017336, mae: 2.495460, mean_q: 3.005877, mean_eps: 0.050000
 1884190/2000000: episode: 2510, duration: 60.261s, episode steps: 992, steps per second:  16, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.016572, mae: 2.489702, mean_q: 2.997434, mean_eps: 0.050000
 1885235/2000000: episode: 2511, duration: 63.702s, episode steps: 1045, steps per second:  16, episode reward: 13.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.278 [0.000, 5.000],  loss: 0.015713, mae: 2.489227, mean_q: 2.996334, mean_eps: 0.050000
 1886228/2000000: episode: 2512, duration: 60.792s, episode steps: 993, steps per second:  16, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.277 [0.000, 5.000],  loss: 0.017381, mae: 2.486540, mean_q: 2.992604, mean_eps: 0.050000
 1887108/2000000: episode: 2513, duration: 53.892s, episode steps: 880, steps per second:  16, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.195 [0.000, 5.000],  loss: 0.015897, mae: 2.485536, mean_q: 2.993145, mean_eps: 0.050000
 1887822/2000000: episode: 2514, duration: 43.831s, episode steps: 714, steps per second:  16, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.877 [0.000, 5.000],  loss: 0.017065, mae: 2.485657, mean_q: 2.993350, mean_eps: 0.050000
 1888617/2000000: episode: 2515, duration: 48.716s, episode steps: 795, steps per second:  16, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.016100, mae: 2.492918, mean_q: 3.001902, mean_eps: 0.050000
 1889312/2000000: episode: 2516, duration: 42.308s, episode steps: 695, steps per second:  16, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.016463, mae: 2.492612, mean_q: 3.000429, mean_eps: 0.050000
 1890272/2000000: episode: 2517, duration: 59.058s, episode steps: 960, steps per second:  16, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.015896, mae: 2.475291, mean_q: 2.980562, mean_eps: 0.050000
 1890779/2000000: episode: 2518, duration: 31.083s, episode steps: 507, steps per second:  16, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.968 [0.000, 5.000],  loss: 0.014959, mae: 2.464698, mean_q: 2.969624, mean_eps: 0.050000
 1891501/2000000: episode: 2519, duration: 44.309s, episode steps: 722, steps per second:  16, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.217 [0.000, 5.000],  loss: 0.015467, mae: 2.465862, mean_q: 2.967988, mean_eps: 0.050000
 1891941/2000000: episode: 2520, duration: 26.905s, episode steps: 440, steps per second:  16, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.957 [0.000, 5.000],  loss: 0.016138, mae: 2.465434, mean_q: 2.968409, mean_eps: 0.050000
 1892581/2000000: episode: 2521, duration: 38.967s, episode steps: 640, steps per second:  16, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.833 [0.000, 5.000],  loss: 0.015773, mae: 2.450381, mean_q: 2.951501, mean_eps: 0.050000
 1892933/2000000: episode: 2522, duration: 21.568s, episode steps: 352, steps per second:  16, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.287 [0.000, 5.000],  loss: 0.015247, mae: 2.459837, mean_q: 2.960300, mean_eps: 0.050000
 1894416/2000000: episode: 2523, duration: 90.356s, episode steps: 1483, steps per second:  16, episode reward: 28.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.800 [0.000, 5.000],  loss: 0.015811, mae: 2.473797, mean_q: 2.978301, mean_eps: 0.050000
 1895099/2000000: episode: 2524, duration: 41.889s, episode steps: 683, steps per second:  16, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.044 [0.000, 5.000],  loss: 0.016651, mae: 2.461948, mean_q: 2.961636, mean_eps: 0.050000
 1896120/2000000: episode: 2525, duration: 62.737s, episode steps: 1021, steps per second:  16, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.161 [0.000, 5.000],  loss: 0.016386, mae: 2.466660, mean_q: 2.968832, mean_eps: 0.050000
 1896728/2000000: episode: 2526, duration: 37.438s, episode steps: 608, steps per second:  16, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.946 [0.000, 5.000],  loss: 0.015163, mae: 2.471561, mean_q: 2.973427, mean_eps: 0.050000
 1897125/2000000: episode: 2527, duration: 24.399s, episode steps: 397, steps per second:  16, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.977 [0.000, 5.000],  loss: 0.017477, mae: 2.469649, mean_q: 2.971588, mean_eps: 0.050000
 1897638/2000000: episode: 2528, duration: 31.364s, episode steps: 513, steps per second:  16, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.017579, mae: 2.469975, mean_q: 2.971446, mean_eps: 0.050000
 1898582/2000000: episode: 2529, duration: 57.726s, episode steps: 944, steps per second:  16, episode reward:  7.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 0.015865, mae: 2.469923, mean_q: 2.973702, mean_eps: 0.050000
 1899087/2000000: episode: 2530, duration: 30.731s, episode steps: 505, steps per second:  16, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.051 [0.000, 5.000],  loss: 0.016472, mae: 2.482518, mean_q: 2.988515, mean_eps: 0.050000
 1900827/2000000: episode: 2531, duration: 106.941s, episode steps: 1740, steps per second:  16, episode reward: 33.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.014686, mae: 2.456408, mean_q: 2.955234, mean_eps: 0.050000
 1901220/2000000: episode: 2532, duration: 24.083s, episode steps: 393, steps per second:  16, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.016325, mae: 2.454991, mean_q: 2.952560, mean_eps: 0.050000
 1901649/2000000: episode: 2533, duration: 26.065s, episode steps: 429, steps per second:  16, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.855 [0.000, 5.000],  loss: 0.015480, mae: 2.449717, mean_q: 2.947397, mean_eps: 0.050000
 1902873/2000000: episode: 2534, duration: 74.623s, episode steps: 1224, steps per second:  16, episode reward: 32.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.940 [0.000, 5.000],  loss: 0.015968, mae: 2.452216, mean_q: 2.953913, mean_eps: 0.050000
 1903863/2000000: episode: 2535, duration: 60.913s, episode steps: 990, steps per second:  16, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.013 [0.000, 5.000],  loss: 0.015241, mae: 2.437018, mean_q: 2.934915, mean_eps: 0.050000
 1904959/2000000: episode: 2536, duration: 66.977s, episode steps: 1096, steps per second:  16, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.016062, mae: 2.447771, mean_q: 2.947124, mean_eps: 0.050000
 1905702/2000000: episode: 2537, duration: 45.398s, episode steps: 743, steps per second:  16, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.820 [0.000, 5.000],  loss: 0.014208, mae: 2.441714, mean_q: 2.939231, mean_eps: 0.050000
 1906325/2000000: episode: 2538, duration: 38.163s, episode steps: 623, steps per second:  16, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.934 [0.000, 5.000],  loss: 0.016367, mae: 2.436407, mean_q: 2.930345, mean_eps: 0.050000
 1907604/2000000: episode: 2539, duration: 77.784s, episode steps: 1279, steps per second:  16, episode reward: 27.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.196 [0.000, 5.000],  loss: 0.015957, mae: 2.442731, mean_q: 2.940427, mean_eps: 0.050000
 1908661/2000000: episode: 2540, duration: 64.865s, episode steps: 1057, steps per second:  16, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.130 [0.000, 5.000],  loss: 0.016428, mae: 2.447541, mean_q: 2.945685, mean_eps: 0.050000
 1909725/2000000: episode: 2541, duration: 65.221s, episode steps: 1064, steps per second:  16, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.737 [0.000, 5.000],  loss: 0.016291, mae: 2.446936, mean_q: 2.945524, mean_eps: 0.050000
 1910126/2000000: episode: 2542, duration: 24.651s, episode steps: 401, steps per second:  16, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.955 [0.000, 5.000],  loss: 0.016361, mae: 2.435694, mean_q: 2.932562, mean_eps: 0.050000
 1911064/2000000: episode: 2543, duration: 57.163s, episode steps: 938, steps per second:  16, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.013439, mae: 2.443984, mean_q: 2.942732, mean_eps: 0.050000
 1911949/2000000: episode: 2544, duration: 53.780s, episode steps: 885, steps per second:  16, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.016631, mae: 2.444563, mean_q: 2.945662, mean_eps: 0.050000
 1912894/2000000: episode: 2545, duration: 57.597s, episode steps: 945, steps per second:  16, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.183 [0.000, 5.000],  loss: 0.015737, mae: 2.445274, mean_q: 2.944113, mean_eps: 0.050000
 1914247/2000000: episode: 2546, duration: 82.475s, episode steps: 1353, steps per second:  16, episode reward: 18.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.016281, mae: 2.454023, mean_q: 2.956152, mean_eps: 0.050000
 1915358/2000000: episode: 2547, duration: 67.928s, episode steps: 1111, steps per second:  16, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.853 [0.000, 5.000],  loss: 0.015555, mae: 2.457402, mean_q: 2.959465, mean_eps: 0.050000
 1915886/2000000: episode: 2548, duration: 32.344s, episode steps: 528, steps per second:  16, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: 0.016203, mae: 2.452079, mean_q: 2.947920, mean_eps: 0.050000
 1916521/2000000: episode: 2549, duration: 38.785s, episode steps: 635, steps per second:  16, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.017916, mae: 2.446414, mean_q: 2.943815, mean_eps: 0.050000
 1917074/2000000: episode: 2550, duration: 33.923s, episode steps: 553, steps per second:  16, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.277 [0.000, 5.000],  loss: 0.015309, mae: 2.443739, mean_q: 2.939196, mean_eps: 0.050000
 1917798/2000000: episode: 2551, duration: 43.947s, episode steps: 724, steps per second:  16, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.721 [0.000, 5.000],  loss: 0.016475, mae: 2.441294, mean_q: 2.936794, mean_eps: 0.050000
 1918346/2000000: episode: 2552, duration: 33.314s, episode steps: 548, steps per second:  16, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.901 [0.000, 5.000],  loss: 0.015149, mae: 2.447174, mean_q: 2.943009, mean_eps: 0.050000
 1919553/2000000: episode: 2553, duration: 73.956s, episode steps: 1207, steps per second:  16, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.015941, mae: 2.445373, mean_q: 2.943480, mean_eps: 0.050000
 1920676/2000000: episode: 2554, duration: 68.928s, episode steps: 1123, steps per second:  16, episode reward: 15.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.014403, mae: 2.430041, mean_q: 2.924767, mean_eps: 0.050000
 1921075/2000000: episode: 2555, duration: 24.500s, episode steps: 399, steps per second:  16, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.877 [0.000, 5.000],  loss: 0.013387, mae: 2.420716, mean_q: 2.912782, mean_eps: 0.050000
 1921762/2000000: episode: 2556, duration: 42.120s, episode steps: 687, steps per second:  16, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.220 [0.000, 5.000],  loss: 0.015381, mae: 2.423596, mean_q: 2.916365, mean_eps: 0.050000
 1922449/2000000: episode: 2557, duration: 42.591s, episode steps: 687, steps per second:  16, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.038 [0.000, 5.000],  loss: 0.014707, mae: 2.421332, mean_q: 2.912317, mean_eps: 0.050000
 1923240/2000000: episode: 2558, duration: 48.466s, episode steps: 791, steps per second:  16, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.014475, mae: 2.425918, mean_q: 2.919343, mean_eps: 0.050000
 1923967/2000000: episode: 2559, duration: 44.994s, episode steps: 727, steps per second:  16, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.017479, mae: 2.420688, mean_q: 2.912077, mean_eps: 0.050000
 1924794/2000000: episode: 2560, duration: 51.005s, episode steps: 827, steps per second:  16, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.947 [0.000, 5.000],  loss: 0.015851, mae: 2.427069, mean_q: 2.921030, mean_eps: 0.050000
 1925981/2000000: episode: 2561, duration: 72.426s, episode steps: 1187, steps per second:  16, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.015711, mae: 2.426179, mean_q: 2.920564, mean_eps: 0.050000
 1927041/2000000: episode: 2562, duration: 64.677s, episode steps: 1060, steps per second:  16, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.015821, mae: 2.427348, mean_q: 2.920830, mean_eps: 0.050000
 1928092/2000000: episode: 2563, duration: 64.503s, episode steps: 1051, steps per second:  16, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.015561, mae: 2.432553, mean_q: 2.927337, mean_eps: 0.050000
 1928932/2000000: episode: 2564, duration: 51.614s, episode steps: 840, steps per second:  16, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.937 [0.000, 5.000],  loss: 0.015926, mae: 2.428059, mean_q: 2.921506, mean_eps: 0.050000
 1929620/2000000: episode: 2565, duration: 42.436s, episode steps: 688, steps per second:  16, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.015758, mae: 2.438369, mean_q: 2.933772, mean_eps: 0.050000
 1930150/2000000: episode: 2566, duration: 32.690s, episode steps: 530, steps per second:  16, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.934 [0.000, 5.000],  loss: 0.015069, mae: 2.425316, mean_q: 2.919063, mean_eps: 0.050000
 1931368/2000000: episode: 2567, duration: 74.744s, episode steps: 1218, steps per second:  16, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 0.014069, mae: 2.420759, mean_q: 2.914364, mean_eps: 0.050000
 1932427/2000000: episode: 2568, duration: 65.065s, episode steps: 1059, steps per second:  16, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.178 [0.000, 5.000],  loss: 0.014711, mae: 2.412405, mean_q: 2.903806, mean_eps: 0.050000
 1933973/2000000: episode: 2569, duration: 94.443s, episode steps: 1546, steps per second:  16, episode reward: 27.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 0.015012, mae: 2.415681, mean_q: 2.908087, mean_eps: 0.050000
 1935069/2000000: episode: 2570, duration: 66.543s, episode steps: 1096, steps per second:  16, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.009 [0.000, 5.000],  loss: 0.016322, mae: 2.409140, mean_q: 2.900044, mean_eps: 0.050000
 1935564/2000000: episode: 2571, duration: 30.604s, episode steps: 495, steps per second:  16, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.857 [0.000, 5.000],  loss: 0.015886, mae: 2.404172, mean_q: 2.892915, mean_eps: 0.050000
 1936260/2000000: episode: 2572, duration: 42.807s, episode steps: 696, steps per second:  16, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.226 [0.000, 5.000],  loss: 0.019405, mae: 2.415905, mean_q: 2.904984, mean_eps: 0.050000
 1937176/2000000: episode: 2573, duration: 58.873s, episode steps: 916, steps per second:  16, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.015351, mae: 2.411576, mean_q: 2.902556, mean_eps: 0.050000
 1938144/2000000: episode: 2574, duration: 60.223s, episode steps: 968, steps per second:  16, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.026 [0.000, 5.000],  loss: 0.016099, mae: 2.423942, mean_q: 2.916927, mean_eps: 0.050000
 1939387/2000000: episode: 2575, duration: 76.493s, episode steps: 1243, steps per second:  16, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.016399, mae: 2.427572, mean_q: 2.921027, mean_eps: 0.050000
 1940626/2000000: episode: 2576, duration: 75.907s, episode steps: 1239, steps per second:  16, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.014835, mae: 2.406357, mean_q: 2.896747, mean_eps: 0.050000
 1941149/2000000: episode: 2577, duration: 31.950s, episode steps: 523, steps per second:  16, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.826 [0.000, 5.000],  loss: 0.014295, mae: 2.411356, mean_q: 2.901766, mean_eps: 0.050000
 1942176/2000000: episode: 2578, duration: 62.672s, episode steps: 1027, steps per second:  16, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.014659, mae: 2.404547, mean_q: 2.894071, mean_eps: 0.050000
 1942638/2000000: episode: 2579, duration: 28.494s, episode steps: 462, steps per second:  16, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.016665, mae: 2.407651, mean_q: 2.895460, mean_eps: 0.050000
 1943557/2000000: episode: 2580, duration: 56.235s, episode steps: 919, steps per second:  16, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.015842, mae: 2.411806, mean_q: 2.903506, mean_eps: 0.050000
 1944201/2000000: episode: 2581, duration: 39.292s, episode steps: 644, steps per second:  16, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.113 [0.000, 5.000],  loss: 0.015171, mae: 2.394638, mean_q: 2.882879, mean_eps: 0.050000
 1945371/2000000: episode: 2582, duration: 71.243s, episode steps: 1170, steps per second:  16, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.278 [0.000, 5.000],  loss: 0.015977, mae: 2.402435, mean_q: 2.890635, mean_eps: 0.050000
 1945941/2000000: episode: 2583, duration: 34.857s, episode steps: 570, steps per second:  16, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.002 [0.000, 5.000],  loss: 0.016431, mae: 2.409546, mean_q: 2.900123, mean_eps: 0.050000
 1946274/2000000: episode: 2584, duration: 20.392s, episode steps: 333, steps per second:  16, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.016031, mae: 2.420426, mean_q: 2.912503, mean_eps: 0.050000
 1946640/2000000: episode: 2585, duration: 22.645s, episode steps: 366, steps per second:  16, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.153 [0.000, 5.000],  loss: 0.015138, mae: 2.412775, mean_q: 2.903336, mean_eps: 0.050000
 1947614/2000000: episode: 2586, duration: 59.739s, episode steps: 974, steps per second:  16, episode reward:  8.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.566 [0.000, 5.000],  loss: 0.015507, mae: 2.393993, mean_q: 2.879891, mean_eps: 0.050000
 1948061/2000000: episode: 2587, duration: 27.306s, episode steps: 447, steps per second:  16, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.098 [0.000, 5.000],  loss: 0.015326, mae: 2.397156, mean_q: 2.884932, mean_eps: 0.050000
 1949058/2000000: episode: 2588, duration: 61.130s, episode steps: 997, steps per second:  16, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.016062, mae: 2.412683, mean_q: 2.903534, mean_eps: 0.050000
 1949565/2000000: episode: 2589, duration: 31.212s, episode steps: 507, steps per second:  16, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.015748, mae: 2.402827, mean_q: 2.894922, mean_eps: 0.050000
 1950728/2000000: episode: 2590, duration: 72.123s, episode steps: 1163, steps per second:  16, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.886 [0.000, 5.000],  loss: 0.014912, mae: 2.398543, mean_q: 2.886650, mean_eps: 0.050000
 1951647/2000000: episode: 2591, duration: 56.471s, episode steps: 919, steps per second:  16, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.015993, mae: 2.407603, mean_q: 2.898949, mean_eps: 0.050000
 1952300/2000000: episode: 2592, duration: 40.352s, episode steps: 653, steps per second:  16, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.741 [0.000, 5.000],  loss: 0.014616, mae: 2.424549, mean_q: 2.917256, mean_eps: 0.050000
 1953363/2000000: episode: 2593, duration: 65.071s, episode steps: 1063, steps per second:  16, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.015225, mae: 2.421139, mean_q: 2.913419, mean_eps: 0.050000
 1953864/2000000: episode: 2594, duration: 30.974s, episode steps: 501, steps per second:  16, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.182 [0.000, 5.000],  loss: 0.015153, mae: 2.411205, mean_q: 2.902489, mean_eps: 0.050000
 1954598/2000000: episode: 2595, duration: 45.277s, episode steps: 734, steps per second:  16, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.059 [0.000, 5.000],  loss: 0.016223, mae: 2.406680, mean_q: 2.896159, mean_eps: 0.050000
 1955423/2000000: episode: 2596, duration: 50.980s, episode steps: 825, steps per second:  16, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.766 [0.000, 5.000],  loss: 0.015342, mae: 2.402826, mean_q: 2.892196, mean_eps: 0.050000
 1955948/2000000: episode: 2597, duration: 32.144s, episode steps: 525, steps per second:  16, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.299 [0.000, 5.000],  loss: 0.015564, mae: 2.396966, mean_q: 2.885865, mean_eps: 0.050000
 1956456/2000000: episode: 2598, duration: 31.198s, episode steps: 508, steps per second:  16, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.015446, mae: 2.409972, mean_q: 2.900519, mean_eps: 0.050000
 1957435/2000000: episode: 2599, duration: 59.994s, episode steps: 979, steps per second:  16, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.015645, mae: 2.412945, mean_q: 2.905863, mean_eps: 0.050000
 1957944/2000000: episode: 2600, duration: 31.244s, episode steps: 509, steps per second:  16, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.972 [0.000, 5.000],  loss: 0.014957, mae: 2.423388, mean_q: 2.917853, mean_eps: 0.050000
 1958460/2000000: episode: 2601, duration: 31.894s, episode steps: 516, steps per second:  16, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.017229, mae: 2.417372, mean_q: 2.908017, mean_eps: 0.050000
 1958997/2000000: episode: 2602, duration: 33.039s, episode steps: 537, steps per second:  16, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.016979, mae: 2.406125, mean_q: 2.895603, mean_eps: 0.050000
 1960030/2000000: episode: 2603, duration: 62.862s, episode steps: 1033, steps per second:  16, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.015937, mae: 2.409209, mean_q: 2.899648, mean_eps: 0.050000
 1960807/2000000: episode: 2604, duration: 47.176s, episode steps: 777, steps per second:  16, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.014332, mae: 2.393191, mean_q: 2.881830, mean_eps: 0.050000
 1961348/2000000: episode: 2605, duration: 33.247s, episode steps: 541, steps per second:  16, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.055 [0.000, 5.000],  loss: 0.015103, mae: 2.410405, mean_q: 2.903134, mean_eps: 0.050000
 1962000/2000000: episode: 2606, duration: 40.213s, episode steps: 652, steps per second:  16, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.262 [0.000, 5.000],  loss: 0.015699, mae: 2.418166, mean_q: 2.915281, mean_eps: 0.050000
 1962878/2000000: episode: 2607, duration: 53.769s, episode steps: 878, steps per second:  16, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.015417, mae: 2.409148, mean_q: 2.900799, mean_eps: 0.050000
 1963893/2000000: episode: 2608, duration: 62.246s, episode steps: 1015, steps per second:  16, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.079 [0.000, 5.000],  loss: 0.015757, mae: 2.395308, mean_q: 2.882366, mean_eps: 0.050000
 1964911/2000000: episode: 2609, duration: 62.130s, episode steps: 1018, steps per second:  16, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.016706, mae: 2.415194, mean_q: 2.906658, mean_eps: 0.050000
 1965987/2000000: episode: 2610, duration: 65.860s, episode steps: 1076, steps per second:  16, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: 0.016506, mae: 2.399859, mean_q: 2.890002, mean_eps: 0.050000
 1966667/2000000: episode: 2611, duration: 41.483s, episode steps: 680, steps per second:  16, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.016423, mae: 2.390696, mean_q: 2.877144, mean_eps: 0.050000
 1967664/2000000: episode: 2612, duration: 61.654s, episode steps: 997, steps per second:  16, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.015859, mae: 2.403769, mean_q: 2.894647, mean_eps: 0.050000
 1968710/2000000: episode: 2613, duration: 64.657s, episode steps: 1046, steps per second:  16, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.015331, mae: 2.395123, mean_q: 2.883004, mean_eps: 0.050000
 1969335/2000000: episode: 2614, duration: 38.077s, episode steps: 625, steps per second:  16, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.015930, mae: 2.396098, mean_q: 2.883462, mean_eps: 0.050000
 1969954/2000000: episode: 2615, duration: 38.183s, episode steps: 619, steps per second:  16, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.701 [0.000, 5.000],  loss: 0.015029, mae: 2.397784, mean_q: 2.886036, mean_eps: 0.050000
 1970569/2000000: episode: 2616, duration: 37.606s, episode steps: 615, steps per second:  16, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.014936, mae: 2.407077, mean_q: 2.897836, mean_eps: 0.050000
 1971160/2000000: episode: 2617, duration: 36.396s, episode steps: 591, steps per second:  16, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.770 [0.000, 5.000],  loss: 0.014446, mae: 2.405319, mean_q: 2.896290, mean_eps: 0.050000
 1972365/2000000: episode: 2618, duration: 73.308s, episode steps: 1205, steps per second:  16, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.101 [0.000, 5.000],  loss: 0.015325, mae: 2.402067, mean_q: 2.890385, mean_eps: 0.050000
 1973516/2000000: episode: 2619, duration: 70.230s, episode steps: 1151, steps per second:  16, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.786 [0.000, 5.000],  loss: 0.014756, mae: 2.415734, mean_q: 2.907794, mean_eps: 0.050000
 1974370/2000000: episode: 2620, duration: 52.358s, episode steps: 854, steps per second:  16, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.015328, mae: 2.398073, mean_q: 2.887237, mean_eps: 0.050000
 1975081/2000000: episode: 2621, duration: 43.552s, episode steps: 711, steps per second:  16, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.128 [0.000, 5.000],  loss: 0.015374, mae: 2.401430, mean_q: 2.891405, mean_eps: 0.050000
 1975814/2000000: episode: 2622, duration: 44.885s, episode steps: 733, steps per second:  16, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.660 [0.000, 5.000],  loss: 0.015957, mae: 2.397769, mean_q: 2.885198, mean_eps: 0.050000
 1976991/2000000: episode: 2623, duration: 71.758s, episode steps: 1177, steps per second:  16, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.291 [0.000, 5.000],  loss: 0.018189, mae: 2.415494, mean_q: 2.909016, mean_eps: 0.050000
 1978241/2000000: episode: 2624, duration: 76.511s, episode steps: 1250, steps per second:  16, episode reward: 13.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.016505, mae: 2.403988, mean_q: 2.892918, mean_eps: 0.050000
 1979335/2000000: episode: 2625, duration: 66.751s, episode steps: 1094, steps per second:  16, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.015247, mae: 2.397755, mean_q: 2.886778, mean_eps: 0.050000
 1980417/2000000: episode: 2626, duration: 66.436s, episode steps: 1082, steps per second:  16, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.128 [0.000, 5.000],  loss: 0.015555, mae: 2.401554, mean_q: 2.890875, mean_eps: 0.050000
 1981141/2000000: episode: 2627, duration: 44.176s, episode steps: 724, steps per second:  16, episode reward:  5.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.014097, mae: 2.418715, mean_q: 2.912259, mean_eps: 0.050000
 1981660/2000000: episode: 2628, duration: 31.979s, episode steps: 519, steps per second:  16, episode reward: 13.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.014563, mae: 2.426219, mean_q: 2.920411, mean_eps: 0.050000
 1982564/2000000: episode: 2629, duration: 55.267s, episode steps: 904, steps per second:  16, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.771 [0.000, 5.000],  loss: 0.015958, mae: 2.407646, mean_q: 2.898424, mean_eps: 0.050000
 1983398/2000000: episode: 2630, duration: 51.056s, episode steps: 834, steps per second:  16, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 3.040 [0.000, 5.000],  loss: 0.016092, mae: 2.393584, mean_q: 2.880949, mean_eps: 0.050000
 1983902/2000000: episode: 2631, duration: 30.645s, episode steps: 504, steps per second:  16, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 3.083 [0.000, 5.000],  loss: 0.015456, mae: 2.408824, mean_q: 2.903092, mean_eps: 0.050000
 1984319/2000000: episode: 2632, duration: 25.352s, episode steps: 417, steps per second:  16, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.705 [0.000, 5.000],  loss: 0.015152, mae: 2.411473, mean_q: 2.902741, mean_eps: 0.050000
 1984989/2000000: episode: 2633, duration: 41.011s, episode steps: 670, steps per second:  16, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.016454, mae: 2.412445, mean_q: 2.903354, mean_eps: 0.050000
 1985509/2000000: episode: 2634, duration: 31.845s, episode steps: 520, steps per second:  16, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.017241, mae: 2.416652, mean_q: 2.910267, mean_eps: 0.050000
 1986185/2000000: episode: 2635, duration: 41.261s, episode steps: 676, steps per second:  16, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.121 [0.000, 5.000],  loss: 0.016307, mae: 2.401518, mean_q: 2.890735, mean_eps: 0.050000
 1986892/2000000: episode: 2636, duration: 43.080s, episode steps: 707, steps per second:  16, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.019759, mae: 2.395512, mean_q: 2.886378, mean_eps: 0.050000
 1987844/2000000: episode: 2637, duration: 58.508s, episode steps: 952, steps per second:  16, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.080 [0.000, 5.000],  loss: 0.017443, mae: 2.417230, mean_q: 2.908551, mean_eps: 0.050000
 1988707/2000000: episode: 2638, duration: 52.964s, episode steps: 863, steps per second:  16, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.016318, mae: 2.407037, mean_q: 2.896748, mean_eps: 0.050000
 1989539/2000000: episode: 2639, duration: 50.944s, episode steps: 832, steps per second:  16, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.529 [0.000, 5.000],  loss: 0.016177, mae: 2.405881, mean_q: 2.897225, mean_eps: 0.050000
 1990672/2000000: episode: 2640, duration: 69.091s, episode steps: 1133, steps per second:  16, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.014748, mae: 2.409728, mean_q: 2.902042, mean_eps: 0.050000
 1991169/2000000: episode: 2641, duration: 30.329s, episode steps: 497, steps per second:  16, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.015988, mae: 2.414805, mean_q: 2.909495, mean_eps: 0.050000
 1991838/2000000: episode: 2642, duration: 40.725s, episode steps: 669, steps per second:  16, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.015219, mae: 2.421023, mean_q: 2.915865, mean_eps: 0.050000
 1992303/2000000: episode: 2643, duration: 28.353s, episode steps: 465, steps per second:  16, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.778 [0.000, 5.000],  loss: 0.015014, mae: 2.432116, mean_q: 2.927548, mean_eps: 0.050000
 1992854/2000000: episode: 2644, duration: 33.845s, episode steps: 551, steps per second:  16, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.318 [0.000, 5.000],  loss: 0.016233, mae: 2.405366, mean_q: 2.897718, mean_eps: 0.050000
 1994321/2000000: episode: 2645, duration: 89.657s, episode steps: 1467, steps per second:  16, episode reward: 24.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.964 [0.000, 5.000],  loss: 0.015478, mae: 2.411911, mean_q: 2.904393, mean_eps: 0.050000
 1995072/2000000: episode: 2646, duration: 46.057s, episode steps: 751, steps per second:  16, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.016153, mae: 2.413041, mean_q: 2.905323, mean_eps: 0.050000
 1995885/2000000: episode: 2647, duration: 51.077s, episode steps: 813, steps per second:  16, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.015333, mae: 2.409979, mean_q: 2.900466, mean_eps: 0.050000
 1996771/2000000: episode: 2648, duration: 54.685s, episode steps: 886, steps per second:  16, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.699 [0.000, 5.000],  loss: 0.015711, mae: 2.415726, mean_q: 2.907265, mean_eps: 0.050000
 1997229/2000000: episode: 2649, duration: 28.253s, episode steps: 458, steps per second:  16, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.876 [0.000, 5.000],  loss: 0.017845, mae: 2.406072, mean_q: 2.898630, mean_eps: 0.050000
 1998056/2000000: episode: 2650, duration: 50.714s, episode steps: 827, steps per second:  16, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.088 [0.000, 5.000],  loss: 0.015830, mae: 2.410704, mean_q: 2.902192, mean_eps: 0.050000
 1998775/2000000: episode: 2651, duration: 44.283s, episode steps: 719, steps per second:  16, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.015444, mae: 2.409386, mean_q: 2.899511, mean_eps: 0.050000
 1999416/2000000: episode: 2652, duration: 39.242s, episode steps: 641, steps per second:  16, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.746 [0.000, 5.000],  loss: 0.015811, mae: 2.399636, mean_q: 2.888995, mean_eps: 0.050000
done, took 106270.351 seconds
